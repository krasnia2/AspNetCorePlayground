[ns_server:info,2020-03-27T19:45:32.261Z,nonode@nohost:<0.118.0>:ns_server:init_logging:150]Started & configured logging
[ns_server:info,2020-03-27T19:45:32.294Z,nonode@nohost:<0.118.0>:ns_server:log_pending:32]Static config terms:
[{error_logger_mf_dir,"/opt/couchbase/var/lib/couchbase/logs"},
 {path_config_bindir,"/opt/couchbase/bin"},
 {path_config_etcdir,"/opt/couchbase/etc/couchbase"},
 {path_config_libdir,"/opt/couchbase/lib"},
 {path_config_datadir,"/opt/couchbase/var/lib/couchbase"},
 {path_config_tmpdir,"/opt/couchbase/var/lib/couchbase/tmp"},
 {path_config_secdir,"/opt/couchbase/etc/security"},
 {nodefile,"/opt/couchbase/var/lib/couchbase/couchbase-server.node"},
 {loglevel_default,debug},
 {loglevel_couchdb,info},
 {loglevel_ns_server,debug},
 {loglevel_error_logger,debug},
 {loglevel_user,debug},
 {loglevel_menelaus,debug},
 {loglevel_ns_doctor,debug},
 {loglevel_stats,debug},
 {loglevel_rebalance,debug},
 {loglevel_cluster,debug},
 {loglevel_views,debug},
 {loglevel_mapreduce_errors,debug},
 {loglevel_xdcr,debug},
 {loglevel_access,info},
 {loglevel_cbas,debug},
 {disk_sink_opts,[{rotation,[{compress,true},
                             {size,41943040},
                             {num_files,10},
                             {buffer_size_max,52428800}]}]},
 {disk_sink_opts_json_rpc,[{rotation,[{compress,true},
                                      {size,41943040},
                                      {num_files,2},
                                      {buffer_size_max,52428800}]}]},
 {net_kernel_verbosity,10}]
[ns_server:warn,2020-03-27T19:45:32.294Z,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter error_logger_mf_dir, which is given from command line
[ns_server:warn,2020-03-27T19:45:32.295Z,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter path_config_bindir, which is given from command line
[ns_server:warn,2020-03-27T19:45:32.295Z,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter path_config_etcdir, which is given from command line
[ns_server:warn,2020-03-27T19:45:32.295Z,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter path_config_libdir, which is given from command line
[ns_server:warn,2020-03-27T19:45:32.295Z,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter path_config_datadir, which is given from command line
[ns_server:warn,2020-03-27T19:45:32.295Z,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter path_config_tmpdir, which is given from command line
[ns_server:warn,2020-03-27T19:45:32.295Z,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter path_config_secdir, which is given from command line
[ns_server:warn,2020-03-27T19:45:32.295Z,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter nodefile, which is given from command line
[ns_server:warn,2020-03-27T19:45:32.295Z,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_default, which is given from command line
[ns_server:warn,2020-03-27T19:45:32.295Z,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_couchdb, which is given from command line
[ns_server:warn,2020-03-27T19:45:32.295Z,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_ns_server, which is given from command line
[ns_server:warn,2020-03-27T19:45:32.295Z,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_error_logger, which is given from command line
[ns_server:warn,2020-03-27T19:45:32.295Z,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_user, which is given from command line
[ns_server:warn,2020-03-27T19:45:32.295Z,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_menelaus, which is given from command line
[ns_server:warn,2020-03-27T19:45:32.296Z,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_ns_doctor, which is given from command line
[ns_server:warn,2020-03-27T19:45:32.296Z,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_stats, which is given from command line
[ns_server:warn,2020-03-27T19:45:32.296Z,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_rebalance, which is given from command line
[ns_server:warn,2020-03-27T19:45:32.296Z,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_cluster, which is given from command line
[ns_server:warn,2020-03-27T19:45:32.296Z,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_views, which is given from command line
[ns_server:warn,2020-03-27T19:45:32.296Z,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_mapreduce_errors, which is given from command line
[ns_server:warn,2020-03-27T19:45:32.296Z,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_xdcr, which is given from command line
[ns_server:warn,2020-03-27T19:45:32.296Z,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_access, which is given from command line
[ns_server:warn,2020-03-27T19:45:32.296Z,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_cbas, which is given from command line
[ns_server:warn,2020-03-27T19:45:32.296Z,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter disk_sink_opts, which is given from command line
[ns_server:warn,2020-03-27T19:45:32.296Z,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter disk_sink_opts_json_rpc, which is given from command line
[ns_server:warn,2020-03-27T19:45:32.296Z,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter net_kernel_verbosity, which is given from command line
[ns_server:info,2020-03-27T19:45:32.314Z,nonode@nohost:dist_manager<0.166.0>:dist_manager:read_address_config_from_path:99]Reading ip config from "/opt/couchbase/var/lib/couchbase/ip_start"
[ns_server:info,2020-03-27T19:45:32.315Z,nonode@nohost:dist_manager<0.166.0>:dist_manager:read_address_config_from_path:99]Reading ip config from "/opt/couchbase/var/lib/couchbase/ip"
[ns_server:info,2020-03-27T19:45:32.316Z,nonode@nohost:dist_manager<0.166.0>:dist_manager:init:196]ip config not found. Looks like we're brand new node
[ns_server:info,2020-03-27T19:45:32.320Z,nonode@nohost:dist_manager<0.166.0>:dist_manager:bringup:249]Attempting to bring up net_kernel with name 'ns_1@cb.local'
[error_logger:info,2020-03-27T19:45:32.337Z,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ssl_dist_admin_sup}
             started: [{pid,<0.170.0>},
                       {id,ssl_pem_cache_dist},
                       {mfargs,{ssl_pem_cache,start_link_dist,[[]]}},
                       {restart_type,permanent},
                       {shutdown,4000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T19:45:32.337Z,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ssl_dist_admin_sup}
             started: [{pid,<0.171.0>},
                       {id,ssl_dist_manager},
                       {mfargs,{ssl_manager,start_link_dist,[[]]}},
                       {restart_type,permanent},
                       {shutdown,4000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T19:45:32.337Z,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ssl_dist_sup}
             started: [{pid,<0.169.0>},
                       {id,ssl_dist_admin_sup},
                       {mfargs,{ssl_dist_admin_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,4000},
                       {child_type,supervisor}]

[error_logger:info,2020-03-27T19:45:32.342Z,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ssl_dist_sup}
             started: [{pid,<0.172.0>},
                       {id,ssl_tls_dist_proxy},
                       {mfargs,{ssl_tls_dist_proxy,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,4000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T19:45:32.347Z,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ssl_dist_connection_sup}
             started: [{pid,<0.174.0>},
                       {id,dist_tls_connection},
                       {mfargs,{tls_connection_sup,start_link_dist,[]}},
                       {restart_type,permanent},
                       {shutdown,4000},
                       {child_type,supervisor}]

[error_logger:info,2020-03-27T19:45:32.348Z,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ssl_dist_connection_sup}
             started: [{pid,<0.175.0>},
                       {id,dist_tls_socket},
                       {mfargs,{ssl_listen_tracker_sup,start_link_dist,[]}},
                       {restart_type,permanent},
                       {shutdown,4000},
                       {child_type,supervisor}]

[error_logger:info,2020-03-27T19:45:32.348Z,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ssl_dist_sup}
             started: [{pid,<0.173.0>},
                       {id,ssl_dist_connection_sup},
                       {mfargs,{ssl_dist_connection_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,4000},
                       {child_type,supervisor}]

[error_logger:info,2020-03-27T19:45:32.348Z,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,net_sup}
             started: [{pid,<0.168.0>},
                       {id,ssl_dist_sup},
                       {mfargs,{ssl_dist_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[ns_server:debug,2020-03-27T19:45:32.348Z,nonode@nohost:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Starting cb_dist with config []
[error_logger:info,2020-03-27T19:45:32.350Z,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,net_sup}
             started: [{pid,<0.176.0>},
                       {id,cb_dist},
                       {mfargs,{cb_dist,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,worker}]

[error_logger:info,2020-03-27T19:45:32.351Z,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,net_sup}
             started: [{pid,<0.177.0>},
                       {id,cb_epmd},
                       {mfargs,{cb_epmd,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,2000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T19:45:32.355Z,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,net_sup}
             started: [{pid,<0.178.0>},
                       {id,auth},
                       {mfargs,{auth,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,2000},
                       {child_type,worker}]

[ns_server:debug,2020-03-27T19:45:32.357Z,nonode@nohost:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Initial protos: [inet_tcp_dist,inet6_tcp_dist], required protos: [inet_tcp_dist]
[ns_server:debug,2020-03-27T19:45:32.357Z,nonode@nohost:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Starting inet_tcp_dist listener on 21100...
[ns_server:debug,2020-03-27T19:45:32.358Z,nonode@nohost:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Starting inet6_tcp_dist listener on 21100...
[ns_server:debug,2020-03-27T19:45:32.361Z,ns_1@cb.local:dist_manager<0.166.0>:dist_manager:configure_net_kernel:293]Set net_kernel vebosity to 10 -> 0
[error_logger:info,2020-03-27T19:45:32.361Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,net_sup}
             started: [{pid,<0.179.0>},
                       {id,net_kernel},
                       {mfargs,
                           {net_kernel,start_link,
                               [['ns_1@cb.local',longnames],false]}},
                       {restart_type,permanent},
                       {shutdown,2000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T19:45:32.361Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,kernel_sup}
             started: [{pid,<0.167.0>},
                       {id,net_sup_dynamic},
                       {mfargs,
                           {erl_distribution,start_link,
                               [['ns_1@cb.local',longnames],false]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,supervisor}]

[ns_server:info,2020-03-27T19:45:32.363Z,ns_1@cb.local:dist_manager<0.166.0>:dist_manager:save_node:175]saving node to "/opt/couchbase/var/lib/couchbase/couchbase-server.node"
[ns_server:debug,2020-03-27T19:45:32.377Z,ns_1@cb.local:dist_manager<0.166.0>:dist_manager:bringup:263]Attempted to save node name to disk: ok
[ns_server:debug,2020-03-27T19:45:32.377Z,ns_1@cb.local:dist_manager<0.166.0>:dist_manager:wait_for_node:270]Waiting for connection to node 'babysitter_of_ns_1@cb.local' to be established
[error_logger:info,2020-03-27T19:45:32.377Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{connect,normal,'babysitter_of_ns_1@cb.local'}}
[ns_server:debug,2020-03-27T19:45:32.378Z,ns_1@cb.local:net_kernel<0.179.0>:cb_dist:info_msg:754]cb_dist: Setting up new connection to 'babysitter_of_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2020-03-27T19:45:32.378Z,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Added connection {con,#Ref<0.1597663237.214695938.137194>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2020-03-27T19:45:32.378Z,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Updated connection: {con,#Ref<0.1597663237.214695938.137194>,
                                  inet_tcp_dist,<0.183.0>,
                                  #Ref<0.1597663237.214695937.137955>}
[ns_server:debug,2020-03-27T19:45:32.388Z,ns_1@cb.local:dist_manager<0.166.0>:dist_manager:wait_for_node:282]Observed node 'babysitter_of_ns_1@cb.local' to come up
[ns_server:info,2020-03-27T19:45:32.390Z,ns_1@cb.local:dist_manager<0.166.0>:dist_manager:save_address_config:162]Deleting irrelevant ip file "/opt/couchbase/var/lib/couchbase/ip_start": {error,
                                                                          enoent}
[ns_server:info,2020-03-27T19:45:32.390Z,ns_1@cb.local:dist_manager<0.166.0>:dist_manager:save_address_config:163]saving ip config to "/opt/couchbase/var/lib/couchbase/ip"
[ns_server:info,2020-03-27T19:45:32.426Z,ns_1@cb.local:dist_manager<0.166.0>:dist_manager:save_address_config:166]Persisted the address successfully
[error_logger:info,2020-03-27T19:45:32.426Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,root_sup}
             started: [{pid,<0.166.0>},
                       {id,dist_manager},
                       {mfargs,{dist_manager,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T19:45:32.440Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.186.0>},
                       {id,local_tasks},
                       {mfargs,{local_tasks,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[ns_server:info,2020-03-27T19:45:32.443Z,ns_1@cb.local:ns_server_cluster_sup<0.185.0>:log_os_info:start_link:25]OS type: {unix,linux} Version: {4,19,76}
Runtime info: [{otp_release,"20"},
               {erl_version,"9.3.3.9"},
               {erl_version_long,
                   "Erlang/OTP 20 [erts-9.3.3.9] [source-d27a01d] [64-bit] [smp:2:2] [ds:2:2:10] [async-threads:16] [kernel-poll:true]\n"},
               {system_arch_raw,"x86_64-unknown-linux-gnu"},
               {system_arch,"x86_64-unknown-linux-gnu"},
               {localtime,{{2020,3,27},{19,45,32}}},
               {memory,
                   [{total,36385840},
                    {processes,9390808},
                    {processes_used,9387192},
                    {system,26995032},
                    {atom,388625},
                    {atom_used,364429},
                    {binary,122480},
                    {code,8250921},
                    {ets,1504840}]},
               {loaded,
                   [ns_info,log_os_info,local_tasks,restartable,
                    ns_server_cluster_sup,ns_cluster,dist_util,ns_node_disco,
                    inet6_tcp,inet6_tcp_dist,re,auth,rand,
                    ssl_dist_connection_sup,ssl_tls_dist_proxy,
                    ssl_dist_admin_sup,ssl_dist_sup,inet_tls_dist,
                    inet_tcp_dist,inet_tcp,gen_tcp,erl_epmd,cb_epmd,gen_udp,
                    inet_hosts,dist_manager,root_sup,path_config,cb_dist,
                    unicode_util,calendar,ale_default_formatter,
                    'ale_logger-metakv','ale_logger-rebalance',
                    'ale_logger-menelaus','ale_logger-stats',
                    'ale_logger-json_rpc','ale_logger-access',
                    'ale_logger-ns_server','ale_logger-user',
                    'ale_logger-ns_doctor','ale_logger-cluster',
                    'ale_logger-xdcr',erl_bits,otp_internal,ns_log_sink,
                    ale_disk_sink,misc,couch_util,ns_server,io_lib_fread,
                    filelib,cpu_sup,memsup,disksup,os_mon,string,io,
                    release_handler,alarm_handler,sasl,timer,tftp_sup,
                    httpd_sup,httpc_handler_sup,httpc_cookie,inets_trace,
                    httpc_manager,httpc,httpc_profile_sup,httpc_sup,ftp_sup,
                    inets_sup,inets_app,ssl,lhttpc_manager,lhttpc_sup,lhttpc,
                    dtls_udp_sup,dtls_connection_sup,ssl_listen_tracker_sup,
                    tls_connection_sup,ssl_connection_sup,ssl_session_cache,
                    ssl_manager,ssl_pkix_db,ssl_pem_cache,ssl_admin_sup,
                    ssl_sup,ssl_app,ale_error_logger_handler,
                    'ale_logger-ale_logger','ale_logger-error_logger',
                    beam_opcodes,maps,beam_dict,beam_asm,beam_validator,
                    beam_z,beam_flatten,beam_trim,beam_record,beam_receive,
                    beam_bsm,beam_peep,beam_dead,beam_split,beam_type,
                    beam_clean,beam_bs,beam_except,beam_block,beam_utils,
                    beam_reorder,beam_jump,beam_a,v3_codegen,v3_life,
                    v3_kernel,sys_core_dsetel,sys_core_bsm,erl_bifs,
                    cerl_clauses,cerl_sets,sys_core_fold,cerl_trees,
                    sys_core_inline,core_lib,cerl,v3_core,erl_expand_records,
                    sofs,erl_internal,sets,ordsets,compile,dynamic_compile,
                    ale_utils,io_lib_pretty,io_lib_format,io_lib,ale_codegen,
                    dict,ale,ale_dynamic_sup,ale_sup,ale_app,ns_bootstrap,
                    child_erlang,orddict,c,erl_signal_handler,kernel_config,
                    user_io,user_sup,supervisor_bridge,standard_error,
                    net_kernel,global_group,erl_distribution,epp,
                    inet_gethost_native,inet_parse,inet,inet_udp,inet_config,
                    inet_db,global,rpc,unicode,os,hipe_unified_loader,
                    gb_trees,gb_sets,binary,erl_anno,proplists,erl_scan,
                    application_master,error_handler,application,code_server,
                    application_controller,error_logger,file_server,erl_eval,
                    code,kernel,gen,proc_lib,gen_server,gen_event,heart,
                    file_io_server,erl_parse,file,lists,ets,filename,
                    supervisor,erl_lint,erts_dirty_process_code_checker,
                    erts_literal_area_collector,erl_tracer,erts_internal,
                    erlang,erl_prim_loader,prim_zip,zlib,prim_file,prim_inet,
                    prim_eval,init,erts_code_purger,otp_ring0]},
               {applications,
                   [{sasl,"SASL  CXC 138 11","3.1.2"},
                    {os_mon,"CPO  CXC 138 46","2.4.4"},
                    {inets,"INETS  CXC 138 49","6.5.2.4"},
                    {crypto,"CRYPTO","4.2.2.2"},
                    {ale,"Another Logger for Erlang","0.0.0"},
                    {lhttpc,"Lightweight HTTP Client","1.3.0"},
                    {stdlib,"ERTS  CXC 138 10","3.4.5.1"},
                    {ssl,"Erlang/OTP SSL application","8.2.6.4"},
                    {kernel,"ERTS  CXC 138 10","5.4.3.2"},
                    {public_key,"Public key infrastructure","1.5.2"},
                    {asn1,"The Erlang ASN1 compiler version 5.0.5.2",
                        "5.0.5.2"},
                    {ns_server,"Couchbase server","6.5.0-4960-enterprise"}]},
               {pre_loaded,
                   [erts_dirty_process_code_checker,
                    erts_literal_area_collector,erl_tracer,erts_internal,
                    erlang,erl_prim_loader,prim_zip,zlib,prim_file,prim_inet,
                    prim_eval,init,erts_code_purger,otp_ring0]},
               {process_count,129},
               {node,'ns_1@cb.local'},
               {nodes,[]},
               {registered,
                   [application_controller,erl_prim_loader,httpd_sup,auth,
                    dtls_udp_sup,cb_dist,dtls_connection_sup,
                    ns_server_cluster_sup,tls_connection_sup,sasl_sup,
                    release_handler,lhttpc_sup,httpc_sup,lhttpc_manager,
                    alarm_handler,httpc_profile_sup,
                    ssl_listen_tracker_supdist,httpc_manager,
                    httpc_handler_sup,ssl_connection_sup_dist,'sink-ns_log',
                    local_tasks,standard_error_sup,ftp_sup,
                    'sink-disk_json_rpc','sink-disk_metakv',inets_sup,
                    'sink-disk_access_int','sink-disk_access',standard_error,
                    'sink-disk_reports',kernel_safe_sup,ale_stats_events,
                    'sink-disk_stats','sink-disk_xdcr',timer_server,
                    'sink-disk_debug',ale_sup,'sink-disk_error',inet_db,
                    'sink-disk_default',ale_dynamic_sup,ssl_pem_cache_dist,
                    rex,global_group,net_sup,ssl_connection_sup,kernel_sup,
                    ssl_admin_sup,tftp_sup,global_name_server,ssl_sup,
                    root_sup,os_mon_sup,erts_code_purger,file_server_2,
                    error_logger,cpu_sup,memsup,erl_epmd,init,disksup,ale,
                    erl_signal_server,net_kernel,dist_manager,ssl_pem_cache,
                    ssl_manager,ssl_dist_admin_sup,ssl_dist_connection_sup,
                    ssl_dist_sup,ssl_tls_dist_proxy,ssl_manager_dist,user,
                    sasl_safe_sup,ssl_listen_tracker_sup,code_server]},
               {cookie,nocookie},
               {wordsize,8},
               {wall_clock,2}]
[ns_server:info,2020-03-27T19:45:32.450Z,ns_1@cb.local:ns_server_cluster_sup<0.185.0>:log_os_info:start_link:27]Manifest:
["<manifest>",
 "  <remote fetch=\"git://github.com/blevesearch/\" name=\"blevesearch\" />",
 "  <remote fetch=\"git://github.com/couchbase/\" name=\"couchbase\" review=\"review.couchbase.org\" />",
 "  <remote fetch=\"ssh://git@github.com/couchbase/\" name=\"couchbase-priv\" review=\"review.couchbase.org\" />",
 "  <remote fetch=\"git://github.com/couchbasedeps/\" name=\"couchbasedeps\" review=\"review.couchbase.org\" />",
 "  <remote fetch=\"git://github.com/couchbaselabs/\" name=\"couchbaselabs\" review=\"review.couchbase.org\" />",
 "  ","  <default remote=\"couchbase\" revision=\"master\" />","  ",
 "  <project groups=\"kv\" name=\"HdrHistogram_c\" path=\"third_party/HdrHistogram_c\" remote=\"couchbasedeps\" revision=\"bc8aef24ea57884464027f841c1ad7436a42c615\" />",
 "  <project groups=\"notdefault,enterprise,analytics\" name=\"analytics-dcp-client\" path=\"analytics/java-dcp-client\" revision=\"691cec38f47eaab04ad81556cc065d22f1eb8749\" />",
 "  <project groups=\"notdefault,enterprise,analytics\" name=\"asterixdb\" path=\"analytics/asterixdb\" revision=\"672a36b64a0632b72aa4b4df59635ceaa0e340de\" />",
 "  <project groups=\"backup,notdefault,enterprise\" name=\"backup\" path=\"goproj/src/github.com/couchbase/backup\" remote=\"couchbase-priv\" revision=\"cfa0f75f28402d2e1aa254b2a374bead19433526\" upstream=\"mad-hatter\" />",
 "  <project groups=\"kv\" name=\"benchmark\" remote=\"couchbasedeps\" revision=\"74b24058ad4914b837200d0341050657ba154e4a\" />",
 "  <project name=\"bitset\" path=\"godeps/src/github.com/willf/bitset\" remote=\"couchbasedeps\" revision=\"28a4168144bb8ac95454e1f51c84da1933681ad4\" />",
 "  <project name=\"blance\" path=\"godeps/src/github.com/couchbase/blance\" revision=\"5cd1345cca3ed72f1e63d41d622fcda73e63fea8\" upstream=\"master\" />",
 "  <project name=\"bleve\" path=\"godeps/src/github.com/blevesearch/bleve\" remote=\"blevesearch\" revision=\"b7a0cb6a1d4fdbaeb7ab5bdec6a9732b995e39a0\" />",
 "  <project name=\"bleve-mapping-ui\" path=\"godeps/src/github.com/blevesearch/bleve-mapping-ui\" remote=\"blevesearch\" revision=\"7987f3c80047347b1e2c3a5fafae8da56daf97d7\" />",
 "  <project name=\"bolt\" path=\"godeps/src/github.com/boltdb/bolt\" remote=\"couchbasedeps\" revision=\"51f99c862475898df9773747d3accd05a7ca33c1\" />",
 "  <project name=\"buffer\" path=\"godeps/src/github.com/tdewolff/buffer\" remote=\"couchbasedeps\" revision=\"43cef5ba7b6ce99cc410632dad46cf1c6c97026e\" />",
 "  <project groups=\"notdefault,build\" name=\"build\" path=\"cbbuild\" revision=\"f2a16b53bb74146f20d18ba2c0443d5f10a9a550\" upstream=\"master\">",
 "    <annotation name=\"RELEASE\" value=\"mad-hatter\" />",
 "    <annotation name=\"PRODUCT\" value=\"couchbase-server\" />",
 "    <annotation name=\"BLD_NUM\" value=\"4960\" />",
 "    <annotation name=\"VERSION\" value=\"6.5.0\" />","  </project>",
 "  <project groups=\"notdefault,enterprise,analytics\" name=\"cbas\" path=\"goproj/src/github.com/couchbase/cbas\" remote=\"couchbase-priv\" revision=\"e3ec01671ca2f253a5f32cf9e258d3be7fdbfe9a\" />",
 "  <project groups=\"notdefault,enterprise,analytics\" name=\"cbas-core\" path=\"analytics\" remote=\"couchbase-priv\" revision=\"c86a9fc60d074711470b112753c5695dee79dcf7\" />",
 "  <project groups=\"analytics\" name=\"cbas-ui\" revision=\"8744108f25c4520b09009ff277d35223e208fe30\" />",
 "  <project name=\"cbauth\" path=\"godeps/src/github.com/couchbase/cbauth\" revision=\"82614adbe4d480de5675d8eee9b21a180a779222\" upstream=\"master\" />",
 "  <project groups=\"backup\" name=\"cbflag\" path=\"godeps/src/github.com/couchbase/cbflag\" revision=\"9892b6db3537c54be7719f47ad25e0d513333b3e\" upstream=\"master\" />",
 "  <project name=\"cbft\" path=\"goproj/src/github.com/couchbase/cbft\" revision=\"ef487dda0baef8a258bac4f7482af3b761e4a8e0\" upstream=\"mad-hatter\" />",
 "  <project groups=\"notdefault,enterprise\" name=\"cbftx\" path=\"goproj/src/github.com/couchbase/cbftx\" remote=\"couchbase-priv\" revision=\"46dbb7c6edac7dfef017ae889d7a5b7536ce904d\" upstream=\"master\" />",
 "  <project name=\"cbgt\" path=\"goproj/src/github.com/couchbase/cbgt\" revision=\"c78e34377d7a8f017328f57a3376642f37458464\" upstream=\"mad-hatter\" />",
 "  <project name=\"cbsummary\" path=\"goproj/src/github.com/couchbase/cbsummary\" revision=\"31ba0584a81d5b293cedfb236109ab95036aa395\" upstream=\"master\" />",
 "  <project groups=\"backup\" name=\"clog\" path=\"godeps/src/github.com/couchbase/clog\" revision=\"b8e6d5d421bcc34f522e3a9a12fd6e09980995b1\" upstream=\"master\" />",
 "  <project name=\"cobra\" path=\"godeps/src/github.com/spf13/cobra\" remote=\"couchbasedeps\" revision=\"0f056af21f5f368e5b0646079d0094a2c64150f7\" />",
 "  <project name=\"context\" path=\"godeps/src/github.com/gorilla/context\" remote=\"couchbasedeps\" revision=\"215affda49addc4c8ef7e2534915df2c8c35c6cd\" />",
 "  <project groups=\"notdefault,kv_ee,enterprise\" name=\"couch_rocks\" remote=\"couchbase-priv\" revision=\"75f37fa46bfe5e445dee077157303968a3e09126\" upstream=\"master\" />",
 "  <project groups=\"kv\" name=\"couchbase-cli\" revision=\"abb0c1036566f4bd579aaadbaaa4e13466a23ef7\" upstream=\"master\" />",
 "  <project name=\"couchdb\" revision=\"fa3c64b1b85ad3145bb7910d3fe7ee90c060247e\" upstream=\"mad-hatter\" />",
 "  <project groups=\"notdefault,packaging\" name=\"couchdbx-app\" revision=\"b2a111967ba02772dc600d5c15a6514e2dea7d68\" upstream=\"master\" />",
 "  <project groups=\"kv\" name=\"couchstore\" revision=\"fff3e20090414206853b2293f17667279dda0337\" />",
 "  <project groups=\"backup\" name=\"crypto\" path=\"godeps/src/golang.org/x/crypto\" remote=\"couchbasedeps\" revision=\"bd6f299fb381e4c3393d1c4b1f0b94f5e77650c8\" />",
 "  <project name=\"cuckoofilter\" path=\"godeps/src/github.com/seiflotfy/cuckoofilter\" remote=\"couchbasedeps\" revision=\"d04838794ab86926d32b124345777e55e6f43974\" />",
 "  <project name=\"cznic-b\" path=\"godeps/src/github.com/cznic/b\" remote=\"couchbasedeps\" revision=\"b96e30f1b7bd34b0b9d8760798d67eca83d7f09e\" />",
 "  <project name=\"docloader\" path=\"goproj/src/github.com/couchbase/docloader\" revision=\"13cf07af78594aff20d00db4633af27d81fc921d\" upstream=\"master\" />",
 "  <project name=\"dparval\" path=\"godeps/src/github.com/couchbase/dparval\" revision=\"9def03782da875a2477c05bf64985db3f19f59ae\" upstream=\"master\" />",
 "  <project groups=\"backup\" name=\"errors\" path=\"godeps/src/github.com/pkg/errors\" remote=\"couchbasedeps\" revision=\"30136e27e2ac8d167177e8a583aa4c3fea5be833\" />",
 "  <project name=\"etcd-bbolt\" path=\"godeps/src/github.com/etcd-io/bbolt\" remote=\"couchbasedeps\" revision=\"7ee3ded59d4835e10f3e7d0f7603c42aa5e83820\" />",
 "  <project groups=\"notdefault,enterprise\" name=\"eventing\" path=\"goproj/src/github.com/couchbase/eventing\" revision=\"dec7a7d51b71309d43d7aea4803cd45f6ad001da\" upstream=\"mad-hatter\" />",
 "  <project groups=\"notdefault,enterprise\" name=\"eventing-ee\" path=\"goproj/src/github.com/couchbase/eventing-ee\" remote=\"couchbase-priv\" revision=\"398acea25e003c1739d3f45f53121bdec857e485\" upstream=\"mad-hatter\" />",
 "  <project name=\"flatbuffers\" path=\"godeps/src/github.com/google/flatbuffers\" remote=\"couchbasedeps\" revision=\"1a8968225130caeddd16e227678e6f8af1926303\" />",
 "  <project groups=\"backup,kv\" name=\"forestdb\" revision=\"4c3b2f9b1d869b6b71556e461d6ee68f941c1ba5\" upstream=\"cb-master\" />",
 "  <project name=\"fwd\" path=\"godeps/src/github.com/philhofer/fwd\" remote=\"couchbasedeps\" revision=\"bb6d471dc95d4fe11e432687f8b70ff496cf3136\" />",
 "  <project name=\"geocouch\" revision=\"92def13f6b049553da1aa1488ce0bde6b7d0f459\" upstream=\"master\" />",
 "  <project name=\"ghistogram\" path=\"godeps/src/github.com/couchbase/ghistogram\" revision=\"d910dd063dd68fb4d2a1ba344440f834ebb4ef62\" upstream=\"master\" />",
 "  <project name=\"go-bindata-assetfs\" path=\"godeps/src/github.com/elazarl/go-bindata-assetfs\" remote=\"couchbasedeps\" revision=\"57eb5e1fc594ad4b0b1dbea7b286d299e0cb43c2\" />",
 "  <project name=\"go-couchbase\" path=\"godeps/src/github.com/couchbase/go-couchbase\" revision=\"12d479a70a3ef189d8fb2424f5e2eea3632c0c9a\" upstream=\"mad-hatter\" />",
 "  <project name=\"go-curl\" path=\"godeps/src/github.com/andelf/go-curl\" remote=\"couchbasedeps\" revision=\"f0b2afc926ec79be5d7f30393b3485352781a705\" upstream=\"20161221-couchbase\" />",
 "  <project name=\"go-genproto\" path=\"godeps/src/google.golang.org/genproto\" remote=\"couchbasedeps\" revision=\"2b5a72b8730b0b16380010cfe5286c42108d88e7\" />",
 "  <project name=\"go-jsonpointer\" path=\"godeps/src/github.com/dustin/go-jsonpointer\" remote=\"couchbasedeps\" revision=\"75939f54b39e7dafae879e61f65438dadc5f288c\" />",
 "  <project name=\"go-metrics\" path=\"godeps/src/github.com/rcrowley/go-metrics\" remote=\"couchbasedeps\" revision=\"dee209f2455f101a5e4e593dea94872d2c62d85d\" />",
 "  <project name=\"go-porterstemmer\" path=\"godeps/src/github.com/blevesearch/go-porterstemmer\" remote=\"blevesearch\" revision=\"23a2c8e5cf1f380f27722c6d2ae8896431dc7d0e\" />",
 "  <project name=\"go-runewidth\" path=\"godeps/src/github.com/mattn/go-runewidth\" remote=\"couchbasedeps\" revision=\"703b5e6b11ae25aeb2af9ebb5d5fdf8fa2575211\" />",
 "  <project name=\"go-slab\" path=\"godeps/src/github.com/couchbase/go-slab\" revision=\"1f5f7f282713ccfab3f46b1610cb8da34bcf676f\" upstream=\"master\" />",
 "  <project groups=\"backup\" name=\"go-sqlite3\" path=\"godeps/src/github.com/mattn/go-sqlite3\" remote=\"couchbasedeps\" revision=\"ad30583d8387ce8118f8605eaeb3b4f7b4ae0ee1\" />",
 "  <project name=\"go-unsnap-stream\" path=\"godeps/src/github.com/glycerine/go-unsnap-stream\" remote=\"couchbasedeps\" revision=\"62a9a9eb44fd8932157b1a8ace2149eff5971af6\" />",
 "  <project name=\"go-zookeeper\" path=\"godeps/src/github.com/samuel/go-zookeeper\" remote=\"couchbasedeps\" revision=\"fa6674abf3f4580b946a01bf7a1ce4ba8766205b\" />",
 "  <project name=\"go_json\" path=\"godeps/src/github.com/couchbase/go_json\" revision=\"d47ffbbc4863b0020bb85c4e181d4044ea184d40\" upstream=\"mad-hatter\" />",
 "  <project name=\"go_n1ql\" path=\"godeps/src/github.com/couchbase/go_n1ql\" revision=\"6cf4e348b127e21f56e53eb8c3faaea56afdc588\" upstream=\"master\" />",
 "  <project groups=\"backup\" name=\"gocb\" path=\"godeps/src/gopkg.in/couchbase/gocb.v1\" revision=\"01c846cb025ddd50a2ef4c82a27992b40c230dbb\" upstream=\"refs/tags/v1.4.2\" />",
 "  <project groups=\"backup\" name=\"gocbconnstr\" path=\"godeps/src/gopkg.in/couchbaselabs/gocbconnstr.v1\" remote=\"couchbaselabs\" revision=\"083dcfef49cfdcb42a0f5ecf8c0c29b0cbaa640f\" upstream=\"master\" />",
 "  <project groups=\"backup\" name=\"gocbcore\" path=\"godeps/src/gopkg.in/couchbase/gocbcore.v7\" revision=\"441cb91f01ce26932514ec10d9e59e568ee27722\" upstream=\"refs/tags/v7.1.14\" />",
 "  <project name=\"godbc\" path=\"godeps/src/github.com/couchbase/godbc\" revision=\"b2aaaa21900ab3e95d37d38fb5a0f320426cbe56\" upstream=\"mad-hatter\" />",
 "  <project name=\"gofarmhash\" path=\"godeps/src/github.com/leemcloughlin/gofarmhash\" remote=\"couchbasedeps\" revision=\"0a055c5b87a8c55ce83459cbf2776b563822a942\" />",
 "  <project groups=\"backup\" name=\"goforestdb\" path=\"godeps/src/github.com/couchbase/goforestdb\" revision=\"0b501227de0e8c55d99ed14e900eea1a1dbaf899\" upstream=\"master\" />",
 "  <project name=\"gojson\" path=\"godeps/src/github.com/dustin/gojson\" remote=\"couchbasedeps\" revision=\"af16e0e771e2ed110f2785564ae33931de8829e4\" />",
 "  <project name=\"gojsonsm\" path=\"godeps/src/github.com/couchbase/gojsonsm\" remote=\"couchbaselabs\" revision=\"eec4953dcb855282c483b8cd4fe03a8074e2f7a1\" upstream=\"master\" />",
 "  <project name=\"golang-pkg-pcre\" path=\"godeps/src/github.com/glenn-brown/golang-pkg-pcre\" remote=\"couchbasedeps\" revision=\"48bb82a8b8ceea98f4e97825b43870f6ba1970d6\" />",
 "  <project groups=\"backup\" name=\"golang-snappy\" path=\"godeps/src/github.com/golang/snappy\" remote=\"couchbasedeps\" revision=\"723cc1e459b8eea2dea4583200fd60757d40097a\" />",
 "  <project name=\"golang-tools\" path=\"godeps/src/golang.org/x/tools\" remote=\"couchbasedeps\" revision=\"a28dfb48e06b2296b66678872c2cb638f0304f20\" />",
 "  <project name=\"goleveldb\" path=\"godeps/src/github.com/syndtr/goleveldb\" remote=\"couchbasedeps\" revision=\"fa5b5c78794bc5c18f330361059f871ae8c2b9d6\" />",
 "  <project name=\"gomemcached\" path=\"godeps/src/github.com/couchbase/gomemcached\" revision=\"2b4197fedf38f694a33465050d1396e03e97db19\" upstream=\"mad-hatter\" />",
 "  <project name=\"gometa\" path=\"goproj/src/github.com/couchbase/gometa\" revision=\"563cdf343321e2025b73852bcf454860a4880300\" upstream=\"mad-hatter\" />",
 "  <project groups=\"kv\" name=\"googletest\" remote=\"couchbasedeps\" revision=\"f397fa5ec6365329b2e82eb2d8c03a7897bbefb5\" />",
 "  <project name=\"goskiplist\" path=\"godeps/src/github.com/ryszard/goskiplist\" remote=\"couchbasedeps\" revision=\"2dfbae5fcf46374f166f8969cb07e167f1be6273\" />",
 "  <project name=\"gosnappy\" path=\"godeps/src/github.com/syndtr/gosnappy\" remote=\"couchbasedeps\" revision=\"156a073208e131d7d2e212cb749feae7c339e846\" />",
 "  <project groups=\"backup\" name=\"goutils\" path=\"godeps/src/github.com/couchbase/goutils\" revision=\"b49639060d85b267c5bdb7d4e3246d4ccca94e79\" upstream=\"mad-hatter\" />",
 "  <project name=\"goxdcr\" path=\"goproj/src/github.com/couchbase/goxdcr\" revision=\"03e000156faeecd5e77eb79fc45d7c73f26b2899\" upstream=\"mad-hatter\" />",
 "  <project name=\"grpc-go\" path=\"godeps/src/google.golang.org/grpc\" remote=\"couchbasedeps\" revision=\"df014850f6dee74ba2fc94874043a9f3f75fbfd8\" upstream=\"refs/tags/v1.17.0\" />",
 "  <project groups=\"kv\" name=\"gsl-lite\" path=\"third_party/gsl-lite\" remote=\"couchbasedeps\" revision=\"57542c7e7ced375346e9ac55dad85b942cfad556\" upstream=\"refs/tags/v0.25.0\" />",
 "  <project name=\"gtreap\" path=\"godeps/src/github.com/steveyen/gtreap\" remote=\"couchbasedeps\" revision=\"0abe01ef9be25c4aedc174758ec2d917314d6d70\" />",
 "  <project name=\"httprouter\" path=\"godeps/src/github.com/julienschmidt/httprouter\" remote=\"couchbasedeps\" revision=\"975b5c4c7c21c0e3d2764200bf2aa8e34657ae6e\" />",
 "  <project name=\"indexing\" path=\"goproj/src/github.com/couchbase/indexing\" revision=\"fc2e1b715bf9c098bf0991af666388dd446edf9b\" upstream=\"mad-hatter\" />",
 "  <project name=\"json-iterator-go\" path=\"godeps/src/github.com/json-iterator/go\" remote=\"couchbasedeps\" revision=\"f7279a603edee96fe7764d3de9c6ff8cf9970994\" />",
 "  <project name=\"jsonparser\" path=\"godeps/src/github.com/buger/jsonparser\" remote=\"couchbasedeps\" revision=\"bf1c66bbce23153d89b23f8960071a680dbef54b\" />",
 "  <project groups=\"backup\" name=\"jsonx\" path=\"godeps/src/gopkg.in/couchbaselabs/jsonx.v1\" remote=\"couchbaselabs\" revision=\"5b7baa20429a46a5543ee259664cc86502738cad\" upstream=\"master\" />",
 "  <project groups=\"kv\" name=\"kv_engine\" revision=\"2a368c39481ff4d42c6f755bd7d185b9a57554ca\" upstream=\"6.5.0\" />",
 "  <project name=\"levigo\" path=\"godeps/src/github.com/jmhodges/levigo\" remote=\"couchbasedeps\" revision=\"1ddad808d437abb2b8a55a950ec2616caa88969b\" />",
 "  <project groups=\"notdefault,enterprise\" name=\"libcouchbase\" revision=\"152e1a18bbcfd75bbb5a1388ed5ee050cde8a56d\" />",
 "  <project name=\"liner\" path=\"godeps/src/github.com/peterh/liner\" remote=\"couchbasedeps\" revision=\"6f820f8f90ce9482ffbd40bb15f9ea9932f4942d\" />",
 "  <project name=\"liner\" path=\"godeps/src/github.com/sbinet/liner\" remote=\"couchbasedeps\" revision=\"d9335eee40a45a4f5d74524c90040d6fe6013d50\" />",
 "  <project groups=\"notdefault,enterprise,kv_ee\" name=\"magma\" remote=\"couchbase-priv\" revision=\"c8e91e0af8b46d0a0e026d23ebbfab4048f670b6\" />",
 "  <project name=\"minify\" path=\"godeps/src/github.com/tdewolff/minify\" remote=\"couchbasedeps\" revision=\"ede45cc53f43891267b1fe7c689db9c76d4ce0fb\" />",
 "  <project name=\"mmap-go\" path=\"godeps/src/github.com/edsrzf/mmap-go\" remote=\"couchbasedeps\" revision=\"935e0e8a636ca4ba70b713f3e38a19e1b77739e8\" />",
 "  <project name=\"mobile-service\" path=\"goproj/src/github.com/couchbase/mobile-service\" revision=\"4672fde0390f115a25f4f4bfe9d1511836de47a7\" upstream=\"master\" />",
 "  <project name=\"moss\" path=\"godeps/src/github.com/couchbase/moss\" revision=\"a0cae174c4987cb28c071e0796e25b58834108d8\" upstream=\"master\" />",
 "  <project name=\"mossScope\" path=\"godeps/src/github.com/couchbase/mossScope\" revision=\"aa48ddbc0e832bc68dde56c4b69e30c5cb3983eb\" upstream=\"master\" />",
 "  <project name=\"mousetrap\" path=\"godeps/src/github.com/inconshreveable/mousetrap\" remote=\"couchbasedeps\" revision=\"76626ae9c91c4f2a10f34cad8ce83ea42c93bb75\" />",
 "  <project name=\"msgp\" path=\"godeps/src/github.com/tinylib/msgp\" remote=\"couchbasedeps\" revision=\"5bb5e1aed7ba5bcc93307153b020e7ffe79b0509\" />",
 "  <project name=\"mux\" path=\"godeps/src/github.com/gorilla/mux\" remote=\"couchbasedeps\" revision=\"043ee6597c29786140136a5747b6a886364f5282\" />",
 "  <project name=\"n1fty\" path=\"godeps/src/github.com/couchbase/n1fty\" revision=\"f28de9b4e73d7acdf3b07b7f7318bb23973f7dc6\" upstream=\"mad-hatter\" />",
 "  <project groups=\"backup\" name=\"net\" path=\"godeps/src/golang.org/x/net\" remote=\"couchbasedeps\" revision=\"44b7c21cbf19450f38b337eb6b6fe4f6496fb5b3\" />",
 "  <project name=\"nitro\" path=\"goproj/src/github.com/couchbase/nitro\" revision=\"4fc6475fb3352618cdf93fead56271bb29d15571\" upstream=\"mad-hatter\" />",
 "  <project name=\"npipe\" path=\"godeps/src/github.com/natefinch/npipe\" remote=\"couchbasedeps\" revision=\"272c8150302e83f23d32a355364578c9c13ab20f\" />",
 "  <project name=\"ns_server\" revision=\"3fe2759eb53c12478f75bd1613f8998401b0635c\" upstream=\"mad-hatter\" />",
 "  <project groups=\"backup\" name=\"opentracing-go\" path=\"godeps/src/github.com/opentracing/opentracing-go\" remote=\"couchbasedeps\" revision=\"1949ddbfd147afd4d964a9f00b24eb291e0e7c38\" />",
 "  <project name=\"parse\" path=\"godeps/src/github.com/tdewolff/parse\" remote=\"couchbasedeps\" revision=\"0334a869253aca4b3a10c56c3f3139b394aec3a9\" />",
 "  <project name=\"participle\" path=\"godeps/src/github.com/alecthomas/participle\" remote=\"couchbasedeps\" revision=\"bf8340a459bd383e5eb7d44a9a1b3af23b6cf8cd\" />",
 "  <project name=\"pflag\" path=\"godeps/src/github.com/spf13/pflag\" remote=\"couchbasedeps\" revision=\"a232f6d9f87afaaa08bafaff5da685f974b83313\" />",
 "  <project groups=\"kv\" name=\"phosphor\" revision=\"53ca1eeae7bd3deea5b7bf48b3d4188b47e530d1\" upstream=\"master\" />",
 "  <project name=\"pierrec-lz4\" path=\"godeps/src/github.com/pierrec/lz4\" remote=\"couchbasedeps\" revision=\"ed8d4cc3b461464e69798080a0092bd028910298\" />",
 "  <project name=\"pierrec-xxHash\" path=\"godeps/src/github.com/pierrec/xxHash\" remote=\"couchbasedeps\" revision=\"a0006b13c722f7f12368c00a3d3c2ae8a999a0c6\" />",
 "  <project groups=\"notdefault,enterprise\" name=\"plasma\" path=\"goproj/src/github.com/couchbase/plasma\" remote=\"couchbase-priv\" revision=\"4aa86645ce4b4673de08f6829b446b9c00cd3f3d\" upstream=\"mad-hatter\" />",
 "  <project groups=\"kv\" name=\"platform\" revision=\"bec44f963f3c4d73d3735380a8107b7292558749\" upstream=\"mad-hatter\" />",
 "  <project groups=\"kv\" name=\"product-texts\" revision=\"7a3aa547b3f5eb3ea28d279a08384609cd2cea7c\" upstream=\"master\" />",
 "  <project name=\"protobuf\" path=\"godeps/src/github.com/golang/protobuf\" remote=\"couchbasedeps\" revision=\"ddf22928ea3c56eb4292a0adbbf5001b1e8e7d0d\" />",
 "  <project name=\"query\" path=\"goproj/src/github.com/couchbase/query\" revision=\"a1708edce7216cdc4f21b4d4dd0eb4001d38e3c0\" upstream=\"mad-hatter\" />",
 "  <project groups=\"notdefault,enterprise\" name=\"query-ee\" path=\"goproj/src/github.com/couchbase/query-ee\" remote=\"couchbase-priv\" revision=\"3ef4ab89910a53b6acfaba4cc7d96091ab33a346\" upstream=\"mad-hatter\" />",
 "  <project name=\"query-ui\" revision=\"d736c5b2b97eeea0bf8170a40cfa7533e168388e\" upstream=\"master\" />",
 "  <project name=\"retriever\" path=\"godeps/src/github.com/couchbase/retriever\" revision=\"e3419088e4d3b4fe3aad3b364fdbe9a154f85f17\" upstream=\"master\" />",
 "  <project name=\"roaring\" path=\"godeps/src/github.com/RoaringBitmap/roaring\" remote=\"couchbasedeps\" revision=\"d0ce1763c3526f65703c395da50da7a7fb2138d5\" />",
 "  <project name=\"segment\" path=\"godeps/src/github.com/blevesearch/segment\" remote=\"blevesearch\" revision=\"762005e7a34fd909a84586299f1dd457371d36ee\" />",
 "  <project groups=\"kv\" name=\"sigar\" revision=\"c33791d6d5de19d6c5575aa33f8e5dba848414d8\" upstream=\"master\" />",
 "  <project name=\"snowballstem\" path=\"godeps/src/github.com/blevesearch/snowballstem\" remote=\"blevesearch\" revision=\"26b06a2c243d4f8ca5db3486f94409dd5b2a7467\" />",
 "  <project groups=\"kv\" name=\"spdlog\" path=\"third_party/spdlog\" remote=\"couchbasedeps\" revision=\"20967a170429d0d37e09a485bc3cf5b153554924\" upstream=\"v1.1.0-couchbase\" />",
 "  <project name=\"strconv\" path=\"godeps/src/github.com/tdewolff/strconv\" remote=\"couchbasedeps\" revision=\"9b189f5be77f33c46776f24dbddb2a7ab32af214\" />",
 "  <project groups=\"kv\" name=\"subjson\" revision=\"ae63ab4b653870e400855f8563da40dda49f0eb3\" upstream=\"master\" />",
 "  <project groups=\"backup\" name=\"sys\" path=\"godeps/src/golang.org/x/sys\" remote=\"couchbasedeps\" revision=\"7fbe1cd0fcc20051e1fcb87fbabec4a1bacaaeba\" />",
 "  <project name=\"testrunner\" revision=\"ee64d41320d14fabe814a241a5cf4f6a6f6e827a\" upstream=\"mad-hatter\" />",
 "  <project groups=\"backup\" name=\"text\" path=\"godeps/src/golang.org/x/text\" remote=\"couchbasedeps\" revision=\"88f656faf3f37f690df1a32515b479415e1a6769\" />",
 "  <project groups=\"kv\" name=\"tlm\" revision=\"7279de40e2a171aeed67b2566bd499d7157df965\">",
 "    <copyfile dest=\"GNUmakefile\" src=\"GNUmakefile\" />",
 "    <copyfile dest=\"Makefile\" src=\"Makefile\" />",
 "    <copyfile dest=\"CMakeLists.txt\" src=\"CMakeLists.txt\" />",
 "    <copyfile dest=\".clang-format\" src=\"dot-clang-format\" />",
 "    <copyfile dest=\"third_party/CMakeLists.txt\" src=\"third-party-CMakeLists.txt\" />",
 "  </project>",
 "  <project groups=\"backup\" name=\"ts\" path=\"godeps/src/github.com/olekukonko/ts\" remote=\"couchbasedeps\" revision=\"ecf753e7c962639ab5a1fb46f7da627d4c0a04b8\" />",
 "  <project groups=\"backup\" name=\"uuid\" path=\"godeps/src/github.com/google/uuid\" remote=\"couchbasedeps\" revision=\"dec09d789f3dba190787f8b4454c7d3c936fed9e\" />",
 "  <project name=\"vellum\" path=\"godeps/src/github.com/couchbase/vellum\" revision=\"ef2e028c01fdb60c46da4067d2e83745b8d54120\" upstream=\"master\" />",
 "  <project groups=\"notdefault,packaging\" name=\"voltron\" remote=\"couchbase-priv\" revision=\"45188488712448a326c8efad0d8c7b00e8afbefe\" upstream=\"master\" />",
 "  <project name=\"zstd\" path=\"godeps/src/github.com/DataDog/zstd\" remote=\"couchbasedeps\" revision=\"aebefd9fcb99f22cd691ef778a12ed68f0e6a1ab\" />",
 "</manifest>"]

[error_logger:info,2020-03-27T19:45:32.453Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.187.0>},
                       {id,timeout_diag_logger},
                       {mfargs,{timeout_diag_logger,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T19:45:32.455Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.188.0>},
                       {id,ns_cookie_manager},
                       {mfargs,{ns_cookie_manager,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T19:45:32.456Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.189.0>},
                       {id,ns_cluster},
                       {mfargs,{ns_cluster,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[ns_server:info,2020-03-27T19:45:32.458Z,ns_1@cb.local:ns_config_sup<0.190.0>:ns_config_sup:init:32]loading static ns_config from "/opt/couchbase/etc/couchbase/config"
[error_logger:info,2020-03-27T19:45:32.458Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_config_sup}
             started: [{pid,<0.191.0>},
                       {id,ns_config_events},
                       {mfargs,
                           {gen_event,start_link,[{local,ns_config_events}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T19:45:32.458Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_config_sup}
             started: [{pid,<0.192.0>},
                       {id,ns_config_events_local},
                       {mfargs,
                           {gen_event,start_link,
                               [{local,ns_config_events_local}]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[ns_server:info,2020-03-27T19:45:32.512Z,ns_1@cb.local:ns_config<0.193.0>:ns_config:load_config:1106]Loading static config from "/opt/couchbase/etc/couchbase/config"
[ns_server:info,2020-03-27T19:45:32.513Z,ns_1@cb.local:ns_config<0.193.0>:ns_config:load_config:1120]Loading dynamic config from "/opt/couchbase/var/lib/couchbase/config/config.dat"
[ns_server:info,2020-03-27T19:45:32.513Z,ns_1@cb.local:ns_config<0.193.0>:ns_config:load_config:1125]No dynamic config file found. Assuming we're brand new node
[ns_server:debug,2020-03-27T19:45:32.518Z,ns_1@cb.local:ns_config<0.193.0>:ns_config:load_config:1128]Here's full dynamic config we loaded:
[[]]
[ns_server:info,2020-03-27T19:45:32.522Z,ns_1@cb.local:ns_config<0.193.0>:ns_config:load_config:1149]Here's full dynamic config we loaded + static & default config:
[{{node,'ns_1@cb.local',{project_intact,is_vulnerable}},
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557532}}]}|
   false]},
 {{node,'ns_1@cb.local',cbas_debug_port},
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557532}}]}|-1]},
 {{node,'ns_1@cb.local',cbas_parent_port},
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557532}}]}|
   9122]},
 {{node,'ns_1@cb.local',cbas_metadata_port},
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557532}}]}|
   9121]},
 {{node,'ns_1@cb.local',cbas_replication_port},
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557532}}]}|
   9120]},
 {{node,'ns_1@cb.local',cbas_metadata_callback_port},
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557532}}]}|
   9119]},
 {{node,'ns_1@cb.local',cbas_messaging_port},
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557532}}]}|
   9118]},
 {{node,'ns_1@cb.local',cbas_result_port},
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557532}}]}|
   9117]},
 {{node,'ns_1@cb.local',cbas_data_port},
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557532}}]}|
   9116]},
 {{node,'ns_1@cb.local',cbas_cluster_port},
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557532}}]}|
   9115]},
 {{node,'ns_1@cb.local',cbas_console_port},
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557532}}]}|
   9114]},
 {{node,'ns_1@cb.local',cbas_cc_client_port},
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557532}}]}|
   9113]},
 {{node,'ns_1@cb.local',cbas_cc_cluster_port},
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557532}}]}|
   9112]},
 {{node,'ns_1@cb.local',cbas_cc_http_port},
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557532}}]}|
   9111]},
 {{node,'ns_1@cb.local',cbas_admin_port},
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557532}}]}|
   9110]},
 {{node,'ns_1@cb.local',cbas_ssl_port},
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557532}}]}|
   18095]},
 {{node,'ns_1@cb.local',cbas_http_port},
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557532}}]}|
   8095]},
 {{node,'ns_1@cb.local',eventing_https_port},
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557532}}]}|
   18096]},
 {{node,'ns_1@cb.local',eventing_debug_port},
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557532}}]}|
   9140]},
 {{node,'ns_1@cb.local',eventing_http_port},
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557532}}]}|
   8096]},
 {{node,'ns_1@cb.local',fts_grpc_ssl_port},
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557532}}]}|
   19130]},
 {{node,'ns_1@cb.local',fts_grpc_port},
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557532}}]}|
   9130]},
 {{node,'ns_1@cb.local',fts_ssl_port},
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557532}}]}|
   18094]},
 {{node,'ns_1@cb.local',fts_http_port},
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557532}}]}|
   8094]},
 {{node,'ns_1@cb.local',indexer_https_port},
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557532}}]}|
   19102]},
 {{node,'ns_1@cb.local',indexer_stmaint_port},
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557532}}]}|
   9105]},
 {{node,'ns_1@cb.local',indexer_stcatchup_port},
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557532}}]}|
   9104]},
 {{node,'ns_1@cb.local',indexer_stinit_port},
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557532}}]}|
   9103]},
 {{node,'ns_1@cb.local',indexer_http_port},
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557532}}]}|
   9102]},
 {{node,'ns_1@cb.local',indexer_scan_port},
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557532}}]}|
   9101]},
 {{node,'ns_1@cb.local',indexer_admin_port},
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557532}}]}|
   9100]},
 {{node,'ns_1@cb.local',ssl_query_port},
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557532}}]}|
   18093]},
 {{node,'ns_1@cb.local',query_port},
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557532}}]}|
   8093]},
 {{node,'ns_1@cb.local',projector_ssl_port},
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557532}}]}|
   9999]},
 {{node,'ns_1@cb.local',projector_port},
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557532}}]}|
   9999]},
 {{node,'ns_1@cb.local',ssl_capi_port},
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557532}}]}|
   18092]},
 {{node,'ns_1@cb.local',capi_port},
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557532}}]}|
   8092]},
 {{node,'ns_1@cb.local',memcached_dedicated_ssl_port},
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557532}}]}|
   11206]},
 {{node,'ns_1@cb.local',xdcr_rest_port},
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557532}}]}|
   9998]},
 {{node,'ns_1@cb.local',ssl_rest_port},
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557532}}]}|
   18091]},
 {{node,'ns_1@cb.local',rest},
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557532}}]},
   {port,8091},
   {port_meta,global}]},
 {rest,[{port,8091}]},
 {password_policy,[{min_length,6},{must_present,[]}]},
 {drop_request_memory_threshold_mib,undefined},
 {{request_limit,capi},undefined},
 {{request_limit,rest},undefined},
 {auto_reprovision_cfg,[{enabled,true},{max_nodes,1},{count,0}]},
 {auto_failover_cfg,[{enabled,true},{timeout,120},{max_nodes,1},{count,0}]},
 {log_redaction_default_cfg,[{redact_level,none}]},
 {replication,[{enabled,true}]},
 {alert_limits,
  [{max_overhead_perc,50},{max_disk_used,90},{max_indexer_ram,75}]},
 {email_alerts,
  [{recipients,["root@localhost"]},
   {sender,"couchbase@localhost"},
   {enabled,false},
   {email_server,
    [{user,[]},{pass,"*****"},{host,"localhost"},{port,25},{encrypt,false}]},
   {alerts,
    [auto_failover_node,auto_failover_maximum_reached,
     auto_failover_other_nodes_down,auto_failover_cluster_too_small,
     auto_failover_disabled,ip,disk,overhead,ep_oom_errors,
     ep_item_commit_failed,audit_dropped_events,indexer_ram_max_usage,
     ep_clock_cas_drift_threshold_exceeded,communication_issue]}]},
 {{node,'ns_1@cb.local',ns_log},
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557532}}]},
   {filename,"/opt/couchbase/var/lib/couchbase/ns_log"}]},
 {{node,'ns_1@cb.local',port_servers},
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557532}}]}]},
 {{node,'ns_1@cb.local',moxi},
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557532}}]},
   {port,0}]},
 {secure_headers,[]},
 {buckets,[{configs,[]}]},
 {cbas_memory_quota,1024},
 {fts_memory_quota,256},
 {memory_quota,292},
 {{node,'ns_1@cb.local',memcached_config},
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557532}}]}|
   {[{interfaces,
      {memcached_config_mgr,omit_missing_mcd_ports,
       [{[{host,<<"*">>},
          {port,port},
          {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
          {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]},
        {[{host,<<"*">>},
          {port,dedicated_port},
          {system,true},
          {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
          {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]},
        {[{host,<<"*">>},
          {port,ssl_port},
          {ssl,
           {[{key,
              <<"/opt/couchbase/var/lib/couchbase/config/memcached-key.pem">>},
             {cert,
              <<"/opt/couchbase/var/lib/couchbase/config/memcached-cert.pem">>}]}},
          {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
          {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]},
        {[{host,<<"*">>},
          {port,dedicated_ssl_port},
          {system,true},
          {ssl,
           {[{key,
              <<"/opt/couchbase/var/lib/couchbase/config/memcached-key.pem">>},
             {cert,
              <<"/opt/couchbase/var/lib/couchbase/config/memcached-cert.pem">>}]}},
          {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
          {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]}]}},
     {ssl_cipher_list,{memcached_config_mgr,get_ssl_cipher_list,[]}},
     {ssl_cipher_order,{memcached_config_mgr,get_ssl_cipher_order,[]}},
     {client_cert_auth,{memcached_config_mgr,client_cert_auth,[]}},
     {ssl_minimum_protocol,{memcached_config_mgr,ssl_minimum_protocol,[]}},
     {connection_idle_time,connection_idle_time},
     {privilege_debug,privilege_debug},
     {breakpad,
      {[{enabled,breakpad_enabled},
        {minidump_dir,{memcached_config_mgr,get_minidump_dir,[]}}]}},
     {opentracing,
      {[{enabled,opentracing_enabled},
        {module,{"~s",[opentracing_module]}},
        {config,{"~s",[opentracing_config]}}]}},
     {admin,{"~s",[admin_user]}},
     {verbosity,verbosity},
     {audit_file,{"~s",[audit_file]}},
     {rbac_file,{"~s",[rbac_file]}},
     {dedupe_nmvb_maps,dedupe_nmvb_maps},
     {tracing_enabled,tracing_enabled},
     {datatype_snappy,{memcached_config_mgr,is_snappy_enabled,[]}},
     {xattr_enabled,true},
     {scramsha_fallback_salt,{memcached_config_mgr,get_fallback_salt,[]}},
     {collections_enabled,{memcached_config_mgr,collections_enabled,[]}},
     {max_connections,max_connections},
     {system_connections,system_connections},
     {num_reader_threads,num_reader_threads},
     {num_writer_threads,num_writer_threads},
     {logger,
      {[{filename,{"~s/~s",[log_path,log_prefix]}},
        {cyclesize,log_cyclesize},
        {sleeptime,log_sleeptime}]}},
     {external_auth_service,
      {memcached_config_mgr,get_external_auth_service,[]}},
     {active_external_users_push_interval,
      {memcached_config_mgr,get_external_users_push_interval,[]}}]}]},
 {{node,'ns_1@cb.local',memcached},
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557532}}]},
   {port,11210},
   {dedicated_port,11209},
   {dedicated_ssl_port,11206},
   {ssl_port,11207},
   {admin_user,"@ns_server"},
   {other_users,
    ["@cbq-engine","@projector","@goxdcr","@index","@fts","@eventing",
     "@cbas"]},
   {admin_pass,"*****"},
   {engines,
    [{membase,
      [{engine,"/opt/couchbase/lib/memcached/ep.so"},
       {static_config_string,"failpartialwarmup=false"}]},
     {memcached,
      [{engine,"/opt/couchbase/lib/memcached/default_engine.so"},
       {static_config_string,"vb0=true"}]}]},
   {config_path,"/opt/couchbase/var/lib/couchbase/config/memcached.json"},
   {audit_file,"/opt/couchbase/var/lib/couchbase/config/audit.json"},
   {rbac_file,"/opt/couchbase/var/lib/couchbase/config/memcached.rbac"},
   {log_path,"/opt/couchbase/var/lib/couchbase/logs"},
   {log_prefix,"memcached.log"},
   {log_generations,20},
   {log_cyclesize,10485760},
   {log_sleeptime,19},
   {log_rotation_period,39003}]},
 {{node,'ns_1@cb.local',memcached_defaults},
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557532}}]},
   {max_connections,65000},
   {system_connections,5000},
   {connection_idle_time,0},
   {verbosity,0},
   {privilege_debug,false},
   {opentracing_enabled,false},
   {opentracing_module,[]},
   {opentracing_config,[]},
   {breakpad_enabled,true},
   {breakpad_minidump_dir_path,"/opt/couchbase/var/lib/couchbase/crash"},
   {dedupe_nmvb_maps,false},
   {tracing_enabled,true},
   {datatype_snappy,true},
   {num_reader_threads,<<"default">>},
   {num_writer_threads,<<"default">>}]},
 {memcached,[]},
 {{node,'ns_1@cb.local',audit},
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557532}}]}]},
 {audit,
  [{auditd_enabled,false},
   {rotate_interval,86400},
   {rotate_size,20971520},
   {disabled,[]},
   {sync,[]},
   {log_path,"/opt/couchbase/var/lib/couchbase/logs"}]},
 {{node,'ns_1@cb.local',isasl},
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557532}}]},
   {path,"/opt/couchbase/var/lib/couchbase/isasl.pw"}]},
 {remote_clusters,[]},
 {rest_creds,null},
 {{metakv,<<"/indexing/settings/config">>},
  <<"{\"indexer.settings.compaction.days_of_week\":\"Sunday,Monday,Tuesday,Wednesday,Thursday,Friday,Saturday\",\"indexer.settings.compaction.interval\":\"00:00,00:00\",\"indexer.settings.compaction.compaction_mode\":\"circular\",\"indexer.settings.log_level\":\"info\",\"indexer.settings.persisted_snapshot.interval\":5000,\"indexer.settings.compaction.min_frag\":30,\"indexer.settings.inmemory_snapshot.interval\":200,\"indexer.settings.max_cpu_percent\":0,\"indexer.settings.storage_mode\":\"\",\"indexer.settings.recovery.max_rollbacks\":2,\"indexer.settings.memory_quota\":536870912,\"indexer.settings.compaction.abort_exceed_interval\":false}">>},
 {{couchdb,max_parallel_replica_indexers},2},
 {{couchdb,max_parallel_indexers},4},
 {{node,'ns_1@cb.local',membership},
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557532}}]}|
   active]},
 {server_groups,
  [[{uuid,<<"0">>},{name,<<"Group 1">>},{nodes,['ns_1@cb.local']}]]},
 {quorum_nodes,['ns_1@cb.local']},
 {nodes_wanted,['ns_1@cb.local']},
 {{node,'ns_1@cb.local',compaction_daemon},
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557532}}]},
   {check_interval,30},
   {min_db_file_size,131072},
   {min_view_file_size,20971520}]},
 {set_view_update_daemon,
  [{update_interval,5000},
   {update_min_changes,5000},
   {replica_update_min_changes,5000}]},
 {autocompaction,
  [{database_fragmentation_threshold,{30,undefined}},
   {view_fragmentation_threshold,{30,undefined}}]},
 {max_bucket_count,30},
 {index_aware_rebalance_disabled,false},
 {{node,'ns_1@cb.local',saslauthd_enabled},
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557532}}]}|
   true]},
 {{node,'ns_1@cb.local',is_enterprise},
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557532}}]}|
   true]},
 {{node,'ns_1@cb.local',config_version},
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557532}}]}|
   {6,5}]},
 {{node,'ns_1@cb.local',uuid},
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557532}}]}|
   <<"60a6bc3db77e6c7b91c556140dcfec71">>]}]
[error_logger:info,2020-03-27T19:45:32.529Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_config_sup}
             started: [{pid,<0.193.0>},
                       {id,ns_config},
                       {mfargs,
                           {ns_config,start_link,
                               ["/opt/couchbase/etc/couchbase/config",
                                ns_config_default]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T19:45:32.530Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_config_sup}
             started: [{pid,<0.199.0>},
                       {id,ns_config_remote},
                       {mfargs,{ns_config_replica,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T19:45:32.531Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_config_sup}
             started: [{pid,<0.200.0>},
                       {id,ns_config_log},
                       {mfargs,{ns_config_log,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T19:45:32.532Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.190.0>},
                       {id,ns_config_sup},
                       {mfargs,{ns_config_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[ns_server:debug,2020-03-27T19:45:32.534Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',erl_external_listeners} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557532}}]},
 {inet,false},
 {inet6,false}]
[error_logger:info,2020-03-27T19:45:32.534Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.202.0>},
                       {id,netconfig_updater},
                       {mfargs,{netconfig_updater,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[ns_server:debug,2020-03-27T19:45:32.534Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',node_encryption} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557532}}]}|false]
[ns_server:debug,2020-03-27T19:45:32.534Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',address_family} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557532}}]}|inet]
[ns_server:debug,2020-03-27T19:45:32.534Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{local_changes_count,<<"60a6bc3db77e6c7b91c556140dcfec71">>} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557532}}]}]
[error_logger:info,2020-03-27T19:45:32.538Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.205.0>},
                       {id,json_rpc_connection_sup},
                       {mfargs,{json_rpc_connection_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-03-27T19:45:32.553Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.208.0>},
                       {name,remote_monitors},
                       {mfargs,{remote_monitors,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-03-27T19:45:32.561Z,ns_1@cb.local:menelaus_barrier<0.209.0>:one_shot_barrier:barrier_body:58]Barrier menelaus_barrier has started
[error_logger:info,2020-03-27T19:45:32.561Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.209.0>},
                       {name,menelaus_barrier},
                       {mfargs,{menelaus_sup,barrier_start_link,[]}},
                       {restart_type,temporary},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T19:45:32.561Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.210.0>},
                       {name,rest_lhttpc_pool},
                       {mfargs,
                           {lhttpc_manager,start_link,
                               [[{name,rest_lhttpc_pool},
                                 {connection_timeout,120000},
                                 {pool_size,20}]]}},
                       {restart_type,{permanent,1}},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T19:45:32.567Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.211.0>},
                       {name,memcached_refresh},
                       {mfargs,{memcached_refresh,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T19:45:32.568Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_ssl_services_sup}
             started: [{pid,<0.213.0>},
                       {id,ssl_service_events},
                       {mfargs,
                           {gen_event,start_link,
                               [{local,ssl_service_events}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-03-27T19:45:32.791Z,ns_1@cb.local:<0.218.0>:goport:handle_eof:582]Stream 'stderr' closed
[ns_server:debug,2020-03-27T19:45:32.791Z,ns_1@cb.local:<0.218.0>:goport:handle_eof:582]Stream 'stdout' closed
[ns_server:info,2020-03-27T19:45:32.791Z,ns_1@cb.local:<0.218.0>:goport:handle_process_exit:563]Port exited with status 0.
[ns_server:debug,2020-03-27T19:45:32.811Z,ns_1@cb.local:ns_ssl_services_setup<0.214.0>:ns_server_cert:generate_cert_and_pkey:83]Generated certificate and private key in 241226 us
[ns_server:debug,2020-03-27T19:45:32.812Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
cert_and_pkey ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557532}}]}|
 {<<"-----BEGIN CERTIFICATE-----\nMIIDAjCCAeqgAwIBAgIIFgBA0G6s0lwwDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciAzNDhlZDI3MDAeFw0xMzAxMDEwMDAwMDBaFw00\nOTEyMzEyMzU5NTlaMCQxIjAgBgNVBAMTGUNvdWNoYmFzZSBTZXJ2ZXIgMzQ4ZWQy\nNzAwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQDh0efJgvfpArvc3iI0\ncHa/zZhS9WoaoNHksFooYqy3AYiZR+8S3IgPS0nfcZGrIWJUPgokUqBqh6u59abc\nvCuyMwLNkkuLOkT6wzUztrAqoCoOEl4"...>>,
  <<"*****">>}]
[ns_server:debug,2020-03-27T19:45:32.812Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{local_changes_count,<<"60a6bc3db77e6c7b91c556140dcfec71">>} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557532}}]}]
[ns_server:info,2020-03-27T19:45:32.814Z,ns_1@cb.local:ns_ssl_services_setup<0.214.0>:ns_ssl_services_setup:maybe_generate_local_cert:620]Failed to read node certificate. Perhaps it wasn't created yet. Error: {error,
                                                                        {badmatch,
                                                                         {error,
                                                                          enoent}}}
[ns_server:debug,2020-03-27T19:45:32.957Z,ns_1@cb.local:<0.222.0>:goport:handle_eof:582]Stream 'stderr' closed
[ns_server:debug,2020-03-27T19:45:32.957Z,ns_1@cb.local:<0.222.0>:goport:handle_eof:582]Stream 'stdout' closed
[ns_server:info,2020-03-27T19:45:32.957Z,ns_1@cb.local:<0.222.0>:goport:handle_process_exit:563]Port exited with status 0.
[ns_server:info,2020-03-27T19:45:33.016Z,ns_1@cb.local:ns_ssl_services_setup<0.214.0>:ns_ssl_services_setup:do_generate_local_cert:608]Saved local cert for node 'ns_1@cb.local'
[ns_server:debug,2020-03-27T19:45:33.093Z,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Restarting tls distribution protocols (if any)
[ns_server:debug,2020-03-27T19:45:33.093Z,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: ignoring closing of inet6_tls_dist because listener is not started
[ns_server:debug,2020-03-27T19:45:33.093Z,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: ignoring closing of inet_tls_dist because listener is not started
[ns_server:info,2020-03-27T19:45:33.116Z,ns_1@cb.local:ns_ssl_services_setup<0.214.0>:ns_ssl_services_setup:init:462]Used ssl options:
[{keyfile,"/opt/couchbase/var/lib/couchbase/config/ssl-cert-key.pem"},
 {certfile,"/opt/couchbase/var/lib/couchbase/config/ssl-cert-key.pem"},
 {versions,['tlsv1.1','tlsv1.2']},
 {cacerts,[<<48,130,3,2,48,130,1,234,160,3,2,1,2,2,8,22,0,64,208,110,172,210,
             92,48,13,6,9,42,134,72,134,247,13,1,1,11,5,0,48,36,49,34,48,32,
             6,3,85,4,3,19,25,67,111,117,99,104,98,97,115,101,32,83,101,114,
             118,101,114,32,51,52,56,101,100,50,55,48,48,30,23,13,49,51,48,
             49,48,49,48,48,48,48,48,48,90,23,13,52,57,49,50,51,49,50,51,53,
             57,53,57,90,48,36,49,34,48,32,6,3,85,4,3,19,25,67,111,117,99,
             104,98,97,115,101,32,83,101,114,118,101,114,32,51,52,56,101,100,
             50,55,48,48,130,1,34,48,13,6,9,42,134,72,134,247,13,1,1,1,5,0,3,
             130,1,15,0,48,130,1,10,2,130,1,1,0,225,209,231,201,130,247,233,
             2,187,220,222,34,52,112,118,191,205,152,82,245,106,26,160,209,
             228,176,90,40,98,172,183,1,136,153,71,239,18,220,136,15,75,73,
             223,113,145,171,33,98,84,62,10,36,82,160,106,135,171,185,245,
             166,220,188,43,178,51,2,205,146,75,139,58,68,250,195,53,51,182,
             176,42,160,42,14,18,94,26,188,161,102,35,66,108,137,96,63,168,
             160,212,254,70,169,139,74,198,244,204,206,194,101,179,132,181,1,
             21,76,117,72,101,24,165,108,212,56,253,47,230,184,143,50,20,121,
             74,247,204,71,247,226,97,249,191,174,226,150,23,149,71,180,209,
             86,216,72,31,202,197,17,62,92,90,116,10,212,69,89,20,91,163,51,
             77,94,156,13,115,23,71,83,252,222,166,85,132,171,214,167,174,78,
             60,63,78,241,115,199,70,186,175,191,94,208,5,216,173,106,248,
             203,56,72,24,219,23,86,67,137,18,221,171,208,1,41,239,84,253,
             158,230,36,78,222,33,120,90,96,209,36,5,230,90,163,19,2,35,165,
             80,128,171,6,90,11,237,162,35,3,201,94,110,201,181,13,49,165,2,
             3,1,0,1,163,56,48,54,48,14,6,3,85,29,15,1,1,255,4,4,3,2,2,164,
             48,19,6,3,85,29,37,4,12,48,10,6,8,43,6,1,5,5,7,3,1,48,15,6,3,85,
             29,19,1,1,255,4,5,48,3,1,1,255,48,13,6,9,42,134,72,134,247,13,1,
             1,11,5,0,3,130,1,1,0,6,81,65,124,71,43,122,15,167,207,16,251,0,
             159,39,98,188,227,29,215,100,107,194,184,208,49,234,49,157,22,
             16,114,226,174,13,150,172,144,2,238,230,8,40,221,26,55,56,132,
             126,233,153,24,185,41,108,27,239,216,82,120,63,12,251,133,139,
             191,7,65,245,131,92,34,201,230,196,53,62,156,158,117,62,220,159,
             192,101,171,131,143,145,151,43,167,227,90,161,173,77,152,27,124,
             67,49,150,67,102,161,221,43,166,252,34,106,208,76,223,73,230,
             216,161,26,132,137,129,146,79,142,69,179,193,194,47,129,207,251,
             4,237,180,247,54,155,40,216,36,79,59,110,188,27,153,208,45,211,
             245,157,64,19,243,225,128,45,126,172,97,164,65,55,250,227,90,
             212,59,153,137,219,195,84,171,109,145,64,111,253,109,120,35,199,
             153,232,120,170,179,167,110,176,116,10,245,79,6,66,158,148,137,
             191,125,65,164,153,94,180,226,83,230,241,236,39,67,40,12,91,44,
             79,54,65,72,249,75,235,222,205,142,112,55,224,47,125,42,235,188,
             14,128,134,44,228,25,163,25,90,246,170,74,163,117,63,44,151>>]},
 {dh,<<48,130,1,8,2,130,1,1,0,152,202,99,248,92,201,35,238,246,5,77,93,120,10,
       118,129,36,52,111,193,167,220,49,229,106,105,152,133,121,157,73,158,
       232,153,197,197,21,171,140,30,207,52,165,45,8,221,162,21,199,183,66,
       211,247,51,224,102,214,190,130,96,253,218,193,35,43,139,145,89,200,250,
       145,92,50,80,134,135,188,205,254,148,122,136,237,220,186,147,187,104,
       159,36,147,217,117,74,35,163,145,249,175,242,18,221,124,54,140,16,246,
       169,84,252,45,47,99,136,30,60,189,203,61,86,225,117,255,4,91,46,110,
       167,173,106,51,65,10,248,94,225,223,73,40,232,140,26,11,67,170,118,190,
       67,31,127,233,39,68,88,132,171,224,62,187,207,160,189,209,101,74,8,205,
       174,146,173,80,105,144,246,25,153,86,36,24,178,163,64,202,221,95,184,
       110,244,32,226,217,34,55,188,230,55,16,216,247,173,246,139,76,187,66,
       211,159,17,46,20,18,48,80,27,250,96,189,29,214,234,241,34,69,254,147,
       103,220,133,40,164,84,8,44,241,61,164,151,9,135,41,60,75,4,202,133,173,
       72,6,69,167,89,112,174,40,229,171,2,1,2>>},
 {ciphers,[{ecdhe_ecdsa,aes_256_gcm,aead,sha384},
           {ecdhe_rsa,aes_256_gcm,aead,sha384},
           {ecdhe_ecdsa,aes_256_cbc,sha384,sha384},
           {ecdhe_rsa,aes_256_cbc,sha384,sha384},
           {ecdh_ecdsa,aes_256_gcm,aead,sha384},
           {ecdh_rsa,aes_256_gcm,aead,sha384},
           {ecdh_ecdsa,aes_256_cbc,sha384,sha384},
           {ecdh_rsa,aes_256_cbc,sha384,sha384},
           {ecdhe_ecdsa,chacha20_poly1305,aead,sha256},
           {ecdhe_rsa,chacha20_poly1305,aead,sha256},
           {dhe_rsa,chacha20_poly1305,aead,sha256},
           {dhe_rsa,aes_256_gcm,aead,sha384},
           {dhe_dss,aes_256_gcm,aead,sha384},
           {dhe_rsa,aes_256_cbc,sha256},
           {dhe_dss,aes_256_cbc,sha256},
           {rsa,aes_256_gcm,aead,sha384},
           {rsa,aes_256_cbc,sha256},
           {ecdhe_ecdsa,aes_128_gcm,aead,sha256},
           {ecdhe_rsa,aes_128_gcm,aead,sha256},
           {ecdhe_ecdsa,aes_128_cbc,sha256,sha256},
           {ecdhe_rsa,aes_128_cbc,sha256,sha256},
           {ecdh_ecdsa,aes_128_gcm,aead,sha256},
           {ecdh_rsa,aes_128_gcm,aead,sha256},
           {ecdh_ecdsa,aes_128_cbc,sha256,sha256},
           {ecdh_rsa,aes_128_cbc,sha256,sha256},
           {dhe_rsa,aes_128_gcm,aead,sha256},
           {dhe_dss,aes_128_gcm,aead,sha256},
           {dhe_rsa,aes_128_cbc,sha256},
           {dhe_dss,aes_128_cbc,sha256},
           {rsa,aes_128_gcm,aead,sha256},
           {rsa,aes_128_cbc,sha256},
           {ecdhe_ecdsa,aes_256_cbc,sha},
           {ecdhe_rsa,aes_256_cbc,sha},
           {dhe_rsa,aes_256_cbc,sha},
           {dhe_dss,aes_256_cbc,sha},
           {ecdh_ecdsa,aes_256_cbc,sha},
           {ecdh_rsa,aes_256_cbc,sha},
           {rsa,aes_256_cbc,sha},
           {ecdhe_ecdsa,aes_128_cbc,sha},
           {ecdhe_rsa,aes_128_cbc,sha},
           {dhe_rsa,aes_128_cbc,sha},
           {dhe_dss,aes_128_cbc,sha},
           {ecdh_ecdsa,aes_128_cbc,sha},
           {ecdh_rsa,aes_128_cbc,sha},
           {rsa,aes_128_cbc,sha},
           {ecdhe_ecdsa,'3des_ede_cbc',sha},
           {ecdhe_rsa,'3des_ede_cbc',sha},
           {dhe_rsa,'3des_ede_cbc',sha},
           {dhe_dss,'3des_ede_cbc',sha},
           {ecdh_ecdsa,'3des_ede_cbc',sha},
           {ecdh_rsa,'3des_ede_cbc',sha},
           {rsa,'3des_ede_cbc',sha}]},
 {honor_cipher_order,true},
 {secure_renegotiate,true},
 {client_renegotiation,false}]
[error_logger:info,2020-03-27T19:45:33.118Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_ssl_services_sup}
             started: [{pid,<0.214.0>},
                       {id,ns_ssl_services_setup},
                       {mfargs,{ns_ssl_services_setup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:info,2020-03-27T19:45:33.135Z,ns_1@cb.local:<0.224.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for cbas
[ns_server:info,2020-03-27T19:45:33.135Z,ns_1@cb.local:<0.224.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for eventing
[ns_server:info,2020-03-27T19:45:33.135Z,ns_1@cb.local:<0.224.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for fts
[ns_server:info,2020-03-27T19:45:33.135Z,ns_1@cb.local:<0.224.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for n1ql
[error_logger:info,2020-03-27T19:45:33.156Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {<0.224.0>,menelaus_web}
             started: [{pid,<0.225.0>},
                       {id,menelaus_web_ipv4},
                       {mfargs,
                        {menelaus_web,http_server,
                         [[{ip,"0.0.0.0"},
                           {name,menelaus_web_ssl_ipv4},
                           {ssl,true},
                           {ssl_opts,
                            [{keyfile,
                              "/opt/couchbase/var/lib/couchbase/config/ssl-cert-key.pem"},
                             {certfile,
                              "/opt/couchbase/var/lib/couchbase/config/ssl-cert-key.pem"},
                             {versions,['tlsv1.1','tlsv1.2']},
                             {cacerts,
                              [<<48,130,3,2,48,130,1,234,160,3,2,1,2,2,8,22,
                                 0,64,208,110,172,210,92,48,13,6,9,42,134,72,
                                 134,247,13,1,1,11,5,0,48,36,49,34,48,32,6,3,
                                 85,4,3,19,25,67,111,117,99,104,98,97,115,
                                 101,32,83,101,114,118,101,114,32,51,52,56,
                                 101,100,50,55,48,48,30,23,13,49,51,48,49,48,
                                 49,48,48,48,48,48,48,90,23,13,52,57,49,50,
                                 51,49,50,51,53,57,53,57,90,48,36,49,34,48,
                                 32,6,3,85,4,3,19,25,67,111,117,99,104,98,97,
                                 115,101,32,83,101,114,118,101,114,32,51,52,
                                 56,101,100,50,55,48,48,130,1,34,48,13,6,9,
                                 42,134,72,134,247,13,1,1,1,5,0,3,130,1,15,0,
                                 48,130,1,10,2,130,1,1,0,225,209,231,201,130,
                                 247,233,2,187,220,222,34,52,112,118,191,205,
                                 152,82,245,106,26,160,209,228,176,90,40,98,
                                 172,183,1,136,153,71,239,18,220,136,15,75,
                                 73,223,113,145,171,33,98,84,62,10,36,82,160,
                                 106,135,171,185,245,166,220,188,43,178,51,2,
                                 205,146,75,139,58,68,250,195,53,51,182,176,
                                 42,160,42,14,18,94,26,188,161,102,35,66,108,
                                 137,96,63,168,160,212,254,70,169,139,74,198,
                                 244,204,206,194,101,179,132,181,1,21,76,117,
                                 72,101,24,165,108,212,56,253,47,230,184,143,
                                 50,20,121,74,247,204,71,247,226,97,249,191,
                                 174,226,150,23,149,71,180,209,86,216,72,31,
                                 202,197,17,62,92,90,116,10,212,69,89,20,91,
                                 163,51,77,94,156,13,115,23,71,83,252,222,
                                 166,85,132,171,214,167,174,78,60,63,78,241,
                                 115,199,70,186,175,191,94,208,5,216,173,106,
                                 248,203,56,72,24,219,23,86,67,137,18,221,
                                 171,208,1,41,239,84,253,158,230,36,78,222,
                                 33,120,90,96,209,36,5,230,90,163,19,2,35,
                                 165,80,128,171,6,90,11,237,162,35,3,201,94,
                                 110,201,181,13,49,165,2,3,1,0,1,163,56,48,
                                 54,48,14,6,3,85,29,15,1,1,255,4,4,3,2,2,164,
                                 48,19,6,3,85,29,37,4,12,48,10,6,8,43,6,1,5,
                                 5,7,3,1,48,15,6,3,85,29,19,1,1,255,4,5,48,3,
                                 1,1,255,48,13,6,9,42,134,72,134,247,13,1,1,
                                 11,5,0,3,130,1,1,0,6,81,65,124,71,43,122,15,
                                 167,207,16,251,0,159,39,98,188,227,29,215,
                                 100,107,194,184,208,49,234,49,157,22,16,114,
                                 226,174,13,150,172,144,2,238,230,8,40,221,
                                 26,55,56,132,126,233,153,24,185,41,108,27,
                                 239,216,82,120,63,12,251,133,139,191,7,65,
                                 245,131,92,34,201,230,196,53,62,156,158,117,
                                 62,220,159,192,101,171,131,143,145,151,43,
                                 167,227,90,161,173,77,152,27,124,67,49,150,
                                 67,102,161,221,43,166,252,34,106,208,76,223,
                                 73,230,216,161,26,132,137,129,146,79,142,69,
                                 179,193,194,47,129,207,251,4,237,180,247,54,
                                 155,40,216,36,79,59,110,188,27,153,208,45,
                                 211,245,157,64,19,243,225,128,45,126,172,97,
                                 164,65,55,250,227,90,212,59,153,137,219,195,
                                 84,171,109,145,64,111,253,109,120,35,199,
                                 153,232,120,170,179,167,110,176,116,10,245,
                                 79,6,66,158,148,137,191,125,65,164,153,94,
                                 180,226,83,230,241,236,39,67,40,12,91,44,79,
                                 54,65,72,249,75,235,222,205,142,112,55,224,
                                 47,125,42,235,188,14,128,134,44,228,25,163,
                                 25,90,246,170,74,163,117,63,44,151>>]},
                             {dh,
                              <<48,130,1,8,2,130,1,1,0,152,202,99,248,92,201,
                                35,238,246,5,77,93,120,10,118,129,36,52,111,
                                193,167,220,49,229,106,105,152,133,121,157,73,
                                158,232,153,197,197,21,171,140,30,207,52,165,
                                45,8,221,162,21,199,183,66,211,247,51,224,102,
                                214,190,130,96,253,218,193,35,43,139,145,89,
                                200,250,145,92,50,80,134,135,188,205,254,148,
                                122,136,237,220,186,147,187,104,159,36,147,
                                217,117,74,35,163,145,249,175,242,18,221,124,
                                54,140,16,246,169,84,252,45,47,99,136,30,60,
                                189,203,61,86,225,117,255,4,91,46,110,167,173,
                                106,51,65,10,248,94,225,223,73,40,232,140,26,
                                11,67,170,118,190,67,31,127,233,39,68,88,132,
                                171,224,62,187,207,160,189,209,101,74,8,205,
                                174,146,173,80,105,144,246,25,153,86,36,24,
                                178,163,64,202,221,95,184,110,244,32,226,217,
                                34,55,188,230,55,16,216,247,173,246,139,76,
                                187,66,211,159,17,46,20,18,48,80,27,250,96,
                                189,29,214,234,241,34,69,254,147,103,220,133,
                                40,164,84,8,44,241,61,164,151,9,135,41,60,75,
                                4,202,133,173,72,6,69,167,89,112,174,40,229,
                                171,2,1,2>>},
                             {ciphers,
                              [{ecdhe_ecdsa,aes_256_gcm,aead,sha384},
                               {ecdhe_rsa,aes_256_gcm,aead,sha384},
                               {ecdhe_ecdsa,aes_256_cbc,sha384,sha384},
                               {ecdhe_rsa,aes_256_cbc,sha384,sha384},
                               {ecdh_ecdsa,aes_256_gcm,aead,sha384},
                               {ecdh_rsa,aes_256_gcm,aead,sha384},
                               {ecdh_ecdsa,aes_256_cbc,sha384,sha384},
                               {ecdh_rsa,aes_256_cbc,sha384,sha384},
                               {ecdhe_ecdsa,chacha20_poly1305,aead,sha256},
                               {ecdhe_rsa,chacha20_poly1305,aead,sha256},
                               {dhe_rsa,chacha20_poly1305,aead,sha256},
                               {dhe_rsa,aes_256_gcm,aead,sha384},
                               {dhe_dss,aes_256_gcm,aead,sha384},
                               {dhe_rsa,aes_256_cbc,sha256},
                               {dhe_dss,aes_256_cbc,sha256},
                               {rsa,aes_256_gcm,aead,sha384},
                               {rsa,aes_256_cbc,sha256},
                               {ecdhe_ecdsa,aes_128_gcm,aead,sha256},
                               {ecdhe_rsa,aes_128_gcm,aead,sha256},
                               {ecdhe_ecdsa,aes_128_cbc,sha256,sha256},
                               {ecdhe_rsa,aes_128_cbc,sha256,sha256},
                               {ecdh_ecdsa,aes_128_gcm,aead,sha256},
                               {ecdh_rsa,aes_128_gcm,aead,sha256},
                               {ecdh_ecdsa,aes_128_cbc,sha256,sha256},
                               {ecdh_rsa,aes_128_cbc,sha256,sha256},
                               {dhe_rsa,aes_128_gcm,aead,sha256},
                               {dhe_dss,aes_128_gcm,aead,sha256},
                               {dhe_rsa,aes_128_cbc,sha256},
                               {dhe_dss,aes_128_cbc,sha256},
                               {rsa,aes_128_gcm,aead,sha256},
                               {rsa,aes_128_cbc,sha256},
                               {ecdhe_ecdsa,aes_256_cbc,sha},
                               {ecdhe_rsa,aes_256_cbc,sha},
                               {dhe_rsa,aes_256_cbc,sha},
                               {dhe_dss,aes_256_cbc,sha},
                               {ecdh_ecdsa,aes_256_cbc,sha},
                               {ecdh_rsa,aes_256_cbc,sha},
                               {rsa,aes_256_cbc,sha},
                               {ecdhe_ecdsa,aes_128_cbc,sha},
                               {ecdhe_rsa,aes_128_cbc,sha},
                               {dhe_rsa,aes_128_cbc,sha},
                               {dhe_dss,aes_128_cbc,sha},
                               {ecdh_ecdsa,aes_128_cbc,sha},
                               {ecdh_rsa,aes_128_cbc,sha},
                               {rsa,aes_128_cbc,sha},
                               {ecdhe_ecdsa,'3des_ede_cbc',sha},
                               {ecdhe_rsa,'3des_ede_cbc',sha},
                               {dhe_rsa,'3des_ede_cbc',sha},
                               {dhe_dss,'3des_ede_cbc',sha},
                               {ecdh_ecdsa,'3des_ede_cbc',sha},
                               {ecdh_rsa,'3des_ede_cbc',sha},
                               {rsa,'3des_ede_cbc',sha}]},
                             {honor_cipher_order,true},
                             {secure_renegotiate,true},
                             {client_renegotiation,false}]},
                           {port,18091}]]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[ns_server:info,2020-03-27T19:45:33.157Z,ns_1@cb.local:<0.224.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for cbas
[ns_server:info,2020-03-27T19:45:33.157Z,ns_1@cb.local:<0.224.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for eventing
[ns_server:info,2020-03-27T19:45:33.157Z,ns_1@cb.local:<0.224.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for fts
[ns_server:info,2020-03-27T19:45:33.157Z,ns_1@cb.local:<0.224.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for n1ql
[ns_server:debug,2020-03-27T19:45:33.158Z,ns_1@cb.local:<0.223.0>:restartable:start_child:98]Started child process <0.224.0>
  MFA: {ns_ssl_services_setup,start_link_rest_service,[]}
[error_logger:info,2020-03-27T19:45:33.158Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {<0.224.0>,menelaus_web}
             started: [{pid,<0.243.0>},
                       {id,menelaus_web_ipv6},
                       {mfargs,
                        {menelaus_web,http_server,
                         [[{ip,"::"},
                           {name,menelaus_web_ssl_ipv6},
                           {ssl,true},
                           {ssl_opts,
                            [{keyfile,
                              "/opt/couchbase/var/lib/couchbase/config/ssl-cert-key.pem"},
                             {certfile,
                              "/opt/couchbase/var/lib/couchbase/config/ssl-cert-key.pem"},
                             {versions,['tlsv1.1','tlsv1.2']},
                             {cacerts,
                              [<<48,130,3,2,48,130,1,234,160,3,2,1,2,2,8,22,
                                 0,64,208,110,172,210,92,48,13,6,9,42,134,72,
                                 134,247,13,1,1,11,5,0,48,36,49,34,48,32,6,3,
                                 85,4,3,19,25,67,111,117,99,104,98,97,115,
                                 101,32,83,101,114,118,101,114,32,51,52,56,
                                 101,100,50,55,48,48,30,23,13,49,51,48,49,48,
                                 49,48,48,48,48,48,48,90,23,13,52,57,49,50,
                                 51,49,50,51,53,57,53,57,90,48,36,49,34,48,
                                 32,6,3,85,4,3,19,25,67,111,117,99,104,98,97,
                                 115,101,32,83,101,114,118,101,114,32,51,52,
                                 56,101,100,50,55,48,48,130,1,34,48,13,6,9,
                                 42,134,72,134,247,13,1,1,1,5,0,3,130,1,15,0,
                                 48,130,1,10,2,130,1,1,0,225,209,231,201,130,
                                 247,233,2,187,220,222,34,52,112,118,191,205,
                                 152,82,245,106,26,160,209,228,176,90,40,98,
                                 172,183,1,136,153,71,239,18,220,136,15,75,
                                 73,223,113,145,171,33,98,84,62,10,36,82,160,
                                 106,135,171,185,245,166,220,188,43,178,51,2,
                                 205,146,75,139,58,68,250,195,53,51,182,176,
                                 42,160,42,14,18,94,26,188,161,102,35,66,108,
                                 137,96,63,168,160,212,254,70,169,139,74,198,
                                 244,204,206,194,101,179,132,181,1,21,76,117,
                                 72,101,24,165,108,212,56,253,47,230,184,143,
                                 50,20,121,74,247,204,71,247,226,97,249,191,
                                 174,226,150,23,149,71,180,209,86,216,72,31,
                                 202,197,17,62,92,90,116,10,212,69,89,20,91,
                                 163,51,77,94,156,13,115,23,71,83,252,222,
                                 166,85,132,171,214,167,174,78,60,63,78,241,
                                 115,199,70,186,175,191,94,208,5,216,173,106,
                                 248,203,56,72,24,219,23,86,67,137,18,221,
                                 171,208,1,41,239,84,253,158,230,36,78,222,
                                 33,120,90,96,209,36,5,230,90,163,19,2,35,
                                 165,80,128,171,6,90,11,237,162,35,3,201,94,
                                 110,201,181,13,49,165,2,3,1,0,1,163,56,48,
                                 54,48,14,6,3,85,29,15,1,1,255,4,4,3,2,2,164,
                                 48,19,6,3,85,29,37,4,12,48,10,6,8,43,6,1,5,
                                 5,7,3,1,48,15,6,3,85,29,19,1,1,255,4,5,48,3,
                                 1,1,255,48,13,6,9,42,134,72,134,247,13,1,1,
                                 11,5,0,3,130,1,1,0,6,81,65,124,71,43,122,15,
                                 167,207,16,251,0,159,39,98,188,227,29,215,
                                 100,107,194,184,208,49,234,49,157,22,16,114,
                                 226,174,13,150,172,144,2,238,230,8,40,221,
                                 26,55,56,132,126,233,153,24,185,41,108,27,
                                 239,216,82,120,63,12,251,133,139,191,7,65,
                                 245,131,92,34,201,230,196,53,62,156,158,117,
                                 62,220,159,192,101,171,131,143,145,151,43,
                                 167,227,90,161,173,77,152,27,124,67,49,150,
                                 67,102,161,221,43,166,252,34,106,208,76,223,
                                 73,230,216,161,26,132,137,129,146,79,142,69,
                                 179,193,194,47,129,207,251,4,237,180,247,54,
                                 155,40,216,36,79,59,110,188,27,153,208,45,
                                 211,245,157,64,19,243,225,128,45,126,172,97,
                                 164,65,55,250,227,90,212,59,153,137,219,195,
                                 84,171,109,145,64,111,253,109,120,35,199,
                                 153,232,120,170,179,167,110,176,116,10,245,
                                 79,6,66,158,148,137,191,125,65,164,153,94,
                                 180,226,83,230,241,236,39,67,40,12,91,44,79,
                                 54,65,72,249,75,235,222,205,142,112,55,224,
                                 47,125,42,235,188,14,128,134,44,228,25,163,
                                 25,90,246,170,74,163,117,63,44,151>>]},
                             {dh,
                              <<48,130,1,8,2,130,1,1,0,152,202,99,248,92,201,
                                35,238,246,5,77,93,120,10,118,129,36,52,111,
                                193,167,220,49,229,106,105,152,133,121,157,73,
                                158,232,153,197,197,21,171,140,30,207,52,165,
                                45,8,221,162,21,199,183,66,211,247,51,224,102,
                                214,190,130,96,253,218,193,35,43,139,145,89,
                                200,250,145,92,50,80,134,135,188,205,254,148,
                                122,136,237,220,186,147,187,104,159,36,147,
                                217,117,74,35,163,145,249,175,242,18,221,124,
                                54,140,16,246,169,84,252,45,47,99,136,30,60,
                                189,203,61,86,225,117,255,4,91,46,110,167,173,
                                106,51,65,10,248,94,225,223,73,40,232,140,26,
                                11,67,170,118,190,67,31,127,233,39,68,88,132,
                                171,224,62,187,207,160,189,209,101,74,8,205,
                                174,146,173,80,105,144,246,25,153,86,36,24,
                                178,163,64,202,221,95,184,110,244,32,226,217,
                                34,55,188,230,55,16,216,247,173,246,139,76,
                                187,66,211,159,17,46,20,18,48,80,27,250,96,
                                189,29,214,234,241,34,69,254,147,103,220,133,
                                40,164,84,8,44,241,61,164,151,9,135,41,60,75,
                                4,202,133,173,72,6,69,167,89,112,174,40,229,
                                171,2,1,2>>},
                             {ciphers,
                              [{ecdhe_ecdsa,aes_256_gcm,aead,sha384},
                               {ecdhe_rsa,aes_256_gcm,aead,sha384},
                               {ecdhe_ecdsa,aes_256_cbc,sha384,sha384},
                               {ecdhe_rsa,aes_256_cbc,sha384,sha384},
                               {ecdh_ecdsa,aes_256_gcm,aead,sha384},
                               {ecdh_rsa,aes_256_gcm,aead,sha384},
                               {ecdh_ecdsa,aes_256_cbc,sha384,sha384},
                               {ecdh_rsa,aes_256_cbc,sha384,sha384},
                               {ecdhe_ecdsa,chacha20_poly1305,aead,sha256},
                               {ecdhe_rsa,chacha20_poly1305,aead,sha256},
                               {dhe_rsa,chacha20_poly1305,aead,sha256},
                               {dhe_rsa,aes_256_gcm,aead,sha384},
                               {dhe_dss,aes_256_gcm,aead,sha384},
                               {dhe_rsa,aes_256_cbc,sha256},
                               {dhe_dss,aes_256_cbc,sha256},
                               {rsa,aes_256_gcm,aead,sha384},
                               {rsa,aes_256_cbc,sha256},
                               {ecdhe_ecdsa,aes_128_gcm,aead,sha256},
                               {ecdhe_rsa,aes_128_gcm,aead,sha256},
                               {ecdhe_ecdsa,aes_128_cbc,sha256,sha256},
                               {ecdhe_rsa,aes_128_cbc,sha256,sha256},
                               {ecdh_ecdsa,aes_128_gcm,aead,sha256},
                               {ecdh_rsa,aes_128_gcm,aead,sha256},
                               {ecdh_ecdsa,aes_128_cbc,sha256,sha256},
                               {ecdh_rsa,aes_128_cbc,sha256,sha256},
                               {dhe_rsa,aes_128_gcm,aead,sha256},
                               {dhe_dss,aes_128_gcm,aead,sha256},
                               {dhe_rsa,aes_128_cbc,sha256},
                               {dhe_dss,aes_128_cbc,sha256},
                               {rsa,aes_128_gcm,aead,sha256},
                               {rsa,aes_128_cbc,sha256},
                               {ecdhe_ecdsa,aes_256_cbc,sha},
                               {ecdhe_rsa,aes_256_cbc,sha},
                               {dhe_rsa,aes_256_cbc,sha},
                               {dhe_dss,aes_256_cbc,sha},
                               {ecdh_ecdsa,aes_256_cbc,sha},
                               {ecdh_rsa,aes_256_cbc,sha},
                               {rsa,aes_256_cbc,sha},
                               {ecdhe_ecdsa,aes_128_cbc,sha},
                               {ecdhe_rsa,aes_128_cbc,sha},
                               {dhe_rsa,aes_128_cbc,sha},
                               {dhe_dss,aes_128_cbc,sha},
                               {ecdh_ecdsa,aes_128_cbc,sha},
                               {ecdh_rsa,aes_128_cbc,sha},
                               {rsa,aes_128_cbc,sha},
                               {ecdhe_ecdsa,'3des_ede_cbc',sha},
                               {ecdhe_rsa,'3des_ede_cbc',sha},
                               {dhe_rsa,'3des_ede_cbc',sha},
                               {dhe_dss,'3des_ede_cbc',sha},
                               {ecdh_ecdsa,'3des_ede_cbc',sha},
                               {ecdh_rsa,'3des_ede_cbc',sha},
                               {rsa,'3des_ede_cbc',sha}]},
                             {honor_cipher_order,true},
                             {secure_renegotiate,true},
                             {client_renegotiation,false}]},
                           {port,18091}]]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T19:45:33.159Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_ssl_services_sup}
             started: [{pid,<0.223.0>},
                       {id,ns_rest_ssl_service},
                       {mfargs,
                           {restartable,start_link,
                               [{ns_ssl_services_setup,
                                    start_link_rest_service,[]},
                                1000]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,worker}]

[error_logger:info,2020-03-27T19:45:33.159Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.212.0>},
                       {name,ns_ssl_services_sup},
                       {mfargs,{ns_ssl_services_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-03-27T19:45:33.168Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.261.0>},
                       {name,ldap_auth_cache},
                       {mfargs,{ldap_auth_cache,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T19:45:33.169Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,users_sup}
             started: [{pid,<0.264.0>},
                       {id,user_storage_events},
                       {mfargs,
                           {gen_event,start_link,
                               [{local,user_storage_events}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T19:45:33.182Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,users_storage_sup}
             started: [{pid,<0.266.0>},
                       {id,users_replicator},
                       {mfargs,{menelaus_users,start_replicator,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-03-27T19:45:33.184Z,ns_1@cb.local:users_replicator<0.266.0>:replicated_storage:wait_for_startup:54]Start waiting for startup
[ns_server:debug,2020-03-27T19:45:33.187Z,ns_1@cb.local:users_storage<0.267.0>:replicated_storage:anounce_startup:68]Announce my startup to <0.266.0>
[ns_server:debug,2020-03-27T19:45:33.187Z,ns_1@cb.local:users_replicator<0.266.0>:replicated_storage:wait_for_startup:57]Received replicated storage registration from <0.267.0>
[ns_server:debug,2020-03-27T19:45:33.189Z,ns_1@cb.local:users_storage<0.267.0>:replicated_dets:open:177]Opening file "/opt/couchbase/var/lib/couchbase/config/users.dets"
[error_logger:info,2020-03-27T19:45:33.189Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,users_storage_sup}
             started: [{pid,<0.267.0>},
                       {id,users_storage},
                       {mfargs,{menelaus_users,start_storage,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T19:45:33.189Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,users_sup}
             started: [{pid,<0.265.0>},
                       {id,users_storage_sup},
                       {mfargs,{users_storage_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[ns_server:debug,2020-03-27T19:45:33.190Z,ns_1@cb.local:compiled_roles_cache<0.269.0>:versioned_cache:init:47]Starting versioned cache compiled_roles_cache
[error_logger:info,2020-03-27T19:45:33.190Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,users_sup}
             started: [{pid,<0.269.0>},
                       {id,compiled_roles_cache},
                       {mfargs,{menelaus_roles,start_compiled_roles_cache,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T19:45:33.197Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,users_sup}
             started: [{pid,<0.272.0>},
                       {id,roles_cache},
                       {mfargs,{roles_cache,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T19:45:33.197Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.263.0>},
                       {name,users_sup},
                       {mfargs,{users_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-03-27T19:45:33.201Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,kernel_safe_sup}
             started: [{pid,<0.276.0>},
                       {id,dets_sup},
                       {mfargs,{dets_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,supervisor}]

[error_logger:info,2020-03-27T19:45:33.201Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,kernel_safe_sup}
             started: [{pid,<0.277.0>},
                       {id,dets},
                       {mfargs,{dets_server,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,2000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T19:45:33.205Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.275.0>},
                       {name,start_couchdb_node},
                       {mfargs,{ns_server_nodes_sup,start_couchdb_node,[]}},
                       {restart_type,{permanent,5}},
                       {shutdown,86400000},
                       {child_type,worker}]

[ns_server:debug,2020-03-27T19:45:33.205Z,ns_1@cb.local:wait_link_to_couchdb_node<0.280.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:152]Waiting for ns_couchdb node to start
[ns_server:debug,2020-03-27T19:45:33.205Z,ns_1@cb.local:net_kernel<0.179.0>:cb_dist:info_msg:754]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2020-03-27T19:45:33.205Z,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Added connection {con,#Ref<0.1597663237.214695938.137535>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2020-03-27T19:45:33.206Z,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Updated connection: {con,#Ref<0.1597663237.214695938.137535>,
                                  inet_tcp_dist,<0.283.0>,
                                  #Ref<0.1597663237.214695938.137539>}
[error_logger:info,2020-03-27T19:45:33.206Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{connect,normal,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2020-03-27T19:45:33.206Z,ns_1@cb.local:<0.281.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:166]ns_couchdb is not ready: {badrpc,nodedown}
[ns_server:debug,2020-03-27T19:45:33.207Z,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Connection down: {con,#Ref<0.1597663237.214695938.137535>,
                               inet_tcp_dist,<0.283.0>,
                               #Ref<0.1597663237.214695938.137539>}
[error_logger:info,2020-03-27T19:45:33.207Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{'EXIT',<0.283.0>,shutdown}}
[error_logger:info,2020-03-27T19:45:33.207Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{net_kernel,913,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:info,2020-03-27T19:45:33.250Z,ns_1@cb.local:users_storage<0.267.0>:replicated_dets:convert_docs_to_55_in_dets:209]Checking for pre 5.5 records in dets: users_storage
[ns_server:debug,2020-03-27T19:45:33.251Z,ns_1@cb.local:users_storage<0.267.0>:replicated_dets:init_after_ack:170]Loading 0 items, 300 words took 61ms
[ns_server:debug,2020-03-27T19:45:33.269Z,ns_1@cb.local:users_replicator<0.266.0>:doc_replicator:loop:60]doing replicate_newnodes_docs
[error_logger:info,2020-03-27T19:45:33.407Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{connect,normal,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2020-03-27T19:45:33.407Z,ns_1@cb.local:net_kernel<0.179.0>:cb_dist:info_msg:754]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2020-03-27T19:45:33.407Z,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Added connection {con,#Ref<0.1597663237.214695937.138183>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2020-03-27T19:45:33.407Z,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Updated connection: {con,#Ref<0.1597663237.214695937.138183>,
                                  inet_tcp_dist,<0.286.0>,
                                  #Ref<0.1597663237.214695937.138187>}
[ns_server:debug,2020-03-27T19:45:33.408Z,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Connection down: {con,#Ref<0.1597663237.214695937.138183>,
                               inet_tcp_dist,<0.286.0>,
                               #Ref<0.1597663237.214695937.138187>}
[error_logger:info,2020-03-27T19:45:33.408Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{'EXIT',<0.286.0>,shutdown}}
[error_logger:info,2020-03-27T19:45:33.408Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{net_kernel,913,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2020-03-27T19:45:33.408Z,ns_1@cb.local:<0.281.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:166]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2020-03-27T19:45:33.609Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{connect,normal,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2020-03-27T19:45:33.609Z,ns_1@cb.local:net_kernel<0.179.0>:cb_dist:info_msg:754]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2020-03-27T19:45:33.609Z,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Added connection {con,#Ref<0.1597663237.214695938.137550>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2020-03-27T19:45:33.609Z,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Updated connection: {con,#Ref<0.1597663237.214695938.137550>,
                                  inet_tcp_dist,<0.289.0>,
                                  #Ref<0.1597663237.214695938.137554>}
[ns_server:debug,2020-03-27T19:45:33.610Z,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Connection down: {con,#Ref<0.1597663237.214695938.137550>,
                               inet_tcp_dist,<0.289.0>,
                               #Ref<0.1597663237.214695938.137554>}
[ns_server:debug,2020-03-27T19:45:33.610Z,ns_1@cb.local:<0.281.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:166]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2020-03-27T19:45:33.610Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{'EXIT',<0.289.0>,shutdown}}
[error_logger:info,2020-03-27T19:45:33.610Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{net_kernel,913,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2020-03-27T19:45:33.810Z,ns_1@cb.local:net_kernel<0.179.0>:cb_dist:info_msg:754]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[error_logger:info,2020-03-27T19:45:33.810Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{connect,normal,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2020-03-27T19:45:33.810Z,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Added connection {con,#Ref<0.1597663237.214695937.138200>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2020-03-27T19:45:33.810Z,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Updated connection: {con,#Ref<0.1597663237.214695937.138200>,
                                  inet_tcp_dist,<0.292.0>,
                                  #Ref<0.1597663237.214695937.138202>}
[ns_server:debug,2020-03-27T19:45:33.810Z,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Connection down: {con,#Ref<0.1597663237.214695937.138200>,
                               inet_tcp_dist,<0.292.0>,
                               #Ref<0.1597663237.214695937.138202>}
[ns_server:debug,2020-03-27T19:45:33.811Z,ns_1@cb.local:<0.281.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:166]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2020-03-27T19:45:33.811Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{'EXIT',<0.292.0>,shutdown}}
[error_logger:info,2020-03-27T19:45:33.811Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{net_kernel,913,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2020-03-27T19:45:34.011Z,ns_1@cb.local:net_kernel<0.179.0>:cb_dist:info_msg:754]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[error_logger:info,2020-03-27T19:45:34.011Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{connect,normal,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2020-03-27T19:45:34.011Z,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Added connection {con,#Ref<0.1597663237.214695937.138212>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2020-03-27T19:45:34.011Z,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Updated connection: {con,#Ref<0.1597663237.214695937.138212>,
                                  inet_tcp_dist,<0.295.0>,
                                  #Ref<0.1597663237.214695937.138216>}
[error_logger:info,2020-03-27T19:45:34.012Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{'EXIT',<0.295.0>,shutdown}}
[ns_server:debug,2020-03-27T19:45:34.012Z,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Connection down: {con,#Ref<0.1597663237.214695937.138212>,
                               inet_tcp_dist,<0.295.0>,
                               #Ref<0.1597663237.214695937.138216>}
[ns_server:debug,2020-03-27T19:45:34.012Z,ns_1@cb.local:<0.281.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:166]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2020-03-27T19:45:34.012Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{net_kernel,913,nodedown,'couchdb_ns_1@cb.local'}}
[error_logger:info,2020-03-27T19:45:34.213Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{connect,normal,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2020-03-27T19:45:34.213Z,ns_1@cb.local:net_kernel<0.179.0>:cb_dist:info_msg:754]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2020-03-27T19:45:34.213Z,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Added connection {con,#Ref<0.1597663237.214695937.138226>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2020-03-27T19:45:34.213Z,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Updated connection: {con,#Ref<0.1597663237.214695937.138226>,
                                  inet_tcp_dist,<0.298.0>,
                                  #Ref<0.1597663237.214695937.138230>}
[ns_server:debug,2020-03-27T19:45:34.400Z,ns_1@cb.local:<0.281.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:166]ns_couchdb is not ready: false
[ns_server:debug,2020-03-27T19:45:34.609Z,ns_1@cb.local:<0.281.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:166]ns_couchdb is not ready: false
[ns_server:debug,2020-03-27T19:45:34.811Z,ns_1@cb.local:<0.281.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:166]ns_couchdb is not ready: false
[ns_server:debug,2020-03-27T19:45:35.013Z,ns_1@cb.local:<0.281.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:166]ns_couchdb is not ready: false
[ns_server:debug,2020-03-27T19:45:35.215Z,ns_1@cb.local:<0.281.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:166]ns_couchdb is not ready: false
[ns_server:debug,2020-03-27T19:45:35.417Z,ns_1@cb.local:<0.281.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:166]ns_couchdb is not ready: false
[ns_server:debug,2020-03-27T19:45:35.622Z,ns_1@cb.local:<0.281.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:166]ns_couchdb is not ready: false
[error_logger:info,2020-03-27T19:45:36.008Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,kernel_safe_sup}
             started: [{pid,<0.307.0>},
                       {id,timer2_server},
                       {mfargs,{timer2,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:info,2020-03-27T19:45:36.208Z,ns_1@cb.local:ns_couchdb_port<0.275.0>:ns_port_server:log:224]ns_couchdb<0.275.0>: Apache CouchDB  (LogLevel=info) is starting.

[ns_server:info,2020-03-27T19:45:36.468Z,ns_1@cb.local:ns_couchdb_port<0.275.0>:ns_port_server:log:224]ns_couchdb<0.275.0>: Apache CouchDB has started. Time to relax.

[error_logger:info,2020-03-27T19:45:36.567Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.280.0>},
                       {name,wait_for_couchdb_node},
                       {mfargs,
                           {erlang,apply,
                               [#Fun<ns_server_nodes_sup.0.58023840>,[]]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-03-27T19:45:36.584Z,ns_1@cb.local:ns_server_nodes_sup<0.207.0>:ns_storage_conf:setup_db_and_ix_paths:64]Initialize db_and_ix_paths variable with [{db_path,
                                           "/opt/couchbase/var/lib/couchbase/data"},
                                          {index_path,
                                           "/opt/couchbase/var/lib/couchbase/data"}]
[ns_server:debug,2020-03-27T19:45:36.584Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{local_changes_count,<<"60a6bc3db77e6c7b91c556140dcfec71">>} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{3,63752557536}}]}]
[ns_server:debug,2020-03-27T19:45:36.585Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',cbas_dirs} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557536}}]},
 "/opt/couchbase/var/lib/couchbase/data"]
[ns_server:debug,2020-03-27T19:45:36.585Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{local_changes_count,<<"60a6bc3db77e6c7b91c556140dcfec71">>} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{4,63752557536}}]}]
[ns_server:debug,2020-03-27T19:45:36.585Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',eventing_dir} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557536}}]},
 47,111,112,116,47,99,111,117,99,104,98,97,115,101,47,118,97,114,47,108,105,
 98,47,99,111,117,99,104,98,97,115,101,47,100,97,116,97]
[error_logger:info,2020-03-27T19:45:36.604Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.311.0>},
                       {name,ns_disksup},
                       {mfargs,{ns_disksup,start_link,[]}},
                       {restart_type,{permanent,4}},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T19:45:36.616Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.312.0>},
                       {name,diag_handler_worker},
                       {mfargs,{work_queue,start_link,[diag_handler_worker]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:info,2020-03-27T19:45:36.630Z,ns_1@cb.local:ns_server_sup<0.310.0>:dir_size:start_link:39]Starting quick version of dir_size with program name: godu
[error_logger:info,2020-03-27T19:45:36.639Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.313.0>},
                       {name,dir_size},
                       {mfargs,{dir_size,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T19:45:36.677Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.315.0>},
                       {name,request_throttler},
                       {mfargs,{request_throttler,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:warn,2020-03-27T19:45:36.695Z,ns_1@cb.local:ns_log<0.316.0>:ns_log:read_logs:91]Couldn't load logs from "/opt/couchbase/var/lib/couchbase/ns_log" (perhaps it's first startup): {error,
                                                                                                 enoent}
[error_logger:info,2020-03-27T19:45:36.695Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.316.0>},
                       {name,ns_log},
                       {mfargs,{ns_log,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T19:45:36.695Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.317.0>},
                       {name,ns_crash_log_consumer},
                       {mfargs,{ns_log,start_link_crash_consumer,[]}},
                       {restart_type,{permanent,4}},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-03-27T19:45:36.752Z,ns_1@cb.local:memcached_passwords<0.318.0>:memcached_cfg:init:62]Init config writer for memcached_passwords, "/opt/couchbase/var/lib/couchbase/isasl.pw"
[ns_server:debug,2020-03-27T19:45:36.756Z,ns_1@cb.local:memcached_passwords<0.318.0>:memcached_cfg:write_cfg:118]Writing config file for: "/opt/couchbase/var/lib/couchbase/isasl.pw"
[ns_server:info,2020-03-27T19:45:36.767Z,ns_1@cb.local:ns_couchdb_port<0.275.0>:ns_port_server:log:224]ns_couchdb<0.275.0>: 209: Booted. Waiting for shutdown request
ns_couchdb<0.275.0>: working as port

[ns_server:debug,2020-03-27T19:45:36.834Z,ns_1@cb.local:users_storage<0.267.0>:replicated_dets:handle_call:302]Suspended by process <0.318.0>
[ns_server:debug,2020-03-27T19:45:36.834Z,ns_1@cb.local:memcached_passwords<0.318.0>:replicated_dets:select_from_dets_locked:350]Starting select with {users_storage,[{{docv2,{auth,{'_',local}},'_','_'},
                                      [],
                                      ['$_']}],
                                    100}
[ns_server:debug,2020-03-27T19:45:36.834Z,ns_1@cb.local:users_storage<0.267.0>:replicated_dets:handle_call:309]Released by process <0.318.0>
[error_logger:info,2020-03-27T19:45:36.858Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.318.0>},
                       {name,memcached_passwords},
                       {mfargs,{memcached_passwords,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-03-27T19:45:36.858Z,ns_1@cb.local:memcached_refresh<0.211.0>:memcached_refresh:handle_cast:55]Refresh of isasl requested
[ns_server:debug,2020-03-27T19:45:36.864Z,ns_1@cb.local:memcached_permissions<0.321.0>:memcached_cfg:init:62]Init config writer for memcached_permissions, "/opt/couchbase/var/lib/couchbase/config/memcached.rbac"
[error_logger:info,2020-03-27T19:45:36.869Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,inet_gethost_native_sup}
             started: [{pid,<0.325.0>},{mfa,{inet_gethost_native,init,[[]]}}]

[error_logger:info,2020-03-27T19:45:36.869Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,kernel_safe_sup}
             started: [{pid,<0.324.0>},
                       {id,inet_gethost_native_sup},
                       {mfargs,{inet_gethost_native,start_link,[]}},
                       {restart_type,temporary},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-03-27T19:45:36.872Z,ns_1@cb.local:memcached_permissions<0.321.0>:memcached_cfg:write_cfg:118]Writing config file for: "/opt/couchbase/var/lib/couchbase/config/memcached.rbac"
[ns_server:debug,2020-03-27T19:45:36.881Z,ns_1@cb.local:users_storage<0.267.0>:replicated_dets:handle_call:302]Suspended by process <0.321.0>
[ns_server:debug,2020-03-27T19:45:36.881Z,ns_1@cb.local:memcached_permissions<0.321.0>:replicated_dets:select_from_dets_locked:350]Starting select with {users_storage,[{{docv2,{user,{'_',local}},'_','_'},
                                      [],
                                      ['$_']}],
                                    100}
[ns_server:debug,2020-03-27T19:45:36.881Z,ns_1@cb.local:users_storage<0.267.0>:replicated_dets:handle_call:309]Released by process <0.321.0>
[error_logger:info,2020-03-27T19:45:36.896Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.321.0>},
                       {name,memcached_permissions},
                       {mfargs,{memcached_permissions,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T19:45:36.901Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.326.0>},
                       {name,ns_email_alert},
                       {mfargs,{ns_email_alert,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-03-27T19:45:36.909Z,ns_1@cb.local:ns_node_disco<0.329.0>:ns_node_disco:init:128]Initting ns_node_disco with []
[ns_server:debug,2020-03-27T19:45:36.909Z,ns_1@cb.local:ns_cookie_manager<0.188.0>:ns_cookie_manager:do_cookie_sync:107]ns_cookie_manager do_cookie_sync
[error_logger:info,2020-03-27T19:45:36.909Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_node_disco_sup}
             started: [{pid,<0.328.0>},
                       {id,ns_node_disco_events},
                       {mfargs,
                           {gen_event,start_link,
                               [{local,ns_node_disco_events}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[user:info,2020-03-27T19:45:36.909Z,ns_1@cb.local:ns_cookie_manager<0.188.0>:ns_cookie_manager:do_cookie_init:84]Initial otp cookie generated: {sanitized,
                                  <<"C4PeZIe8LLStUy+d1HIN638gh0wTZW33h1wdSpFkvzc=">>}
[ns_server:debug,2020-03-27T19:45:36.909Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{local_changes_count,<<"60a6bc3db77e6c7b91c556140dcfec71">>} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{5,63752557536}}]}]
[ns_server:debug,2020-03-27T19:45:36.909Z,ns_1@cb.local:<0.330.0>:ns_node_disco:do_nodes_wanted_updated_fun:214]ns_node_disco: nodes_wanted updated: ['ns_1@cb.local'], with cookie: {sanitized,
                                                                      <<"C4PeZIe8LLStUy+d1HIN638gh0wTZW33h1wdSpFkvzc=">>}
[ns_server:debug,2020-03-27T19:45:36.909Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
otp ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557536}}]},
 {cookie,{sanitized,<<"C4PeZIe8LLStUy+d1HIN638gh0wTZW33h1wdSpFkvzc=">>}}]
[ns_server:warn,2020-03-27T19:45:36.921Z,ns_1@cb.local:memcached_refresh<0.211.0>:ns_memcached:connect:1101]Unable to connect: {error,{badmatch,{error,econnrefused}}}.
[ns_server:debug,2020-03-27T19:45:36.921Z,ns_1@cb.local:memcached_refresh<0.211.0>:memcached_refresh:handle_info:93]Refresh of [isasl] failed. Retry in 1000 ms.
[ns_server:debug,2020-03-27T19:45:36.921Z,ns_1@cb.local:memcached_refresh<0.211.0>:memcached_refresh:handle_cast:55]Refresh of rbac requested
[ns_server:debug,2020-03-27T19:45:36.922Z,ns_1@cb.local:<0.330.0>:ns_node_disco:do_nodes_wanted_updated_fun:220]ns_node_disco: nodes_wanted pong: ['ns_1@cb.local'], with cookie: {sanitized,
                                                                   <<"C4PeZIe8LLStUy+d1HIN638gh0wTZW33h1wdSpFkvzc=">>}
[error_logger:info,2020-03-27T19:45:36.922Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_node_disco_sup}
             started: [{pid,<0.329.0>},
                       {id,ns_node_disco},
                       {mfargs,{ns_node_disco,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:warn,2020-03-27T19:45:36.924Z,ns_1@cb.local:memcached_refresh<0.211.0>:ns_memcached:connect:1101]Unable to connect: {error,{badmatch,{error,econnrefused}}}.
[ns_server:debug,2020-03-27T19:45:36.924Z,ns_1@cb.local:memcached_refresh<0.211.0>:memcached_refresh:handle_info:93]Refresh of [rbac,isasl] failed. Retry in 1000 ms.
[error_logger:info,2020-03-27T19:45:36.933Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_node_disco_sup}
             started: [{pid,<0.332.0>},
                       {id,ns_node_disco_log},
                       {mfargs,{ns_node_disco_log,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T19:45:36.958Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_node_disco_sup}
             started: [{pid,<0.333.0>},
                       {id,ns_node_disco_conf_events},
                       {mfargs,{ns_node_disco_conf_events,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T19:45:36.975Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_node_disco_sup}
             started: [{pid,<0.334.0>},
                       {id,ns_config_rep_merger},
                       {mfargs,{ns_config_rep,start_link_merger,[]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[ns_server:debug,2020-03-27T19:45:36.975Z,ns_1@cb.local:ns_config_rep<0.335.0>:ns_config_rep:init:71]init pulling
[ns_server:debug,2020-03-27T19:45:36.975Z,ns_1@cb.local:ns_config_rep<0.335.0>:ns_config_rep:init:73]init pushing
[ns_server:debug,2020-03-27T19:45:36.985Z,ns_1@cb.local:ns_config_rep<0.335.0>:ns_config_rep:init:77]init reannouncing
[ns_server:debug,2020-03-27T19:45:36.988Z,ns_1@cb.local:ns_config_events<0.191.0>:ns_node_disco_conf_events:handle_event:50]ns_node_disco_conf_events config on otp
[ns_server:debug,2020-03-27T19:45:36.988Z,ns_1@cb.local:ns_config_events<0.191.0>:ns_node_disco_conf_events:handle_event:44]ns_node_disco_conf_events config on nodes_wanted
[ns_server:debug,2020-03-27T19:45:37.000Z,ns_1@cb.local:ns_cookie_manager<0.188.0>:ns_cookie_manager:do_cookie_sync:107]ns_cookie_manager do_cookie_sync
[ns_server:debug,2020-03-27T19:45:37.000Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
otp ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557536}}]},
 {cookie,{sanitized,<<"C4PeZIe8LLStUy+d1HIN638gh0wTZW33h1wdSpFkvzc=">>}}]
[ns_server:debug,2020-03-27T19:45:37.000Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',eventing_dir} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557536}}]},
 47,111,112,116,47,99,111,117,99,104,98,97,115,101,47,118,97,114,47,108,105,
 98,47,99,111,117,99,104,98,97,115,101,47,100,97,116,97]
[ns_server:debug,2020-03-27T19:45:37.001Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',cbas_dirs} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557536}}]},
 "/opt/couchbase/var/lib/couchbase/data"]
[ns_server:debug,2020-03-27T19:45:37.001Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
cert_and_pkey ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557532}}]}|
 {<<"-----BEGIN CERTIFICATE-----\nMIIDAjCCAeqgAwIBAgIIFgBA0G6s0lwwDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciAzNDhlZDI3MDAeFw0xMzAxMDEwMDAwMDBaFw00\nOTEyMzEyMzU5NTlaMCQxIjAgBgNVBAMTGUNvdWNoYmFzZSBTZXJ2ZXIgMzQ4ZWQy\nNzAwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQDh0efJgvfpArvc3iI0\ncHa/zZhS9WoaoNHksFooYqy3AYiZR+8S3IgPS0nfcZGrIWJUPgokUqBqh6u59abc\nvCuyMwLNkkuLOkT6wzUztrAqoCoOEl4"...>>,
  <<"*****">>}]
[ns_server:debug,2020-03-27T19:45:37.001Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',erl_external_listeners} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557532}}]},
 {inet,false},
 {inet6,false}]
[ns_server:debug,2020-03-27T19:45:37.001Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',node_encryption} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557532}}]}|false]
[ns_server:debug,2020-03-27T19:45:37.001Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',address_family} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557532}}]}|inet]
[ns_server:debug,2020-03-27T19:45:37.001Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
alert_limits ->
[{max_overhead_perc,50},{max_disk_used,90},{max_indexer_ram,75}]
[ns_server:debug,2020-03-27T19:45:37.001Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
audit ->
[{auditd_enabled,false},
 {rotate_interval,86400},
 {rotate_size,20971520},
 {disabled,[]},
 {sync,[]},
 {log_path,"/opt/couchbase/var/lib/couchbase/logs"}]
[ns_server:debug,2020-03-27T19:45:37.001Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
auto_failover_cfg ->
[{enabled,true},{timeout,120},{max_nodes,1},{count,0}]
[ns_server:debug,2020-03-27T19:45:37.002Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
auto_reprovision_cfg ->
[{enabled,true},{max_nodes,1},{count,0}]
[ns_server:debug,2020-03-27T19:45:37.002Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
autocompaction ->
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}}]
[ns_server:debug,2020-03-27T19:45:37.002Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
buckets ->
[[],{configs,[]}]
[ns_server:debug,2020-03-27T19:45:37.002Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
cbas_memory_quota ->
1024
[ns_server:debug,2020-03-27T19:45:37.002Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
drop_request_memory_threshold_mib ->
undefined
[ns_server:debug,2020-03-27T19:45:37.002Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
email_alerts ->
[{recipients,["root@localhost"]},
 {sender,"couchbase@localhost"},
 {enabled,false},
 {email_server,[{user,[]},
                {pass,"*****"},
                {host,"localhost"},
                {port,25},
                {encrypt,false}]},
 {alerts,[auto_failover_node,auto_failover_maximum_reached,
          auto_failover_other_nodes_down,auto_failover_cluster_too_small,
          auto_failover_disabled,ip,disk,overhead,ep_oom_errors,
          ep_item_commit_failed,audit_dropped_events,indexer_ram_max_usage,
          ep_clock_cas_drift_threshold_exceeded,communication_issue]}]
[ns_server:debug,2020-03-27T19:45:37.002Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
fts_memory_quota ->
256
[ns_server:debug,2020-03-27T19:45:37.002Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
index_aware_rebalance_disabled ->
false
[ns_server:debug,2020-03-27T19:45:37.002Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
log_redaction_default_cfg ->
[{redact_level,none}]
[ns_server:debug,2020-03-27T19:45:37.002Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
max_bucket_count ->
30
[ns_server:debug,2020-03-27T19:45:37.002Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
memcached ->
[]
[ns_server:debug,2020-03-27T19:45:37.002Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
memory_quota ->
292
[ns_server:debug,2020-03-27T19:45:37.002Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
nodes_wanted ->
['ns_1@cb.local']
[ns_server:debug,2020-03-27T19:45:37.002Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
password_policy ->
[{min_length,6},{must_present,[]}]
[ns_server:debug,2020-03-27T19:45:37.002Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
quorum_nodes ->
['ns_1@cb.local']
[ns_server:debug,2020-03-27T19:45:37.002Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
remote_clusters ->
[]
[ns_server:debug,2020-03-27T19:45:37.002Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
replication ->
[{enabled,true}]
[ns_server:debug,2020-03-27T19:45:37.002Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
rest ->
[{port,8091}]
[ns_server:debug,2020-03-27T19:45:37.003Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
rest_creds ->
null
[ns_server:debug,2020-03-27T19:45:37.003Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
secure_headers ->
[]
[ns_server:debug,2020-03-27T19:45:37.003Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
server_groups ->
[[{uuid,<<"0">>},{name,<<"Group 1">>},{nodes,['ns_1@cb.local']}]]
[ns_server:debug,2020-03-27T19:45:37.003Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
set_view_update_daemon ->
[{update_interval,5000},
 {update_min_changes,5000},
 {replica_update_min_changes,5000}]
[ns_server:debug,2020-03-27T19:45:37.003Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{couchdb,max_parallel_indexers} ->
4
[ns_server:debug,2020-03-27T19:45:37.003Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{couchdb,max_parallel_replica_indexers} ->
2
[ns_server:debug,2020-03-27T19:45:37.004Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{metakv,<<"/indexing/settings/config">>} ->
<<"{\"indexer.settings.compaction.days_of_week\":\"Sunday,Monday,Tuesday,Wednesday,Thursday,Friday,Saturday\",\"indexer.settings.compaction.interval\":\"00:00,00:00\",\"indexer.settings.compaction.compaction_mode\":\"circular\",\"indexer.settings.log_level\":\"info\",\"indexer.settings.persisted_snapshot.interval\":5000,\"indexer.settings.compaction.min_frag\":30,\"indexer.settings.inmemory_snapshot.interval\":200,\"in"...>>
[ns_server:debug,2020-03-27T19:45:37.004Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{request_limit,capi} ->
undefined
[ns_server:debug,2020-03-27T19:45:37.004Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{request_limit,rest} ->
undefined
[ns_server:debug,2020-03-27T19:45:37.004Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',audit} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557532}}]}]
[ns_server:debug,2020-03-27T19:45:37.004Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',capi_port} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557532}}]}|8092]
[ns_server:debug,2020-03-27T19:45:37.004Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',cbas_admin_port} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557532}}]}|9110]
[ns_server:debug,2020-03-27T19:45:37.004Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',cbas_cc_client_port} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557532}}]}|9113]
[ns_server:debug,2020-03-27T19:45:37.004Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',cbas_cc_cluster_port} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557532}}]}|9112]
[ns_server:debug,2020-03-27T19:45:37.004Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',cbas_cc_http_port} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557532}}]}|9111]
[ns_server:debug,2020-03-27T19:45:37.004Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',cbas_cluster_port} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557532}}]}|9115]
[ns_server:debug,2020-03-27T19:45:37.004Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',cbas_console_port} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557532}}]}|9114]
[ns_server:debug,2020-03-27T19:45:37.004Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',cbas_data_port} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557532}}]}|9116]
[ns_server:debug,2020-03-27T19:45:37.004Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',cbas_debug_port} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557532}}]}|-1]
[ns_server:debug,2020-03-27T19:45:37.004Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',cbas_http_port} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557532}}]}|8095]
[ns_server:debug,2020-03-27T19:45:37.004Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',cbas_messaging_port} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557532}}]}|9118]
[ns_server:debug,2020-03-27T19:45:37.004Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',cbas_metadata_callback_port} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557532}}]}|9119]
[ns_server:debug,2020-03-27T19:45:37.004Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',cbas_metadata_port} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557532}}]}|9121]
[ns_server:debug,2020-03-27T19:45:37.004Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',cbas_parent_port} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557532}}]}|9122]
[ns_server:debug,2020-03-27T19:45:37.004Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',cbas_replication_port} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557532}}]}|9120]
[ns_server:debug,2020-03-27T19:45:37.004Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',cbas_result_port} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557532}}]}|9117]
[ns_server:debug,2020-03-27T19:45:37.004Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',cbas_ssl_port} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557532}}]}|18095]
[ns_server:debug,2020-03-27T19:45:37.005Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',compaction_daemon} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557532}}]},
 {check_interval,30},
 {min_db_file_size,131072},
 {min_view_file_size,20971520}]
[ns_server:debug,2020-03-27T19:45:37.005Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',config_version} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557532}}]}|{6,5}]
[ns_server:debug,2020-03-27T19:45:37.005Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',eventing_debug_port} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557532}}]}|9140]
[ns_server:debug,2020-03-27T19:45:37.005Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',eventing_http_port} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557532}}]}|8096]
[ns_server:debug,2020-03-27T19:45:37.005Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',eventing_https_port} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557532}}]}|18096]
[ns_server:debug,2020-03-27T19:45:37.005Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',fts_grpc_port} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557532}}]}|9130]
[ns_server:debug,2020-03-27T19:45:37.005Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',fts_grpc_ssl_port} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557532}}]}|19130]
[ns_server:debug,2020-03-27T19:45:37.005Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',fts_http_port} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557532}}]}|8094]
[ns_server:debug,2020-03-27T19:45:37.005Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',fts_ssl_port} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557532}}]}|18094]
[ns_server:debug,2020-03-27T19:45:37.005Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',indexer_admin_port} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557532}}]}|9100]
[ns_server:debug,2020-03-27T19:45:37.005Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',indexer_http_port} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557532}}]}|9102]
[ns_server:debug,2020-03-27T19:45:37.005Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',indexer_https_port} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557532}}]}|19102]
[ns_server:debug,2020-03-27T19:45:37.005Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',indexer_scan_port} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557532}}]}|9101]
[ns_server:debug,2020-03-27T19:45:37.005Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',indexer_stcatchup_port} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557532}}]}|9104]
[ns_server:debug,2020-03-27T19:45:37.005Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',indexer_stinit_port} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557532}}]}|9103]
[ns_server:debug,2020-03-27T19:45:37.005Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',indexer_stmaint_port} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557532}}]}|9105]
[ns_server:debug,2020-03-27T19:45:37.005Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',is_enterprise} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557532}}]}|true]
[ns_server:debug,2020-03-27T19:45:37.006Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',isasl} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557532}}]},
 {path,"/opt/couchbase/var/lib/couchbase/isasl.pw"}]
[ns_server:debug,2020-03-27T19:45:37.006Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',membership} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557532}}]}|
 active]
[ns_server:debug,2020-03-27T19:45:37.006Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',memcached} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557532}}]},
 {port,11210},
 {dedicated_port,11209},
 {dedicated_ssl_port,11206},
 {ssl_port,11207},
 {admin_user,"@ns_server"},
 {other_users,["@cbq-engine","@projector","@goxdcr","@index","@fts",
               "@eventing","@cbas"]},
 {admin_pass,"*****"},
 {engines,[{membase,[{engine,"/opt/couchbase/lib/memcached/ep.so"},
                     {static_config_string,"failpartialwarmup=false"}]},
           {memcached,[{engine,"/opt/couchbase/lib/memcached/default_engine.so"},
                       {static_config_string,"vb0=true"}]}]},
 {config_path,"/opt/couchbase/var/lib/couchbase/config/memcached.json"},
 {audit_file,"/opt/couchbase/var/lib/couchbase/config/audit.json"},
 {rbac_file,"/opt/couchbase/var/lib/couchbase/config/memcached.rbac"},
 {log_path,"/opt/couchbase/var/lib/couchbase/logs"},
 {log_prefix,"memcached.log"},
 {log_generations,20},
 {log_cyclesize,10485760},
 {log_sleeptime,19},
 {log_rotation_period,39003}]
[ns_server:debug,2020-03-27T19:45:37.007Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',memcached_config} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557532}}]}|
 {[{interfaces,
    {memcached_config_mgr,omit_missing_mcd_ports,
     [{[{host,<<"*">>},
        {port,port},
        {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
        {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]},
      {[{host,<<"*">>},
        {port,dedicated_port},
        {system,true},
        {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
        {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]},
      {[{host,<<"*">>},
        {port,ssl_port},
        {ssl,
         {[{key,
            <<"/opt/couchbase/var/lib/couchbase/config/memcached-key.pem">>},
           {cert,
            <<"/opt/couchbase/var/lib/couchbase/config/memcached-cert.pem">>}]}},
        {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
        {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]},
      {[{host,<<"*">>},
        {port,dedicated_ssl_port},
        {system,true},
        {ssl,
         {[{key,
            <<"/opt/couchbase/var/lib/couchbase/config/memcached-key.pem">>},
           {cert,
            <<"/opt/couchbase/var/lib/couchbase/config/memcached-cert.pem">>}]}},
        {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
        {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]}]}},
   {ssl_cipher_list,{memcached_config_mgr,get_ssl_cipher_list,[]}},
   {ssl_cipher_order,{memcached_config_mgr,get_ssl_cipher_order,[]}},
   {client_cert_auth,{memcached_config_mgr,client_cert_auth,[]}},
   {ssl_minimum_protocol,{memcached_config_mgr,ssl_minimum_protocol,[]}},
   {connection_idle_time,connection_idle_time},
   {privilege_debug,privilege_debug},
   {breakpad,
    {[{enabled,breakpad_enabled},
      {minidump_dir,{memcached_config_mgr,get_minidump_dir,[]}}]}},
   {opentracing,
    {[{enabled,opentracing_enabled},
      {module,{"~s",[opentracing_module]}},
      {config,{"~s",[opentracing_config]}}]}},
   {admin,{"~s",[admin_user]}},
   {verbosity,verbosity},
   {audit_file,{"~s",[audit_file]}},
   {rbac_file,{"~s",[rbac_file]}},
   {dedupe_nmvb_maps,dedupe_nmvb_maps},
   {tracing_enabled,tracing_enabled},
   {datatype_snappy,{memcached_config_mgr,is_snappy_enabled,[]}},
   {xattr_enabled,true},
   {scramsha_fallback_salt,{memcached_config_mgr,get_fallback_salt,[]}},
   {collections_enabled,{memcached_config_mgr,collections_enabled,[]}},
   {max_connections,max_connections},
   {system_connections,system_connections},
   {num_reader_threads,num_reader_threads},
   {num_writer_threads,num_writer_threads},
   {logger,
    {[{filename,{"~s/~s",[log_path,log_prefix]}},
      {cyclesize,log_cyclesize},
      {sleeptime,log_sleeptime}]}},
   {external_auth_service,{memcached_config_mgr,get_external_auth_service,[]}},
   {active_external_users_push_interval,
    {memcached_config_mgr,get_external_users_push_interval,[]}}]}]
[ns_server:debug,2020-03-27T19:45:37.007Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',memcached_dedicated_ssl_port} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557532}}]}|11206]
[ns_server:debug,2020-03-27T19:45:37.007Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',memcached_defaults} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557532}}]},
 {max_connections,65000},
 {system_connections,5000},
 {connection_idle_time,0},
 {verbosity,0},
 {privilege_debug,false},
 {opentracing_enabled,false},
 {opentracing_module,[]},
 {opentracing_config,[]},
 {breakpad_enabled,true},
 {breakpad_minidump_dir_path,"/opt/couchbase/var/lib/couchbase/crash"},
 {dedupe_nmvb_maps,false},
 {tracing_enabled,true},
 {datatype_snappy,true},
 {num_reader_threads,<<"default">>},
 {num_writer_threads,<<"default">>}]
[ns_server:debug,2020-03-27T19:45:37.007Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',moxi} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557532}}]},
 {port,0}]
[ns_server:debug,2020-03-27T19:45:37.008Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',ns_log} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557532}}]},
 {filename,"/opt/couchbase/var/lib/couchbase/ns_log"}]
[ns_server:debug,2020-03-27T19:45:37.008Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',port_servers} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557532}}]}]
[ns_server:debug,2020-03-27T19:45:37.008Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',projector_port} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557532}}]}|9999]
[ns_server:debug,2020-03-27T19:45:37.008Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',projector_ssl_port} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557532}}]}|9999]
[ns_server:debug,2020-03-27T19:45:37.008Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',query_port} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557532}}]}|8093]
[ns_server:debug,2020-03-27T19:45:37.008Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',rest} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557532}}]},
 {port,8091},
 {port_meta,global}]
[ns_server:debug,2020-03-27T19:45:37.008Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',saslauthd_enabled} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557532}}]}|true]
[ns_server:debug,2020-03-27T19:45:37.008Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',ssl_capi_port} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557532}}]}|18092]
[ns_server:debug,2020-03-27T19:45:37.008Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',ssl_query_port} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557532}}]}|18093]
[ns_server:debug,2020-03-27T19:45:37.008Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',ssl_rest_port} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557532}}]}|18091]
[ns_server:debug,2020-03-27T19:45:37.008Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',uuid} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557532}}]}|
 <<"60a6bc3db77e6c7b91c556140dcfec71">>]
[ns_server:debug,2020-03-27T19:45:37.008Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',xdcr_rest_port} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557532}}]}|9998]
[ns_server:debug,2020-03-27T19:45:37.008Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',{project_intact,is_vulnerable}} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557532}}]}|false]
[ns_server:debug,2020-03-27T19:45:37.008Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{local_changes_count,<<"60a6bc3db77e6c7b91c556140dcfec71">>} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{5,63752557536}}]}]
[ns_server:debug,2020-03-27T19:45:37.009Z,ns_1@cb.local:ns_cookie_manager<0.188.0>:ns_cookie_manager:do_cookie_sync:107]ns_cookie_manager do_cookie_sync
[ns_server:debug,2020-03-27T19:45:36.989Z,ns_1@cb.local:compiled_roles_cache<0.269.0>:versioned_cache:handle_info:92]Flushing cache compiled_roles_cache due to version change from undefined to {undefined,
                                                                             {0,
                                                                              4055119168},
                                                                             {0,
                                                                              4055119168},
                                                                             false,
                                                                             []}
[ns_server:debug,2020-03-27T19:45:37.017Z,ns_1@cb.local:memcached_passwords<0.318.0>:memcached_cfg:write_cfg:118]Writing config file for: "/opt/couchbase/var/lib/couchbase/isasl.pw"
[ns_server:debug,2020-03-27T19:45:37.021Z,ns_1@cb.local:<0.341.0>:ns_node_disco:do_nodes_wanted_updated_fun:214]ns_node_disco: nodes_wanted updated: ['ns_1@cb.local'], with cookie: {sanitized,
                                                                      <<"C4PeZIe8LLStUy+d1HIN638gh0wTZW33h1wdSpFkvzc=">>}
[ns_server:debug,2020-03-27T19:45:37.021Z,ns_1@cb.local:<0.342.0>:ns_node_disco:do_nodes_wanted_updated_fun:214]ns_node_disco: nodes_wanted updated: ['ns_1@cb.local'], with cookie: {sanitized,
                                                                      <<"C4PeZIe8LLStUy+d1HIN638gh0wTZW33h1wdSpFkvzc=">>}
[ns_server:debug,2020-03-27T19:45:37.021Z,ns_1@cb.local:<0.341.0>:ns_node_disco:do_nodes_wanted_updated_fun:220]ns_node_disco: nodes_wanted pong: ['ns_1@cb.local'], with cookie: {sanitized,
                                                                   <<"C4PeZIe8LLStUy+d1HIN638gh0wTZW33h1wdSpFkvzc=">>}
[ns_server:debug,2020-03-27T19:45:37.022Z,ns_1@cb.local:<0.342.0>:ns_node_disco:do_nodes_wanted_updated_fun:220]ns_node_disco: nodes_wanted pong: ['ns_1@cb.local'], with cookie: {sanitized,
                                                                   <<"C4PeZIe8LLStUy+d1HIN638gh0wTZW33h1wdSpFkvzc=">>}
[error_logger:info,2020-03-27T19:45:37.023Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_node_disco_sup}
             started: [{pid,<0.335.0>},
                       {id,ns_config_rep},
                       {mfargs,{ns_config_rep,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T19:45:37.023Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.327.0>},
                       {name,ns_node_disco_sup},
                       {mfargs,{ns_node_disco_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[ns_server:debug,2020-03-27T19:45:37.023Z,ns_1@cb.local:ns_config_rep<0.335.0>:ns_config_rep:do_push_keys:321]Replicating some config keys ([alert_limits,audit,auto_failover_cfg,
                               auto_reprovision_cfg,autocompaction,buckets,
                               cbas_memory_quota,cert_and_pkey,
                               drop_request_memory_threshold_mib,email_alerts,
                               fts_memory_quota,
                               index_aware_rebalance_disabled,
                               log_redaction_default_cfg,max_bucket_count,
                               memcached,memory_quota,nodes_wanted,otp,
                               password_policy,quorum_nodes,remote_clusters,
                               replication,rest,rest_creds,secure_headers,
                               server_groups,set_view_update_daemon,
                               {couchdb,max_parallel_indexers},
                               {couchdb,max_parallel_replica_indexers},
                               {local_changes_count,
                                   <<"60a6bc3db77e6c7b91c556140dcfec71">>},
                               {metakv,<<"/indexing/settings/config">>},
                               {request_limit,capi},
                               {request_limit,rest},
                               {node,'ns_1@cb.local',address_family},
                               {node,'ns_1@cb.local',audit},
                               {node,'ns_1@cb.local',capi_port},
                               {node,'ns_1@cb.local',cbas_admin_port},
                               {node,'ns_1@cb.local',cbas_cc_client_port},
                               {node,'ns_1@cb.local',cbas_cc_cluster_port},
                               {node,'ns_1@cb.local',cbas_cc_http_port},
                               {node,'ns_1@cb.local',cbas_cluster_port},
                               {node,'ns_1@cb.local',cbas_console_port},
                               {node,'ns_1@cb.local',cbas_data_port},
                               {node,'ns_1@cb.local',cbas_debug_port},
                               {node,'ns_1@cb.local',cbas_dirs},
                               {node,'ns_1@cb.local',cbas_http_port},
                               {node,'ns_1@cb.local',cbas_messaging_port},
                               {node,'ns_1@cb.local',
                                   cbas_metadata_callback_port},
                               {node,'ns_1@cb.local',cbas_metadata_port},
                               {node,'ns_1@cb.local',cbas_parent_port},
                               {node,'ns_1@cb.local',cbas_replication_port},
                               {node,'ns_1@cb.local',cbas_result_port},
                               {node,'ns_1@cb.local',cbas_ssl_port},
                               {node,'ns_1@cb.local',compaction_daemon},
                               {node,'ns_1@cb.local',config_version},
                               {node,'ns_1@cb.local',erl_external_listeners},
                               {node,'ns_1@cb.local',eventing_debug_port},
                               {node,'ns_1@cb.local',eventing_dir},
                               {node,'ns_1@cb.local',eventing_http_port},
                               {node,'ns_1@cb.local',eventing_https_port},
                               {node,'ns_1@cb.local',fts_grpc_port},
                               {node,'ns_1@cb.local',fts_grpc_ssl_port},
                               {node,'ns_1@cb.local',fts_http_port},
                               {node,'ns_1@cb.local',fts_ssl_port}]..)
[ns_server:debug,2020-03-27T19:45:37.064Z,ns_1@cb.local:ns_ssl_services_setup<0.214.0>:ns_ssl_services_setup:trigger_ssl_reload:594]Notify services [capi_ssl_service] about secure_headers_changed change
[ns_server:debug,2020-03-27T19:45:37.064Z,ns_1@cb.local:ns_ssl_services_setup<0.214.0>:ns_ssl_services_setup:notify_services:740]Going to notify following services: [capi_ssl_service]
[ns_server:info,2020-03-27T19:45:37.094Z,ns_1@cb.local:<0.350.0>:ns_ssl_services_setup:notify_service:772]Successfully notified service capi_ssl_service
[ns_server:info,2020-03-27T19:45:37.094Z,ns_1@cb.local:ns_ssl_services_setup<0.214.0>:ns_ssl_services_setup:notify_services:756]Succesfully notified services [capi_ssl_service]
[error_logger:info,2020-03-27T19:45:37.115Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.351.0>},
                       {name,vbucket_map_mirror},
                       {mfargs,{vbucket_map_mirror,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[ns_server:debug,2020-03-27T19:45:37.148Z,ns_1@cb.local:users_storage<0.267.0>:replicated_dets:handle_call:302]Suspended by process <0.318.0>
[ns_server:debug,2020-03-27T19:45:37.148Z,ns_1@cb.local:memcached_passwords<0.318.0>:replicated_dets:select_from_dets_locked:350]Starting select with {users_storage,[{{docv2,{auth,{'_',local}},'_','_'},
                                      [],
                                      ['$_']}],
                                    100}
[ns_server:debug,2020-03-27T19:45:37.148Z,ns_1@cb.local:users_storage<0.267.0>:replicated_dets:handle_call:309]Released by process <0.318.0>
[error_logger:info,2020-03-27T19:45:37.145Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.353.0>},
                       {name,bucket_info_cache},
                       {mfargs,{bucket_info_cache,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[error_logger:info,2020-03-27T19:45:37.148Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.356.0>},
                       {name,ns_tick_event},
                       {mfargs,{gen_event,start_link,[{local,ns_tick_event}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T19:45:37.149Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.357.0>},
                       {name,buckets_events},
                       {mfargs,
                           {gen_event,start_link,[{local,buckets_events}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T19:45:37.149Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.358.0>},
                       {name,ns_stats_event},
                       {mfargs,
                           {gen_event,start_link,[{local,ns_stats_event}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T19:45:37.155Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.359.0>},
                       {name,samples_loader_tasks},
                       {mfargs,{samples_loader_tasks,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-03-27T19:45:37.180Z,ns_1@cb.local:memcached_refresh<0.211.0>:memcached_refresh:handle_cast:55]Refresh of isasl requested
[ns_server:warn,2020-03-27T19:45:37.185Z,ns_1@cb.local:memcached_refresh<0.211.0>:ns_memcached:connect:1101]Unable to connect: {error,{badmatch,{error,econnrefused}}}.
[ns_server:debug,2020-03-27T19:45:37.186Z,ns_1@cb.local:memcached_refresh<0.211.0>:memcached_refresh:handle_info:93]Refresh of [rbac,isasl] failed. Retry in 1000 ms.
[error_logger:info,2020-03-27T19:45:37.192Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_heart_sup}
             started: [{pid,<0.361.0>},
                       {id,ns_heart},
                       {mfargs,{ns_heart,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T19:45:37.192Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_heart_sup}
             started: [{pid,<0.365.0>},
                       {id,ns_heart_slow_updater},
                       {mfargs,{ns_heart,start_link_slow_updater,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T19:45:37.192Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.360.0>},
                       {name,ns_heart_sup},
                       {mfargs,{ns_heart_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[ns_server:debug,2020-03-27T19:45:37.199Z,ns_1@cb.local:ns_heart<0.361.0>:ns_heart:grab_latest_stats:263]Ignoring failure to grab "@system" stats:
{'EXIT',{badarg,[{ets,last,['stats_archiver-@system-minute'],[]},
                 {stats_archiver,latest_sample,2,
                                 [{file,"src/stats_archiver.erl"},{line,120}]},
                 {ns_heart,grab_latest_stats,1,
                           [{file,"src/ns_heart.erl"},{line,259}]},
                 {ns_heart,'-current_status_slow_inner/0-lc$^0/1-0-',1,
                           [{file,"src/ns_heart.erl"},{line,282}]},
                 {ns_heart,current_status_slow_inner,0,
                           [{file,"src/ns_heart.erl"},{line,282}]},
                 {ns_heart,current_status_slow,1,
                           [{file,"src/ns_heart.erl"},{line,250}]},
                 {ns_heart,update_current_status,1,
                           [{file,"src/ns_heart.erl"},{line,187}]},
                 {ns_heart,handle_info,2,
                           [{file,"src/ns_heart.erl"},{line,118}]}]}}

[ns_server:debug,2020-03-27T19:45:37.201Z,ns_1@cb.local:ns_heart<0.361.0>:ns_heart:grab_latest_stats:263]Ignoring failure to grab "@system-processes" stats:
{'EXIT',{badarg,[{ets,last,['stats_archiver-@system-processes-minute'],[]},
                 {stats_archiver,latest_sample,2,
                                 [{file,"src/stats_archiver.erl"},{line,120}]},
                 {ns_heart,grab_latest_stats,1,
                           [{file,"src/ns_heart.erl"},{line,259}]},
                 {ns_heart,'-current_status_slow_inner/0-lc$^0/1-0-',1,
                           [{file,"src/ns_heart.erl"},{line,282}]},
                 {ns_heart,'-current_status_slow_inner/0-lc$^0/1-0-',1,
                           [{file,"src/ns_heart.erl"},{line,282}]},
                 {ns_heart,current_status_slow_inner,0,
                           [{file,"src/ns_heart.erl"},{line,282}]},
                 {ns_heart,current_status_slow,1,
                           [{file,"src/ns_heart.erl"},{line,250}]},
                 {ns_heart,update_current_status,1,
                           [{file,"src/ns_heart.erl"},{line,187}]}]}}

[error_logger:info,2020-03-27T19:45:37.205Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_doctor_sup}
             started: [{pid,<0.369.0>},
                       {id,ns_doctor_events},
                       {mfargs,
                           {gen_event,start_link,[{local,ns_doctor_events}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-03-27T19:45:37.221Z,ns_1@cb.local:<0.366.0>:restartable:start_child:98]Started child process <0.368.0>
  MFA: {ns_doctor_sup,start_link,[]}
[error_logger:info,2020-03-27T19:45:37.222Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_doctor_sup}
             started: [{pid,<0.370.0>},
                       {id,ns_doctor},
                       {mfargs,{ns_doctor,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T19:45:37.222Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.366.0>},
                       {name,ns_doctor_sup},
                       {mfargs,
                           {restartable,start_link,
                               [{ns_doctor_sup,start_link,[]},infinity]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-03-27T19:45:37.224Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.373.0>},
                       {name,master_activity_events},
                       {mfargs,
                           {gen_event,start_link,
                               [{local,master_activity_events}]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[error_logger:info,2020-03-27T19:45:37.234Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.374.0>},
                       {name,xdcr_ckpt_store},
                       {mfargs,{simple_store,start_link,[xdcr_ckpt_data]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T19:45:37.234Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.375.0>},
                       {name,metakv_worker},
                       {mfargs,{work_queue,start_link,[metakv_worker]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T19:45:37.235Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.376.0>},
                       {name,index_events},
                       {mfargs,{gen_event,start_link,[{local,index_events}]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[error_logger:info,2020-03-27T19:45:37.235Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.377.0>},
                       {name,index_settings_manager},
                       {mfargs,{index_settings_manager,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T19:45:37.241Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.379.0>},
                       {name,query_settings_manager},
                       {mfargs,{query_settings_manager,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T19:45:37.254Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.382.0>},
                       {name,eventing_settings_manager},
                       {mfargs,{eventing_settings_manager,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T19:45:37.254Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.384.0>},
                       {name,audit_events},
                       {mfargs,{gen_event,start_link,[{local,audit_events}]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[ns_server:debug,2020-03-27T19:45:37.277Z,ns_1@cb.local:ns_heart<0.361.0>:goxdcr_rest:get_from_goxdcr:140]Goxdcr is temporary not available. Return empty list.
[error_logger:info,2020-03-27T19:45:37.281Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,menelaus_sup}
             started: [{pid,<0.386.0>},
                       {id,menelaus_ui_auth},
                       {mfargs,{menelaus_ui_auth,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T19:45:37.281Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,menelaus_sup}
             started: [{pid,<0.388.0>},
                       {id,scram_sha},
                       {mfargs,{scram_sha,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[ns_server:debug,2020-03-27T19:45:37.286Z,ns_1@cb.local:ns_heart<0.361.0>:cluster_logs_collection_task:maybe_build_cluster_logs_task:46]Ignoring exception trying to read cluster_logs_collection_task_status table: error:badarg
[error_logger:info,2020-03-27T19:45:37.298Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,menelaus_sup}
             started: [{pid,<0.389.0>},
                       {id,menelaus_local_auth},
                       {mfargs,{menelaus_local_auth,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T19:45:37.332Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,menelaus_sup}
             started: [{pid,<0.395.0>},
                       {id,menelaus_web_cache},
                       {mfargs,{menelaus_web_cache,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T19:45:37.354Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,menelaus_sup}
             started: [{pid,<0.397.0>},
                       {id,menelaus_stats_gatherer},
                       {mfargs,{menelaus_stats_gatherer,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[ns_server:info,2020-03-27T19:45:37.356Z,ns_1@cb.local:<0.401.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for cbas
[ns_server:info,2020-03-27T19:45:37.360Z,ns_1@cb.local:<0.401.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for eventing
[ns_server:info,2020-03-27T19:45:37.360Z,ns_1@cb.local:<0.401.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for fts
[ns_server:info,2020-03-27T19:45:37.360Z,ns_1@cb.local:<0.401.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for n1ql
[ns_server:info,2020-03-27T19:45:37.361Z,ns_1@cb.local:<0.401.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for cbas
[error_logger:info,2020-03-27T19:45:37.355Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,menelaus_sup}
             started: [{pid,<0.398.0>},
                       {id,json_rpc_events},
                       {mfargs,
                           {gen_event,start_link,[{local,json_rpc_events}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:info,2020-03-27T19:45:37.368Z,ns_1@cb.local:<0.401.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for eventing
[error_logger:info,2020-03-27T19:45:37.368Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {<0.401.0>,menelaus_web}
             started: [{pid,<0.402.0>},
                       {id,menelaus_web_ipv4},
                       {mfargs,
                           {menelaus_web,http_server,
                               [[{ip,"0.0.0.0"},{name,menelaus_web_ipv4}]]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[ns_server:info,2020-03-27T19:45:37.368Z,ns_1@cb.local:<0.401.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for fts
[ns_server:info,2020-03-27T19:45:37.368Z,ns_1@cb.local:<0.401.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for n1ql
[ns_server:debug,2020-03-27T19:45:37.369Z,ns_1@cb.local:<0.400.0>:restartable:start_child:98]Started child process <0.401.0>
  MFA: {menelaus_web,start_link,[]}
[error_logger:info,2020-03-27T19:45:37.369Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {<0.401.0>,menelaus_web}
             started: [{pid,<0.419.0>},
                       {id,menelaus_web_ipv6},
                       {mfargs,
                           {menelaus_web,http_server,
                               [[{ip,"::"},{name,menelaus_web_ipv6}]]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T19:45:37.369Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,menelaus_sup}
             started: [{pid,<0.400.0>},
                       {id,menelaus_web},
                       {mfargs,
                           {restartable,start_link,
                               [{menelaus_web,start_link,[]},infinity]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[ns_server:debug,2020-03-27T19:45:37.370Z,ns_1@cb.local:ns_heart_slow_status_updater<0.365.0>:ns_heart:grab_latest_stats:263]Ignoring failure to grab "@system" stats:
{'EXIT',{badarg,[{ets,last,['stats_archiver-@system-minute'],[]},
                 {stats_archiver,latest_sample,2,
                                 [{file,"src/stats_archiver.erl"},{line,120}]},
                 {ns_heart,grab_latest_stats,1,
                           [{file,"src/ns_heart.erl"},{line,259}]},
                 {ns_heart,'-current_status_slow_inner/0-lc$^0/1-0-',1,
                           [{file,"src/ns_heart.erl"},{line,282}]},
                 {ns_heart,current_status_slow_inner,0,
                           [{file,"src/ns_heart.erl"},{line,282}]},
                 {ns_heart,current_status_slow,1,
                           [{file,"src/ns_heart.erl"},{line,250}]},
                 {ns_heart,slow_updater_loop,0,
                           [{file,"src/ns_heart.erl"},{line,244}]},
                 {proc_lib,init_p_do_apply,3,
                           [{file,"proc_lib.erl"},{line,247}]}]}}

[ns_server:debug,2020-03-27T19:45:37.371Z,ns_1@cb.local:ns_heart_slow_status_updater<0.365.0>:ns_heart:grab_latest_stats:263]Ignoring failure to grab "@system-processes" stats:
{'EXIT',{badarg,[{ets,last,['stats_archiver-@system-processes-minute'],[]},
                 {stats_archiver,latest_sample,2,
                                 [{file,"src/stats_archiver.erl"},{line,120}]},
                 {ns_heart,grab_latest_stats,1,
                           [{file,"src/ns_heart.erl"},{line,259}]},
                 {ns_heart,'-current_status_slow_inner/0-lc$^0/1-0-',1,
                           [{file,"src/ns_heart.erl"},{line,282}]},
                 {ns_heart,'-current_status_slow_inner/0-lc$^0/1-0-',1,
                           [{file,"src/ns_heart.erl"},{line,282}]},
                 {ns_heart,current_status_slow_inner,0,
                           [{file,"src/ns_heart.erl"},{line,282}]},
                 {ns_heart,current_status_slow,1,
                           [{file,"src/ns_heart.erl"},{line,250}]},
                 {ns_heart,slow_updater_loop,0,
                           [{file,"src/ns_heart.erl"},{line,244}]}]}}

[error_logger:info,2020-03-27T19:45:37.371Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,menelaus_sup}
             started: [{pid,<0.444.0>},
                       {id,menelaus_event},
                       {mfargs,{menelaus_event,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[ns_server:debug,2020-03-27T19:45:37.372Z,ns_1@cb.local:ns_heart_slow_status_updater<0.365.0>:goxdcr_rest:get_from_goxdcr:140]Goxdcr is temporary not available. Return empty list.
[ns_server:debug,2020-03-27T19:45:37.372Z,ns_1@cb.local:ns_heart_slow_status_updater<0.365.0>:cluster_logs_collection_task:maybe_build_cluster_logs_task:46]Ignoring exception trying to read cluster_logs_collection_task_status table: error:badarg
[error_logger:info,2020-03-27T19:45:37.373Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,menelaus_sup}
             started: [{pid,<0.447.0>},
                       {id,hot_keys_keeper},
                       {mfargs,{hot_keys_keeper,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T19:45:37.385Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,menelaus_sup}
             started: [{pid,<0.448.0>},
                       {id,menelaus_web_alerts_srv},
                       {mfargs,{menelaus_web_alerts_srv,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[user:info,2020-03-27T19:45:37.390Z,ns_1@cb.local:ns_server_sup<0.310.0>:menelaus_sup:start_link:48]Couchbase Server has started on web port 8091 on node 'ns_1@cb.local'. Version: "6.5.0-4960-enterprise".
[error_logger:info,2020-03-27T19:45:37.390Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,menelaus_sup}
             started: [{pid,<0.454.0>},
                       {id,menelaus_cbauth},
                       {mfargs,{menelaus_cbauth,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T19:45:37.393Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.385.0>},
                       {name,menelaus},
                       {mfargs,{menelaus_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-03-27T19:45:37.399Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.461.0>},
                       {name,ns_ports_setup},
                       {mfargs,{ns_ports_setup,start,[]}},
                       {restart_type,{permanent,4}},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[error_logger:info,2020-03-27T19:45:37.402Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,service_agent_sup}
             started: [{pid,<0.468.0>},
                       {id,service_agent_children_sup},
                       {mfargs,
                           {supervisor,start_link,
                               [{local,service_agent_children_sup},
                                service_agent_sup,child]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-03-27T19:45:37.403Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,service_agent_sup}
             started: [{pid,<0.469.0>},
                       {id,service_agent_worker},
                       {mfargs,
                           {erlang,apply,
                               [#Fun<service_agent_sup.0.107373856>,[]]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T19:45:37.403Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.467.0>},
                       {name,service_agent_sup},
                       {mfargs,{service_agent_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[ns_server:debug,2020-03-27T19:45:37.419Z,ns_1@cb.local:ns_ports_setup<0.461.0>:ns_ports_manager:set_dynamic_children:54]Setting children [memcached,saslauthd_port,goxdcr]
[error_logger:info,2020-03-27T19:45:37.451Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.471.0>},
                       {name,ns_memcached_sockets_pool},
                       {mfargs,{ns_memcached_sockets_pool,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-03-27T19:45:37.527Z,ns_1@cb.local:memcached_auth_server<0.472.0>:memcached_auth_server:reconnect:233]Skipping creation of 'Auth provider' connection because external users are disabled
[error_logger:info,2020-03-27T19:45:37.527Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.472.0>},
                       {name,memcached_auth_server},
                       {mfargs,{memcached_auth_server,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-03-27T19:45:37.527Z,ns_1@cb.local:ns_audit_cfg<0.475.0>:ns_audit_cfg:write_audit_json:259]Writing new content to "/opt/couchbase/var/lib/couchbase/config/audit.json", Params [{descriptors_path,
                                                                                      "/opt/couchbase/etc/security"},
                                                                                     {version,
                                                                                      1},
                                                                                     {auditd_enabled,
                                                                                      false},
                                                                                     {disabled,
                                                                                      []},
                                                                                     {log_path,
                                                                                      "/opt/couchbase/var/lib/couchbase/logs"},
                                                                                     {rotate_interval,
                                                                                      86400},
                                                                                     {rotate_size,
                                                                                      20971520},
                                                                                     {sync,
                                                                                      []}]
[ns_server:debug,2020-03-27T19:45:37.545Z,ns_1@cb.local:ns_audit_cfg<0.475.0>:ns_audit_cfg:notify_memcached:170]Instruct memcached to reload audit config
[error_logger:info,2020-03-27T19:45:37.546Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.475.0>},
                       {name,ns_audit_cfg},
                       {mfargs,{ns_audit_cfg,start_link,[]}},
                       {restart_type,{permanent,4}},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:warn,2020-03-27T19:45:37.553Z,ns_1@cb.local:<0.478.0>:ns_memcached:connect:1104]Unable to connect: {error,{badmatch,{error,econnrefused}}}, retrying.
[error_logger:info,2020-03-27T19:45:37.569Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.479.0>},
                       {name,ns_audit},
                       {mfargs,{ns_audit,start_link,[]}},
                       {restart_type,{permanent,4}},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-03-27T19:45:37.569Z,ns_1@cb.local:memcached_config_mgr<0.480.0>:memcached_config_mgr:init:49]waiting for completion of initial ns_ports_setup round
[error_logger:info,2020-03-27T19:45:37.571Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.480.0>},
                       {name,memcached_config_mgr},
                       {mfargs,{memcached_config_mgr,start_link,[]}},
                       {restart_type,{permanent,4}},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:info,2020-03-27T19:45:37.585Z,ns_1@cb.local:<0.481.0>:ns_memcached_log_rotator:init:42]Starting log rotator on "/opt/couchbase/var/lib/couchbase/logs"/"memcached.log"* with an initial period of 39003ms
[error_logger:info,2020-03-27T19:45:37.586Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.481.0>},
                       {name,ns_memcached_log_rotator},
                       {mfargs,{ns_memcached_log_rotator,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T19:45:37.598Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.482.0>},
                       {name,testconditions_store},
                       {mfargs,{simple_store,start_link,[testconditions]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T19:45:37.601Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.483.0>},
                       {name,terse_cluster_info_uploader},
                       {mfargs,{terse_cluster_info_uploader,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-03-27T19:45:37.609Z,ns_1@cb.local:terse_cluster_info_uploader<0.483.0>:terse_cluster_info_uploader:handle_info:48]Refreshing terse cluster info with <<"{\"rev\":5,\"nodesExt\":[{\"services\":{\"mgmt\":8091,\"mgmtSSL\":18091,\"kv\":11210,\"kvSSL\":11207,\"capi\":8092,\"capiSSL\":18092,\"projector\":9999,\"projector\":9999},\"thisNode\":true}]}">>
[ns_server:warn,2020-03-27T19:45:37.611Z,ns_1@cb.local:<0.487.0>:ns_memcached:connect:1104]Unable to connect: {error,{badmatch,{error,econnrefused}}}, retrying.
[error_logger:info,2020-03-27T19:45:37.612Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_bucket_worker_sup}
             started: [{pid,<0.488.0>},
                       {id,ns_bucket_sup},
                       {mfargs,{ns_bucket_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-03-27T19:45:37.615Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_bucket_worker_sup}
             started: [{pid,<0.489.0>},
                       {id,ns_bucket_worker},
                       {mfargs,{ns_bucket_worker,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T19:45:37.615Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.485.0>},
                       {name,ns_bucket_worker_sup},
                       {mfargs,{ns_bucket_worker_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-03-27T19:45:37.627Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.491.0>},
                       {name,system_stats_collector},
                       {mfargs,{system_stats_collector,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-03-27T19:45:37.629Z,ns_1@cb.local:ns_ports_setup<0.461.0>:ns_ports_setup:set_children:85]Monitor ns_child_ports_sup <12939.109.0>
[ns_server:debug,2020-03-27T19:45:37.630Z,ns_1@cb.local:memcached_config_mgr<0.480.0>:memcached_config_mgr:init:51]ns_ports_setup seems to be ready
[error_logger:info,2020-03-27T19:45:37.635Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.495.0>},
                       {name,{stats_archiver,"@system"}},
                       {mfargs,{stats_archiver,start_link,["@system"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T19:45:37.647Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.498.0>},
                       {name,{stats_reader,"@system"}},
                       {mfargs,{stats_reader,start_link,["@system"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T19:45:37.650Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.499.0>},
                       {name,{stats_archiver,"@system-processes"}},
                       {mfargs,
                           {stats_archiver,start_link,["@system-processes"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T19:45:37.651Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.501.0>},
                       {name,{stats_reader,"@system-processes"}},
                       {mfargs,
                           {stats_reader,start_link,["@system-processes"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T19:45:37.651Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.502.0>},
                       {name,{stats_archiver,"@query"}},
                       {mfargs,{stats_archiver,start_link,["@query"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T19:45:37.652Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.504.0>},
                       {name,{stats_reader,"@query"}},
                       {mfargs,{stats_reader,start_link,["@query"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-03-27T19:45:37.653Z,ns_1@cb.local:memcached_config_mgr<0.480.0>:memcached_config_mgr:find_port_pid_loop:137]Found memcached port <12939.116.0>
[error_logger:info,2020-03-27T19:45:37.662Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.506.0>},
                       {name,query_stats_collector},
                       {mfargs,{query_stats_collector,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T19:45:37.662Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.508.0>},
                       {name,{stats_archiver,"@global"}},
                       {mfargs,{stats_archiver,start_link,["@global"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T19:45:37.662Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.510.0>},
                       {name,{stats_reader,"@global"}},
                       {mfargs,{stats_reader,start_link,["@global"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T19:45:37.687Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.513.0>},
                       {name,global_stats_collector},
                       {mfargs,{global_stats_collector,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-03-27T19:45:37.705Z,ns_1@cb.local:memcached_config_mgr<0.480.0>:memcached_config_mgr:init:82]wrote memcached config to /opt/couchbase/var/lib/couchbase/config/memcached.json. Will activate memcached port server
[ns_server:debug,2020-03-27T19:45:37.709Z,ns_1@cb.local:memcached_config_mgr<0.480.0>:memcached_config_mgr:init:86]activated memcached port server
[error_logger:info,2020-03-27T19:45:37.715Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.515.0>},
                       {name,goxdcr_status_keeper},
                       {mfargs,{goxdcr_status_keeper,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T19:45:37.722Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,services_stats_sup}
             started: [{pid,<0.518.0>},
                       {id,service_stats_children_sup},
                       {mfargs,
                           {supervisor,start_link,
                               [{local,service_stats_children_sup},
                                services_stats_sup,child]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[ns_server:debug,2020-03-27T19:45:37.724Z,ns_1@cb.local:goxdcr_status_keeper<0.515.0>:goxdcr_rest:get_from_goxdcr:140]Goxdcr is temporary not available. Return empty list.
[ns_server:debug,2020-03-27T19:45:37.726Z,ns_1@cb.local:goxdcr_status_keeper<0.515.0>:goxdcr_rest:get_from_goxdcr:140]Goxdcr is temporary not available. Return empty list.
[error_logger:info,2020-03-27T19:45:37.727Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,service_status_keeper_sup}
             started: [{pid,<0.521.0>},
                       {id,service_status_keeper_worker},
                       {mfargs,
                           {work_queue,start_link,
                               [service_status_keeper_worker]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T19:45:37.738Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,service_status_keeper_sup}
             started: [{pid,<0.522.0>},
                       {id,service_status_keeper_index},
                       {mfargs,{service_index,start_keeper,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T19:45:37.746Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,service_status_keeper_sup}
             started: [{pid,<0.525.0>},
                       {id,service_status_keeper_fts},
                       {mfargs,{service_fts,start_keeper,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T19:45:37.749Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,service_status_keeper_sup}
             started: [{pid,<0.528.0>},
                       {id,service_status_keeper_eventing},
                       {mfargs,{service_eventing,start_keeper,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T19:45:37.750Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,services_stats_sup}
             started: [{pid,<0.520.0>},
                       {id,service_status_keeper_sup},
                       {mfargs,{service_status_keeper_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-03-27T19:45:37.750Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,services_stats_sup}
             started: [{pid,<0.531.0>},
                       {id,service_stats_worker},
                       {mfargs,
                           {erlang,apply,
                               [#Fun<services_stats_sup.0.108537742>,[]]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T19:45:37.750Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.517.0>},
                       {name,services_stats_sup},
                       {mfargs,{services_stats_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[ns_server:debug,2020-03-27T19:45:37.775Z,ns_1@cb.local:<0.535.0>:new_concurrency_throttle:init:115]init concurrent throttle process, pid: <0.535.0>, type: kv_throttle# of available token: 1
[ns_server:debug,2020-03-27T19:45:37.781Z,ns_1@cb.local:compaction_daemon<0.533.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2020-03-27T19:45:37.781Z,ns_1@cb.local:compaction_daemon<0.533.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[error_logger:info,2020-03-27T19:45:37.782Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.533.0>},
                       {name,compaction_daemon},
                       {mfargs,{compaction_daemon,start_link,[]}},
                       {restart_type,{permanent,4}},
                       {shutdown,86400000},
                       {child_type,worker}]

[ns_server:debug,2020-03-27T19:45:37.782Z,ns_1@cb.local:compaction_daemon<0.533.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2020-03-27T19:45:37.782Z,ns_1@cb.local:compaction_daemon<0.533.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2020-03-27T19:45:37.782Z,ns_1@cb.local:compaction_daemon<0.533.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_master. Rescheduling compaction.
[ns_server:debug,2020-03-27T19:45:37.782Z,ns_1@cb.local:compaction_daemon<0.533.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_master too soon. Next run will be in 3600s
[error_logger:info,2020-03-27T19:45:37.790Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,cluster_logs_sup}
             started: [{pid,<0.537.0>},
                       {id,ets_holder},
                       {mfargs,
                           {cluster_logs_collection_task,
                               start_link_ets_holder,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T19:45:37.790Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.536.0>},
                       {name,cluster_logs_sup},
                       {mfargs,{cluster_logs_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-03-27T19:45:37.790Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.538.0>},
                       {name,leader_events},
                       {mfargs,{gen_event,start_link,[{local,leader_events}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T19:45:37.814Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,leader_leases_sup}
             started: [{pid,<0.542.0>},
                       {id,leader_activities},
                       {mfargs,{leader_activities,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,10000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T19:45:37.834Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,leader_leases_sup}
             started: [{pid,<0.543.0>},
                       {id,leader_lease_agent},
                       {mfargs,{leader_lease_agent,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T19:45:37.834Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,leader_services_sup}
             started: [{pid,<0.541.0>},
                       {id,leader_leases_sup},
                       {mfargs,{leader_leases_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-03-27T19:45:37.843Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,leader_registry_sup}
             started: [{pid,<0.545.0>},
                       {id,leader_registry_server},
                       {mfargs,{leader_registry_server,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-03-27T19:45:37.847Z,ns_1@cb.local:leader_registry_sup<0.544.0>:mb_master:check_master_takeover_needed:283]Sending master node question to the following nodes: []
[ns_server:debug,2020-03-27T19:45:37.847Z,ns_1@cb.local:leader_registry_sup<0.544.0>:mb_master:check_master_takeover_needed:285]Got replies: []
[ns_server:debug,2020-03-27T19:45:37.847Z,ns_1@cb.local:leader_registry_sup<0.544.0>:mb_master:check_master_takeover_needed:291]Was unable to discover master, not going to force mastership takeover
[user:info,2020-03-27T19:45:37.864Z,ns_1@cb.local:mb_master<0.548.0>:mb_master:init:103]I'm the only node, so I'm the master.
[ns_server:debug,2020-03-27T19:45:37.864Z,ns_1@cb.local:leader_registry<0.545.0>:leader_registry_server:handle_new_leader:241]New leader is 'ns_1@cb.local'. Invalidating name cache.
[ns_server:debug,2020-03-27T19:45:37.905Z,ns_1@cb.local:mb_master<0.548.0>:master_activity_events:submit_cast:82]Failed to send master activity event: {error,badarg}
[ns_server:debug,2020-03-27T19:45:37.909Z,ns_1@cb.local:leader_lease_acquirer<0.551.0>:leader_utils:wait_cluster_is_55:54]Delaying start since cluster is not fully upgraded to 5.5 yet.
[error_logger:info,2020-03-27T19:45:37.909Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,mb_master_sup}
             started: [{pid,<0.551.0>},
                       {id,leader_lease_acquirer},
                       {mfargs,{leader_lease_acquirer,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,10000},
                       {child_type,worker}]

[ns_server:debug,2020-03-27T19:45:37.923Z,ns_1@cb.local:leader_quorum_nodes_manager<0.553.0>:leader_utils:wait_cluster_is_55:54]Delaying start since cluster is not fully upgraded to 5.5 yet.
[error_logger:info,2020-03-27T19:45:37.923Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,mb_master_sup}
             started: [{pid,<0.553.0>},
                       {id,leader_quorum_nodes_manager},
                       {mfargs,{leader_quorum_nodes_manager,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:info,2020-03-27T19:45:37.931Z,ns_1@cb.local:mb_master_sup<0.550.0>:misc:start_singleton:857]start_singleton(gen_server, start_link, [{via,leader_registry,ns_tick},
                                         ns_tick,[],[]]): started as <0.555.0> on 'ns_1@cb.local'

[error_logger:info,2020-03-27T19:45:37.931Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,mb_master_sup}
             started: [{pid,<0.555.0>},
                       {id,ns_tick},
                       {mfargs,{ns_tick,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,10},
                       {child_type,worker}]

[error_logger:info,2020-03-27T19:45:37.937Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_orchestrator_sup}
             started: [{pid,<0.557.0>},
                       {id,compat_mode_events},
                       {mfargs,
                           {gen_event,start_link,
                               [{local,compat_mode_events}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-03-27T19:45:37.952Z,ns_1@cb.local:memcached_refresh<0.211.0>:memcached_refresh:handle_info:89]Refresh of [rbac,isasl] succeeded
[ns_server:debug,2020-03-27T19:45:37.954Z,ns_1@cb.local:ns_config<0.193.0>:ns_config:do_upgrade_config:757]Upgrading config by changes:
[{set,cluster_compat_version,[5,0]}]

[ns_server:info,2020-03-27T19:45:37.954Z,ns_1@cb.local:ns_config<0.193.0>:ns_online_config_upgrader:do_upgrade_config:46]Performing online config upgrade to [5,1]
[ns_server:debug,2020-03-27T19:45:37.955Z,ns_1@cb.local:ns_config<0.193.0>:ns_config:do_upgrade_config:757]Upgrading config by changes:
[{set,cluster_compat_version,[5,1]},
 {set,client_cert_auth,[{state,"disable"},{prefixes,[]}]},
 {set,buckets,[{configs,[]}]}]

[ns_server:info,2020-03-27T19:45:37.962Z,ns_1@cb.local:ns_config<0.193.0>:ns_online_config_upgrader:do_upgrade_config:46]Performing online config upgrade to [5,5]
[ns_server:debug,2020-03-27T19:45:37.962Z,ns_1@cb.local:ns_config<0.193.0>:ns_config:do_upgrade_config:757]Upgrading config by changes:
[{set,cluster_compat_version,[5,5]},
 {set,auto_failover_cfg,
      [{enabled,true},
       {timeout,120},
       {count,0},
       {failover_on_data_disk_issues,[{enabled,false},{timePeriod,120}]},
       {failover_server_group,false},
       {max_count,1},
       {failed_over_server_groups,[]}]},
 {set,{metakv,<<"/query/settings/config">>},
      <<"{\"query.settings.curl_whitelist\":{\"all_access\":false,\"allowed_urls\":[],\"disallowed_urls\":[]},\"query.settings.tmp_space_dir\":\"/opt/couchbase/var/lib/couchbase/tmp\",\"query.settings.tmp_space_size\":5120}">>},
 {set,{metakv,<<"/eventing/settings/config">>},<<"{\"ram_quota\":256}">>},
 {set,buckets,[{configs,[]}]},
 {delete,{rbac_upgrade,[5,5]}},
 {set,audit,
      [{enabled,[]},
       {disabled_users,[]},
       {auditd_enabled,false},
       {rotate_interval,86400},
       {rotate_size,20971520},
       {disabled,[]},
       {sync,[]},
       {log_path,"/opt/couchbase/var/lib/couchbase/logs"}]},
 {set,quorum_nodes,['ns_1@cb.local']},
 {set,scramsha_fallback_salt,<<176,211,90,3,15,137,19,29,193,47,34,182>>}]

[ns_server:info,2020-03-27T19:45:37.962Z,ns_1@cb.local:ns_config<0.193.0>:ns_online_config_upgrader:do_upgrade_config:46]Performing online config upgrade to [6,0]
[ns_server:debug,2020-03-27T19:45:37.962Z,ns_1@cb.local:ns_config<0.193.0>:ns_config:do_upgrade_config:757]Upgrading config by changes:
[{set,cluster_compat_version,[6,0]}]

[ns_server:info,2020-03-27T19:45:37.968Z,ns_1@cb.local:ns_config<0.193.0>:ns_online_config_upgrader:do_upgrade_config:46]Performing online config upgrade to [6,5]
[ns_server:debug,2020-03-27T19:45:37.978Z,ns_1@cb.local:ns_config<0.193.0>:ns_config:do_upgrade_config:757]Upgrading config by changes:
[{set,cluster_compat_version,[6,5]},
 {set,audit_decriptors,
      [{8243,
        [{name,<<"mutate document">>},
         {description,<<"Document was mutated via the REST API">>},
         {enabled,true},
         {module,ns_server}]},
       {8255,
        [{name,<<"read document">>},
         {description,<<"Document was read via the REST API">>},
         {enabled,false},
         {module,ns_server}]},
       {8257,
        [{name,<<"alert email sent">>},
         {description,<<"An alert email was successfully sent">>},
         {enabled,true},
         {module,ns_server}]},
       {20480,
        [{name,<<"opened DCP connection">>},
         {description,<<"opened DCP connection">>},
         {enabled,true},
         {module,memcached}]},
       {20482,
        [{name,<<"external memcached bucket flush">>},
         {description,<<"External user flushed the content of a memcached bucket">>},
         {enabled,true},
         {module,memcached}]},
       {20483,
        [{name,<<"invalid packet">>},
         {description,<<"Rejected an invalid packet">>},
         {enabled,true},
         {module,memcached}]},
       {20485,
        [{name,<<"authentication succeeded">>},
         {description,<<"Authentication to the cluster succeeded">>},
         {enabled,false},
         {module,memcached}]},
       {20488,
        [{name,<<"document read">>},
         {description,<<"Document was read">>},
         {enabled,false},
         {module,memcached}]},
       {20489,
        [{name,<<"document locked">>},
         {description,<<"Document was locked">>},
         {enabled,false},
         {module,memcached}]},
       {20490,
        [{name,<<"document modify">>},
         {description,<<"Document was modified">>},
         {enabled,false},
         {module,memcached}]},
       {20491,
        [{name,<<"document delete">>},
         {description,<<"Document was deleted">>},
         {enabled,false},
         {module,memcached}]},
       {20492,
        [{name,<<"select bucket">>},
         {description,<<"The specified bucket was selected">>},
         {enabled,true},
         {module,memcached}]},
       {28672,
        [{name,<<"SELECT statement">>},
         {description,<<"A N1QL SELECT statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28673,
        [{name,<<"EXPLAIN statement">>},
         {description,<<"A N1QL EXPLAIN statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28674,
        [{name,<<"PREPARE statement">>},
         {description,<<"A N1QL PREPARE statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28675,
        [{name,<<"INFER statement">>},
         {description,<<"A N1QL INFER statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28676,
        [{name,<<"INSERT statement">>},
         {description,<<"A N1QL INSERT statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28677,
        [{name,<<"UPSERT statement">>},
         {description,<<"A N1QL UPSERT statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28678,
        [{name,<<"DELETE statement">>},
         {description,<<"A N1QL DELETE statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28679,
        [{name,<<"UPDATE statement">>},
         {description,<<"A N1QL UPDATE statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28680,
        [{name,<<"MERGE statement">>},
         {description,<<"A N1QL MERGE statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28681,
        [{name,<<"CREATE INDEX statement">>},
         {description,<<"A N1QL CREATE INDEX statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28682,
        [{name,<<"DROP INDEX statement">>},
         {description,<<"A N1QL DROP INDEX statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28683,
        [{name,<<"ALTER INDEX statement">>},
         {description,<<"A N1QL ALTER INDEX statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28684,
        [{name,<<"BUILD INDEX statement">>},
         {description,<<"A N1QL BUILD INDEX statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28685,
        [{name,<<"GRANT ROLE statement">>},
         {description,<<"A N1QL GRANT ROLE statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28686,
        [{name,<<"REVOKE ROLE statement">>},
         {description,<<"A N1QL REVOKE ROLE statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28687,
        [{name,<<"UNRECOGNIZED statement">>},
         {description,<<"An unrecognized statement was received by the N1QL query engine">>},
         {enabled,false},
         {module,n1ql}]},
       {28688,
        [{name,<<"CREATE PRIMARY INDEX statement">>},
         {description,<<"A N1QL CREATE PRIMARY INDEX statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28689,
        [{name,<<"/admin/stats API request">>},
         {description,<<"An HTTP request was made to the API at /admin/stats.">>},
         {enabled,false},
         {module,n1ql}]},
       {28690,
        [{name,<<"/admin/vitals API request">>},
         {description,<<"An HTTP request was made to the API at /admin/vitals.">>},
         {enabled,false},
         {module,n1ql}]},
       {28691,
        [{name,<<"/admin/prepareds API request">>},
         {description,<<"An HTTP request was made to the API at /admin/prepareds.">>},
         {enabled,false},
         {module,n1ql}]},
       {28692,
        [{name,<<"/admin/active_requests API request">>},
         {description,<<"An HTTP request was made to the API at /admin/active_requests.">>},
         {enabled,false},
         {module,n1ql}]},
       {28693,
        [{name,<<"/admin/indexes/prepareds API request">>},
         {description,<<"An HTTP request was made to the API at /admin/indexes/prepareds.">>},
         {enabled,false},
         {module,n1ql}]},
       {28694,
        [{name,<<"/admin/indexes/active_requests API request">>},
         {description,<<"An HTTP request was made to the API at /admin/indexes/active_requests.">>},
         {enabled,false},
         {module,n1ql}]},
       {28695,
        [{name,<<"/admin/indexes/completed_requests API request">>},
         {description,<<"An HTTP request was made to the API at /admin/indexes/completed_requests.">>},
         {enabled,false},
         {module,n1ql}]},
       {28697,
        [{name,<<"/admin/ping API request">>},
         {description,<<"An HTTP request was made to the API at /admin/ping.">>},
         {enabled,false},
         {module,n1ql}]},
       {28698,
        [{name,<<"/admin/config API request">>},
         {description,<<"An HTTP request was made to the API at /admin/config.">>},
         {enabled,false},
         {module,n1ql}]},
       {28699,
        [{name,<<"/admin/ssl_cert API request">>},
         {description,<<"An HTTP request was made to the API at /admin/ssl_cert.">>},
         {enabled,false},
         {module,n1ql}]},
       {28700,
        [{name,<<"/admin/settings API request">>},
         {description,<<"An HTTP request was made to the API at /admin/settings.">>},
         {enabled,false},
         {module,n1ql}]},
       {28701,
        [{name,<<"/admin/clusters API request">>},
         {description,<<"An HTTP request was made to the API at /admin/clusters.">>},
         {enabled,false},
         {module,n1ql}]},
       {28702,
        [{name,<<"/admin/completed_requests API request">>},
         {description,<<"An HTTP request was made to the API at /admin/completed_requests.">>},
         {enabled,false},
         {module,n1ql}]},
       {28704,
        [{name,<<"/admin/functions API request">>},
         {description,<<"An HTTP request was made to the API at /admin/functions.">>},
         {enabled,false},
         {module,n1ql}]},
       {28705,
        [{name,<<"/admin/indexes/functions API request">>},
         {description,<<"An HTTP request was made to the API at /admin/indexes/functions.">>},
         {enabled,false},
         {module,n1ql}]},
       {32768,
        [{name,<<"Create Function">>},
         {description,<<"Eventing function definition was created or updated">>},
         {enabled,true},
         {module,eventing}]},
       {32769,
        [{name,<<"Delete Function">>},
         {description,<<"Eventing function definition was deleted">>},
         {enabled,true},
         {module,eventing}]},
       {32770,
        [{name,<<"Fetch Functions">>},
         {description,<<"Eventing function definition was read">>},
         {enabled,false},
         {module,eventing}]},
       {32771,
        [{name,<<"List Deployed">>},
         {description,<<"Eventing deployed functions list was read">>},
         {enabled,false},
         {module,eventing}]},
       {32772,
        [{name,<<"Fetch Drafts">>},
         {description,<<"Eventing function draft definitions were read">>},
         {enabled,false},
         {module,eventing}]},
       {32773,
        [{name,<<"Delete Drafts">>},
         {description,<<"Eventing function draft definitions were deleted">>},
         {enabled,true},
         {module,eventing}]},
       {32774,
        [{name,<<"Save Draft">>},
         {description,<<"Save a draft definition to the store">>},
         {enabled,true},
         {module,eventing}]},
       {32775,
        [{name,<<"Start Debug">>},
         {description,<<"Start eventing function debugger">>},
         {enabled,true},
         {module,eventing}]},
       {32776,
        [{name,<<"Stop Debug">>},
         {description,<<"Stop eventing function debugger">>},
         {enabled,true},
         {module,eventing}]},
       {32777,
        [{name,<<"Start Tracing">>},
         {description,<<"Start tracing eventing function execution">>},
         {enabled,true},
         {module,eventing}]},
       {32778,
        [{name,<<"Stop Tracing">>},
         {description,<<"Stop tracing eventing function execution">>},
         {enabled,true},
         {module,eventing}]},
       {32779,
        [{name,<<"Set Settings">>},
         {description,<<"Save settings for a given app">>},
         {enabled,true},
         {module,eventing}]},
       {32780,
        [{name,<<"Fetch Config">>},
         {description,<<"Get config for eventing">>},
         {enabled,false},
         {module,eventing}]},
       {32781,
        [{name,<<"Save Config">>},
         {description,<<"Save config for eventing">>},
         {enabled,true},
         {module,eventing}]},
       {32782,
        [{name,<<"Cleanup Eventing">>},
         {description,<<"Clears up app definitions and settings from metakv">>},
         {enabled,true},
         {module,eventing}]},
       {32783,
        [{name,<<"Get Settings">>},
         {description,<<"Get settings for a given app">>},
         {enabled,false},
         {module,eventing}]},
       {32784,
        [{name,<<"Import Functions">>},
         {description,<<"Import a list of functions">>},
         {enabled,false},
         {module,eventing}]},
       {32785,
        [{name,<<"Export Functions">>},
         {description,<<"Export the list of functions">>},
         {enabled,false},
         {module,eventing}]},
       {32786,
        [{name,<<"List Running">>},
         {description,<<"Eventing running function list was read">>},
         {enabled,false},
         {module,eventing}]},
       {36865,
        [{name,<<"Service configuration change">>},
         {description,<<"A successful service configuration change was made.">>},
         {enabled,true},
         {module,analytics}]},
       {36866,
        [{name,<<"Node configuration change">>},
         {description,<<"A successful node configuration change was made.">>},
         {enabled,true},
         {module,analytics}]},
       {40960,
        [{name,<<"Create Design Doc">>},
         {description,<<"Design Doc is Created">>},
         {enabled,true},
         {module,view_engine}]},
       {40961,
        [{name,<<"Delete Design Doc">>},
         {description,<<"Design Doc is Deleted">>},
         {enabled,true},
         {module,view_engine}]},
       {40962,
        [{name,<<"Query DDoc Meta Data">>},
         {description,<<"Design Doc Meta Data Query Request">>},
         {enabled,true},
         {module,view_engine}]},
       {40963,
        [{name,<<"View Query">>},
         {description,<<"View Query Request">>},
         {enabled,false},
         {module,view_engine}]},
       {40964,
        [{name,<<"Update Design Doc">>},
         {description,<<"Design Doc is Updated">>},
         {enabled,true},
         {module,view_engine}]}]},
 {set,auto_failover_cfg,
      [{enabled,true},
       {timeout,120},
       {count,0},
       {failover_on_data_disk_issues,[{enabled,false},{timePeriod,120}]},
       {failover_server_group,false},
       {max_count,1},
       {failed_over_server_groups,[]},
       {can_abort_rebalance,true}]},
 {set,max_bucket_count,30},
 {set,retry_rebalance,
      [{enabled,false},{after_time_period,300},{max_attempts,1}]},
 {set,{metakv,<<"/query/settings/config">>},
      <<"{\"timeout\":0,\"n1ql-feat-ctrl\":12,\"max-parallelism\":1,\"query.settings.curl_whitelist\":{\"all_access\":false,\"allowed_urls\":[],\"disallowed_urls\":[]},\"query.settings.tmp_space_dir\":\"/opt/couchbase/var/lib/couchbase/tmp\",\"completed-limit\":4000,\"prepared-limit\":16384,\"pipeline-batch\":16,\"pipeline-cap\":512,\"scan-cap\":512,\"loglevel\":\"info\",\"completed-threshold\":1000,\"query.settings.tmp_space_size\":5120}">>}]

[ns_server:debug,2020-03-27T19:45:37.980Z,ns_1@cb.local:compiled_roles_cache<0.269.0>:versioned_cache:handle_info:92]Flushing cache compiled_roles_cache due to version change from {undefined,
                                                                {0,4055119168},
                                                                {0,4055119168},
                                                                false,[]} to {[6,
                                                                               5],
                                                                              {0,
                                                                               4055119168},
                                                                              {0,
                                                                               4055119168},
                                                                              false,
                                                                              []}
[ns_server:debug,2020-03-27T19:45:37.982Z,ns_1@cb.local:leader_quorum_nodes_manager<0.553.0>:leader_utils:wait_cluster_is_55_loop:78]Cluster upgraded to 5.5. Starting.
[ns_server:debug,2020-03-27T19:45:37.982Z,ns_1@cb.local:leader_lease_acquirer<0.551.0>:leader_utils:wait_cluster_is_55_loop:78]Cluster upgraded to 5.5. Starting.
[ns_server:debug,2020-03-27T19:45:37.982Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
retry_rebalance ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557537}}]},
 {enabled,false},
 {after_time_period,300},
 {max_attempts,1}]
[ns_server:debug,2020-03-27T19:45:37.982Z,ns_1@cb.local:memcached_permissions<0.321.0>:memcached_cfg:write_cfg:118]Writing config file for: "/opt/couchbase/var/lib/couchbase/config/memcached.rbac"
[ns_server:debug,2020-03-27T19:45:37.986Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
audit_decriptors ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557537}}]},
 {8243,
  [{name,<<"mutate document">>},
   {description,<<"Document was mutated via the REST API">>},
   {enabled,true},
   {module,ns_server}]},
 {8255,
  [{name,<<"read document">>},
   {description,<<"Document was read via the REST API">>},
   {enabled,false},
   {module,ns_server}]},
 {8257,
  [{name,<<"alert email sent">>},
   {description,<<"An alert email was successfully sent">>},
   {enabled,true},
   {module,ns_server}]},
 {20480,
  [{name,<<"opened DCP connection">>},
   {description,<<"opened DCP connection">>},
   {enabled,true},
   {module,memcached}]},
 {20482,
  [{name,<<"external memcached bucket flush">>},
   {description,<<"External user flushed the content of a memcached bucket">>},
   {enabled,true},
   {module,memcached}]},
 {20483,
  [{name,<<"invalid packet">>},
   {description,<<"Rejected an invalid packet">>},
   {enabled,true},
   {module,memcached}]},
 {20485,
  [{name,<<"authentication succeeded">>},
   {description,<<"Authentication to the cluster succeeded">>},
   {enabled,false},
   {module,memcached}]},
 {20488,
  [{name,<<"document read">>},
   {description,<<"Document was read">>},
   {enabled,false},
   {module,memcached}]},
 {20489,
  [{name,<<"document locked">>},
   {description,<<"Document was locked">>},
   {enabled,false},
   {module,memcached}]},
 {20490,
  [{name,<<"document modify">>},
   {description,<<"Document was modified">>},
   {enabled,false},
   {module,memcached}]},
 {20491,
  [{name,<<"document delete">>},
   {description,<<"Document was deleted">>},
   {enabled,false},
   {module,memcached}]},
 {20492,
  [{name,<<"select bucket">>},
   {description,<<"The specified bucket was selected">>},
   {enabled,true},
   {module,memcached}]},
 {28672,
  [{name,<<"SELECT statement">>},
   {description,<<"A N1QL SELECT statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28673,
  [{name,<<"EXPLAIN statement">>},
   {description,<<"A N1QL EXPLAIN statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28674,
  [{name,<<"PREPARE statement">>},
   {description,<<"A N1QL PREPARE statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28675,
  [{name,<<"INFER statement">>},
   {description,<<"A N1QL INFER statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28676,
  [{name,<<"INSERT statement">>},
   {description,<<"A N1QL INSERT statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28677,
  [{name,<<"UPSERT statement">>},
   {description,<<"A N1QL UPSERT statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28678,
  [{name,<<"DELETE statement">>},
   {description,<<"A N1QL DELETE statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28679,
  [{name,<<"UPDATE statement">>},
   {description,<<"A N1QL UPDATE statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28680,
  [{name,<<"MERGE statement">>},
   {description,<<"A N1QL MERGE statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28681,
  [{name,<<"CREATE INDEX statement">>},
   {description,<<"A N1QL CREATE INDEX statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28682,
  [{name,<<"DROP INDEX statement">>},
   {description,<<"A N1QL DROP INDEX statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28683,
  [{name,<<"ALTER INDEX statement">>},
   {description,<<"A N1QL ALTER INDEX statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28684,
  [{name,<<"BUILD INDEX statement">>},
   {description,<<"A N1QL BUILD INDEX statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28685,
  [{name,<<"GRANT ROLE statement">>},
   {description,<<"A N1QL GRANT ROLE statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28686,
  [{name,<<"REVOKE ROLE statement">>},
   {description,<<"A N1QL REVOKE ROLE statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28687,
  [{name,<<"UNRECOGNIZED statement">>},
   {description,<<"An unrecognized statement was received by the N1QL query engine">>},
   {enabled,false},
   {module,n1ql}]},
 {28688,
  [{name,<<"CREATE PRIMARY INDEX statement">>},
   {description,<<"A N1QL CREATE PRIMARY INDEX statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28689,
  [{name,<<"/admin/stats API request">>},
   {description,<<"An HTTP request was made to the API at /admin/stats.">>},
   {enabled,false},
   {module,n1ql}]},
 {28690,
  [{name,<<"/admin/vitals API request">>},
   {description,<<"An HTTP request was made to the API at /admin/vitals.">>},
   {enabled,false},
   {module,n1ql}]},
 {28691,
  [{name,<<"/admin/prepareds API request">>},
   {description,<<"An HTTP request was made to the API at /admin/prepareds.">>},
   {enabled,false},
   {module,n1ql}]},
 {28692,
  [{name,<<"/admin/active_requests API request">>},
   {description,<<"An HTTP request was made to the API at /admin/active_requests.">>},
   {enabled,false},
   {module,n1ql}]},
 {28693,
  [{name,<<"/admin/indexes/prepareds API request">>},
   {description,<<"An HTTP request was made to the API at /admin/indexes/prepareds.">>},
   {enabled,false},
   {module,n1ql}]},
 {28694,
  [{name,<<"/admin/indexes/active_requests API request">>},
   {description,<<"An HTTP request was made to the API at /admin/indexes/active_requests.">>},
   {enabled,false},
   {module,n1ql}]},
 {28695,
  [{name,<<"/admin/indexes/completed_requests API request">>},
   {description,<<"An HTTP request was made to the API at /admin/indexes/completed_requests.">>},
   {enabled,false},
   {module,n1ql}]},
 {28697,
  [{name,<<"/admin/ping API request">>},
   {description,<<"An HTTP request was made to the API at /admin/ping.">>},
   {enabled,false},
   {module,n1ql}]},
 {28698,
  [{name,<<"/admin/config API request">>},
   {description,<<"An HTTP request was made to the API at /admin/config.">>},
   {enabled,false},
   {module,n1ql}]},
 {28699,
  [{name,<<"/admin/ssl_cert API request">>},
   {description,<<"An HTTP request was made to the API at /admin/ssl_cert.">>},
   {enabled,false},
   {module,n1ql}]},
 {28700,
  [{name,<<"/admin/settings API request">>},
   {description,<<"An HTTP request was made to the API at /admin/settings.">>},
   {enabled,false},
   {module,n1ql}]},
 {28701,
  [{name,<<"/admin/clusters API request">>},
   {description,<<"An HTTP request was made to the API at /admin/clusters.">>},
   {enabled,false},
   {module,n1ql}]},
 {28702,
  [{name,<<"/admin/completed_requests API request">>},
   {description,<<"An HTTP request was made to the API at /admin/completed_requests.">>},
   {enabled,false},
   {module,n1ql}]},
 {28704,
  [{name,<<"/admin/functions API request">>},
   {description,<<"An HTTP request was made to the API at /admin/functions.">>},
   {enabled,false},
   {module,n1ql}]},
 {28705,
  [{name,<<"/admin/indexes/functions API request">>},
   {description,<<"An HTTP request was made to the API at /admin/indexes/functions.">>},
   {enabled,false},
   {module,n1ql}]},
 {32768,
  [{name,<<"Create Function">>},
   {description,<<"Eventing function definition was created or updated">>},
   {enabled,true},
   {module,eventing}]},
 {32769,
  [{name,<<"Delete Function">>},
   {description,<<"Eventing function definition was deleted">>},
   {enabled,true},
   {module,eventing}]},
 {32770,
  [{name,<<"Fetch Functions">>},
   {description,<<"Eventing function definition was read">>},
   {enabled,false},
   {module,eventing}]},
 {32771,
  [{name,<<"List Deployed">>},
   {description,<<"Eventing deployed functions list was read">>},
   {enabled,false},
   {module,eventing}]},
 {32772,
  [{name,<<"Fetch Drafts">>},
   {description,<<"Eventing function draft definitions were read">>},
   {enabled,false},
   {module,eventing}]},
 {32773,
  [{name,<<"Delete Drafts">>},
   {description,<<"Eventing function draft definitions were deleted">>},
   {enabled,true},
   {module,eventing}]},
 {32774,
  [{name,<<"Save Draft">>},
   {description,<<"Save a draft definition to the store">>},
   {enabled,true},
   {module,eventing}]},
 {32775,
  [{name,<<"Start Debug">>},
   {description,<<"Start eventing function debugger">>},
   {enabled,true},
   {module,eventing}]},
 {32776,
  [{name,<<"Stop Debug">>},
   {description,<<"Stop eventing function debugger">>},
   {enabled,true},
   {module,eventing}]},
 {32777,
  [{name,<<"Start Tracing">>},
   {description,<<"Start tracing eventing function execution">>},
   {enabled,true},
   {module,eventing}]},
 {32778,
  [{name,<<"Stop Tracing">>},
   {description,<<"Stop tracing eventing function execution">>},
   {enabled,true},
   {module,eventing}]},
 {32779,
  [{name,<<"Set Settings">>},
   {description,<<"Save settings for a given app">>},
   {enabled,true},
   {module,eventing}]},
 {32780,
  [{name,<<"Fetch Config">>},
   {description,<<"Get config for eventing">>},
   {enabled,false},
   {module,eventing}]},
 {32781,
  [{name,<<"Save Config">>},
   {description,<<"Save config for eventing">>},
   {enabled,true},
   {module,eventing}]},
 {32782,
  [{name,<<"Cleanup Eventing">>},
   {description,<<"Clears up app definitions and settings from metakv">>},
   {enabled,true},
   {module,eventing}]},
 {32783,
  [{name,<<"Get Settings">>},
   {description,<<"Get settings for a given app">>},
   {enabled,false},
   {module,eventing}]},
 {32784,
  [{name,<<"Import Functions">>},
   {description,<<"Import a list of functions">>},
   {enabled,false},
   {module,eventing}]},
 {32785,
  [{name,<<"Export Functions">>},
   {description,<<"Export the list of functions">>},
   {enabled,false},
   {module,eventing}]},
 {32786,
  [{name,<<"List Running">>},
   {description,<<"Eventing running function list was read">>},
   {enabled,false},
   {module,eventing}]},
 {36865,
  [{name,<<"Service configuration change">>},
   {description,<<"A successful service configuration change was made.">>},
   {enabled,true},
   {module,analytics}]},
 {36866,
  [{name,<<"Node configuration change">>},
   {description,<<"A successful node configuration change was made.">>},
   {enabled,true},
   {module,analytics}]},
 {40960,
  [{name,<<"Create Design Doc">>},
   {description,<<"Design Doc is Created">>},
   {enabled,true},
   {module,view_engine}]},
 {40961,
  [{name,<<"Delete Design Doc">>},
   {description,<<"Design Doc is Deleted">>},
   {enabled,true},
   {module,view_engine}]},
 {40962,
  [{name,<<"Query DDoc Meta Data">>},
   {description,<<"Design Doc Meta Data Query Request">>},
   {enabled,true},
   {module,view_engine}]},
 {40963,
  [{name,<<"View Query">>},
   {description,<<"View Query Request">>},
   {enabled,false},
   {module,view_engine}]},
 {40964,
  [{name,<<"Update Design Doc">>},
   {description,<<"Design Doc is Updated">>},
   {enabled,true},
   {module,view_engine}]}]
[ns_server:debug,2020-03-27T19:45:37.986Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
scramsha_fallback_salt ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557537}}]}|
 <<176,211,90,3,15,137,19,29,193,47,34,182>>]
[ns_server:debug,2020-03-27T19:45:37.986Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{metakv,<<"/eventing/settings/config">>} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557537}}]}|
 <<"{\"ram_quota\":256}">>]
[ns_server:debug,2020-03-27T19:45:37.986Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{metakv,<<"/query/settings/config">>} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557537}}]}|
 <<"{\"timeout\":0,\"n1ql-feat-ctrl\":12,\"max-parallelism\":1,\"query.settings.curl_whitelist\":{\"all_access\":false,\"allowed_urls\":[],\"disallowed_urls\":[]},\"query.settings.tmp_space_dir\":\"/opt/couchbase/var/lib/couchbase/tmp\",\"completed-limit\":4000,\"prepared-limit\":16384,\"pipeline-batch\":16,\"pipeline-cap\":512,\"scan-cap\":512,\"loglevel\":\"info\",\"completed-threshold\":1000,\"query.settings.tmp_space_si"...>>]
[ns_server:debug,2020-03-27T19:45:37.988Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
client_cert_auth ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557537}}]},
 {state,"disable"},
 {prefixes,[]}]
[ns_server:debug,2020-03-27T19:45:37.988Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
cluster_compat_version ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{5,63752557537}}]},6,5]
[ns_server:debug,2020-03-27T19:45:37.988Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
audit ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557537}}]},
 {enabled,[]},
 {disabled_users,[]},
 {auditd_enabled,false},
 {rotate_interval,86400},
 {rotate_size,20971520},
 {disabled,[]},
 {sync,[]},
 {log_path,"/opt/couchbase/var/lib/couchbase/logs"}]
[ns_server:debug,2020-03-27T19:45:37.989Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
auto_failover_cfg ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557537}}]},
 {enabled,true},
 {timeout,120},
 {count,0},
 {failover_on_data_disk_issues,[{enabled,false},{timePeriod,120}]},
 {failover_server_group,false},
 {max_count,1},
 {failed_over_server_groups,[]},
 {can_abort_rebalance,true}]
[ns_server:debug,2020-03-27T19:45:37.989Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
buckets ->
[[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557537}}],{configs,[]}]
[ns_server:debug,2020-03-27T19:45:37.989Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
max_bucket_count ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557537}}]}|30]
[ns_server:debug,2020-03-27T19:45:37.989Z,ns_1@cb.local:leader_quorum_nodes_manager<0.553.0>:leader_quorum_nodes_manager:pull_config:114]Attempting to pull config from nodes:
[]
[ns_server:debug,2020-03-27T19:45:37.989Z,ns_1@cb.local:ns_config_rep<0.335.0>:ns_config_rep:do_push_keys:321]Replicating some config keys ([audit,audit_decriptors,auto_failover_cfg,
                               buckets,client_cert_auth,
                               cluster_compat_version,max_bucket_count,
                               quorum_nodes,retry_rebalance,
                               scramsha_fallback_salt,
                               {local_changes_count,
                                   <<"60a6bc3db77e6c7b91c556140dcfec71">>},
                               {metakv,<<"/eventing/settings/config">>},
                               {metakv,<<"/query/settings/config">>}]..)
[ns_server:debug,2020-03-27T19:45:37.992Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
quorum_nodes ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557537}}]},
 'ns_1@cb.local']
[ns_server:debug,2020-03-27T19:45:37.997Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{local_changes_count,<<"60a6bc3db77e6c7b91c556140dcfec71">>} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{6,63752557537}}]}]
[ns_server:debug,2020-03-27T19:45:37.999Z,ns_1@cb.local:leader_quorum_nodes_manager<0.553.0>:leader_quorum_nodes_manager:pull_config:119]Pulled config successfully.
[ns_server:debug,2020-03-27T19:45:38.003Z,ns_1@cb.local:memcached_config_mgr<0.480.0>:memcached_config_mgr:apply_changed_memcached_config:179]New memcached config is hot-reloadable.
[ns_server:debug,2020-03-27T19:45:38.008Z,ns_1@cb.local:users_storage<0.267.0>:replicated_dets:handle_call:302]Suspended by process <0.321.0>
[ns_server:debug,2020-03-27T19:45:38.008Z,ns_1@cb.local:memcached_permissions<0.321.0>:replicated_dets:select_from_dets_locked:350]Starting select with {users_storage,[{{docv2,{user,{'_',local}},'_','_'},
                                      [],
                                      ['$_']}],
                                    100}
[ns_server:debug,2020-03-27T19:45:38.009Z,ns_1@cb.local:users_storage<0.267.0>:replicated_dets:handle_call:309]Released by process <0.321.0>
[ns_server:debug,2020-03-27T19:45:38.009Z,ns_1@cb.local:ns_config_rep<0.335.0>:ns_config_rep:handle_call:122]Got full synchronization request from 'ns_1@cb.local'
[ns_server:debug,2020-03-27T19:45:38.009Z,ns_1@cb.local:ns_config_rep<0.335.0>:ns_config_rep:handle_call:128]Fully synchronized config in 21 us
[user:warn,2020-03-27T19:45:38.010Z,ns_1@cb.local:compat_mode_manager<0.558.0>:compat_mode_manager:handle_consider_switching_compat_mode:49]Changed cluster compat mode from undefined to [6,5]
[error_logger:info,2020-03-27T19:45:38.014Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_orchestrator_sup}
             started: [{pid,<0.558.0>},
                       {id,compat_mode_manager},
                       {mfargs,{compat_mode_manager,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-03-27T19:45:38.015Z,ns_1@cb.local:memcached_config_mgr<0.480.0>:memcached_config_mgr:do_read_current_memcached_config:287]Got enoent while trying to read active memcached config from /opt/couchbase/var/lib/couchbase/config/memcached.json.prev
[ns_server:debug,2020-03-27T19:45:38.024Z,ns_1@cb.local:memcached_refresh<0.211.0>:memcached_refresh:handle_cast:55]Refresh of rbac requested
[ns_server:debug,2020-03-27T19:45:38.029Z,ns_1@cb.local:ns_ssl_services_setup<0.214.0>:ns_ssl_services_setup:trigger_ssl_reload:594]Notify services [ssl_service,capi_ssl_service] about client_cert_auth change
[ns_server:debug,2020-03-27T19:45:38.029Z,ns_1@cb.local:ns_ssl_services_setup<0.214.0>:ns_ssl_services_setup:notify_services:740]Going to notify following services: [capi_ssl_service,ssl_service]
[ns_server:debug,2020-03-27T19:45:38.037Z,ns_1@cb.local:<0.223.0>:restartable:loop:71]Restarting child <0.224.0>
  MFA: {ns_ssl_services_setup,start_link_rest_service,[]}
  Shutdown policy: 1000
  Caller: {<0.582.0>,#Ref<0.1597663237.214695938.138355>}
[ns_server:debug,2020-03-27T19:45:38.040Z,ns_1@cb.local:memcached_refresh<0.211.0>:memcached_refresh:handle_info:89]Refresh of [rbac] succeeded
[ns_server:debug,2020-03-27T19:45:38.042Z,ns_1@cb.local:<0.223.0>:restartable:shutdown_child:120]Successfully terminated process <0.224.0>
[ns_server:info,2020-03-27T19:45:38.060Z,ns_1@cb.local:<0.581.0>:ns_ssl_services_setup:notify_service:772]Successfully notified service capi_ssl_service
[ns_server:debug,2020-03-27T19:45:38.085Z,ns_1@cb.local:leader_lease_agent<0.543.0>:leader_lease_agent:do_handle_acquire_lease:149]Granting lease to {lease_holder,<<"952aa8a36b9840f3391de772fe18e797">>,
                                'ns_1@cb.local'} for 15000ms
[error_logger:info,2020-03-27T19:45:38.095Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_orchestrator_child_sup}
             started: [{pid,<0.588.0>},
                       {id,ns_janitor_server},
                       {mfargs,{ns_janitor_server,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:info,2020-03-27T19:45:38.101Z,ns_1@cb.local:<0.585.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for cbas
[ns_server:info,2020-03-27T19:45:38.102Z,ns_1@cb.local:<0.585.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for eventing
[ns_server:info,2020-03-27T19:45:38.103Z,ns_1@cb.local:<0.585.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for fts
[ns_server:info,2020-03-27T19:45:38.103Z,ns_1@cb.local:<0.585.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for n1ql
[error_logger:info,2020-03-27T19:45:38.105Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {<0.585.0>,menelaus_web}
             started: [{pid,<0.589.0>},
                       {id,menelaus_web_ipv4},
                       {mfargs,
                        {menelaus_web,http_server,
                         [[{ip,"0.0.0.0"},
                           {name,menelaus_web_ssl_ipv4},
                           {ssl,true},
                           {ssl_opts,
                            [{keyfile,
                              "/opt/couchbase/var/lib/couchbase/config/ssl-cert-key.pem"},
                             {certfile,
                              "/opt/couchbase/var/lib/couchbase/config/ssl-cert-key.pem"},
                             {versions,['tlsv1.1','tlsv1.2']},
                             {cacerts,
                              [<<48,130,3,2,48,130,1,234,160,3,2,1,2,2,8,22,
                                 0,64,208,110,172,210,92,48,13,6,9,42,134,72,
                                 134,247,13,1,1,11,5,0,48,36,49,34,48,32,6,3,
                                 85,4,3,19,25,67,111,117,99,104,98,97,115,
                                 101,32,83,101,114,118,101,114,32,51,52,56,
                                 101,100,50,55,48,48,30,23,13,49,51,48,49,48,
                                 49,48,48,48,48,48,48,90,23,13,52,57,49,50,
                                 51,49,50,51,53,57,53,57,90,48,36,49,34,48,
                                 32,6,3,85,4,3,19,25,67,111,117,99,104,98,97,
                                 115,101,32,83,101,114,118,101,114,32,51,52,
                                 56,101,100,50,55,48,48,130,1,34,48,13,6,9,
                                 42,134,72,134,247,13,1,1,1,5,0,3,130,1,15,0,
                                 48,130,1,10,2,130,1,1,0,225,209,231,201,130,
                                 247,233,2,187,220,222,34,52,112,118,191,205,
                                 152,82,245,106,26,160,209,228,176,90,40,98,
                                 172,183,1,136,153,71,239,18,220,136,15,75,
                                 73,223,113,145,171,33,98,84,62,10,36,82,160,
                                 106,135,171,185,245,166,220,188,43,178,51,2,
                                 205,146,75,139,58,68,250,195,53,51,182,176,
                                 42,160,42,14,18,94,26,188,161,102,35,66,108,
                                 137,96,63,168,160,212,254,70,169,139,74,198,
                                 244,204,206,194,101,179,132,181,1,21,76,117,
                                 72,101,24,165,108,212,56,253,47,230,184,143,
                                 50,20,121,74,247,204,71,247,226,97,249,191,
                                 174,226,150,23,149,71,180,209,86,216,72,31,
                                 202,197,17,62,92,90,116,10,212,69,89,20,91,
                                 163,51,77,94,156,13,115,23,71,83,252,222,
                                 166,85,132,171,214,167,174,78,60,63,78,241,
                                 115,199,70,186,175,191,94,208,5,216,173,106,
                                 248,203,56,72,24,219,23,86,67,137,18,221,
                                 171,208,1,41,239,84,253,158,230,36,78,222,
                                 33,120,90,96,209,36,5,230,90,163,19,2,35,
                                 165,80,128,171,6,90,11,237,162,35,3,201,94,
                                 110,201,181,13,49,165,2,3,1,0,1,163,56,48,
                                 54,48,14,6,3,85,29,15,1,1,255,4,4,3,2,2,164,
                                 48,19,6,3,85,29,37,4,12,48,10,6,8,43,6,1,5,
                                 5,7,3,1,48,15,6,3,85,29,19,1,1,255,4,5,48,3,
                                 1,1,255,48,13,6,9,42,134,72,134,247,13,1,1,
                                 11,5,0,3,130,1,1,0,6,81,65,124,71,43,122,15,
                                 167,207,16,251,0,159,39,98,188,227,29,215,
                                 100,107,194,184,208,49,234,49,157,22,16,114,
                                 226,174,13,150,172,144,2,238,230,8,40,221,
                                 26,55,56,132,126,233,153,24,185,41,108,27,
                                 239,216,82,120,63,12,251,133,139,191,7,65,
                                 245,131,92,34,201,230,196,53,62,156,158,117,
                                 62,220,159,192,101,171,131,143,145,151,43,
                                 167,227,90,161,173,77,152,27,124,67,49,150,
                                 67,102,161,221,43,166,252,34,106,208,76,223,
                                 73,230,216,161,26,132,137,129,146,79,142,69,
                                 179,193,194,47,129,207,251,4,237,180,247,54,
                                 155,40,216,36,79,59,110,188,27,153,208,45,
                                 211,245,157,64,19,243,225,128,45,126,172,97,
                                 164,65,55,250,227,90,212,59,153,137,219,195,
                                 84,171,109,145,64,111,253,109,120,35,199,
                                 153,232,120,170,179,167,110,176,116,10,245,
                                 79,6,66,158,148,137,191,125,65,164,153,94,
                                 180,226,83,230,241,236,39,67,40,12,91,44,79,
                                 54,65,72,249,75,235,222,205,142,112,55,224,
                                 47,125,42,235,188,14,128,134,44,228,25,163,
                                 25,90,246,170,74,163,117,63,44,151>>]},
                             {dh,
                              <<48,130,1,8,2,130,1,1,0,152,202,99,248,92,201,
                                35,238,246,5,77,93,120,10,118,129,36,52,111,
                                193,167,220,49,229,106,105,152,133,121,157,73,
                                158,232,153,197,197,21,171,140,30,207,52,165,
                                45,8,221,162,21,199,183,66,211,247,51,224,102,
                                214,190,130,96,253,218,193,35,43,139,145,89,
                                200,250,145,92,50,80,134,135,188,205,254,148,
                                122,136,237,220,186,147,187,104,159,36,147,
                                217,117,74,35,163,145,249,175,242,18,221,124,
                                54,140,16,246,169,84,252,45,47,99,136,30,60,
                                189,203,61,86,225,117,255,4,91,46,110,167,173,
                                106,51,65,10,248,94,225,223,73,40,232,140,26,
                                11,67,170,118,190,67,31,127,233,39,68,88,132,
                                171,224,62,187,207,160,189,209,101,74,8,205,
                                174,146,173,80,105,144,246,25,153,86,36,24,
                                178,163,64,202,221,95,184,110,244,32,226,217,
                                34,55,188,230,55,16,216,247,173,246,139,76,
                                187,66,211,159,17,46,20,18,48,80,27,250,96,
                                189,29,214,234,241,34,69,254,147,103,220,133,
                                40,164,84,8,44,241,61,164,151,9,135,41,60,75,
                                4,202,133,173,72,6,69,167,89,112,174,40,229,
                                171,2,1,2>>},
                             {ciphers,
                              [{ecdhe_ecdsa,aes_256_gcm,aead,sha384},
                               {ecdhe_rsa,aes_256_gcm,aead,sha384},
                               {ecdhe_ecdsa,aes_256_cbc,sha384,sha384},
                               {ecdhe_rsa,aes_256_cbc,sha384,sha384},
                               {ecdh_ecdsa,aes_256_gcm,aead,sha384},
                               {ecdh_rsa,aes_256_gcm,aead,sha384},
                               {ecdh_ecdsa,aes_256_cbc,sha384,sha384},
                               {ecdh_rsa,aes_256_cbc,sha384,sha384},
                               {ecdhe_ecdsa,chacha20_poly1305,aead,sha256},
                               {ecdhe_rsa,chacha20_poly1305,aead,sha256},
                               {dhe_rsa,chacha20_poly1305,aead,sha256},
                               {dhe_rsa,aes_256_gcm,aead,sha384},
                               {dhe_dss,aes_256_gcm,aead,sha384},
                               {dhe_rsa,aes_256_cbc,sha256},
                               {dhe_dss,aes_256_cbc,sha256},
                               {rsa,aes_256_gcm,aead,sha384},
                               {rsa,aes_256_cbc,sha256},
                               {ecdhe_ecdsa,aes_128_gcm,aead,sha256},
                               {ecdhe_rsa,aes_128_gcm,aead,sha256},
                               {ecdhe_ecdsa,aes_128_cbc,sha256,sha256},
                               {ecdhe_rsa,aes_128_cbc,sha256,sha256},
                               {ecdh_ecdsa,aes_128_gcm,aead,sha256},
                               {ecdh_rsa,aes_128_gcm,aead,sha256},
                               {ecdh_ecdsa,aes_128_cbc,sha256,sha256},
                               {ecdh_rsa,aes_128_cbc,sha256,sha256},
                               {dhe_rsa,aes_128_gcm,aead,sha256},
                               {dhe_dss,aes_128_gcm,aead,sha256},
                               {dhe_rsa,aes_128_cbc,sha256},
                               {dhe_dss,aes_128_cbc,sha256},
                               {rsa,aes_128_gcm,aead,sha256},
                               {rsa,aes_128_cbc,sha256},
                               {ecdhe_ecdsa,aes_256_cbc,sha},
                               {ecdhe_rsa,aes_256_cbc,sha},
                               {dhe_rsa,aes_256_cbc,sha},
                               {dhe_dss,aes_256_cbc,sha},
                               {ecdh_ecdsa,aes_256_cbc,sha},
                               {ecdh_rsa,aes_256_cbc,sha},
                               {rsa,aes_256_cbc,sha},
                               {ecdhe_ecdsa,aes_128_cbc,sha},
                               {ecdhe_rsa,aes_128_cbc,sha},
                               {dhe_rsa,aes_128_cbc,sha},
                               {dhe_dss,aes_128_cbc,sha},
                               {ecdh_ecdsa,aes_128_cbc,sha},
                               {ecdh_rsa,aes_128_cbc,sha},
                               {rsa,aes_128_cbc,sha},
                               {ecdhe_ecdsa,'3des_ede_cbc',sha},
                               {ecdhe_rsa,'3des_ede_cbc',sha},
                               {dhe_rsa,'3des_ede_cbc',sha},
                               {dhe_dss,'3des_ede_cbc',sha},
                               {ecdh_ecdsa,'3des_ede_cbc',sha},
                               {ecdh_rsa,'3des_ede_cbc',sha},
                               {rsa,'3des_ede_cbc',sha}]},
                             {honor_cipher_order,true},
                             {secure_renegotiate,true},
                             {client_renegotiation,false}]},
                           {port,18091}]]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[ns_server:info,2020-03-27T19:45:38.110Z,ns_1@cb.local:ns_orchestrator_child_sup<0.584.0>:misc:start_singleton:857]start_singleton(gen_server, start_link, [{via,leader_registry,
                                          auto_reprovision},
                                         auto_reprovision,[],[]]): started as <0.607.0> on 'ns_1@cb.local'

[error_logger:info,2020-03-27T19:45:38.110Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_orchestrator_child_sup}
             started: [{pid,<0.607.0>},
                       {id,auto_reprovision},
                       {mfargs,{auto_reprovision,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:info,2020-03-27T19:45:38.114Z,ns_1@cb.local:ns_orchestrator_child_sup<0.584.0>:misc:start_singleton:857]start_singleton(gen_server, start_link, [{via,leader_registry,auto_rebalance},
                                         auto_rebalance,[],[]]): started as <0.608.0> on 'ns_1@cb.local'

[error_logger:info,2020-03-27T19:45:38.114Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_orchestrator_child_sup}
             started: [{pid,<0.608.0>},
                       {id,auto_rebalance},
                       {mfargs,{auto_rebalance,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:info,2020-03-27T19:45:38.114Z,ns_1@cb.local:ns_orchestrator_child_sup<0.584.0>:misc:start_singleton:857]start_singleton(gen_statem, start_link, [{via,leader_registry,ns_orchestrator},
                                         ns_orchestrator,[],[]]): started as <0.609.0> on 'ns_1@cb.local'

[error_logger:info,2020-03-27T19:45:38.114Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_orchestrator_child_sup}
             started: [{pid,<0.609.0>},
                       {id,ns_orchestrator},
                       {mfargs,{ns_orchestrator,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T19:45:38.115Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_orchestrator_sup}
             started: [{pid,<0.584.0>},
                       {id,ns_orchestrator_child_sup},
                       {mfargs,{ns_orchestrator_child_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[ns_server:debug,2020-03-27T19:45:38.134Z,ns_1@cb.local:<0.613.0>:auto_failover:init:185]init auto_failover.
[user:info,2020-03-27T19:45:38.134Z,ns_1@cb.local:<0.613.0>:auto_failover:handle_call:216]Enabled auto-failover with timeout 120 and max count 1
[ns_server:info,2020-03-27T19:45:38.133Z,ns_1@cb.local:<0.585.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for cbas
[ns_server:info,2020-03-27T19:45:38.136Z,ns_1@cb.local:<0.583.0>:leader_lease_acquire_worker:handle_fresh_lease_acquired:302]Acquired lease from node 'ns_1@cb.local' (lease uuid: <<"952aa8a36b9840f3391de772fe18e797">>)
[ns_server:info,2020-03-27T19:45:38.139Z,ns_1@cb.local:<0.585.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for eventing
[ns_server:debug,2020-03-27T19:45:38.142Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{local_changes_count,<<"60a6bc3db77e6c7b91c556140dcfec71">>} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{7,63752557538}}]}]
[ns_server:info,2020-03-27T19:45:38.142Z,ns_1@cb.local:ns_orchestrator_sup<0.556.0>:misc:start_singleton:857]start_singleton(gen_server, start_link, [{via,leader_registry,auto_failover},
                                         auto_failover,[],[]]): started as <0.613.0> on 'ns_1@cb.local'

[ns_server:debug,2020-03-27T19:45:38.142Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
auto_failover_cfg ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557537}}]},
 {enabled,true},
 {timeout,120},
 {count,0},
 {failover_on_data_disk_issues,[{enabled,false},{timePeriod,120}]},
 {failover_server_group,false},
 {max_count,1},
 {failed_over_server_groups,[]},
 {can_abort_rebalance,true}]
[error_logger:info,2020-03-27T19:45:38.142Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_orchestrator_sup}
             started: [{pid,<0.613.0>},
                       {id,auto_failover},
                       {mfargs,{auto_failover,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-03-27T19:45:38.142Z,ns_1@cb.local:ns_config_rep<0.335.0>:ns_config_rep:do_push_keys:321]Replicating some config keys ([auto_failover_cfg,
                               {local_changes_count,
                                   <<"60a6bc3db77e6c7b91c556140dcfec71">>}]..)
[ns_server:info,2020-03-27T19:45:38.144Z,ns_1@cb.local:<0.585.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for fts
[error_logger:info,2020-03-27T19:45:38.144Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,mb_master_sup}
             started: [{pid,<0.556.0>},
                       {id,ns_orchestrator_sup},
                       {mfargs,{ns_orchestrator_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[ns_server:info,2020-03-27T19:45:38.144Z,ns_1@cb.local:mb_master_sup<0.550.0>:misc:start_singleton:857]start_singleton(work_queue, start_link, [{via,leader_registry,collections}]): started as <0.620.0> on 'ns_1@cb.local'

[error_logger:info,2020-03-27T19:45:38.144Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,mb_master_sup}
             started: [{pid,<0.620.0>},
                       {id,collections},
                       {mfargs,{collections,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[user:info,2020-03-27T19:45:38.149Z,ns_1@cb.local:memcached_config_mgr<0.480.0>:memcached_config_mgr:hot_reload_config:248]Hot-reloaded memcached.json for config change of the following keys: [<<"client_cert_auth">>,
                                                                      <<"datatype_snappy">>,
                                                                      <<"scramsha_fallback_salt">>]
[ns_server:info,2020-03-27T19:45:38.153Z,ns_1@cb.local:<0.585.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for n1ql
[ns_server:debug,2020-03-27T19:45:38.156Z,ns_1@cb.local:<0.223.0>:restartable:start_child:98]Started child process <0.585.0>
  MFA: {ns_ssl_services_setup,start_link_rest_service,[]}
[error_logger:info,2020-03-27T19:45:38.155Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {<0.585.0>,menelaus_web}
             started: [{pid,<0.621.0>},
                       {id,menelaus_web_ipv6},
                       {mfargs,
                        {menelaus_web,http_server,
                         [[{ip,"::"},
                           {name,menelaus_web_ssl_ipv6},
                           {ssl,true},
                           {ssl_opts,
                            [{keyfile,
                              "/opt/couchbase/var/lib/couchbase/config/ssl-cert-key.pem"},
                             {certfile,
                              "/opt/couchbase/var/lib/couchbase/config/ssl-cert-key.pem"},
                             {versions,['tlsv1.1','tlsv1.2']},
                             {cacerts,
                              [<<48,130,3,2,48,130,1,234,160,3,2,1,2,2,8,22,
                                 0,64,208,110,172,210,92,48,13,6,9,42,134,72,
                                 134,247,13,1,1,11,5,0,48,36,49,34,48,32,6,3,
                                 85,4,3,19,25,67,111,117,99,104,98,97,115,
                                 101,32,83,101,114,118,101,114,32,51,52,56,
                                 101,100,50,55,48,48,30,23,13,49,51,48,49,48,
                                 49,48,48,48,48,48,48,90,23,13,52,57,49,50,
                                 51,49,50,51,53,57,53,57,90,48,36,49,34,48,
                                 32,6,3,85,4,3,19,25,67,111,117,99,104,98,97,
                                 115,101,32,83,101,114,118,101,114,32,51,52,
                                 56,101,100,50,55,48,48,130,1,34,48,13,6,9,
                                 42,134,72,134,247,13,1,1,1,5,0,3,130,1,15,0,
                                 48,130,1,10,2,130,1,1,0,225,209,231,201,130,
                                 247,233,2,187,220,222,34,52,112,118,191,205,
                                 152,82,245,106,26,160,209,228,176,90,40,98,
                                 172,183,1,136,153,71,239,18,220,136,15,75,
                                 73,223,113,145,171,33,98,84,62,10,36,82,160,
                                 106,135,171,185,245,166,220,188,43,178,51,2,
                                 205,146,75,139,58,68,250,195,53,51,182,176,
                                 42,160,42,14,18,94,26,188,161,102,35,66,108,
                                 137,96,63,168,160,212,254,70,169,139,74,198,
                                 244,204,206,194,101,179,132,181,1,21,76,117,
                                 72,101,24,165,108,212,56,253,47,230,184,143,
                                 50,20,121,74,247,204,71,247,226,97,249,191,
                                 174,226,150,23,149,71,180,209,86,216,72,31,
                                 202,197,17,62,92,90,116,10,212,69,89,20,91,
                                 163,51,77,94,156,13,115,23,71,83,252,222,
                                 166,85,132,171,214,167,174,78,60,63,78,241,
                                 115,199,70,186,175,191,94,208,5,216,173,106,
                                 248,203,56,72,24,219,23,86,67,137,18,221,
                                 171,208,1,41,239,84,253,158,230,36,78,222,
                                 33,120,90,96,209,36,5,230,90,163,19,2,35,
                                 165,80,128,171,6,90,11,237,162,35,3,201,94,
                                 110,201,181,13,49,165,2,3,1,0,1,163,56,48,
                                 54,48,14,6,3,85,29,15,1,1,255,4,4,3,2,2,164,
                                 48,19,6,3,85,29,37,4,12,48,10,6,8,43,6,1,5,
                                 5,7,3,1,48,15,6,3,85,29,19,1,1,255,4,5,48,3,
                                 1,1,255,48,13,6,9,42,134,72,134,247,13,1,1,
                                 11,5,0,3,130,1,1,0,6,81,65,124,71,43,122,15,
                                 167,207,16,251,0,159,39,98,188,227,29,215,
                                 100,107,194,184,208,49,234,49,157,22,16,114,
                                 226,174,13,150,172,144,2,238,230,8,40,221,
                                 26,55,56,132,126,233,153,24,185,41,108,27,
                                 239,216,82,120,63,12,251,133,139,191,7,65,
                                 245,131,92,34,201,230,196,53,62,156,158,117,
                                 62,220,159,192,101,171,131,143,145,151,43,
                                 167,227,90,161,173,77,152,27,124,67,49,150,
                                 67,102,161,221,43,166,252,34,106,208,76,223,
                                 73,230,216,161,26,132,137,129,146,79,142,69,
                                 179,193,194,47,129,207,251,4,237,180,247,54,
                                 155,40,216,36,79,59,110,188,27,153,208,45,
                                 211,245,157,64,19,243,225,128,45,126,172,97,
                                 164,65,55,250,227,90,212,59,153,137,219,195,
                                 84,171,109,145,64,111,253,109,120,35,199,
                                 153,232,120,170,179,167,110,176,116,10,245,
                                 79,6,66,158,148,137,191,125,65,164,153,94,
                                 180,226,83,230,241,236,39,67,40,12,91,44,79,
                                 54,65,72,249,75,235,222,205,142,112,55,224,
                                 47,125,42,235,188,14,128,134,44,228,25,163,
                                 25,90,246,170,74,163,117,63,44,151>>]},
                             {dh,
                              <<48,130,1,8,2,130,1,1,0,152,202,99,248,92,201,
                                35,238,246,5,77,93,120,10,118,129,36,52,111,
                                193,167,220,49,229,106,105,152,133,121,157,73,
                                158,232,153,197,197,21,171,140,30,207,52,165,
                                45,8,221,162,21,199,183,66,211,247,51,224,102,
                                214,190,130,96,253,218,193,35,43,139,145,89,
                                200,250,145,92,50,80,134,135,188,205,254,148,
                                122,136,237,220,186,147,187,104,159,36,147,
                                217,117,74,35,163,145,249,175,242,18,221,124,
                                54,140,16,246,169,84,252,45,47,99,136,30,60,
                                189,203,61,86,225,117,255,4,91,46,110,167,173,
                                106,51,65,10,248,94,225,223,73,40,232,140,26,
                                11,67,170,118,190,67,31,127,233,39,68,88,132,
                                171,224,62,187,207,160,189,209,101,74,8,205,
                                174,146,173,80,105,144,246,25,153,86,36,24,
                                178,163,64,202,221,95,184,110,244,32,226,217,
                                34,55,188,230,55,16,216,247,173,246,139,76,
                                187,66,211,159,17,46,20,18,48,80,27,250,96,
                                189,29,214,234,241,34,69,254,147,103,220,133,
                                40,164,84,8,44,241,61,164,151,9,135,41,60,75,
                                4,202,133,173,72,6,69,167,89,112,174,40,229,
                                171,2,1,2>>},
                             {ciphers,
                              [{ecdhe_ecdsa,aes_256_gcm,aead,sha384},
                               {ecdhe_rsa,aes_256_gcm,aead,sha384},
                               {ecdhe_ecdsa,aes_256_cbc,sha384,sha384},
                               {ecdhe_rsa,aes_256_cbc,sha384,sha384},
                               {ecdh_ecdsa,aes_256_gcm,aead,sha384},
                               {ecdh_rsa,aes_256_gcm,aead,sha384},
                               {ecdh_ecdsa,aes_256_cbc,sha384,sha384},
                               {ecdh_rsa,aes_256_cbc,sha384,sha384},
                               {ecdhe_ecdsa,chacha20_poly1305,aead,sha256},
                               {ecdhe_rsa,chacha20_poly1305,aead,sha256},
                               {dhe_rsa,chacha20_poly1305,aead,sha256},
                               {dhe_rsa,aes_256_gcm,aead,sha384},
                               {dhe_dss,aes_256_gcm,aead,sha384},
                               {dhe_rsa,aes_256_cbc,sha256},
                               {dhe_dss,aes_256_cbc,sha256},
                               {rsa,aes_256_gcm,aead,sha384},
                               {rsa,aes_256_cbc,sha256},
                               {ecdhe_ecdsa,aes_128_gcm,aead,sha256},
                               {ecdhe_rsa,aes_128_gcm,aead,sha256},
                               {ecdhe_ecdsa,aes_128_cbc,sha256,sha256},
                               {ecdhe_rsa,aes_128_cbc,sha256,sha256},
                               {ecdh_ecdsa,aes_128_gcm,aead,sha256},
                               {ecdh_rsa,aes_128_gcm,aead,sha256},
                               {ecdh_ecdsa,aes_128_cbc,sha256,sha256},
                               {ecdh_rsa,aes_128_cbc,sha256,sha256},
                               {dhe_rsa,aes_128_gcm,aead,sha256},
                               {dhe_dss,aes_128_gcm,aead,sha256},
                               {dhe_rsa,aes_128_cbc,sha256},
                               {dhe_dss,aes_128_cbc,sha256},
                               {rsa,aes_128_gcm,aead,sha256},
                               {rsa,aes_128_cbc,sha256},
                               {ecdhe_ecdsa,aes_256_cbc,sha},
                               {ecdhe_rsa,aes_256_cbc,sha},
                               {dhe_rsa,aes_256_cbc,sha},
                               {dhe_dss,aes_256_cbc,sha},
                               {ecdh_ecdsa,aes_256_cbc,sha},
                               {ecdh_rsa,aes_256_cbc,sha},
                               {rsa,aes_256_cbc,sha},
                               {ecdhe_ecdsa,aes_128_cbc,sha},
                               {ecdhe_rsa,aes_128_cbc,sha},
                               {dhe_rsa,aes_128_cbc,sha},
                               {dhe_dss,aes_128_cbc,sha},
                               {ecdh_ecdsa,aes_128_cbc,sha},
                               {ecdh_rsa,aes_128_cbc,sha},
                               {rsa,aes_128_cbc,sha},
                               {ecdhe_ecdsa,'3des_ede_cbc',sha},
                               {ecdhe_rsa,'3des_ede_cbc',sha},
                               {dhe_rsa,'3des_ede_cbc',sha},
                               {dhe_dss,'3des_ede_cbc',sha},
                               {ecdh_ecdsa,'3des_ede_cbc',sha},
                               {ecdh_rsa,'3des_ede_cbc',sha},
                               {rsa,'3des_ede_cbc',sha}]},
                             {honor_cipher_order,true},
                             {secure_renegotiate,true},
                             {client_renegotiation,false}]},
                           {port,18091}]]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[ns_server:info,2020-03-27T19:45:38.157Z,ns_1@cb.local:<0.582.0>:ns_ssl_services_setup:notify_service:772]Successfully notified service ssl_service
[ns_server:info,2020-03-27T19:45:38.158Z,ns_1@cb.local:ns_ssl_services_setup<0.214.0>:ns_ssl_services_setup:notify_services:756]Succesfully notified services [ssl_service,capi_ssl_service]
[ns_server:debug,2020-03-27T19:45:38.158Z,ns_1@cb.local:<0.639.0>:license_reporting:init:66]Starting license_reporting server
[ns_server:info,2020-03-27T19:45:38.159Z,ns_1@cb.local:mb_master_sup<0.550.0>:misc:start_singleton:857]start_singleton(gen_server, start_link, [{via,leader_registry,
                                          license_reporting},
                                         license_reporting,[],[]]): started as <0.639.0> on 'ns_1@cb.local'

[error_logger:info,2020-03-27T19:45:38.159Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,mb_master_sup}
             started: [{pid,<0.639.0>},
                       {id,license_reporting},
                       {mfargs,{license_reporting,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T19:45:38.159Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,leader_registry_sup}
             started: [{pid,<0.548.0>},
                       {id,mb_master},
                       {mfargs,{mb_master,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-03-27T19:45:38.159Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,leader_services_sup}
             started: [{pid,<0.544.0>},
                       {id,leader_registry_sup},
                       {mfargs,{leader_registry_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[ns_server:debug,2020-03-27T19:45:38.159Z,ns_1@cb.local:<0.539.0>:restartable:start_child:98]Started child process <0.540.0>
  MFA: {leader_services_sup,start_link,[]}
[error_logger:info,2020-03-27T19:45:38.159Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.539.0>},
                       {name,leader_services_sup},
                       {mfargs,
                           {restartable,start_link,
                               [{leader_services_sup,start_link,[]},
                                infinity]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-03-27T19:45:38.183Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.641.0>},
                       {name,ns_tick_agent},
                       {mfargs,{ns_tick_agent,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T19:45:38.183Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.643.0>},
                       {name,master_activity_events_ingress},
                       {mfargs,
                           {gen_event,start_link,
                               [{local,master_activity_events_ingress}]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[error_logger:info,2020-03-27T19:45:38.183Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.644.0>},
                       {name,master_activity_events_timestamper},
                       {mfargs,
                           {master_activity_events,start_link_timestamper,[]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[error_logger:info,2020-03-27T19:45:38.186Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.645.0>},
                       {name,master_activity_events_pids_watcher},
                       {mfargs,
                           {master_activity_events_pids_watcher,start_link,
                               []}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[error_logger:info,2020-03-27T19:45:38.194Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.646.0>},
                       {name,master_activity_events_keeper},
                       {mfargs,{master_activity_events_keeper,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[error_logger:info,2020-03-27T19:45:38.206Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,health_monitor_sup}
             started: [{pid,<0.649.0>},
                       {id,ns_server_monitor},
                       {mfargs,{ns_server_monitor,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T19:45:38.206Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,health_monitor_sup}
             started: [{pid,<0.651.0>},
                       {id,service_monitor_children_sup},
                       {mfargs,
                           {supervisor,start_link,
                               [{local,service_monitor_children_sup},
                                health_monitor_sup,child]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-03-27T19:45:38.209Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,health_monitor_sup}
             started: [{pid,<0.652.0>},
                       {id,service_monitor_worker},
                       {mfargs,
                           {erlang,apply,
                               [#Fun<health_monitor_sup.0.112499759>,[]]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T19:45:38.211Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,health_monitor_sup}
             started: [{pid,<0.658.0>},
                       {id,node_monitor},
                       {mfargs,{node_monitor,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T19:45:38.214Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,health_monitor_sup}
             started: [{pid,<0.664.0>},
                       {id,node_status_analyzer},
                       {mfargs,{node_status_analyzer,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T19:45:38.214Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.648.0>},
                       {name,health_monitor_sup},
                       {mfargs,{health_monitor_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-03-27T19:45:38.219Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.666.0>},
                       {name,rebalance_agent},
                       {mfargs,{rebalance_agent,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T19:45:38.227Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.667.0>},
                       {name,ns_rebalance_report_manager},
                       {mfargs,{ns_rebalance_report_manager,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-03-27T19:45:38.228Z,ns_1@cb.local:ns_server_nodes_sup<0.207.0>:one_shot_barrier:notify:27]Notifying on barrier menelaus_barrier
[error_logger:info,2020-03-27T19:45:38.228Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.310.0>},
                       {name,ns_server_sup},
                       {mfargs,{ns_server_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[ns_server:debug,2020-03-27T19:45:38.228Z,ns_1@cb.local:menelaus_barrier<0.209.0>:one_shot_barrier:barrier_body:62]Barrier menelaus_barrier got notification from <0.207.0>
[ns_server:debug,2020-03-27T19:45:38.228Z,ns_1@cb.local:ns_server_nodes_sup<0.207.0>:one_shot_barrier:notify:32]Successfuly notified on barrier menelaus_barrier
[ns_server:debug,2020-03-27T19:45:38.228Z,ns_1@cb.local:<0.206.0>:restartable:start_child:98]Started child process <0.207.0>
  MFA: {ns_server_nodes_sup,start_link,[]}
[error_logger:info,2020-03-27T19:45:38.228Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.206.0>},
                       {id,ns_server_nodes_sup},
                       {mfargs,
                           {restartable,start_link,
                               [{ns_server_nodes_sup,start_link,[]},
                                infinity]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[ns_server:debug,2020-03-27T19:45:38.231Z,ns_1@cb.local:<0.5.0>:child_erlang:child_loop:130]142: Entered child_loop
[error_logger:info,2020-03-27T19:45:38.231Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.669.0>},
                       {id,remote_api},
                       {mfargs,{remote_api,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T19:45:38.231Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,root_sup}
             started: [{pid,<0.185.0>},
                       {id,ns_server_cluster_sup},
                       {mfargs,{ns_server_cluster_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-03-27T19:45:38.232Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
         application: ns_server
          started_at: 'ns_1@cb.local'

[ns_server:debug,2020-03-27T19:45:38.233Z,ns_1@cb.local:compiled_roles_cache<0.269.0>:menelaus_roles:build_compiled_roles:753]Compile roles for user {"@",admin}
[ns_server:debug,2020-03-27T19:45:38.245Z,ns_1@cb.local:json_rpc_connection-goxdcr-cbauth<0.670.0>:json_rpc_connection:init:73]Observed revrpc connection: label "goxdcr-cbauth", handling process <0.670.0>
[ns_server:debug,2020-03-27T19:45:38.246Z,ns_1@cb.local:json_rpc_connection-saslauthd-saslauthd-port<0.671.0>:json_rpc_connection:init:73]Observed revrpc connection: label "saslauthd-saslauthd-port", handling process <0.671.0>
[ns_server:debug,2020-03-27T19:45:38.245Z,ns_1@cb.local:menelaus_cbauth<0.454.0>:menelaus_cbauth:handle_cast:107]Observed json rpc process {"goxdcr-cbauth",<0.670.0>} started
[ns_server:debug,2020-03-27T19:45:38.250Z,ns_1@cb.local:compiled_roles_cache<0.269.0>:menelaus_roles:build_compiled_roles:753]Compile roles for user {"@goxdcr-cbauth",admin}
[ns_server:debug,2020-03-27T19:45:38.568Z,ns_1@cb.local:ns_audit_cfg<0.475.0>:ns_audit_cfg:write_audit_json:259]Writing new content to "/opt/couchbase/var/lib/couchbase/config/audit.json", Params [{descriptors_path,
                                                                                      "/opt/couchbase/etc/security"},
                                                                                     {version,
                                                                                      2},
                                                                                     {uuid,
                                                                                      "48537283"},
                                                                                     {event_states,
                                                                                      {[]}},
                                                                                     {filtering_enabled,
                                                                                      true},
                                                                                     {disabled_userids,
                                                                                      []},
                                                                                     {auditd_enabled,
                                                                                      false},
                                                                                     {log_path,
                                                                                      "/opt/couchbase/var/lib/couchbase/logs"},
                                                                                     {rotate_interval,
                                                                                      86400},
                                                                                     {rotate_size,
                                                                                      20971520},
                                                                                     {sync,
                                                                                      []}]
[ns_server:debug,2020-03-27T19:45:38.591Z,ns_1@cb.local:ns_audit_cfg<0.475.0>:ns_audit_cfg:notify_memcached:170]Instruct memcached to reload audit config
[ns_server:debug,2020-03-27T19:45:38.618Z,ns_1@cb.local:terse_cluster_info_uploader<0.483.0>:terse_cluster_info_uploader:handle_info:48]Refreshing terse cluster info with <<"{\"rev\":7,\"nodesExt\":[{\"services\":{\"mgmt\":8091,\"mgmtSSL\":18091,\"kv\":11210,\"kvSSL\":11207,\"capi\":8092,\"capiSSL\":18092,\"projector\":9999,\"projector\":9999},\"thisNode\":true}],\"clusterCapabilitiesVer\":[1,0],\"clusterCapabilities\":{\"n1ql\":[\"enhancedPreparedStatements\"]}}">>
[ns_server:debug,2020-03-27T19:45:39.135Z,ns_1@cb.local:<0.613.0>:auto_failover_logic:log_master_activity:177]Transitioned node {'ns_1@cb.local',<<"60a6bc3db77e6c7b91c556140dcfec71">>} state new -> up
[ns_server:debug,2020-03-27T19:45:42.194Z,ns_1@cb.local:ns_heart_slow_status_updater<0.365.0>:goxdcr_rest:get_from_goxdcr:140]Goxdcr is temporary not available. Return empty list.
[ns_server:debug,2020-03-27T19:46:07.784Z,ns_1@cb.local:compaction_daemon<0.533.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2020-03-27T19:46:07.784Z,ns_1@cb.local:compaction_daemon<0.533.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2020-03-27T19:46:07.784Z,ns_1@cb.local:compaction_daemon<0.533.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2020-03-27T19:46:07.784Z,ns_1@cb.local:compaction_daemon<0.533.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2020-03-27T19:46:23.668Z,ns_1@cb.local:compiled_roles_cache<0.269.0>:menelaus_roles:build_compiled_roles:753]Compile roles for user {[],wrong_token}
[ns_server:debug,2020-03-27T19:46:37.786Z,ns_1@cb.local:compaction_daemon<0.533.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2020-03-27T19:46:37.787Z,ns_1@cb.local:compaction_daemon<0.533.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2020-03-27T19:46:37.787Z,ns_1@cb.local:compaction_daemon<0.533.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2020-03-27T19:46:37.788Z,ns_1@cb.local:compaction_daemon<0.533.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2020-03-27T19:46:48.169Z,ns_1@cb.local:ldap_auth_cache<0.261.0>:active_cache:cleanup:231]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:info,2020-03-27T19:47:05.135Z,ns_1@cb.local:netconfig_updater<0.202.0>:netconfig_updater:apply_config_unprotected:158]Node is going to apply the following settings: [{externalListeners,
                                                 [{inet,false},
                                                  {inet6,false}]}]
[ns_server:debug,2020-03-27T19:47:05.172Z,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Updated cb_dist config "/opt/couchbase/var/lib/couchbase/config/dist_cfg": [{external_listeners,
                                                                                      [inet_tcp_dist,
                                                                                       inet6_tcp_dist]},
                                                                                     {preferred_external_proto,
                                                                                      inet_tcp_dist},
                                                                                     {preferred_local_proto,
                                                                                      inet_tcp_dist}]
[ns_server:debug,2020-03-27T19:47:05.174Z,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Reloading configuration: [{external_listeners,
                                       [inet_tcp_dist,inet6_tcp_dist]},
                                   {preferred_external_proto,inet_tcp_dist},
                                   {preferred_local_proto,inet_tcp_dist}]
[ns_server:debug,2020-03-27T19:47:05.175Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{local_changes_count,<<"60a6bc3db77e6c7b91c556140dcfec71">>} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{8,63752557625}}]}]
[ns_server:debug,2020-03-27T19:47:05.175Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',address_family} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557532}}]}|inet]
[ns_server:debug,2020-03-27T19:47:05.175Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{local_changes_count,<<"60a6bc3db77e6c7b91c556140dcfec71">>} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{9,63752557625}}]}]
[ns_server:info,2020-03-27T19:47:05.175Z,ns_1@cb.local:netconfig_updater<0.202.0>:netconfig_updater:apply_config_unprotected:187]Node network settings ([{externalListeners,[{inet,false},{inet6,false}]}]) successfully applied
[ns_server:debug,2020-03-27T19:47:05.175Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',node_encryption} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557532}}]}|false]
[ns_server:debug,2020-03-27T19:47:05.176Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{local_changes_count,<<"60a6bc3db77e6c7b91c556140dcfec71">>} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{10,63752557625}}]}]
[ns_server:debug,2020-03-27T19:47:05.176Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',erl_external_listeners} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557532}}]},
 {inet,false},
 {inet6,false}]
[ns_server:debug,2020-03-27T19:47:05.176Z,ns_1@cb.local:ns_config_rep<0.335.0>:ns_config_rep:do_push_keys:321]Replicating some config keys ([{local_changes_count,
                                   <<"60a6bc3db77e6c7b91c556140dcfec71">>},
                               {node,'ns_1@cb.local',address_family},
                               {node,'ns_1@cb.local',erl_external_listeners},
                               {node,'ns_1@cb.local',node_encryption}]..)
[cluster:info,2020-03-27T19:47:05.234Z,ns_1@cb.local:ns_cluster<0.189.0>:ns_cluster:handle_call:355]Changing address to "127.0.0.1" due to client request
[cluster:info,2020-03-27T19:47:05.235Z,ns_1@cb.local:ns_cluster<0.189.0>:ns_cluster:do_change_address:596]Change of address to "127.0.0.1" is requested.
[ns_server:debug,2020-03-27T19:47:05.235Z,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Closing listener inet_tcp_dist
[ns_server:debug,2020-03-27T19:47:05.235Z,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Closing listener inet6_tcp_dist
[error_logger:info,2020-03-27T19:47:05.235Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{net_kernel,626,nodedown,'ns_1@cb.local'}}
[ns_server:debug,2020-03-27T19:47:05.237Z,nonode@nohost:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Connection down: {con,#Ref<0.1597663237.214695937.138226>,
                               inet_tcp_dist,<0.298.0>,
                               #Ref<0.1597663237.214695937.138230>}
[ns_server:debug,2020-03-27T19:47:05.237Z,nonode@nohost:<0.317.0>:misc:delaying_crash:1608]Delaying crash exit:{{nodedown,'babysitter_of_ns_1@cb.local'},
                     {gen_server,call,
                                 [{ns_crash_log,'babysitter_of_ns_1@cb.local'},
                                  consume,infinity]}} by 1000ms
Stacktrace: [{gen_server,call,3,[{file,"gen_server.erl"},{line,214}]},
             {ns_log,crash_consumption_loop,0,
                     [{file,"src/ns_log.erl"},{line,62}]},
             {misc,delaying_crash,2,[{file,"src/misc.erl"},{line,1605}]},
             {proc_lib,init_p,3,[{file,"proc_lib.erl"},{line,232}]}]
[ns_server:debug,2020-03-27T19:47:05.237Z,nonode@nohost:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Connection down: {con,#Ref<0.1597663237.214695938.137194>,
                               inet_tcp_dist,<0.183.0>,
                               #Ref<0.1597663237.214695937.137955>}
[user:warn,2020-03-27T19:47:05.238Z,nonode@nohost:ns_node_disco<0.329.0>:ns_node_disco:handle_info:188]Node nonode@nohost saw that node 'ns_1@cb.local' went down. Details: [{nodedown_reason,
                                                                       net_kernel_terminated}]
[error_logger:error,2020-03-27T19:47:05.238Z,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]cb_dist: terminating with reason: shutdown
[ns_server:debug,2020-03-27T19:47:05.238Z,nonode@nohost:<0.2473.0>:dist_manager:teardown:306]Got nodedown msg {nodedown,'ns_1@cb.local',
                           [{nodedown_reason,net_kernel_terminated}]} after terminating net kernel
[ns_server:info,2020-03-27T19:47:05.238Z,nonode@nohost:dist_manager<0.166.0>:dist_manager:do_adjust_address:322]Adjusted IP to "127.0.0.1"
[ns_server:info,2020-03-27T19:47:05.238Z,nonode@nohost:dist_manager<0.166.0>:dist_manager:bringup:249]Attempting to bring up net_kernel with name 'ns_1@127.0.0.1'
[ns_server:error,2020-03-27T19:47:05.251Z,nonode@nohost:ns_config<0.193.0>:ns_config:handle_info:900]Saving ns_config failed. Trying to ignore: {distribution_not_started,
                                            [{auth,set_cookie,2,
                                              [{file,"auth.erl"},{line,120}]},
                                             {ns_server,get_babysitter_node,
                                              0,
                                              [{file,"src/ns_server.erl"},
                                               {line,280}]},
                                             {encryption_service,
                                              maybe_clear_backup_key,1,
                                              [{file,
                                                "src/encryption_service.erl"},
                                               {line,76}]},
                                             {proc_lib,init_p_do_apply,3,
                                              [{file,"proc_lib.erl"},
                                               {line,247}]}]}
[error_logger:error,2020-03-27T19:47:05.256Z,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================CRASH REPORT=========================
  crasher:
    initial call: ns_config_default:encrypt_and_save/1
    pid: <0.2469.0>
    registered_name: []
    exception error: distribution_not_started
      in function  auth:set_cookie/2 (auth.erl, line 120)
      in call from ns_server:get_babysitter_node/0 (src/ns_server.erl, line 280)
      in call from encryption_service:maybe_clear_backup_key/1 (src/encryption_service.erl, line 76)
    ancestors: [ns_config,ns_config_sup,ns_server_cluster_sup,root_sup,
                  <0.118.0>]
    message_queue_len: 0
    messages: []
    links: [<0.193.0>]
    dictionary: []
    trap_exit: false
    status: running
    heap_size: 6772
    stack_size: 27
    reductions: 135124
  neighbours:

[error_logger:info,2020-03-27T19:47:05.256Z,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ssl_dist_admin_sup}
             started: [{pid,<0.2477.0>},
                       {id,ssl_pem_cache_dist},
                       {mfargs,{ssl_pem_cache,start_link_dist,[[]]}},
                       {restart_type,permanent},
                       {shutdown,4000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T19:47:05.257Z,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ssl_dist_admin_sup}
             started: [{pid,<0.2478.0>},
                       {id,ssl_dist_manager},
                       {mfargs,{ssl_manager,start_link_dist,[[]]}},
                       {restart_type,permanent},
                       {shutdown,4000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T19:47:05.257Z,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ssl_dist_sup}
             started: [{pid,<0.2476.0>},
                       {id,ssl_dist_admin_sup},
                       {mfargs,{ssl_dist_admin_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,4000},
                       {child_type,supervisor}]

[error_logger:info,2020-03-27T19:47:05.257Z,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ssl_dist_sup}
             started: [{pid,<0.2479.0>},
                       {id,ssl_tls_dist_proxy},
                       {mfargs,{ssl_tls_dist_proxy,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,4000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T19:47:05.257Z,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ssl_dist_connection_sup}
             started: [{pid,<0.2481.0>},
                       {id,dist_tls_connection},
                       {mfargs,{tls_connection_sup,start_link_dist,[]}},
                       {restart_type,permanent},
                       {shutdown,4000},
                       {child_type,supervisor}]

[error_logger:info,2020-03-27T19:47:05.257Z,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ssl_dist_connection_sup}
             started: [{pid,<0.2482.0>},
                       {id,dist_tls_socket},
                       {mfargs,{ssl_listen_tracker_sup,start_link_dist,[]}},
                       {restart_type,permanent},
                       {shutdown,4000},
                       {child_type,supervisor}]

[error_logger:info,2020-03-27T19:47:05.257Z,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ssl_dist_sup}
             started: [{pid,<0.2480.0>},
                       {id,ssl_dist_connection_sup},
                       {mfargs,{ssl_dist_connection_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,4000},
                       {child_type,supervisor}]

[error_logger:info,2020-03-27T19:47:05.257Z,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,net_sup}
             started: [{pid,<0.2475.0>},
                       {id,ssl_dist_sup},
                       {mfargs,{ssl_dist_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[ns_server:debug,2020-03-27T19:47:05.264Z,nonode@nohost:cb_dist<0.2483.0>:cb_dist:info_msg:754]cb_dist: Starting cb_dist with config [{external_listeners,
                                        [inet_tcp_dist,inet6_tcp_dist]},
                                       {preferred_external_proto,
                                        inet_tcp_dist},
                                       {preferred_local_proto,inet_tcp_dist}]
[error_logger:info,2020-03-27T19:47:05.264Z,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,net_sup}
             started: [{pid,<0.2483.0>},
                       {id,cb_dist},
                       {mfargs,{cb_dist,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,worker}]

[error_logger:info,2020-03-27T19:47:05.267Z,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,net_sup}
             started: [{pid,<0.2484.0>},
                       {id,cb_epmd},
                       {mfargs,{cb_epmd,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,2000},
                       {child_type,worker}]

[ns_server:debug,2020-03-27T19:47:05.267Z,nonode@nohost:cb_dist<0.2483.0>:cb_dist:info_msg:754]cb_dist: Initial protos: [inet_tcp_dist,inet6_tcp_dist], required protos: [inet_tcp_dist]
[ns_server:debug,2020-03-27T19:47:05.267Z,nonode@nohost:cb_dist<0.2483.0>:cb_dist:info_msg:754]cb_dist: Starting inet_tcp_dist listener on 21100...
[error_logger:info,2020-03-27T19:47:05.267Z,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,net_sup}
             started: [{pid,<0.2485.0>},
                       {id,auth},
                       {mfargs,{auth,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,2000},
                       {child_type,worker}]

[ns_server:debug,2020-03-27T19:47:05.268Z,nonode@nohost:cb_dist<0.2483.0>:cb_dist:info_msg:754]cb_dist: Starting inet6_tcp_dist listener on 21100...
[user:info,2020-03-27T19:47:05.268Z,ns_1@127.0.0.1:ns_node_disco<0.329.0>:ns_node_disco:handle_info:182]Node 'ns_1@127.0.0.1' saw that node 'ns_1@127.0.0.1' came up. Tags: []
[ns_server:debug,2020-03-27T19:47:05.268Z,ns_1@127.0.0.1:<0.268.0>:doc_replicator:nodeup_monitoring_loop:124]got nodeup event. Considering ddocs replication
[ns_server:debug,2020-03-27T19:47:05.268Z,ns_1@127.0.0.1:users_replicator<0.266.0>:doc_replicator:loop:60]doing replicate_newnodes_docs
[error_logger:info,2020-03-27T19:47:05.268Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,net_sup}
             started: [{pid,<0.2486.0>},
                       {id,net_kernel},
                       {mfargs,
                           {net_kernel,start_link,
                               [['ns_1@127.0.0.1',longnames],false]}},
                       {restart_type,permanent},
                       {shutdown,2000},
                       {child_type,worker}]

[ns_server:debug,2020-03-27T19:47:05.269Z,ns_1@127.0.0.1:dist_manager<0.166.0>:dist_manager:configure_net_kernel:293]Set net_kernel vebosity to 10 -> 0
[error_logger:info,2020-03-27T19:47:05.269Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,kernel_sup}
             started: [{pid,<0.2474.0>},
                       {id,net_sup_dynamic},
                       {mfargs,
                           {erl_distribution,start_link,
                               [['ns_1@127.0.0.1',longnames],false]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,supervisor}]

[ns_server:info,2020-03-27T19:47:05.269Z,ns_1@127.0.0.1:dist_manager<0.166.0>:dist_manager:save_node:175]saving node to "/opt/couchbase/var/lib/couchbase/couchbase-server.node"
[ns_server:debug,2020-03-27T19:47:05.294Z,ns_1@127.0.0.1:dist_manager<0.166.0>:dist_manager:bringup:263]Attempted to save node name to disk: ok
[ns_server:debug,2020-03-27T19:47:05.294Z,ns_1@127.0.0.1:dist_manager<0.166.0>:dist_manager:wait_for_node:270]Waiting for connection to node 'babysitter_of_ns_1@cb.local' to be established
[ns_server:debug,2020-03-27T19:47:05.295Z,ns_1@127.0.0.1:net_kernel<0.2486.0>:cb_dist:info_msg:754]cb_dist: Setting up new connection to 'babysitter_of_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2020-03-27T19:47:05.295Z,ns_1@127.0.0.1:cb_dist<0.2483.0>:cb_dist:info_msg:754]cb_dist: Added connection {con,#Ref<0.1597663237.214695937.147021>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2020-03-27T19:47:05.295Z,ns_1@127.0.0.1:cb_dist<0.2483.0>:cb_dist:info_msg:754]cb_dist: Updated connection: {con,#Ref<0.1597663237.214695937.147021>,
                                  inet_tcp_dist,<0.2490.0>,
                                  #Ref<0.1597663237.214695937.147025>}
[error_logger:info,2020-03-27T19:47:05.295Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{connect,normal,'babysitter_of_ns_1@cb.local'}}
[ns_server:debug,2020-03-27T19:47:05.297Z,ns_1@127.0.0.1:dist_manager<0.166.0>:dist_manager:wait_for_node:282]Observed node 'babysitter_of_ns_1@cb.local' to come up
[ns_server:info,2020-03-27T19:47:05.297Z,ns_1@127.0.0.1:dist_manager<0.166.0>:dist_manager:do_adjust_address:326]Re-setting cookie {{sanitized,<<"C4PeZIe8LLStUy+d1HIN638gh0wTZW33h1wdSpFkvzc=">>},
                   'ns_1@127.0.0.1'}
[ns_server:info,2020-03-27T19:47:05.322Z,ns_1@127.0.0.1:dist_manager<0.166.0>:dist_manager:save_address_config:162]Deleting irrelevant ip file "/opt/couchbase/var/lib/couchbase/ip_start": {error,
                                                                          enoent}
[ns_server:info,2020-03-27T19:47:05.322Z,ns_1@127.0.0.1:dist_manager<0.166.0>:dist_manager:save_address_config:163]saving ip config to "/opt/couchbase/var/lib/couchbase/ip"
[ns_server:info,2020-03-27T19:47:05.336Z,ns_1@127.0.0.1:dist_manager<0.166.0>:dist_manager:save_address_config:166]Persisted the address successfully
[ns_server:debug,2020-03-27T19:47:05.336Z,ns_1@127.0.0.1:dist_manager<0.166.0>:dist_manager:complete_rename:370]Renaming node from 'ns_1@cb.local' to 'ns_1@127.0.0.1' in config
[ns_server:debug,2020-03-27T19:47:05.336Z,ns_1@127.0.0.1:net_kernel<0.2486.0>:cb_dist:info_msg:754]cb_dist: Setting up new connection to 'ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2020-03-27T19:47:05.337Z,ns_1@127.0.0.1:cb_dist<0.2483.0>:cb_dist:info_msg:754]cb_dist: Added connection {con,#Ref<0.1597663237.214695937.147043>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2020-03-27T19:47:05.336Z,ns_1@127.0.0.1:ns_config<0.193.0>:dist_manager:rename_node_in_config:392]renaming node conf {node,'ns_1@cb.local',erl_external_listeners} -> {node,
                                                                     'ns_1@127.0.0.1',
                                                                     erl_external_listeners}:
  [{inet,false},{inet6,false}] ->
  [{inet,false},{inet6,false}]
[ns_server:debug,2020-03-27T19:47:05.337Z,ns_1@127.0.0.1:cb_dist<0.2483.0>:cb_dist:info_msg:754]cb_dist: Updated connection: {con,#Ref<0.1597663237.214695937.147043>,
                                  inet_tcp_dist,<0.2492.0>,
                                  #Ref<0.1597663237.214695937.147047>}
[ns_server:debug,2020-03-27T19:47:05.337Z,ns_1@127.0.0.1:ns_config<0.193.0>:dist_manager:rename_node_in_config:392]renaming node conf {node,'ns_1@cb.local',node_encryption} -> {node,
                                                              'ns_1@127.0.0.1',
                                                              node_encryption}:
  false ->
  false
[error_logger:info,2020-03-27T19:47:05.337Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{connect,normal,'ns_1@cb.local'}}
[ns_server:debug,2020-03-27T19:47:05.337Z,ns_1@127.0.0.1:ns_config<0.193.0>:dist_manager:rename_node_in_config:392]renaming node conf {node,'ns_1@cb.local',address_family} -> {node,
                                                             'ns_1@127.0.0.1',
                                                             address_family}:
  inet ->
  inet
[ns_server:debug,2020-03-27T19:47:05.337Z,ns_1@127.0.0.1:cb_dist<0.2483.0>:cb_dist:info_msg:754]cb_dist: Accepted new connection from <0.2487.0>: {con,
                                                   #Ref<0.1597663237.214695937.147050>,
                                                   inet_tcp_dist,undefined,
                                                   undefined}
[ns_server:debug,2020-03-27T19:47:05.337Z,ns_1@127.0.0.1:net_kernel<0.2486.0>:cb_dist:info_msg:754]cb_dist: Accepting connection from acceptor <0.2487.0> using module inet_tcp_dist
[ns_server:debug,2020-03-27T19:47:05.337Z,ns_1@127.0.0.1:cb_dist<0.2483.0>:cb_dist:info_msg:754]cb_dist: Updated connection: {con,#Ref<0.1597663237.214695937.147050>,
                                  inet_tcp_dist,<0.2494.0>,
                                  #Ref<0.1597663237.214695937.147052>}
[ns_server:debug,2020-03-27T19:47:05.338Z,ns_1@127.0.0.1:cb_dist<0.2483.0>:cb_dist:info_msg:754]cb_dist: Connection down: {con,#Ref<0.1597663237.214695937.147043>,
                               inet_tcp_dist,<0.2492.0>,
                               #Ref<0.1597663237.214695937.147047>}
[error_logger:info,2020-03-27T19:47:05.338Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{'EXIT',<0.2492.0>,
                    {recv_challenge_failed,no_node,"ns_1@127.0.0.1"}}}
[error_logger:info,2020-03-27T19:47:05.338Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{net_kernel,913,nodedown,'ns_1@cb.local'}}
[ns_server:debug,2020-03-27T19:47:05.338Z,ns_1@127.0.0.1:cb_dist<0.2483.0>:cb_dist:info_msg:754]cb_dist: Connection down: {con,#Ref<0.1597663237.214695937.147050>,
                               inet_tcp_dist,<0.2494.0>,
                               #Ref<0.1597663237.214695937.147052>}
[error_logger:info,2020-03-27T19:47:05.338Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{'EXIT',<0.2494.0>,{recv_challenge_reply_failed,{error,closed}}}}
[error_logger:info,2020-03-27T19:47:05.338Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{net_kernel,913,nodedown,'ns_1@127.0.0.1'}}
[ns_server:debug,2020-03-27T19:47:05.339Z,ns_1@127.0.0.1:ns_config<0.193.0>:dist_manager:rename_node_in_config:392]renaming node conf {node,'ns_1@cb.local',eventing_dir} -> {node,
                                                           'ns_1@127.0.0.1',
                                                           eventing_dir}:
  "/opt/couchbase/var/lib/couchbase/data" ->
  "/opt/couchbase/var/lib/couchbase/data"
[ns_server:debug,2020-03-27T19:47:05.340Z,ns_1@127.0.0.1:ns_config<0.193.0>:dist_manager:rename_node_in_config:392]renaming node conf {node,'ns_1@cb.local',cbas_dirs} -> {node,
                                                        'ns_1@127.0.0.1',
                                                        cbas_dirs}:
  ["/opt/couchbase/var/lib/couchbase/data"] ->
  ["/opt/couchbase/var/lib/couchbase/data"]
[ns_server:debug,2020-03-27T19:47:05.340Z,ns_1@127.0.0.1:ns_config<0.193.0>:dist_manager:rename_node_in_config:392]renaming node conf nodes_wanted -> nodes_wanted:
  ['ns_1@cb.local'] ->
  ['ns_1@127.0.0.1']
[ns_server:debug,2020-03-27T19:47:05.340Z,ns_1@127.0.0.1:ns_config<0.193.0>:dist_manager:rename_node_in_config:392]renaming node conf quorum_nodes -> quorum_nodes:
  ['ns_1@cb.local'] ->
  ['ns_1@127.0.0.1']
[ns_server:debug,2020-03-27T19:47:05.341Z,ns_1@127.0.0.1:ns_config<0.193.0>:dist_manager:rename_node_in_config:392]renaming node conf server_groups -> server_groups:
  [[{uuid,<<"0">>},{name,<<"Group 1">>},{nodes,['ns_1@cb.local']}]] ->
  [[{uuid,<<"0">>},{name,<<"Group 1">>},{nodes,['ns_1@127.0.0.1']}]]
[ns_server:debug,2020-03-27T19:47:05.341Z,ns_1@127.0.0.1:ns_config<0.193.0>:dist_manager:rename_node_in_config:392]renaming node conf {node,'ns_1@cb.local',audit} -> {node,'ns_1@127.0.0.1',
                                                    audit}:
  [] ->
  []
[ns_server:debug,2020-03-27T19:47:05.341Z,ns_1@127.0.0.1:ns_config<0.193.0>:dist_manager:rename_node_in_config:392]renaming node conf {node,'ns_1@cb.local',capi_port} -> {node,
                                                        'ns_1@127.0.0.1',
                                                        capi_port}:
  8092 ->
  8092
[ns_server:debug,2020-03-27T19:47:05.341Z,ns_1@127.0.0.1:ns_config<0.193.0>:dist_manager:rename_node_in_config:392]renaming node conf {node,'ns_1@cb.local',cbas_admin_port} -> {node,
                                                              'ns_1@127.0.0.1',
                                                              cbas_admin_port}:
  9110 ->
  9110
[ns_server:debug,2020-03-27T19:47:05.341Z,ns_1@127.0.0.1:ns_config<0.193.0>:dist_manager:rename_node_in_config:392]renaming node conf {node,'ns_1@cb.local',cbas_cc_client_port} -> {node,
                                                                  'ns_1@127.0.0.1',
                                                                  cbas_cc_client_port}:
  9113 ->
  9113
[ns_server:debug,2020-03-27T19:47:05.341Z,ns_1@127.0.0.1:ns_config<0.193.0>:dist_manager:rename_node_in_config:392]renaming node conf {node,'ns_1@cb.local',cbas_cc_cluster_port} -> {node,
                                                                   'ns_1@127.0.0.1',
                                                                   cbas_cc_cluster_port}:
  9112 ->
  9112
[ns_server:debug,2020-03-27T19:47:05.341Z,ns_1@127.0.0.1:ns_config<0.193.0>:dist_manager:rename_node_in_config:392]renaming node conf {node,'ns_1@cb.local',cbas_cc_http_port} -> {node,
                                                                'ns_1@127.0.0.1',
                                                                cbas_cc_http_port}:
  9111 ->
  9111
[ns_server:debug,2020-03-27T19:47:05.342Z,ns_1@127.0.0.1:ns_config<0.193.0>:dist_manager:rename_node_in_config:392]renaming node conf {node,'ns_1@cb.local',cbas_cluster_port} -> {node,
                                                                'ns_1@127.0.0.1',
                                                                cbas_cluster_port}:
  9115 ->
  9115
[ns_server:debug,2020-03-27T19:47:05.342Z,ns_1@127.0.0.1:ns_config<0.193.0>:dist_manager:rename_node_in_config:392]renaming node conf {node,'ns_1@cb.local',cbas_console_port} -> {node,
                                                                'ns_1@127.0.0.1',
                                                                cbas_console_port}:
  9114 ->
  9114
[ns_server:debug,2020-03-27T19:47:05.342Z,ns_1@127.0.0.1:ns_config<0.193.0>:dist_manager:rename_node_in_config:392]renaming node conf {node,'ns_1@cb.local',cbas_data_port} -> {node,
                                                             'ns_1@127.0.0.1',
                                                             cbas_data_port}:
  9116 ->
  9116
[ns_server:debug,2020-03-27T19:47:05.342Z,ns_1@127.0.0.1:ns_config<0.193.0>:dist_manager:rename_node_in_config:392]renaming node conf {node,'ns_1@cb.local',cbas_debug_port} -> {node,
                                                              'ns_1@127.0.0.1',
                                                              cbas_debug_port}:
  -1 ->
  -1
[ns_server:debug,2020-03-27T19:47:05.342Z,ns_1@127.0.0.1:ns_config<0.193.0>:dist_manager:rename_node_in_config:392]renaming node conf {node,'ns_1@cb.local',cbas_http_port} -> {node,
                                                             'ns_1@127.0.0.1',
                                                             cbas_http_port}:
  8095 ->
  8095
[ns_server:debug,2020-03-27T19:47:05.342Z,ns_1@127.0.0.1:ns_config<0.193.0>:dist_manager:rename_node_in_config:392]renaming node conf {node,'ns_1@cb.local',cbas_messaging_port} -> {node,
                                                                  'ns_1@127.0.0.1',
                                                                  cbas_messaging_port}:
  9118 ->
  9118
[ns_server:debug,2020-03-27T19:47:05.342Z,ns_1@127.0.0.1:ns_config<0.193.0>:dist_manager:rename_node_in_config:392]renaming node conf {node,'ns_1@cb.local',cbas_metadata_callback_port} -> {node,
                                                                          'ns_1@127.0.0.1',
                                                                          cbas_metadata_callback_port}:
  9119 ->
  9119
[ns_server:debug,2020-03-27T19:47:05.342Z,ns_1@127.0.0.1:ns_config<0.193.0>:dist_manager:rename_node_in_config:392]renaming node conf {node,'ns_1@cb.local',cbas_metadata_port} -> {node,
                                                                 'ns_1@127.0.0.1',
                                                                 cbas_metadata_port}:
  9121 ->
  9121
[ns_server:debug,2020-03-27T19:47:05.342Z,ns_1@127.0.0.1:ns_config<0.193.0>:dist_manager:rename_node_in_config:392]renaming node conf {node,'ns_1@cb.local',cbas_parent_port} -> {node,
                                                               'ns_1@127.0.0.1',
                                                               cbas_parent_port}:
  9122 ->
  9122
[ns_server:debug,2020-03-27T19:47:05.342Z,ns_1@127.0.0.1:ns_config<0.193.0>:dist_manager:rename_node_in_config:392]renaming node conf {node,'ns_1@cb.local',cbas_replication_port} -> {node,
                                                                    'ns_1@127.0.0.1',
                                                                    cbas_replication_port}:
  9120 ->
  9120
[ns_server:debug,2020-03-27T19:47:05.342Z,ns_1@127.0.0.1:ns_config<0.193.0>:dist_manager:rename_node_in_config:392]renaming node conf {node,'ns_1@cb.local',cbas_result_port} -> {node,
                                                               'ns_1@127.0.0.1',
                                                               cbas_result_port}:
  9117 ->
  9117
[ns_server:debug,2020-03-27T19:47:05.342Z,ns_1@127.0.0.1:ns_config<0.193.0>:dist_manager:rename_node_in_config:392]renaming node conf {node,'ns_1@cb.local',cbas_ssl_port} -> {node,
                                                            'ns_1@127.0.0.1',
                                                            cbas_ssl_port}:
  18095 ->
  18095
[ns_server:debug,2020-03-27T19:47:05.342Z,ns_1@127.0.0.1:ns_config<0.193.0>:dist_manager:rename_node_in_config:392]renaming node conf {node,'ns_1@cb.local',compaction_daemon} -> {node,
                                                                'ns_1@127.0.0.1',
                                                                compaction_daemon}:
  [{check_interval,30},
   {min_db_file_size,131072},
   {min_view_file_size,20971520}] ->
  [{check_interval,30},
   {min_db_file_size,131072},
   {min_view_file_size,20971520}]
[ns_server:debug,2020-03-27T19:47:05.342Z,ns_1@127.0.0.1:ns_config<0.193.0>:dist_manager:rename_node_in_config:392]renaming node conf {node,'ns_1@cb.local',config_version} -> {node,
                                                             'ns_1@127.0.0.1',
                                                             config_version}:
  {6,5} ->
  {6,5}
[ns_server:debug,2020-03-27T19:47:05.342Z,ns_1@127.0.0.1:ns_config<0.193.0>:dist_manager:rename_node_in_config:392]renaming node conf {node,'ns_1@cb.local',eventing_debug_port} -> {node,
                                                                  'ns_1@127.0.0.1',
                                                                  eventing_debug_port}:
  9140 ->
  9140
[ns_server:debug,2020-03-27T19:47:05.342Z,ns_1@127.0.0.1:ns_config<0.193.0>:dist_manager:rename_node_in_config:392]renaming node conf {node,'ns_1@cb.local',eventing_http_port} -> {node,
                                                                 'ns_1@127.0.0.1',
                                                                 eventing_http_port}:
  8096 ->
  8096
[ns_server:debug,2020-03-27T19:47:05.343Z,ns_1@127.0.0.1:ns_config<0.193.0>:dist_manager:rename_node_in_config:392]renaming node conf {node,'ns_1@cb.local',eventing_https_port} -> {node,
                                                                  'ns_1@127.0.0.1',
                                                                  eventing_https_port}:
  18096 ->
  18096
[ns_server:debug,2020-03-27T19:47:05.343Z,ns_1@127.0.0.1:ns_config<0.193.0>:dist_manager:rename_node_in_config:392]renaming node conf {node,'ns_1@cb.local',fts_grpc_port} -> {node,
                                                            'ns_1@127.0.0.1',
                                                            fts_grpc_port}:
  9130 ->
  9130
[ns_server:debug,2020-03-27T19:47:05.343Z,ns_1@127.0.0.1:ns_config<0.193.0>:dist_manager:rename_node_in_config:392]renaming node conf {node,'ns_1@cb.local',fts_grpc_ssl_port} -> {node,
                                                                'ns_1@127.0.0.1',
                                                                fts_grpc_ssl_port}:
  19130 ->
  19130
[ns_server:debug,2020-03-27T19:47:05.343Z,ns_1@127.0.0.1:ns_config<0.193.0>:dist_manager:rename_node_in_config:392]renaming node conf {node,'ns_1@cb.local',fts_http_port} -> {node,
                                                            'ns_1@127.0.0.1',
                                                            fts_http_port}:
  8094 ->
  8094
[ns_server:debug,2020-03-27T19:47:05.343Z,ns_1@127.0.0.1:ns_config<0.193.0>:dist_manager:rename_node_in_config:392]renaming node conf {node,'ns_1@cb.local',fts_ssl_port} -> {node,
                                                           'ns_1@127.0.0.1',
                                                           fts_ssl_port}:
  18094 ->
  18094
[ns_server:debug,2020-03-27T19:47:05.343Z,ns_1@127.0.0.1:ns_config<0.193.0>:dist_manager:rename_node_in_config:392]renaming node conf {node,'ns_1@cb.local',indexer_admin_port} -> {node,
                                                                 'ns_1@127.0.0.1',
                                                                 indexer_admin_port}:
  9100 ->
  9100
[ns_server:debug,2020-03-27T19:47:05.343Z,ns_1@127.0.0.1:ns_config<0.193.0>:dist_manager:rename_node_in_config:392]renaming node conf {node,'ns_1@cb.local',indexer_http_port} -> {node,
                                                                'ns_1@127.0.0.1',
                                                                indexer_http_port}:
  9102 ->
  9102
[ns_server:debug,2020-03-27T19:47:05.343Z,ns_1@127.0.0.1:ns_config<0.193.0>:dist_manager:rename_node_in_config:392]renaming node conf {node,'ns_1@cb.local',indexer_https_port} -> {node,
                                                                 'ns_1@127.0.0.1',
                                                                 indexer_https_port}:
  19102 ->
  19102
[ns_server:debug,2020-03-27T19:47:05.343Z,ns_1@127.0.0.1:ns_config<0.193.0>:dist_manager:rename_node_in_config:392]renaming node conf {node,'ns_1@cb.local',indexer_scan_port} -> {node,
                                                                'ns_1@127.0.0.1',
                                                                indexer_scan_port}:
  9101 ->
  9101
[ns_server:debug,2020-03-27T19:47:05.343Z,ns_1@127.0.0.1:ns_config<0.193.0>:dist_manager:rename_node_in_config:392]renaming node conf {node,'ns_1@cb.local',indexer_stcatchup_port} -> {node,
                                                                     'ns_1@127.0.0.1',
                                                                     indexer_stcatchup_port}:
  9104 ->
  9104
[ns_server:debug,2020-03-27T19:47:05.343Z,ns_1@127.0.0.1:ns_config<0.193.0>:dist_manager:rename_node_in_config:392]renaming node conf {node,'ns_1@cb.local',indexer_stinit_port} -> {node,
                                                                  'ns_1@127.0.0.1',
                                                                  indexer_stinit_port}:
  9103 ->
  9103
[ns_server:debug,2020-03-27T19:47:05.343Z,ns_1@127.0.0.1:ns_config<0.193.0>:dist_manager:rename_node_in_config:392]renaming node conf {node,'ns_1@cb.local',indexer_stmaint_port} -> {node,
                                                                   'ns_1@127.0.0.1',
                                                                   indexer_stmaint_port}:
  9105 ->
  9105
[ns_server:debug,2020-03-27T19:47:05.344Z,ns_1@127.0.0.1:ns_config<0.193.0>:dist_manager:rename_node_in_config:392]renaming node conf {node,'ns_1@cb.local',is_enterprise} -> {node,
                                                            'ns_1@127.0.0.1',
                                                            is_enterprise}:
  true ->
  true
[ns_server:debug,2020-03-27T19:47:05.350Z,ns_1@127.0.0.1:cb_dist<0.2483.0>:cb_dist:info_msg:754]cb_dist: Accepted new connection from <0.2487.0>: {con,
                                                   #Ref<0.1597663237.214695937.147058>,
                                                   inet_tcp_dist,undefined,
                                                   undefined}
[ns_server:debug,2020-03-27T19:47:05.350Z,ns_1@127.0.0.1:net_kernel<0.2486.0>:cb_dist:info_msg:754]cb_dist: Accepting connection from acceptor <0.2487.0> using module inet_tcp_dist
[ns_server:debug,2020-03-27T19:47:05.350Z,ns_1@127.0.0.1:cb_dist<0.2483.0>:cb_dist:info_msg:754]cb_dist: Updated connection: {con,#Ref<0.1597663237.214695937.147058>,
                                  inet_tcp_dist,<0.2496.0>,
                                  #Ref<0.1597663237.214695937.147060>}
[ns_server:debug,2020-03-27T19:47:05.350Z,ns_1@127.0.0.1:ns_config<0.193.0>:dist_manager:rename_node_in_config:392]renaming node conf {node,'ns_1@cb.local',isasl} -> {node,'ns_1@127.0.0.1',
                                                    isasl}:
  [{path,"/opt/couchbase/var/lib/couchbase/isasl.pw"}] ->
  [{path,"/opt/couchbase/var/lib/couchbase/isasl.pw"}]
[ns_server:debug,2020-03-27T19:47:05.351Z,ns_1@127.0.0.1:ns_config<0.193.0>:dist_manager:rename_node_in_config:392]renaming node conf {node,'ns_1@cb.local',membership} -> {node,
                                                         'ns_1@127.0.0.1',
                                                         membership}:
  active ->
  active
[ns_server:debug,2020-03-27T19:47:05.353Z,ns_1@127.0.0.1:cb_dist<0.2483.0>:cb_dist:info_msg:754]cb_dist: Connection down: {con,#Ref<0.1597663237.214695937.147058>,
                               inet_tcp_dist,<0.2496.0>,
                               #Ref<0.1597663237.214695937.147060>}
[error_logger:info,2020-03-27T19:47:05.353Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{'EXIT',<0.2496.0>,{recv_challenge_reply_failed,{error,closed}}}}
[ns_server:debug,2020-03-27T19:47:05.355Z,ns_1@127.0.0.1:ns_config<0.193.0>:dist_manager:rename_node_in_config:392]renaming node conf {node,'ns_1@cb.local',memcached} -> {node,
                                                        'ns_1@127.0.0.1',
                                                        memcached}:
  [{port,11210},
   {dedicated_port,11209},
   {dedicated_ssl_port,11206},
   {ssl_port,11207},
   {admin_user,"@ns_server"},
   {other_users,["@cbq-engine","@projector","@goxdcr","@index","@fts",
                 "@eventing","@cbas"]},
   {admin_pass,"*****"},
   {engines,[{membase,[{engine,"/opt/couchbase/lib/memcached/ep.so"},
                       {static_config_string,"failpartialwarmup=false"}]},
             {memcached,[{engine,"/opt/couchbase/lib/memcached/default_engine.so"},
                         {static_config_string,"vb0=true"}]}]},
   {config_path,"/opt/couchbase/var/lib/couchbase/config/memcached.json"},
   {audit_file,"/opt/couchbase/var/lib/couchbase/config/audit.json"},
   {rbac_file,"/opt/couchbase/var/lib/couchbase/config/memcached.rbac"},
   {log_path,"/opt/couchbase/var/lib/couchbase/logs"},
   {log_prefix,"memcached.log"},
   {log_generations,20},
   {log_cyclesize,10485760},
   {log_sleeptime,19},
   {log_rotation_period,39003}] ->
  [{port,11210},
   {dedicated_port,11209},
   {dedicated_ssl_port,11206},
   {ssl_port,11207},
   {admin_user,"@ns_server"},
   {other_users,["@cbq-engine","@projector","@goxdcr","@index","@fts",
                 "@eventing","@cbas"]},
   {admin_pass,"*****"},
   {engines,[{membase,[{engine,"/opt/couchbase/lib/memcached/ep.so"},
                       {static_config_string,"failpartialwarmup=false"}]},
             {memcached,[{engine,"/opt/couchbase/lib/memcached/default_engine.so"},
                         {static_config_string,"vb0=true"}]}]},
   {config_path,"/opt/couchbase/var/lib/couchbase/config/memcached.json"},
   {audit_file,"/opt/couchbase/var/lib/couchbase/config/audit.json"},
   {rbac_file,"/opt/couchbase/var/lib/couchbase/config/memcached.rbac"},
   {log_path,"/opt/couchbase/var/lib/couchbase/logs"},
   {log_prefix,"memcached.log"},
   {log_generations,20},
   {log_cyclesize,10485760},
   {log_sleeptime,19},
   {log_rotation_period,39003}]
[ns_server:debug,2020-03-27T19:47:05.357Z,ns_1@127.0.0.1:ns_config<0.193.0>:dist_manager:rename_node_in_config:392]renaming node conf {node,'ns_1@cb.local',memcached_config} -> {node,
                                                               'ns_1@127.0.0.1',
                                                               memcached_config}:
  {[{interfaces,
        {memcached_config_mgr,omit_missing_mcd_ports,
            [{[{host,<<"*">>},
               {port,port},
               {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
               {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]},
             {[{host,<<"*">>},
               {port,dedicated_port},
               {system,true},
               {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
               {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]},
             {[{host,<<"*">>},
               {port,ssl_port},
               {ssl,
                   {[{key,
                         <<"/opt/couchbase/var/lib/couchbase/config/memcached-key.pem">>},
                     {cert,
                         <<"/opt/couchbase/var/lib/couchbase/config/memcached-cert.pem">>}]}},
               {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
               {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]},
             {[{host,<<"*">>},
               {port,dedicated_ssl_port},
               {system,true},
               {ssl,
                   {[{key,
                         <<"/opt/couchbase/var/lib/couchbase/config/memcached-key.pem">>},
                     {cert,
                         <<"/opt/couchbase/var/lib/couchbase/config/memcached-cert.pem">>}]}},
               {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
               {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]}]}},
    {ssl_cipher_list,{memcached_config_mgr,get_ssl_cipher_list,[]}},
    {ssl_cipher_order,{memcached_config_mgr,get_ssl_cipher_order,[]}},
    {client_cert_auth,{memcached_config_mgr,client_cert_auth,[]}},
    {ssl_minimum_protocol,{memcached_config_mgr,ssl_minimum_protocol,[]}},
    {connection_idle_time,connection_idle_time},
    {privilege_debug,privilege_debug},
    {breakpad,
        {[{enabled,breakpad_enabled},
          {minidump_dir,{memcached_config_mgr,get_minidump_dir,[]}}]}},
    {opentracing,
        {[{enabled,opentracing_enabled},
          {module,{"~s",[opentracing_module]}},
          {config,{"~s",[opentracing_config]}}]}},
    {admin,{"~s",[admin_user]}},
    {verbosity,verbosity},
    {audit_file,{"~s",[audit_file]}},
    {rbac_file,{"~s",[rbac_file]}},
    {dedupe_nmvb_maps,dedupe_nmvb_maps},
    {tracing_enabled,tracing_enabled},
    {datatype_snappy,{memcached_config_mgr,is_snappy_enabled,[]}},
    {xattr_enabled,true},
    {scramsha_fallback_salt,{memcached_config_mgr,get_fallback_salt,[]}},
    {collections_enabled,{memcached_config_mgr,collections_enabled,[]}},
    {max_connections,max_connections},
    {system_connections,system_connections},
    {num_reader_threads,num_reader_threads},
    {num_writer_threads,num_writer_threads},
    {logger,
        {[{filename,{"~s/~s",[log_path,log_prefix]}},
          {cyclesize,log_cyclesize},
          {sleeptime,log_sleeptime}]}},
    {external_auth_service,
        {memcached_config_mgr,get_external_auth_service,[]}},
    {active_external_users_push_interval,
        {memcached_config_mgr,get_external_users_push_interval,[]}}]} ->
  {[{interfaces,
        {memcached_config_mgr,omit_missing_mcd_ports,
            [{[{host,<<"*">>},
               {port,port},
               {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
               {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]},
             {[{host,<<"*">>},
               {port,dedicated_port},
               {system,true},
               {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
               {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]},
             {[{host,<<"*">>},
               {port,ssl_port},
               {ssl,
                   {[{key,
                         <<"/opt/couchbase/var/lib/couchbase/config/memcached-key.pem">>},
                     {cert,
                         <<"/opt/couchbase/var/lib/couchbase/config/memcached-cert.pem">>}]}},
               {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
               {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]},
             {[{host,<<"*">>},
               {port,dedicated_ssl_port},
               {system,true},
               {ssl,
                   {[{key,
                         <<"/opt/couchbase/var/lib/couchbase/config/memcached-key.pem">>},
                     {cert,
                         <<"/opt/couchbase/var/lib/couchbase/config/memcached-cert.pem">>}]}},
               {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
               {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]}]}},
    {ssl_cipher_list,{memcached_config_mgr,get_ssl_cipher_list,[]}},
    {ssl_cipher_order,{memcached_config_mgr,get_ssl_cipher_order,[]}},
    {client_cert_auth,{memcached_config_mgr,client_cert_auth,[]}},
    {ssl_minimum_protocol,{memcached_config_mgr,ssl_minimum_protocol,[]}},
    {connection_idle_time,connection_idle_time},
    {privilege_debug,privilege_debug},
    {breakpad,
        {[{enabled,breakpad_enabled},
          {minidump_dir,{memcached_config_mgr,get_minidump_dir,[]}}]}},
    {opentracing,
        {[{enabled,opentracing_enabled},
          {module,{"~s",[opentracing_module]}},
          {config,{"~s",[opentracing_config]}}]}},
    {admin,{"~s",[admin_user]}},
    {verbosity,verbosity},
    {audit_file,{"~s",[audit_file]}},
    {rbac_file,{"~s",[rbac_file]}},
    {dedupe_nmvb_maps,dedupe_nmvb_maps},
    {tracing_enabled,tracing_enabled},
    {datatype_snappy,{memcached_config_mgr,is_snappy_enabled,[]}},
    {xattr_enabled,true},
    {scramsha_fallback_salt,{memcached_config_mgr,get_fallback_salt,[]}},
    {collections_enabled,{memcached_config_mgr,collections_enabled,[]}},
    {max_connections,max_connections},
    {system_connections,system_connections},
    {num_reader_threads,num_reader_threads},
    {num_writer_threads,num_writer_threads},
    {logger,
        {[{filename,{"~s/~s",[log_path,log_prefix]}},
          {cyclesize,log_cyclesize},
          {sleeptime,log_sleeptime}]}},
    {external_auth_service,
        {memcached_config_mgr,get_external_auth_service,[]}},
    {active_external_users_push_interval,
        {memcached_config_mgr,get_external_users_push_interval,[]}}]}
[ns_server:debug,2020-03-27T19:47:05.359Z,ns_1@127.0.0.1:ns_config<0.193.0>:dist_manager:rename_node_in_config:392]renaming node conf {node,'ns_1@cb.local',memcached_dedicated_ssl_port} -> {node,
                                                                           'ns_1@127.0.0.1',
                                                                           memcached_dedicated_ssl_port}:
  11206 ->
  11206
[ns_server:debug,2020-03-27T19:47:05.359Z,ns_1@127.0.0.1:ns_config<0.193.0>:dist_manager:rename_node_in_config:392]renaming node conf {node,'ns_1@cb.local',memcached_defaults} -> {node,
                                                                 'ns_1@127.0.0.1',
                                                                 memcached_defaults}:
  [{max_connections,65000},
   {system_connections,5000},
   {connection_idle_time,0},
   {verbosity,0},
   {privilege_debug,false},
   {opentracing_enabled,false},
   {opentracing_module,[]},
   {opentracing_config,[]},
   {breakpad_enabled,true},
   {breakpad_minidump_dir_path,"/opt/couchbase/var/lib/couchbase/crash"},
   {dedupe_nmvb_maps,false},
   {tracing_enabled,true},
   {datatype_snappy,true},
   {num_reader_threads,<<"default">>},
   {num_writer_threads,<<"default">>}] ->
  [{max_connections,65000},
   {system_connections,5000},
   {connection_idle_time,0},
   {verbosity,0},
   {privilege_debug,false},
   {opentracing_enabled,false},
   {opentracing_module,[]},
   {opentracing_config,[]},
   {breakpad_enabled,true},
   {breakpad_minidump_dir_path,"/opt/couchbase/var/lib/couchbase/crash"},
   {dedupe_nmvb_maps,false},
   {tracing_enabled,true},
   {datatype_snappy,true},
   {num_reader_threads,<<"default">>},
   {num_writer_threads,<<"default">>}]
[ns_server:debug,2020-03-27T19:47:05.360Z,ns_1@127.0.0.1:ns_config<0.193.0>:dist_manager:rename_node_in_config:392]renaming node conf {node,'ns_1@cb.local',moxi} -> {node,'ns_1@127.0.0.1',moxi}:
  [{port,0}] ->
  [{port,0}]
[ns_server:debug,2020-03-27T19:47:05.361Z,ns_1@127.0.0.1:ns_config<0.193.0>:dist_manager:rename_node_in_config:392]renaming node conf {node,'ns_1@cb.local',ns_log} -> {node,'ns_1@127.0.0.1',
                                                     ns_log}:
  [{filename,"/opt/couchbase/var/lib/couchbase/ns_log"}] ->
  [{filename,"/opt/couchbase/var/lib/couchbase/ns_log"}]
[ns_server:debug,2020-03-27T19:47:05.361Z,ns_1@127.0.0.1:ns_config<0.193.0>:dist_manager:rename_node_in_config:392]renaming node conf {node,'ns_1@cb.local',port_servers} -> {node,
                                                           'ns_1@127.0.0.1',
                                                           port_servers}:
  [] ->
  []
[ns_server:debug,2020-03-27T19:47:05.361Z,ns_1@127.0.0.1:ns_config<0.193.0>:dist_manager:rename_node_in_config:392]renaming node conf {node,'ns_1@cb.local',projector_port} -> {node,
                                                             'ns_1@127.0.0.1',
                                                             projector_port}:
  9999 ->
  9999
[ns_server:debug,2020-03-27T19:47:05.362Z,ns_1@127.0.0.1:ns_config<0.193.0>:dist_manager:rename_node_in_config:392]renaming node conf {node,'ns_1@cb.local',projector_ssl_port} -> {node,
                                                                 'ns_1@127.0.0.1',
                                                                 projector_ssl_port}:
  9999 ->
  9999
[ns_server:debug,2020-03-27T19:47:05.362Z,ns_1@127.0.0.1:ns_config<0.193.0>:dist_manager:rename_node_in_config:392]renaming node conf {node,'ns_1@cb.local',query_port} -> {node,
                                                         'ns_1@127.0.0.1',
                                                         query_port}:
  8093 ->
  8093
[ns_server:debug,2020-03-27T19:47:05.362Z,ns_1@127.0.0.1:ns_config<0.193.0>:dist_manager:rename_node_in_config:392]renaming node conf {node,'ns_1@cb.local',rest} -> {node,'ns_1@127.0.0.1',rest}:
  [{port,8091},{port_meta,global}] ->
  [{port,8091},{port_meta,global}]
[ns_server:debug,2020-03-27T19:47:05.362Z,ns_1@127.0.0.1:ns_config<0.193.0>:dist_manager:rename_node_in_config:392]renaming node conf {node,'ns_1@cb.local',saslauthd_enabled} -> {node,
                                                                'ns_1@127.0.0.1',
                                                                saslauthd_enabled}:
  true ->
  true
[ns_server:debug,2020-03-27T19:47:05.362Z,ns_1@127.0.0.1:ns_config<0.193.0>:dist_manager:rename_node_in_config:392]renaming node conf {node,'ns_1@cb.local',ssl_capi_port} -> {node,
                                                            'ns_1@127.0.0.1',
                                                            ssl_capi_port}:
  18092 ->
  18092
[ns_server:debug,2020-03-27T19:47:05.362Z,ns_1@127.0.0.1:ns_config<0.193.0>:dist_manager:rename_node_in_config:392]renaming node conf {node,'ns_1@cb.local',ssl_query_port} -> {node,
                                                             'ns_1@127.0.0.1',
                                                             ssl_query_port}:
  18093 ->
  18093
[ns_server:debug,2020-03-27T19:47:05.363Z,ns_1@127.0.0.1:ns_config<0.193.0>:dist_manager:rename_node_in_config:392]renaming node conf {node,'ns_1@cb.local',ssl_rest_port} -> {node,
                                                            'ns_1@127.0.0.1',
                                                            ssl_rest_port}:
  18091 ->
  18091
[ns_server:debug,2020-03-27T19:47:05.363Z,ns_1@127.0.0.1:ns_config<0.193.0>:dist_manager:rename_node_in_config:392]renaming node conf {node,'ns_1@cb.local',uuid} -> {node,'ns_1@127.0.0.1',uuid}:
  <<"60a6bc3db77e6c7b91c556140dcfec71">> ->
  <<"60a6bc3db77e6c7b91c556140dcfec71">>
[ns_server:debug,2020-03-27T19:47:05.363Z,ns_1@127.0.0.1:ns_config<0.193.0>:dist_manager:rename_node_in_config:392]renaming node conf {node,'ns_1@cb.local',xdcr_rest_port} -> {node,
                                                             'ns_1@127.0.0.1',
                                                             xdcr_rest_port}:
  9998 ->
  9998
[ns_server:debug,2020-03-27T19:47:05.363Z,ns_1@127.0.0.1:ns_config<0.193.0>:dist_manager:rename_node_in_config:392]renaming node conf {node,'ns_1@cb.local',{project_intact,is_vulnerable}} -> {node,
                                                                             'ns_1@127.0.0.1',
                                                                             {project_intact,
                                                                              is_vulnerable}}:
  false ->
  false
[ns_server:debug,2020-03-27T19:47:05.365Z,ns_1@127.0.0.1:terse_cluster_info_uploader<0.483.0>:terse_cluster_info_uploader:handle_info:48]Refreshing terse cluster info with <<"{\"rev\":11,\"nodesExt\":[{\"services\":{\"mgmt\":8091,\"mgmtSSL\":18091,\"kv\":11210,\"kvSSL\":11207,\"capi\":8092,\"capiSSL\":18092,\"projector\":9999,\"projector\":9999},\"thisNode\":true}],\"clusterCapabilitiesVer\":[1,0],\"clusterCapabilities\":{\"n1ql\":[\"enhancedPreparedStatements\"]}}">>
[ns_server:debug,2020-03-27T19:47:05.364Z,ns_1@127.0.0.1:ns_config_rep<0.335.0>:ns_config_rep:do_push_keys:321]Replicating some config keys ([nodes_wanted,quorum_nodes,server_groups,
                               {local_changes_count,
                                   <<"60a6bc3db77e6c7b91c556140dcfec71">>},
                               {node,'ns_1@127.0.0.1',address_family},
                               {node,'ns_1@127.0.0.1',audit},
                               {node,'ns_1@127.0.0.1',capi_port},
                               {node,'ns_1@127.0.0.1',cbas_admin_port},
                               {node,'ns_1@127.0.0.1',cbas_cc_client_port},
                               {node,'ns_1@127.0.0.1',cbas_cc_cluster_port},
                               {node,'ns_1@127.0.0.1',cbas_cc_http_port},
                               {node,'ns_1@127.0.0.1',cbas_cluster_port},
                               {node,'ns_1@127.0.0.1',cbas_console_port},
                               {node,'ns_1@127.0.0.1',cbas_data_port},
                               {node,'ns_1@127.0.0.1',cbas_debug_port},
                               {node,'ns_1@127.0.0.1',cbas_dirs},
                               {node,'ns_1@127.0.0.1',cbas_http_port},
                               {node,'ns_1@127.0.0.1',cbas_messaging_port},
                               {node,'ns_1@127.0.0.1',
                                   cbas_metadata_callback_port},
                               {node,'ns_1@127.0.0.1',cbas_metadata_port},
                               {node,'ns_1@127.0.0.1',cbas_parent_port},
                               {node,'ns_1@127.0.0.1',cbas_replication_port},
                               {node,'ns_1@127.0.0.1',cbas_result_port},
                               {node,'ns_1@127.0.0.1',cbas_ssl_port},
                               {node,'ns_1@127.0.0.1',compaction_daemon},
                               {node,'ns_1@127.0.0.1',config_version},
                               {node,'ns_1@127.0.0.1',erl_external_listeners},
                               {node,'ns_1@127.0.0.1',eventing_debug_port},
                               {node,'ns_1@127.0.0.1',eventing_dir},
                               {node,'ns_1@127.0.0.1',eventing_http_port},
                               {node,'ns_1@127.0.0.1',eventing_https_port},
                               {node,'ns_1@127.0.0.1',fts_grpc_port},
                               {node,'ns_1@127.0.0.1',fts_grpc_ssl_port},
                               {node,'ns_1@127.0.0.1',fts_http_port},
                               {node,'ns_1@127.0.0.1',fts_ssl_port},
                               {node,'ns_1@127.0.0.1',indexer_admin_port},
                               {node,'ns_1@127.0.0.1',indexer_http_port},
                               {node,'ns_1@127.0.0.1',indexer_https_port},
                               {node,'ns_1@127.0.0.1',indexer_scan_port},
                               {node,'ns_1@127.0.0.1',indexer_stcatchup_port},
                               {node,'ns_1@127.0.0.1',indexer_stinit_port},
                               {node,'ns_1@127.0.0.1',indexer_stmaint_port},
                               {node,'ns_1@127.0.0.1',is_enterprise},
                               {node,'ns_1@127.0.0.1',isasl},
                               {node,'ns_1@127.0.0.1',membership},
                               {node,'ns_1@127.0.0.1',memcached},
                               {node,'ns_1@127.0.0.1',memcached_config},
                               {node,'ns_1@127.0.0.1',
                                   memcached_dedicated_ssl_port},
                               {node,'ns_1@127.0.0.1',memcached_defaults},
                               {node,'ns_1@127.0.0.1',moxi},
                               {node,'ns_1@127.0.0.1',node_encryption},
                               {node,'ns_1@127.0.0.1',ns_log},
                               {node,'ns_1@127.0.0.1',port_servers},
                               {node,'ns_1@127.0.0.1',projector_port},
                               {node,'ns_1@127.0.0.1',projector_ssl_port},
                               {node,'ns_1@127.0.0.1',query_port},
                               {node,'ns_1@127.0.0.1',rest},
                               {node,'ns_1@127.0.0.1',saslauthd_enabled},
                               {node,'ns_1@127.0.0.1',ssl_capi_port},
                               {node,'ns_1@127.0.0.1',ssl_query_port},
                               {node,'ns_1@127.0.0.1',ssl_rest_port},
                               {node,'ns_1@127.0.0.1',uuid},
                               {node,'ns_1@127.0.0.1',xdcr_rest_port},
                               {node,'ns_1@127.0.0.1',
                                   {project_intact,is_vulnerable}}]..)
[ns_server:info,2020-03-27T19:47:05.366Z,ns_1@127.0.0.1:ns_ssl_services_setup<0.214.0>:ns_ssl_services_setup:handle_info:542]Got certificate and pkey change
[ns_server:debug,2020-03-27T19:47:05.366Z,ns_1@127.0.0.1:ns_config_events<0.191.0>:ns_node_disco_conf_events:handle_event:44]ns_node_disco_conf_events config on nodes_wanted
[ns_server:debug,2020-03-27T19:47:05.366Z,ns_1@127.0.0.1:mb_master<0.548.0>:mb_master:update_peers:577]List of peers has changed from ['ns_1@cb.local'] to ['ns_1@127.0.0.1']
[ns_server:warn,2020-03-27T19:47:05.366Z,ns_1@127.0.0.1:leader_quorum_nodes_manager<0.553.0>:leader_quorum_nodes_manager:handle_quorum_nodes_updated:170]Somebody else updated the quorum nodes when we are the master node.
Our quorum nodes: ['ns_1@cb.local']
Their quorum nodes: ['ns_1@127.0.0.1']
[ns_server:debug,2020-03-27T19:47:05.368Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{local_changes_count,<<"60a6bc3db77e6c7b91c556140dcfec71">>} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{11,63752557625}}]}]
[ns_server:debug,2020-03-27T19:47:05.369Z,ns_1@127.0.0.1:ns_cookie_manager<0.188.0>:ns_cookie_manager:do_cookie_sync:107]ns_cookie_manager do_cookie_sync
[ns_server:debug,2020-03-27T19:47:05.369Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',{project_intact,is_vulnerable}} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|false]
[error_logger:error,2020-03-27T19:47:05.367Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]** Generic server leader_quorum_nodes_manager terminating 
** Last message in was quorum_nodes_updated
** When Server state == {state,{set,1,16,16,8,80,48,
                                    {[],[],[],[],[],[],[],[],[],[],[],[],[],
                                     [],[],[]},
                                    {{[],[],[],[],[],[],[],[],[],[],[],[],
                                      ['ns_1@cb.local'],
                                      [],[],[]}}}}
** Reason for termination == 
** {{quorum_nodes_update_conflict,['ns_1@cb.local'],['ns_1@127.0.0.1']},
    [{leader_quorum_nodes_manager,handle_quorum_nodes_updated,1,
         [{file,"src/leader_quorum_nodes_manager.erl"},{line,175}]},
     {leader_quorum_nodes_manager,handle_info,2,
         [{file,"src/leader_quorum_nodes_manager.erl"},{line,95}]},
     {gen_server2,handle_info,2,[{file,"src/gen_server2.erl"},{line,228}]},
     {gen_server,try_dispatch,4,[{file,"gen_server.erl"},{line,616}]},
     {gen_server,handle_msg,6,[{file,"gen_server.erl"},{line,686}]},
     {proc_lib,init_p_do_apply,3,[{file,"proc_lib.erl"},{line,247}]}]}

[ns_server:debug,2020-03-27T19:47:05.369Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',xdcr_rest_port} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|9998]
[ns_server:debug,2020-03-27T19:47:05.370Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',uuid} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
 <<"60a6bc3db77e6c7b91c556140dcfec71">>]
[ns_server:debug,2020-03-27T19:47:05.371Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',ssl_rest_port} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|18091]
[ns_server:debug,2020-03-27T19:47:05.372Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',ssl_query_port} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|18093]
[ns_server:debug,2020-03-27T19:47:05.372Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',ssl_capi_port} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|18092]
[ns_server:debug,2020-03-27T19:47:05.372Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',saslauthd_enabled} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|true]
[ns_server:debug,2020-03-27T19:47:05.373Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',rest} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]},
 {port,8091},
 {port_meta,global}]
[ns_server:debug,2020-03-27T19:47:05.374Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',query_port} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|8093]
[ns_server:debug,2020-03-27T19:47:05.374Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',projector_ssl_port} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|9999]
[ns_server:debug,2020-03-27T19:47:05.374Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',projector_port} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|9999]
[ns_server:debug,2020-03-27T19:47:05.374Z,ns_1@127.0.0.1:<0.2502.0>:ns_node_disco:do_nodes_wanted_updated_fun:214]ns_node_disco: nodes_wanted updated: ['ns_1@127.0.0.1'], with cookie: {sanitized,
                                                                       <<"C4PeZIe8LLStUy+d1HIN638gh0wTZW33h1wdSpFkvzc=">>}
[ns_server:debug,2020-03-27T19:47:05.374Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',port_servers} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}]
[ns_server:debug,2020-03-27T19:47:05.374Z,ns_1@127.0.0.1:dist_manager<0.166.0>:dist_manager:wait_for_node:270]Waiting for connection to node 'couchdb_ns_1@cb.local' to be established
[ns_server:debug,2020-03-27T19:47:05.374Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',ns_log} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]},
 {filename,"/opt/couchbase/var/lib/couchbase/ns_log"}]
[error_logger:info,2020-03-27T19:47:05.375Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{connect,normal,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2020-03-27T19:47:05.374Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',moxi} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]},
 {port,0}]
[ns_server:debug,2020-03-27T19:47:05.375Z,ns_1@127.0.0.1:net_kernel<0.2486.0>:cb_dist:info_msg:754]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2020-03-27T19:47:05.375Z,ns_1@127.0.0.1:<0.2502.0>:ns_node_disco:do_nodes_wanted_updated_fun:220]ns_node_disco: nodes_wanted pong: ['ns_1@127.0.0.1'], with cookie: {sanitized,
                                                                    <<"C4PeZIe8LLStUy+d1HIN638gh0wTZW33h1wdSpFkvzc=">>}
[ns_server:debug,2020-03-27T19:47:05.375Z,ns_1@127.0.0.1:cb_dist<0.2483.0>:cb_dist:info_msg:754]cb_dist: Added connection {con,#Ref<0.1597663237.214695937.147122>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2020-03-27T19:47:05.375Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',memcached_defaults} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]},
 {max_connections,65000},
 {system_connections,5000},
 {connection_idle_time,0},
 {verbosity,0},
 {privilege_debug,false},
 {opentracing_enabled,false},
 {opentracing_module,[]},
 {opentracing_config,[]},
 {breakpad_enabled,true},
 {breakpad_minidump_dir_path,"/opt/couchbase/var/lib/couchbase/crash"},
 {dedupe_nmvb_maps,false},
 {tracing_enabled,true},
 {datatype_snappy,true},
 {num_reader_threads,<<"default">>},
 {num_writer_threads,<<"default">>}]
[ns_server:debug,2020-03-27T19:47:05.375Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',memcached_dedicated_ssl_port} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|11206]
[ns_server:debug,2020-03-27T19:47:05.375Z,ns_1@127.0.0.1:cb_dist<0.2483.0>:cb_dist:info_msg:754]cb_dist: Updated connection: {con,#Ref<0.1597663237.214695937.147122>,
                                  inet_tcp_dist,<0.2505.0>,
                                  #Ref<0.1597663237.214695937.147126>}
[ns_server:debug,2020-03-27T19:47:05.377Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',memcached_config} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
 {[{interfaces,
    {memcached_config_mgr,omit_missing_mcd_ports,
     [{[{host,<<"*">>},
        {port,port},
        {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
        {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]},
      {[{host,<<"*">>},
        {port,dedicated_port},
        {system,true},
        {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
        {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]},
      {[{host,<<"*">>},
        {port,ssl_port},
        {ssl,
         {[{key,
            <<"/opt/couchbase/var/lib/couchbase/config/memcached-key.pem">>},
           {cert,
            <<"/opt/couchbase/var/lib/couchbase/config/memcached-cert.pem">>}]}},
        {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
        {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]},
      {[{host,<<"*">>},
        {port,dedicated_ssl_port},
        {system,true},
        {ssl,
         {[{key,
            <<"/opt/couchbase/var/lib/couchbase/config/memcached-key.pem">>},
           {cert,
            <<"/opt/couchbase/var/lib/couchbase/config/memcached-cert.pem">>}]}},
        {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
        {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]}]}},
   {ssl_cipher_list,{memcached_config_mgr,get_ssl_cipher_list,[]}},
   {ssl_cipher_order,{memcached_config_mgr,get_ssl_cipher_order,[]}},
   {client_cert_auth,{memcached_config_mgr,client_cert_auth,[]}},
   {ssl_minimum_protocol,{memcached_config_mgr,ssl_minimum_protocol,[]}},
   {connection_idle_time,connection_idle_time},
   {privilege_debug,privilege_debug},
   {breakpad,
    {[{enabled,breakpad_enabled},
      {minidump_dir,{memcached_config_mgr,get_minidump_dir,[]}}]}},
   {opentracing,
    {[{enabled,opentracing_enabled},
      {module,{"~s",[opentracing_module]}},
      {config,{"~s",[opentracing_config]}}]}},
   {admin,{"~s",[admin_user]}},
   {verbosity,verbosity},
   {audit_file,{"~s",[audit_file]}},
   {rbac_file,{"~s",[rbac_file]}},
   {dedupe_nmvb_maps,dedupe_nmvb_maps},
   {tracing_enabled,tracing_enabled},
   {datatype_snappy,{memcached_config_mgr,is_snappy_enabled,[]}},
   {xattr_enabled,true},
   {scramsha_fallback_salt,{memcached_config_mgr,get_fallback_salt,[]}},
   {collections_enabled,{memcached_config_mgr,collections_enabled,[]}},
   {max_connections,max_connections},
   {system_connections,system_connections},
   {num_reader_threads,num_reader_threads},
   {num_writer_threads,num_writer_threads},
   {logger,
    {[{filename,{"~s/~s",[log_path,log_prefix]}},
      {cyclesize,log_cyclesize},
      {sleeptime,log_sleeptime}]}},
   {external_auth_service,{memcached_config_mgr,get_external_auth_service,[]}},
   {active_external_users_push_interval,
    {memcached_config_mgr,get_external_users_push_interval,[]}}]}]
[ns_server:debug,2020-03-27T19:47:05.377Z,ns_1@127.0.0.1:terse_cluster_info_uploader<0.483.0>:terse_cluster_info_uploader:handle_info:48]Refreshing terse cluster info with <<"{\"rev\":11,\"nodesExt\":[{\"services\":{\"mgmt\":8091,\"mgmtSSL\":18091,\"kv\":11210,\"kvSSL\":11207,\"capi\":8092,\"capiSSL\":18092,\"projector\":9999,\"projector\":9999},\"thisNode\":true}],\"clusterCapabilitiesVer\":[1,0],\"clusterCapabilities\":{\"n1ql\":[\"enhancedPreparedStatements\"]}}">>
[ns_server:debug,2020-03-27T19:47:05.378Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',memcached} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]},
 {port,11210},
 {dedicated_port,11209},
 {dedicated_ssl_port,11206},
 {ssl_port,11207},
 {admin_user,"@ns_server"},
 {other_users,["@cbq-engine","@projector","@goxdcr","@index","@fts",
               "@eventing","@cbas"]},
 {admin_pass,"*****"},
 {engines,[{membase,[{engine,"/opt/couchbase/lib/memcached/ep.so"},
                     {static_config_string,"failpartialwarmup=false"}]},
           {memcached,[{engine,"/opt/couchbase/lib/memcached/default_engine.so"},
                       {static_config_string,"vb0=true"}]}]},
 {config_path,"/opt/couchbase/var/lib/couchbase/config/memcached.json"},
 {audit_file,"/opt/couchbase/var/lib/couchbase/config/audit.json"},
 {rbac_file,"/opt/couchbase/var/lib/couchbase/config/memcached.rbac"},
 {log_path,"/opt/couchbase/var/lib/couchbase/logs"},
 {log_prefix,"memcached.log"},
 {log_generations,20},
 {log_cyclesize,10485760},
 {log_sleeptime,19},
 {log_rotation_period,39003}]
[ns_server:debug,2020-03-27T19:47:05.378Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',membership} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
 active]
[ns_server:debug,2020-03-27T19:47:05.379Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',isasl} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]},
 {path,"/opt/couchbase/var/lib/couchbase/isasl.pw"}]
[ns_server:debug,2020-03-27T19:47:05.380Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',is_enterprise} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|true]
[ns_server:debug,2020-03-27T19:47:05.381Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',indexer_stmaint_port} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|9105]
[ns_server:debug,2020-03-27T19:47:05.381Z,ns_1@127.0.0.1:dist_manager<0.166.0>:dist_manager:wait_for_node:282]Observed node 'couchdb_ns_1@cb.local' to come up
[ns_server:debug,2020-03-27T19:47:05.381Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',indexer_stinit_port} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|9103]
[ns_server:debug,2020-03-27T19:47:05.382Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',indexer_stcatchup_port} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|9104]
[ns_server:debug,2020-03-27T19:47:05.383Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',indexer_scan_port} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|9101]
[ns_server:debug,2020-03-27T19:47:05.383Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',indexer_https_port} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|19102]
[ns_server:debug,2020-03-27T19:47:05.383Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',indexer_http_port} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|9102]
[ns_server:debug,2020-03-27T19:47:05.384Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',indexer_admin_port} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|9100]
[ns_server:debug,2020-03-27T19:47:05.384Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',fts_ssl_port} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|18094]
[ns_server:debug,2020-03-27T19:47:05.384Z,ns_1@127.0.0.1:leader_activities<0.542.0>:leader_activities:handle_internal_process_down:511]Process {quorum_nodes_manager,<0.553.0>} terminated with reason {quorum_nodes_update_conflict,
                                                                 ['ns_1@cb.local'],
                                                                 ['ns_1@127.0.0.1']}
[ns_server:debug,2020-03-27T19:47:05.384Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',fts_http_port} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|8094]
[ns_server:debug,2020-03-27T19:47:05.384Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',fts_grpc_ssl_port} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|19130]
[ns_server:debug,2020-03-27T19:47:05.384Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',fts_grpc_port} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|9130]
[ns_server:debug,2020-03-27T19:47:05.384Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',eventing_https_port} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|18096]
[ns_server:debug,2020-03-27T19:47:05.385Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',eventing_http_port} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|8096]
[ns_server:debug,2020-03-27T19:47:05.384Z,ns_1@127.0.0.1:<0.570.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.553.0>} exited with reason {quorum_nodes_update_conflict,
                                                                                ['ns_1@cb.local'],
                                                                                ['ns_1@127.0.0.1']}
[ns_server:debug,2020-03-27T19:47:05.385Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',eventing_debug_port} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|9140]
[ns_server:debug,2020-03-27T19:47:05.385Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',config_version} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|{6,5}]
[ns_server:debug,2020-03-27T19:47:05.385Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',compaction_daemon} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]},
 {check_interval,30},
 {min_db_file_size,131072},
 {min_view_file_size,20971520}]
[ns_server:debug,2020-03-27T19:47:05.386Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',cbas_ssl_port} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|18095]
[ns_server:debug,2020-03-27T19:47:05.386Z,ns_1@127.0.0.1:leader_quorum_nodes_manager<0.2513.0>:leader_quorum_nodes_manager:pull_config:114]Attempting to pull config from nodes:
[]
[ns_server:debug,2020-03-27T19:47:05.386Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',cbas_result_port} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|9117]
[ns_server:debug,2020-03-27T19:47:05.386Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',cbas_replication_port} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|9120]
[ns_server:debug,2020-03-27T19:47:05.386Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',cbas_parent_port} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|9122]
[ns_server:debug,2020-03-27T19:47:05.386Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',cbas_metadata_port} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|9121]
[error_logger:error,2020-03-27T19:47:05.386Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================CRASH REPORT=========================
  crasher:
    initial call: leader_quorum_nodes_manager:init/1
    pid: <0.553.0>
    registered_name: leader_quorum_nodes_manager
    exception exit: {quorum_nodes_update_conflict,
                        ['ns_1@cb.local'],
                        ['ns_1@127.0.0.1']}
      in function  leader_quorum_nodes_manager:handle_quorum_nodes_updated/1 (src/leader_quorum_nodes_manager.erl, line 175)
      in call from leader_quorum_nodes_manager:handle_info/2 (src/leader_quorum_nodes_manager.erl, line 95)
      in call from gen_server2:handle_info/2 (src/gen_server2.erl, line 228)
      in call from gen_server:try_dispatch/4 (gen_server.erl, line 616)
      in call from gen_server:handle_msg/6 (gen_server.erl, line 686)
    ancestors: [mb_master_sup,mb_master,leader_registry_sup,
                  leader_services_sup,<0.539.0>,ns_server_sup,
                  ns_server_nodes_sup,<0.206.0>,ns_server_cluster_sup,
                  root_sup,<0.118.0>]
    message_queue_len: 0
    messages: []
    links: [<0.550.0>,<0.570.0>]
    dictionary: [{{'$gen_server2',module},leader_quorum_nodes_manager},
                  {{'$gen_server2',{have_callback,handle_cast}},false},
                  {{'$gen_server2',{have_callback,handle_info}},true},
                  {{'$gen_server2',{have_callback,terminate}},false},
                  {{'$gen_server2',{have_callback,code_change}},false},
                  {{'$gen_server2',{have_callback,handle_job_death}},false},
                  {{'$gen_server2',{have_callback,init}},true},
                  {{'$gen_server2',{have_callback,handle_call}},true}]
    trap_exit: true
    status: running
    heap_size: 2586
    stack_size: 27
    reductions: 7866
  neighbours:

[ns_server:debug,2020-03-27T19:47:05.386Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',cbas_metadata_callback_port} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|9119]
[ns_server:debug,2020-03-27T19:47:05.387Z,ns_1@127.0.0.1:terse_cluster_info_uploader<0.483.0>:terse_cluster_info_uploader:handle_info:48]Refreshing terse cluster info with <<"{\"rev\":11,\"nodesExt\":[{\"services\":{\"mgmt\":8091,\"mgmtSSL\":18091,\"kv\":11210,\"kvSSL\":11207,\"capi\":8092,\"capiSSL\":18092,\"projector\":9999,\"projector\":9999},\"thisNode\":true}],\"clusterCapabilitiesVer\":[1,0],\"clusterCapabilities\":{\"n1ql\":[\"enhancedPreparedStatements\"]}}">>
[ns_server:debug,2020-03-27T19:47:05.386Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',cbas_messaging_port} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|9118]
[ns_server:debug,2020-03-27T19:47:05.387Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',cbas_http_port} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|8095]
[error_logger:error,2020-03-27T19:47:05.386Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================SUPERVISOR REPORT=========================
     Supervisor: {local,mb_master_sup}
     Context:    child_terminated
     Reason:     {quorum_nodes_update_conflict,
                     ['ns_1@cb.local'],
                     ['ns_1@127.0.0.1']}
     Offender:   [{pid,<0.553.0>},
                  {id,leader_quorum_nodes_manager},
                  {mfargs,{leader_quorum_nodes_manager,start_link,[]}},
                  {restart_type,permanent},
                  {shutdown,1000},
                  {child_type,worker}]


[ns_server:debug,2020-03-27T19:47:05.387Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',cbas_debug_port} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|-1]
[ns_server:debug,2020-03-27T19:47:05.387Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',cbas_data_port} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|9116]
[error_logger:info,2020-03-27T19:47:05.387Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,mb_master_sup}
             started: [{pid,<0.2513.0>},
                       {id,leader_quorum_nodes_manager},
                       {mfargs,{leader_quorum_nodes_manager,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-03-27T19:47:05.388Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',cbas_console_port} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|9114]
[ns_server:debug,2020-03-27T19:47:05.388Z,ns_1@127.0.0.1:leader_quorum_nodes_manager<0.2513.0>:leader_quorum_nodes_manager:pull_config:119]Pulled config successfully.
[ns_server:debug,2020-03-27T19:47:05.389Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',cbas_cluster_port} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|9115]
[ns_server:debug,2020-03-27T19:47:05.389Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',cbas_cc_http_port} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|9111]
[ns_server:debug,2020-03-27T19:47:05.389Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',cbas_cc_cluster_port} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|9112]
[ns_server:debug,2020-03-27T19:47:05.389Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',cbas_cc_client_port} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|9113]
[ns_server:debug,2020-03-27T19:47:05.389Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',cbas_admin_port} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|9110]
[ns_server:debug,2020-03-27T19:47:05.389Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',capi_port} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|8092]
[ns_server:debug,2020-03-27T19:47:05.390Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',audit} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}]
[ns_server:debug,2020-03-27T19:47:05.390Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
server_groups ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557625}}]},
 [{uuid,<<"0">>},{name,<<"Group 1">>},{nodes,['ns_1@127.0.0.1']}]]
[ns_server:debug,2020-03-27T19:47:05.390Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
quorum_nodes ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]},
 'ns_1@127.0.0.1']
[ns_server:debug,2020-03-27T19:47:05.390Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
nodes_wanted ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557625}}]},
 'ns_1@127.0.0.1']
[ns_server:debug,2020-03-27T19:47:05.391Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',cbas_dirs} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]},
 "/opt/couchbase/var/lib/couchbase/data"]
[ns_server:debug,2020-03-27T19:47:05.391Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',eventing_dir} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]},
 47,111,112,116,47,99,111,117,99,104,98,97,115,101,47,118,97,114,47,108,105,
 98,47,99,111,117,99,104,98,97,115,101,47,100,97,116,97]
[ns_server:debug,2020-03-27T19:47:05.391Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',address_family} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|inet]
[ns_server:debug,2020-03-27T19:47:05.391Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',node_encryption} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|false]
[ns_server:debug,2020-03-27T19:47:05.391Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',erl_external_listeners} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]},
 {inet,false},
 {inet6,false}]
[error_logger:info,2020-03-27T19:47:05.393Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{connect,normal,'ns_1@cb.local'}}
[ns_server:debug,2020-03-27T19:47:05.394Z,ns_1@127.0.0.1:net_kernel<0.2486.0>:cb_dist:info_msg:754]cb_dist: Setting up new connection to 'ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2020-03-27T19:47:05.394Z,ns_1@127.0.0.1:cb_dist<0.2483.0>:cb_dist:info_msg:754]cb_dist: Added connection {con,#Ref<0.1597663237.214695938.146662>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2020-03-27T19:47:05.395Z,ns_1@127.0.0.1:cb_dist<0.2483.0>:cb_dist:info_msg:754]cb_dist: Updated connection: {con,#Ref<0.1597663237.214695938.146662>,
                                  inet_tcp_dist,<0.2527.0>,
                                  #Ref<0.1597663237.214695938.146666>}
[ns_server:debug,2020-03-27T19:47:05.395Z,ns_1@127.0.0.1:dist_manager<0.166.0>:dist_manager:complete_rename:373]Node 'ns_1@cb.local' has been renamed to 'ns_1@127.0.0.1'.
[ns_server:debug,2020-03-27T19:47:05.396Z,ns_1@127.0.0.1:cb_dist<0.2483.0>:cb_dist:info_msg:754]cb_dist: Accepted new connection from <0.2487.0>: {con,
                                                   #Ref<0.1597663237.214695938.146673>,
                                                   inet_tcp_dist,undefined,
                                                   undefined}
[ns_server:debug,2020-03-27T19:47:05.396Z,ns_1@127.0.0.1:net_kernel<0.2486.0>:cb_dist:info_msg:754]cb_dist: Accepting connection from acceptor <0.2487.0> using module inet_tcp_dist
[ns_server:debug,2020-03-27T19:47:05.396Z,ns_1@127.0.0.1:cb_dist<0.2483.0>:cb_dist:info_msg:754]cb_dist: Updated connection: {con,#Ref<0.1597663237.214695938.146673>,
                                  inet_tcp_dist,<0.2530.0>,
                                  #Ref<0.1597663237.214695938.146675>}
[error_logger:info,2020-03-27T19:47:05.396Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{'EXIT',<0.2527.0>,
                    {recv_challenge_failed,no_node,"ns_1@127.0.0.1"}}}
[ns_server:debug,2020-03-27T19:47:05.396Z,ns_1@127.0.0.1:cb_dist<0.2483.0>:cb_dist:info_msg:754]cb_dist: Connection down: {con,#Ref<0.1597663237.214695938.146662>,
                               inet_tcp_dist,<0.2527.0>,
                               #Ref<0.1597663237.214695938.146666>}
[ns_server:debug,2020-03-27T19:47:05.396Z,ns_1@127.0.0.1:cb_dist<0.2483.0>:cb_dist:info_msg:754]cb_dist: Connection down: {con,#Ref<0.1597663237.214695938.146673>,
                               inet_tcp_dist,<0.2530.0>,
                               #Ref<0.1597663237.214695938.146675>}
[error_logger:info,2020-03-27T19:47:05.396Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{net_kernel,913,nodedown,'ns_1@cb.local'}}
[error_logger:info,2020-03-27T19:47:05.396Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{'EXIT',<0.2530.0>,{recv_challenge_reply_failed,{error,closed}}}}
[error_logger:info,2020-03-27T19:47:05.397Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{net_kernel,913,nodedown,'ns_1@127.0.0.1'}}
[ns_server:info,2020-03-27T19:47:05.400Z,ns_1@127.0.0.1:ns_ssl_services_setup<0.214.0>:ns_ssl_services_setup:maybe_generate_local_cert:617]Detected existing node certificate that did not match cluster certificate. Will re-generate
[ns_server:debug,2020-03-27T19:47:05.472Z,ns_1@127.0.0.1:<0.2534.0>:goport:handle_eof:582]Stream 'stderr' closed
[ns_server:debug,2020-03-27T19:47:05.473Z,ns_1@127.0.0.1:<0.2534.0>:goport:handle_eof:582]Stream 'stdout' closed
[ns_server:info,2020-03-27T19:47:05.473Z,ns_1@127.0.0.1:<0.2534.0>:goport:handle_process_exit:563]Port exited with status 0.
[ns_server:info,2020-03-27T19:47:05.507Z,ns_1@127.0.0.1:ns_ssl_services_setup<0.214.0>:ns_ssl_services_setup:do_generate_local_cert:608]Saved local cert for node 'ns_1@127.0.0.1'
[ns_server:debug,2020-03-27T19:47:05.573Z,ns_1@127.0.0.1:cb_dist<0.2483.0>:cb_dist:info_msg:754]cb_dist: Restarting tls distribution protocols (if any)
[ns_server:debug,2020-03-27T19:47:05.573Z,ns_1@127.0.0.1:cb_dist<0.2483.0>:cb_dist:info_msg:754]cb_dist: ignoring closing of inet6_tls_dist because listener is not started
[ns_server:debug,2020-03-27T19:47:05.573Z,ns_1@127.0.0.1:cb_dist<0.2483.0>:cb_dist:info_msg:754]cb_dist: ignoring closing of inet_tls_dist because listener is not started
[ns_server:info,2020-03-27T19:47:05.573Z,ns_1@127.0.0.1:ns_ssl_services_setup<0.214.0>:ns_ssl_services_setup:handle_info:545]Wrote new pem file
[ns_server:debug,2020-03-27T19:47:05.573Z,ns_1@127.0.0.1:ns_ssl_services_setup<0.214.0>:ns_ssl_services_setup:notify_services:740]Going to notify following services: [ssl_service,capi_ssl_service,xdcr_proxy,
                                     memcached,event]
[ns_server:info,2020-03-27T19:47:05.573Z,ns_1@127.0.0.1:<0.2545.0>:ns_ssl_services_setup:notify_service:772]Successfully notified service memcached
[ns_server:info,2020-03-27T19:47:05.573Z,ns_1@127.0.0.1:<0.2546.0>:ns_ssl_services_setup:notify_service:772]Successfully notified service event
[ns_server:debug,2020-03-27T19:47:05.573Z,ns_1@127.0.0.1:memcached_refresh<0.211.0>:memcached_refresh:handle_cast:55]Refresh of ssl_certs requested
[ns_server:debug,2020-03-27T19:47:05.574Z,ns_1@127.0.0.1:<0.223.0>:restartable:loop:71]Restarting child <0.585.0>
  MFA: {ns_ssl_services_setup,start_link_rest_service,[]}
  Shutdown policy: 1000
  Caller: {<0.2542.0>,#Ref<0.1597663237.214695938.146927>}
[ns_server:debug,2020-03-27T19:47:05.573Z,ns_1@127.0.0.1:<0.2544.0>:ns_ports_manager:restart_port_by_name:43]Requesting restart of port xdcr_proxy
[ns_server:info,2020-03-27T19:47:05.575Z,ns_1@127.0.0.1:<0.2544.0>:ns_ssl_services_setup:notify_service:772]Successfully notified service xdcr_proxy
[ns_server:debug,2020-03-27T19:47:05.576Z,ns_1@127.0.0.1:<0.223.0>:restartable:shutdown_child:120]Successfully terminated process <0.585.0>
[ns_server:debug,2020-03-27T19:47:05.577Z,ns_1@127.0.0.1:memcached_refresh<0.211.0>:memcached_refresh:handle_info:89]Refresh of [ssl_certs] succeeded
[ns_server:info,2020-03-27T19:47:05.591Z,ns_1@127.0.0.1:<0.2543.0>:ns_ssl_services_setup:notify_service:772]Successfully notified service capi_ssl_service
[ns_server:info,2020-03-27T19:47:05.593Z,ns_1@127.0.0.1:<0.2547.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for cbas
[ns_server:info,2020-03-27T19:47:05.594Z,ns_1@127.0.0.1:<0.2547.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for eventing
[ns_server:info,2020-03-27T19:47:05.594Z,ns_1@127.0.0.1:<0.2547.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for fts
[ns_server:info,2020-03-27T19:47:05.594Z,ns_1@127.0.0.1:<0.2547.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for n1ql
[error_logger:info,2020-03-27T19:47:05.597Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {<0.2547.0>,menelaus_web}
             started: [{pid,<0.2548.0>},
                       {id,menelaus_web_ipv4},
                       {mfargs,
                        {menelaus_web,http_server,
                         [[{ip,"0.0.0.0"},
                           {name,menelaus_web_ssl_ipv4},
                           {ssl,true},
                           {ssl_opts,
                            [{keyfile,
                              "/opt/couchbase/var/lib/couchbase/config/ssl-cert-key.pem"},
                             {certfile,
                              "/opt/couchbase/var/lib/couchbase/config/ssl-cert-key.pem"},
                             {versions,['tlsv1.1','tlsv1.2']},
                             {cacerts,
                              [<<48,130,3,2,48,130,1,234,160,3,2,1,2,2,8,22,
                                 0,64,208,110,172,210,92,48,13,6,9,42,134,72,
                                 134,247,13,1,1,11,5,0,48,36,49,34,48,32,6,3,
                                 85,4,3,19,25,67,111,117,99,104,98,97,115,
                                 101,32,83,101,114,118,101,114,32,51,52,56,
                                 101,100,50,55,48,48,30,23,13,49,51,48,49,48,
                                 49,48,48,48,48,48,48,90,23,13,52,57,49,50,
                                 51,49,50,51,53,57,53,57,90,48,36,49,34,48,
                                 32,6,3,85,4,3,19,25,67,111,117,99,104,98,97,
                                 115,101,32,83,101,114,118,101,114,32,51,52,
                                 56,101,100,50,55,48,48,130,1,34,48,13,6,9,
                                 42,134,72,134,247,13,1,1,1,5,0,3,130,1,15,0,
                                 48,130,1,10,2,130,1,1,0,225,209,231,201,130,
                                 247,233,2,187,220,222,34,52,112,118,191,205,
                                 152,82,245,106,26,160,209,228,176,90,40,98,
                                 172,183,1,136,153,71,239,18,220,136,15,75,
                                 73,223,113,145,171,33,98,84,62,10,36,82,160,
                                 106,135,171,185,245,166,220,188,43,178,51,2,
                                 205,146,75,139,58,68,250,195,53,51,182,176,
                                 42,160,42,14,18,94,26,188,161,102,35,66,108,
                                 137,96,63,168,160,212,254,70,169,139,74,198,
                                 244,204,206,194,101,179,132,181,1,21,76,117,
                                 72,101,24,165,108,212,56,253,47,230,184,143,
                                 50,20,121,74,247,204,71,247,226,97,249,191,
                                 174,226,150,23,149,71,180,209,86,216,72,31,
                                 202,197,17,62,92,90,116,10,212,69,89,20,91,
                                 163,51,77,94,156,13,115,23,71,83,252,222,
                                 166,85,132,171,214,167,174,78,60,63,78,241,
                                 115,199,70,186,175,191,94,208,5,216,173,106,
                                 248,203,56,72,24,219,23,86,67,137,18,221,
                                 171,208,1,41,239,84,253,158,230,36,78,222,
                                 33,120,90,96,209,36,5,230,90,163,19,2,35,
                                 165,80,128,171,6,90,11,237,162,35,3,201,94,
                                 110,201,181,13,49,165,2,3,1,0,1,163,56,48,
                                 54,48,14,6,3,85,29,15,1,1,255,4,4,3,2,2,164,
                                 48,19,6,3,85,29,37,4,12,48,10,6,8,43,6,1,5,
                                 5,7,3,1,48,15,6,3,85,29,19,1,1,255,4,5,48,3,
                                 1,1,255,48,13,6,9,42,134,72,134,247,13,1,1,
                                 11,5,0,3,130,1,1,0,6,81,65,124,71,43,122,15,
                                 167,207,16,251,0,159,39,98,188,227,29,215,
                                 100,107,194,184,208,49,234,49,157,22,16,114,
                                 226,174,13,150,172,144,2,238,230,8,40,221,
                                 26,55,56,132,126,233,153,24,185,41,108,27,
                                 239,216,82,120,63,12,251,133,139,191,7,65,
                                 245,131,92,34,201,230,196,53,62,156,158,117,
                                 62,220,159,192,101,171,131,143,145,151,43,
                                 167,227,90,161,173,77,152,27,124,67,49,150,
                                 67,102,161,221,43,166,252,34,106,208,76,223,
                                 73,230,216,161,26,132,137,129,146,79,142,69,
                                 179,193,194,47,129,207,251,4,237,180,247,54,
                                 155,40,216,36,79,59,110,188,27,153,208,45,
                                 211,245,157,64,19,243,225,128,45,126,172,97,
                                 164,65,55,250,227,90,212,59,153,137,219,195,
                                 84,171,109,145,64,111,253,109,120,35,199,
                                 153,232,120,170,179,167,110,176,116,10,245,
                                 79,6,66,158,148,137,191,125,65,164,153,94,
                                 180,226,83,230,241,236,39,67,40,12,91,44,79,
                                 54,65,72,249,75,235,222,205,142,112,55,224,
                                 47,125,42,235,188,14,128,134,44,228,25,163,
                                 25,90,246,170,74,163,117,63,44,151>>]},
                             {dh,
                              <<48,130,1,8,2,130,1,1,0,152,202,99,248,92,201,
                                35,238,246,5,77,93,120,10,118,129,36,52,111,
                                193,167,220,49,229,106,105,152,133,121,157,73,
                                158,232,153,197,197,21,171,140,30,207,52,165,
                                45,8,221,162,21,199,183,66,211,247,51,224,102,
                                214,190,130,96,253,218,193,35,43,139,145,89,
                                200,250,145,92,50,80,134,135,188,205,254,148,
                                122,136,237,220,186,147,187,104,159,36,147,
                                217,117,74,35,163,145,249,175,242,18,221,124,
                                54,140,16,246,169,84,252,45,47,99,136,30,60,
                                189,203,61,86,225,117,255,4,91,46,110,167,173,
                                106,51,65,10,248,94,225,223,73,40,232,140,26,
                                11,67,170,118,190,67,31,127,233,39,68,88,132,
                                171,224,62,187,207,160,189,209,101,74,8,205,
                                174,146,173,80,105,144,246,25,153,86,36,24,
                                178,163,64,202,221,95,184,110,244,32,226,217,
                                34,55,188,230,55,16,216,247,173,246,139,76,
                                187,66,211,159,17,46,20,18,48,80,27,250,96,
                                189,29,214,234,241,34,69,254,147,103,220,133,
                                40,164,84,8,44,241,61,164,151,9,135,41,60,75,
                                4,202,133,173,72,6,69,167,89,112,174,40,229,
                                171,2,1,2>>},
                             {ciphers,
                              [{ecdhe_ecdsa,aes_256_gcm,aead,sha384},
                               {ecdhe_rsa,aes_256_gcm,aead,sha384},
                               {ecdhe_ecdsa,aes_256_cbc,sha384,sha384},
                               {ecdhe_rsa,aes_256_cbc,sha384,sha384},
                               {ecdh_ecdsa,aes_256_gcm,aead,sha384},
                               {ecdh_rsa,aes_256_gcm,aead,sha384},
                               {ecdh_ecdsa,aes_256_cbc,sha384,sha384},
                               {ecdh_rsa,aes_256_cbc,sha384,sha384},
                               {ecdhe_ecdsa,chacha20_poly1305,aead,sha256},
                               {ecdhe_rsa,chacha20_poly1305,aead,sha256},
                               {dhe_rsa,chacha20_poly1305,aead,sha256},
                               {dhe_rsa,aes_256_gcm,aead,sha384},
                               {dhe_dss,aes_256_gcm,aead,sha384},
                               {dhe_rsa,aes_256_cbc,sha256},
                               {dhe_dss,aes_256_cbc,sha256},
                               {rsa,aes_256_gcm,aead,sha384},
                               {rsa,aes_256_cbc,sha256},
                               {ecdhe_ecdsa,aes_128_gcm,aead,sha256},
                               {ecdhe_rsa,aes_128_gcm,aead,sha256},
                               {ecdhe_ecdsa,aes_128_cbc,sha256,sha256},
                               {ecdhe_rsa,aes_128_cbc,sha256,sha256},
                               {ecdh_ecdsa,aes_128_gcm,aead,sha256},
                               {ecdh_rsa,aes_128_gcm,aead,sha256},
                               {ecdh_ecdsa,aes_128_cbc,sha256,sha256},
                               {ecdh_rsa,aes_128_cbc,sha256,sha256},
                               {dhe_rsa,aes_128_gcm,aead,sha256},
                               {dhe_dss,aes_128_gcm,aead,sha256},
                               {dhe_rsa,aes_128_cbc,sha256},
                               {dhe_dss,aes_128_cbc,sha256},
                               {rsa,aes_128_gcm,aead,sha256},
                               {rsa,aes_128_cbc,sha256},
                               {ecdhe_ecdsa,aes_256_cbc,sha},
                               {ecdhe_rsa,aes_256_cbc,sha},
                               {dhe_rsa,aes_256_cbc,sha},
                               {dhe_dss,aes_256_cbc,sha},
                               {ecdh_ecdsa,aes_256_cbc,sha},
                               {ecdh_rsa,aes_256_cbc,sha},
                               {rsa,aes_256_cbc,sha},
                               {ecdhe_ecdsa,aes_128_cbc,sha},
                               {ecdhe_rsa,aes_128_cbc,sha},
                               {dhe_rsa,aes_128_cbc,sha},
                               {dhe_dss,aes_128_cbc,sha},
                               {ecdh_ecdsa,aes_128_cbc,sha},
                               {ecdh_rsa,aes_128_cbc,sha},
                               {rsa,aes_128_cbc,sha},
                               {ecdhe_ecdsa,'3des_ede_cbc',sha},
                               {ecdhe_rsa,'3des_ede_cbc',sha},
                               {dhe_rsa,'3des_ede_cbc',sha},
                               {dhe_dss,'3des_ede_cbc',sha},
                               {ecdh_ecdsa,'3des_ede_cbc',sha},
                               {ecdh_rsa,'3des_ede_cbc',sha},
                               {rsa,'3des_ede_cbc',sha}]},
                             {honor_cipher_order,true},
                             {secure_renegotiate,true},
                             {client_renegotiation,false}]},
                           {port,18091}]]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[ns_server:info,2020-03-27T19:47:05.599Z,ns_1@127.0.0.1:<0.2547.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for cbas
[ns_server:info,2020-03-27T19:47:05.600Z,ns_1@127.0.0.1:<0.2547.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for eventing
[ns_server:info,2020-03-27T19:47:05.600Z,ns_1@127.0.0.1:<0.2547.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for fts
[ns_server:info,2020-03-27T19:47:05.600Z,ns_1@127.0.0.1:<0.2547.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for n1ql
[ns_server:debug,2020-03-27T19:47:05.601Z,ns_1@127.0.0.1:<0.223.0>:restartable:start_child:98]Started child process <0.2547.0>
  MFA: {ns_ssl_services_setup,start_link_rest_service,[]}
[ns_server:info,2020-03-27T19:47:05.601Z,ns_1@127.0.0.1:<0.2542.0>:ns_ssl_services_setup:notify_service:772]Successfully notified service ssl_service
[ns_server:info,2020-03-27T19:47:05.602Z,ns_1@127.0.0.1:ns_ssl_services_setup<0.214.0>:ns_ssl_services_setup:notify_services:756]Succesfully notified services [event,memcached,xdcr_proxy,capi_ssl_service,
                               ssl_service]
[error_logger:info,2020-03-27T19:47:05.601Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {<0.2547.0>,menelaus_web}
             started: [{pid,<0.2566.0>},
                       {id,menelaus_web_ipv6},
                       {mfargs,
                        {menelaus_web,http_server,
                         [[{ip,"::"},
                           {name,menelaus_web_ssl_ipv6},
                           {ssl,true},
                           {ssl_opts,
                            [{keyfile,
                              "/opt/couchbase/var/lib/couchbase/config/ssl-cert-key.pem"},
                             {certfile,
                              "/opt/couchbase/var/lib/couchbase/config/ssl-cert-key.pem"},
                             {versions,['tlsv1.1','tlsv1.2']},
                             {cacerts,
                              [<<48,130,3,2,48,130,1,234,160,3,2,1,2,2,8,22,
                                 0,64,208,110,172,210,92,48,13,6,9,42,134,72,
                                 134,247,13,1,1,11,5,0,48,36,49,34,48,32,6,3,
                                 85,4,3,19,25,67,111,117,99,104,98,97,115,
                                 101,32,83,101,114,118,101,114,32,51,52,56,
                                 101,100,50,55,48,48,30,23,13,49,51,48,49,48,
                                 49,48,48,48,48,48,48,90,23,13,52,57,49,50,
                                 51,49,50,51,53,57,53,57,90,48,36,49,34,48,
                                 32,6,3,85,4,3,19,25,67,111,117,99,104,98,97,
                                 115,101,32,83,101,114,118,101,114,32,51,52,
                                 56,101,100,50,55,48,48,130,1,34,48,13,6,9,
                                 42,134,72,134,247,13,1,1,1,5,0,3,130,1,15,0,
                                 48,130,1,10,2,130,1,1,0,225,209,231,201,130,
                                 247,233,2,187,220,222,34,52,112,118,191,205,
                                 152,82,245,106,26,160,209,228,176,90,40,98,
                                 172,183,1,136,153,71,239,18,220,136,15,75,
                                 73,223,113,145,171,33,98,84,62,10,36,82,160,
                                 106,135,171,185,245,166,220,188,43,178,51,2,
                                 205,146,75,139,58,68,250,195,53,51,182,176,
                                 42,160,42,14,18,94,26,188,161,102,35,66,108,
                                 137,96,63,168,160,212,254,70,169,139,74,198,
                                 244,204,206,194,101,179,132,181,1,21,76,117,
                                 72,101,24,165,108,212,56,253,47,230,184,143,
                                 50,20,121,74,247,204,71,247,226,97,249,191,
                                 174,226,150,23,149,71,180,209,86,216,72,31,
                                 202,197,17,62,92,90,116,10,212,69,89,20,91,
                                 163,51,77,94,156,13,115,23,71,83,252,222,
                                 166,85,132,171,214,167,174,78,60,63,78,241,
                                 115,199,70,186,175,191,94,208,5,216,173,106,
                                 248,203,56,72,24,219,23,86,67,137,18,221,
                                 171,208,1,41,239,84,253,158,230,36,78,222,
                                 33,120,90,96,209,36,5,230,90,163,19,2,35,
                                 165,80,128,171,6,90,11,237,162,35,3,201,94,
                                 110,201,181,13,49,165,2,3,1,0,1,163,56,48,
                                 54,48,14,6,3,85,29,15,1,1,255,4,4,3,2,2,164,
                                 48,19,6,3,85,29,37,4,12,48,10,6,8,43,6,1,5,
                                 5,7,3,1,48,15,6,3,85,29,19,1,1,255,4,5,48,3,
                                 1,1,255,48,13,6,9,42,134,72,134,247,13,1,1,
                                 11,5,0,3,130,1,1,0,6,81,65,124,71,43,122,15,
                                 167,207,16,251,0,159,39,98,188,227,29,215,
                                 100,107,194,184,208,49,234,49,157,22,16,114,
                                 226,174,13,150,172,144,2,238,230,8,40,221,
                                 26,55,56,132,126,233,153,24,185,41,108,27,
                                 239,216,82,120,63,12,251,133,139,191,7,65,
                                 245,131,92,34,201,230,196,53,62,156,158,117,
                                 62,220,159,192,101,171,131,143,145,151,43,
                                 167,227,90,161,173,77,152,27,124,67,49,150,
                                 67,102,161,221,43,166,252,34,106,208,76,223,
                                 73,230,216,161,26,132,137,129,146,79,142,69,
                                 179,193,194,47,129,207,251,4,237,180,247,54,
                                 155,40,216,36,79,59,110,188,27,153,208,45,
                                 211,245,157,64,19,243,225,128,45,126,172,97,
                                 164,65,55,250,227,90,212,59,153,137,219,195,
                                 84,171,109,145,64,111,253,109,120,35,199,
                                 153,232,120,170,179,167,110,176,116,10,245,
                                 79,6,66,158,148,137,191,125,65,164,153,94,
                                 180,226,83,230,241,236,39,67,40,12,91,44,79,
                                 54,65,72,249,75,235,222,205,142,112,55,224,
                                 47,125,42,235,188,14,128,134,44,228,25,163,
                                 25,90,246,170,74,163,117,63,44,151>>]},
                             {dh,
                              <<48,130,1,8,2,130,1,1,0,152,202,99,248,92,201,
                                35,238,246,5,77,93,120,10,118,129,36,52,111,
                                193,167,220,49,229,106,105,152,133,121,157,73,
                                158,232,153,197,197,21,171,140,30,207,52,165,
                                45,8,221,162,21,199,183,66,211,247,51,224,102,
                                214,190,130,96,253,218,193,35,43,139,145,89,
                                200,250,145,92,50,80,134,135,188,205,254,148,
                                122,136,237,220,186,147,187,104,159,36,147,
                                217,117,74,35,163,145,249,175,242,18,221,124,
                                54,140,16,246,169,84,252,45,47,99,136,30,60,
                                189,203,61,86,225,117,255,4,91,46,110,167,173,
                                106,51,65,10,248,94,225,223,73,40,232,140,26,
                                11,67,170,118,190,67,31,127,233,39,68,88,132,
                                171,224,62,187,207,160,189,209,101,74,8,205,
                                174,146,173,80,105,144,246,25,153,86,36,24,
                                178,163,64,202,221,95,184,110,244,32,226,217,
                                34,55,188,230,55,16,216,247,173,246,139,76,
                                187,66,211,159,17,46,20,18,48,80,27,250,96,
                                189,29,214,234,241,34,69,254,147,103,220,133,
                                40,164,84,8,44,241,61,164,151,9,135,41,60,75,
                                4,202,133,173,72,6,69,167,89,112,174,40,229,
                                171,2,1,2>>},
                             {ciphers,
                              [{ecdhe_ecdsa,aes_256_gcm,aead,sha384},
                               {ecdhe_rsa,aes_256_gcm,aead,sha384},
                               {ecdhe_ecdsa,aes_256_cbc,sha384,sha384},
                               {ecdhe_rsa,aes_256_cbc,sha384,sha384},
                               {ecdh_ecdsa,aes_256_gcm,aead,sha384},
                               {ecdh_rsa,aes_256_gcm,aead,sha384},
                               {ecdh_ecdsa,aes_256_cbc,sha384,sha384},
                               {ecdh_rsa,aes_256_cbc,sha384,sha384},
                               {ecdhe_ecdsa,chacha20_poly1305,aead,sha256},
                               {ecdhe_rsa,chacha20_poly1305,aead,sha256},
                               {dhe_rsa,chacha20_poly1305,aead,sha256},
                               {dhe_rsa,aes_256_gcm,aead,sha384},
                               {dhe_dss,aes_256_gcm,aead,sha384},
                               {dhe_rsa,aes_256_cbc,sha256},
                               {dhe_dss,aes_256_cbc,sha256},
                               {rsa,aes_256_gcm,aead,sha384},
                               {rsa,aes_256_cbc,sha256},
                               {ecdhe_ecdsa,aes_128_gcm,aead,sha256},
                               {ecdhe_rsa,aes_128_gcm,aead,sha256},
                               {ecdhe_ecdsa,aes_128_cbc,sha256,sha256},
                               {ecdhe_rsa,aes_128_cbc,sha256,sha256},
                               {ecdh_ecdsa,aes_128_gcm,aead,sha256},
                               {ecdh_rsa,aes_128_gcm,aead,sha256},
                               {ecdh_ecdsa,aes_128_cbc,sha256,sha256},
                               {ecdh_rsa,aes_128_cbc,sha256,sha256},
                               {dhe_rsa,aes_128_gcm,aead,sha256},
                               {dhe_dss,aes_128_gcm,aead,sha256},
                               {dhe_rsa,aes_128_cbc,sha256},
                               {dhe_dss,aes_128_cbc,sha256},
                               {rsa,aes_128_gcm,aead,sha256},
                               {rsa,aes_128_cbc,sha256},
                               {ecdhe_ecdsa,aes_256_cbc,sha},
                               {ecdhe_rsa,aes_256_cbc,sha},
                               {dhe_rsa,aes_256_cbc,sha},
                               {dhe_dss,aes_256_cbc,sha},
                               {ecdh_ecdsa,aes_256_cbc,sha},
                               {ecdh_rsa,aes_256_cbc,sha},
                               {rsa,aes_256_cbc,sha},
                               {ecdhe_ecdsa,aes_128_cbc,sha},
                               {ecdhe_rsa,aes_128_cbc,sha},
                               {dhe_rsa,aes_128_cbc,sha},
                               {dhe_dss,aes_128_cbc,sha},
                               {ecdh_ecdsa,aes_128_cbc,sha},
                               {ecdh_rsa,aes_128_cbc,sha},
                               {rsa,aes_128_cbc,sha},
                               {ecdhe_ecdsa,'3des_ede_cbc',sha},
                               {ecdhe_rsa,'3des_ede_cbc',sha},
                               {dhe_rsa,'3des_ede_cbc',sha},
                               {dhe_dss,'3des_ede_cbc',sha},
                               {ecdh_ecdsa,'3des_ede_cbc',sha},
                               {ecdh_rsa,'3des_ede_cbc',sha},
                               {rsa,'3des_ede_cbc',sha}]},
                             {honor_cipher_order,true},
                             {secure_renegotiate,true},
                             {client_renegotiation,false}]},
                           {port,18091}]]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[cluster:debug,2020-03-27T19:47:05.607Z,ns_1@127.0.0.1:<0.2471.0>:ns_cluster:maybe_rename:635]Renamed node from 'ns_1@cb.local' to 'ns_1@127.0.0.1'.
[ns_server:debug,2020-03-27T19:47:05.607Z,ns_1@127.0.0.1:wait_link_to_couchdb_node<0.280.0>:ns_server_nodes_sup:wait_link_to_couchdb_node_loop:196]Link to couchdb node was unpaused.
[ns_server:debug,2020-03-27T19:47:05.607Z,ns_1@127.0.0.1:wait_link_to_couchdb_node<0.280.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:152]Waiting for ns_couchdb node to start
[ns_server:debug,2020-03-27T19:47:05.607Z,ns_1@127.0.0.1:ns_node_disco_events<0.328.0>:ns_node_disco_rep_events:handle_event:42]Detected a new nodes (['ns_1@127.0.0.1']).  Moving config around.
[ns_server:info,2020-03-27T19:47:05.608Z,ns_1@127.0.0.1:ns_node_disco_events<0.328.0>:ns_node_disco_log:handle_event:46]ns_node_disco_log: nodes changed: ['ns_1@127.0.0.1']
[ns_server:debug,2020-03-27T19:47:05.608Z,ns_1@127.0.0.1:memcached_config_mgr<0.480.0>:memcached_config_mgr:handle_info:163]Got DOWN with reason: unpaused from memcached port server: <12939.116.0>. Shutting down
[ns_server:debug,2020-03-27T19:47:05.609Z,ns_1@127.0.0.1:<0.366.0>:restartable:loop:71]Restarting child <0.368.0>
  MFA: {ns_doctor_sup,start_link,[]}
  Shutdown policy: infinity
  Caller: {<0.189.0>,#Ref<0.1597663237.214695937.147315>}
[ns_server:debug,2020-03-27T19:47:05.609Z,ns_1@127.0.0.1:ns_ports_setup<0.461.0>:ns_ports_setup:children_loop_continue:118]Remote monitor <12939.109.0> was unpaused after node name change. Restart loop.
[ns_server:warn,2020-03-27T19:47:05.609Z,ns_1@127.0.0.1:<0.2584.0>:leader_lease_acquire_worker:handle_lease_already_acquired:232]Failed to acquire lease from 'ns_1@127.0.0.1' because its already taken by {'ns_1@cb.local',
                                                                            <<"952aa8a36b9840f3391de772fe18e797">>} (valid for 13551ms)
[ns_server:debug,2020-03-27T19:47:05.609Z,ns_1@127.0.0.1:<0.512.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.480.0>} exited with reason {shutdown,
                                                                                {memcached_port_server_down,
                                                                                 <12939.116.0>,
                                                                                 unpaused}}
[ns_server:debug,2020-03-27T19:47:05.610Z,ns_1@127.0.0.1:memcached_config_mgr<0.2589.0>:memcached_config_mgr:init:49]waiting for completion of initial ns_ports_setup round
[ns_server:debug,2020-03-27T19:47:05.610Z,ns_1@127.0.0.1:<0.371.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.370.0>} exited with reason shutdown
[error_logger:error,2020-03-27T19:47:05.610Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================SUPERVISOR REPORT=========================
     Supervisor: {local,ns_server_sup}
     Context:    child_terminated
     Reason:     {shutdown,
                     {memcached_port_server_down,<12939.116.0>,unpaused}}
     Offender:   [{pid,<0.480.0>},
                  {name,memcached_config_mgr},
                  {mfargs,{memcached_config_mgr,start_link,[]}},
                  {restart_type,{permanent,4}},
                  {shutdown,1000},
                  {child_type,worker}]


[error_logger:info,2020-03-27T19:47:05.610Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.2589.0>},
                       {name,memcached_config_mgr},
                       {mfargs,{memcached_config_mgr,start_link,[]}},
                       {restart_type,{permanent,4}},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-03-27T19:47:05.611Z,ns_1@127.0.0.1:<0.366.0>:restartable:shutdown_child:120]Successfully terminated process <0.368.0>
[error_logger:info,2020-03-27T19:47:05.611Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_doctor_sup}
             started: [{pid,<0.2592.0>},
                       {id,ns_doctor_events},
                       {mfargs,
                           {gen_event,start_link,[{local,ns_doctor_events}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-03-27T19:47:05.611Z,ns_1@127.0.0.1:ns_ports_setup<0.461.0>:ns_ports_manager:set_dynamic_children:54]Setting children [memcached,saslauthd_port,goxdcr]
[error_logger:info,2020-03-27T19:47:05.613Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_doctor_sup}
             started: [{pid,<0.2593.0>},
                       {id,ns_doctor},
                       {mfargs,{ns_doctor,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-03-27T19:47:05.614Z,ns_1@127.0.0.1:<0.366.0>:restartable:start_child:98]Started child process <0.2591.0>
  MFA: {ns_doctor_sup,start_link,[]}
[ns_server:debug,2020-03-27T19:47:05.614Z,ns_1@127.0.0.1:<0.539.0>:restartable:loop:71]Restarting child <0.540.0>
  MFA: {leader_services_sup,start_link,[]}
  Shutdown policy: infinity
  Caller: {<0.189.0>,#Ref<0.1597663237.214695938.147000>}
[ns_server:debug,2020-03-27T19:47:05.614Z,ns_1@127.0.0.1:ns_ports_setup<0.461.0>:ns_ports_setup:set_children:85]Monitor ns_child_ports_sup <12939.109.0>
[ns_server:info,2020-03-27T19:47:05.614Z,ns_1@127.0.0.1:mb_master<0.548.0>:mb_master:terminate:327]Synchronously shutting down child mb_master_sup
[ns_server:info,2020-03-27T19:47:05.614Z,ns_1@127.0.0.1:leader_registry<0.545.0>:leader_registry_server:handle_down:253]Process <0.639.0> registered as 'license_reporting' terminated.
[ns_server:debug,2020-03-27T19:47:05.614Z,ns_1@127.0.0.1:<0.640.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.639.0>} exited with reason shutdown
[ns_server:debug,2020-03-27T19:47:05.614Z,ns_1@127.0.0.1:memcached_config_mgr<0.2589.0>:memcached_config_mgr:init:51]ns_ports_setup seems to be ready
[ns_server:info,2020-03-27T19:47:05.615Z,ns_1@127.0.0.1:leader_registry<0.545.0>:leader_registry_server:handle_down:253]Process <0.620.0> registered as 'collections' terminated.
[ns_server:info,2020-03-27T19:47:05.615Z,ns_1@127.0.0.1:leader_registry<0.545.0>:leader_registry_server:handle_down:253]Process <0.613.0> registered as 'auto_failover' terminated.
[ns_server:debug,2020-03-27T19:47:05.615Z,ns_1@127.0.0.1:<0.614.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {compat_mode_events,<0.613.0>} exited with reason shutdown
[ns_server:debug,2020-03-27T19:47:05.615Z,ns_1@127.0.0.1:memcached_config_mgr<0.2589.0>:memcached_config_mgr:find_port_pid_loop:137]Found memcached port <12939.116.0>
[ns_server:info,2020-03-27T19:47:05.615Z,ns_1@127.0.0.1:leader_registry<0.545.0>:leader_registry_server:handle_down:253]Process <0.609.0> registered as 'ns_orchestrator' terminated.
[ns_server:info,2020-03-27T19:47:05.615Z,ns_1@127.0.0.1:leader_registry<0.545.0>:leader_registry_server:handle_down:253]Process <0.608.0> registered as 'auto_rebalance' terminated.
[ns_server:info,2020-03-27T19:47:05.615Z,ns_1@127.0.0.1:leader_registry<0.545.0>:leader_registry_server:handle_down:253]Process <0.607.0> registered as 'auto_reprovision' terminated.
[ns_server:info,2020-03-27T19:47:05.617Z,ns_1@127.0.0.1:leader_registry<0.545.0>:leader_registry_server:handle_down:253]Process <0.555.0> registered as 'ns_tick' terminated.
[ns_server:debug,2020-03-27T19:47:05.617Z,ns_1@127.0.0.1:leader_activities<0.542.0>:leader_activities:handle_internal_process_down:511]Process {quorum_nodes_manager,<0.2513.0>} terminated with reason shutdown
[ns_server:debug,2020-03-27T19:47:05.617Z,ns_1@127.0.0.1:leader_activities<0.542.0>:leader_activities:handle_internal_process_down:511]Process {acquirer,<0.551.0>} terminated with reason shutdown
[ns_server:debug,2020-03-27T19:47:05.617Z,ns_1@127.0.0.1:leader_lease_agent<0.543.0>:leader_lease_agent:handle_abolish_lease:255]Received abolish lease request from {lease_holder,
                                     <<"952aa8a36b9840f3391de772fe18e797">>,
                                     'ns_1@127.0.0.1'} when lease is {lease,
                                                                      {lease_holder,
                                                                       <<"952aa8a36b9840f3391de772fe18e797">>,
                                                                       'ns_1@cb.local'},
                                                                      -576460658211180480,
                                                                      -576460643211180480,
                                                                      {timer,
                                                                       #Ref<0.1597663237.214695937.146792>,
                                                                       {lease_expired,
                                                                        {lease_holder,
                                                                         <<"952aa8a36b9840f3391de772fe18e797">>,
                                                                         'ns_1@cb.local'}}},
                                                                      active}
[ns_server:debug,2020-03-27T19:47:05.617Z,ns_1@127.0.0.1:leader_lease_agent<0.543.0>:leader_lease_agent:handle_abolish_lease:260]Expiring abolished lease
[ns_server:debug,2020-03-27T19:47:05.617Z,ns_1@127.0.0.1:<0.563.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_node_disco_events,<0.551.0>} exited with reason shutdown
[ns_server:debug,2020-03-27T19:47:05.617Z,ns_1@127.0.0.1:<0.2522.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.2513.0>} exited with reason shutdown
[ns_server:debug,2020-03-27T19:47:05.617Z,ns_1@127.0.0.1:leader_registry<0.545.0>:leader_registry_server:handle_new_leader:241]New leader is undefined. Invalidating name cache.
[ns_server:debug,2020-03-27T19:47:05.617Z,ns_1@127.0.0.1:<0.549.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.548.0>} exited with reason shutdown
[ns_server:debug,2020-03-27T19:47:05.617Z,ns_1@127.0.0.1:<0.546.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {leader_events,<0.545.0>} exited with reason shutdown
[ns_server:debug,2020-03-27T19:47:05.627Z,ns_1@127.0.0.1:leader_activities<0.542.0>:leader_activities:handle_internal_process_down:511]Process {agent,<0.543.0>} terminated with reason shutdown
[ns_server:debug,2020-03-27T19:47:05.627Z,ns_1@127.0.0.1:<0.539.0>:restartable:shutdown_child:120]Successfully terminated process <0.540.0>
[error_logger:info,2020-03-27T19:47:05.628Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,leader_leases_sup}
             started: [{pid,<0.2606.0>},
                       {id,leader_activities},
                       {mfargs,{leader_activities,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,10000},
                       {child_type,worker}]

[ns_server:debug,2020-03-27T19:47:05.629Z,ns_1@127.0.0.1:memcached_config_mgr<0.2589.0>:memcached_config_mgr:do_read_current_memcached_config:287]Got enoent while trying to read active memcached config from /opt/couchbase/var/lib/couchbase/config/memcached.json.prev
[ns_server:debug,2020-03-27T19:47:05.631Z,ns_1@127.0.0.1:memcached_config_mgr<0.2589.0>:memcached_config_mgr:init:89]found memcached port to be already active
[error_logger:info,2020-03-27T19:47:05.632Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,leader_leases_sup}
             started: [{pid,<0.2607.0>},
                       {id,leader_lease_agent},
                       {mfargs,{leader_lease_agent,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T19:47:05.632Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,leader_services_sup}
             started: [{pid,<0.2605.0>},
                       {id,leader_leases_sup},
                       {mfargs,{leader_leases_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[ns_server:debug,2020-03-27T19:47:05.632Z,ns_1@127.0.0.1:leader_registry_sup<0.2610.0>:mb_master:check_master_takeover_needed:283]Sending master node question to the following nodes: []
[ns_server:debug,2020-03-27T19:47:05.632Z,ns_1@127.0.0.1:leader_registry_sup<0.2610.0>:mb_master:check_master_takeover_needed:285]Got replies: []
[ns_server:debug,2020-03-27T19:47:05.632Z,ns_1@127.0.0.1:leader_registry_sup<0.2610.0>:mb_master:check_master_takeover_needed:291]Was unable to discover master, not going to force mastership takeover
[error_logger:info,2020-03-27T19:47:05.632Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,leader_registry_sup}
             started: [{pid,<0.2611.0>},
                       {id,leader_registry_server},
                       {mfargs,{leader_registry_server,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[user:info,2020-03-27T19:47:05.632Z,ns_1@127.0.0.1:mb_master<0.2614.0>:mb_master:init:103]I'm the only node, so I'm the master.
[ns_server:debug,2020-03-27T19:47:05.633Z,ns_1@127.0.0.1:leader_registry<0.2611.0>:leader_registry_server:handle_new_leader:241]New leader is 'ns_1@127.0.0.1'. Invalidating name cache.
[ns_server:debug,2020-03-27T19:47:05.633Z,ns_1@127.0.0.1:leader_quorum_nodes_manager<0.2622.0>:leader_quorum_nodes_manager:pull_config:114]Attempting to pull config from nodes:
[]
[error_logger:info,2020-03-27T19:47:05.633Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,mb_master_sup}
             started: [{pid,<0.2620.0>},
                       {id,leader_lease_acquirer},
                       {mfargs,{leader_lease_acquirer,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,10000},
                       {child_type,worker}]

[ns_server:info,2020-03-27T19:47:05.633Z,ns_1@127.0.0.1:mb_master_sup<0.2618.0>:misc:start_singleton:857]start_singleton(gen_server, start_link, [{via,leader_registry,ns_tick},
                                         ns_tick,[],[]]): started as <0.2624.0> on 'ns_1@127.0.0.1'

[error_logger:info,2020-03-27T19:47:05.633Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,mb_master_sup}
             started: [{pid,<0.2622.0>},
                       {id,leader_quorum_nodes_manager},
                       {mfargs,{leader_quorum_nodes_manager,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T19:47:05.633Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,mb_master_sup}
             started: [{pid,<0.2624.0>},
                       {id,ns_tick},
                       {mfargs,{ns_tick,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,10},
                       {child_type,worker}]

[ns_server:debug,2020-03-27T19:47:05.633Z,ns_1@127.0.0.1:leader_lease_agent<0.2607.0>:leader_lease_agent:do_handle_acquire_lease:149]Granting lease to {lease_holder,<<"989da350031a7b3cec93d0674a7161ac">>,
                                'ns_1@127.0.0.1'} for 15000ms
[error_logger:info,2020-03-27T19:47:05.633Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_orchestrator_sup}
             started: [{pid,<0.2628.0>},
                       {id,compat_mode_events},
                       {mfargs,
                           {gen_event,start_link,
                               [{local,compat_mode_events}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:info,2020-03-27T19:47:05.633Z,ns_1@127.0.0.1:ns_orchestrator_child_sup<0.2632.0>:misc:start_singleton:857]start_singleton(gen_server, start_link, [{via,leader_registry,
                                          auto_reprovision},
                                         auto_reprovision,[],[]]): started as <0.2634.0> on 'ns_1@127.0.0.1'

[ns_server:info,2020-03-27T19:47:05.633Z,ns_1@127.0.0.1:ns_orchestrator_child_sup<0.2632.0>:misc:start_singleton:857]start_singleton(gen_server, start_link, [{via,leader_registry,auto_rebalance},
                                         auto_rebalance,[],[]]): started as <0.2636.0> on 'ns_1@127.0.0.1'

[error_logger:info,2020-03-27T19:47:05.633Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_orchestrator_sup}
             started: [{pid,<0.2631.0>},
                       {id,compat_mode_manager},
                       {mfargs,{compat_mode_manager,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:info,2020-03-27T19:47:05.634Z,ns_1@127.0.0.1:ns_orchestrator_child_sup<0.2632.0>:misc:start_singleton:857]start_singleton(gen_statem, start_link, [{via,leader_registry,ns_orchestrator},
                                         ns_orchestrator,[],[]]): started as <0.2637.0> on 'ns_1@127.0.0.1'

[ns_server:debug,2020-03-27T19:47:05.634Z,ns_1@127.0.0.1:leader_quorum_nodes_manager<0.2622.0>:leader_quorum_nodes_manager:pull_config:119]Pulled config successfully.
[error_logger:info,2020-03-27T19:47:05.634Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_orchestrator_child_sup}
             started: [{pid,<0.2633.0>},
                       {id,ns_janitor_server},
                       {mfargs,{ns_janitor_server,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T19:47:05.634Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_orchestrator_child_sup}
             started: [{pid,<0.2634.0>},
                       {id,auto_reprovision},
                       {mfargs,{auto_reprovision,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T19:47:05.634Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_orchestrator_child_sup}
             started: [{pid,<0.2636.0>},
                       {id,auto_rebalance},
                       {mfargs,{auto_rebalance,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-03-27T19:47:05.634Z,ns_1@127.0.0.1:<0.2640.0>:auto_failover:init:185]init auto_failover.
[user:info,2020-03-27T19:47:05.634Z,ns_1@127.0.0.1:<0.2640.0>:auto_failover:handle_call:216]Enabled auto-failover with timeout 120 and max count 1
[error_logger:info,2020-03-27T19:47:05.634Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_orchestrator_child_sup}
             started: [{pid,<0.2637.0>},
                       {id,ns_orchestrator},
                       {mfargs,{ns_orchestrator,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T19:47:05.635Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_orchestrator_sup}
             started: [{pid,<0.2632.0>},
                       {id,ns_orchestrator_child_sup},
                       {mfargs,{ns_orchestrator_child_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[ns_server:debug,2020-03-27T19:47:05.636Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{local_changes_count,<<"60a6bc3db77e6c7b91c556140dcfec71">>} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{12,63752557625}}]}]
[ns_server:info,2020-03-27T19:47:05.636Z,ns_1@127.0.0.1:ns_orchestrator_sup<0.2627.0>:misc:start_singleton:857]start_singleton(gen_server, start_link, [{via,leader_registry,auto_failover},
                                         auto_failover,[],[]]): started as <0.2640.0> on 'ns_1@127.0.0.1'

[error_logger:info,2020-03-27T19:47:05.636Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_orchestrator_sup}
             started: [{pid,<0.2640.0>},
                       {id,auto_failover},
                       {mfargs,{auto_failover,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:info,2020-03-27T19:47:05.636Z,ns_1@127.0.0.1:mb_master_sup<0.2618.0>:misc:start_singleton:857]start_singleton(work_queue, start_link, [{via,leader_registry,collections}]): started as <0.2644.0> on 'ns_1@127.0.0.1'

[ns_server:debug,2020-03-27T19:47:05.636Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
auto_failover_cfg ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557537}}]},
 {enabled,true},
 {timeout,120},
 {count,0},
 {failover_on_data_disk_issues,[{enabled,false},{timePeriod,120}]},
 {failover_server_group,false},
 {max_count,1},
 {failed_over_server_groups,[]},
 {can_abort_rebalance,true}]
[ns_server:debug,2020-03-27T19:47:05.636Z,ns_1@127.0.0.1:<0.2645.0>:license_reporting:init:66]Starting license_reporting server
[error_logger:info,2020-03-27T19:47:05.636Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,mb_master_sup}
             started: [{pid,<0.2627.0>},
                       {id,ns_orchestrator_sup},
                       {mfargs,{ns_orchestrator_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[ns_server:info,2020-03-27T19:47:05.636Z,ns_1@127.0.0.1:mb_master_sup<0.2618.0>:misc:start_singleton:857]start_singleton(gen_server, start_link, [{via,leader_registry,
                                          license_reporting},
                                         license_reporting,[],[]]): started as <0.2645.0> on 'ns_1@127.0.0.1'

[ns_server:debug,2020-03-27T19:47:05.636Z,ns_1@127.0.0.1:<0.539.0>:restartable:start_child:98]Started child process <0.2604.0>
  MFA: {leader_services_sup,start_link,[]}
[cluster:info,2020-03-27T19:47:05.637Z,ns_1@127.0.0.1:ns_cluster<0.189.0>:ns_cluster:do_change_address:602]Renamed node. New name is 'ns_1@127.0.0.1'.
[ns_server:debug,2020-03-27T19:47:05.637Z,ns_1@127.0.0.1:ns_config_rep<0.335.0>:ns_config_rep:do_push_keys:321]Replicating some config keys ([auto_failover_cfg,
                               {local_changes_count,
                                   <<"60a6bc3db77e6c7b91c556140dcfec71">>}]..)
[error_logger:info,2020-03-27T19:47:05.637Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,mb_master_sup}
             started: [{pid,<0.2644.0>},
                       {id,collections},
                       {mfargs,{collections,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T19:47:05.638Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,mb_master_sup}
             started: [{pid,<0.2645.0>},
                       {id,license_reporting},
                       {mfargs,{license_reporting,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T19:47:05.639Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,leader_registry_sup}
             started: [{pid,<0.2614.0>},
                       {id,mb_master},
                       {mfargs,{mb_master,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[ns_server:debug,2020-03-27T19:47:05.639Z,ns_1@127.0.0.1:ns_audit<0.479.0>:ns_audit:handle_call:125]Audit rename_node: [{hostname,<<"127.0.0.1">>},
                    {node,'ns_1@cb.local'},
                    {real_userid,{[{domain,wrong_token},
                                   {user,<<"<ud></ud>">>}]}},
                    {sessionid,<<"3c9673d43ade3c5c4478657f38eb6001">>},
                    {remote,{[{ip,<<"192.168.0.1">>},{port,54264}]}},
                    {timestamp,<<"2020-03-27T19:47:05.638Z">>}]
[error_logger:info,2020-03-27T19:47:05.639Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,leader_services_sup}
             started: [{pid,<0.2610.0>},
                       {id,leader_registry_sup},
                       {mfargs,{leader_registry_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[ns_server:info,2020-03-27T19:47:05.649Z,ns_1@127.0.0.1:<0.2623.0>:leader_lease_acquire_worker:handle_fresh_lease_acquired:302]Acquired lease from node 'ns_1@127.0.0.1' (lease uuid: <<"989da350031a7b3cec93d0674a7161ac">>)
[ns_server:debug,2020-03-27T19:47:05.653Z,ns_1@127.0.0.1:ns_config_rep<0.335.0>:ns_config_rep:do_push_keys:321]Replicating some config keys ([{local_changes_count,
                                   <<"60a6bc3db77e6c7b91c556140dcfec71">>},
                               {metakv,<<"/indexing/settings/config">>}]..)
[ns_server:debug,2020-03-27T19:47:05.653Z,ns_1@127.0.0.1:ns_audit<0.479.0>:ns_audit:handle_call:125]Audit modify_index_storage_mode: [{storageMode,<<"plasma">>},
                                  {real_userid,
                                      {[{domain,wrong_token},
                                        {user,<<"<ud></ud>">>}]}},
                                  {sessionid,
                                      <<"3c9673d43ade3c5c4478657f38eb6001">>},
                                  {remote,
                                      {[{ip,<<"192.168.0.1">>},{port,54264}]}},
                                  {timestamp,<<"2020-03-27T19:47:05.653Z">>}]
[ns_server:debug,2020-03-27T19:47:05.653Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{metakv,<<"/indexing/settings/config">>} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557625}}]}|
 <<"{\"indexer.settings.compaction.days_of_week\":\"Sunday,Monday,Tuesday,Wednesday,Thursday,Friday,Saturday\",\"indexer.settings.compaction.interval\":\"00:00,00:00\",\"indexer.settings.persisted_snapshot.interval\":5000,\"indexer.settings.log_level\":\"info\",\"indexer.settings.compaction.compaction_mode\":\"circular\",\"indexer.settings.compaction.min_frag\":30,\"indexer.settings.inmemory_snapshot.interval\""...>>]
[ns_server:debug,2020-03-27T19:47:05.653Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{local_changes_count,<<"60a6bc3db77e6c7b91c556140dcfec71">>} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{13,63752557625}}]}]
[ns_server:debug,2020-03-27T19:47:05.663Z,ns_1@127.0.0.1:ns_audit<0.479.0>:ns_audit:handle_call:125]Audit cluster_settings: [{cluster_name,<<"BaseCluster">>},
                         {quotas,{[{kv,292},
                                   {index,512},
                                   {fts,256},
                                   {cbas,1024},
                                   {eventing,256}]}},
                         {real_userid,{[{domain,wrong_token},
                                        {user,<<"<ud></ud>">>}]}},
                         {sessionid,<<"3c9673d43ade3c5c4478657f38eb6001">>},
                         {remote,{[{ip,<<"192.168.0.1">>},{port,54264}]}},
                         {timestamp,<<"2020-03-27T19:47:05.663Z">>}]
[ns_server:debug,2020-03-27T19:47:05.664Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{metakv,<<"/indexing/settings/config">>} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
 <<"{\"indexer.settings.compaction.days_of_week\":\"Sunday,Monday,Tuesday,Wednesday,Thursday,Friday,Saturday\",\"indexer.settings.compaction.interval\":\"00:00,00:00\",\"indexer.settings.compaction.compaction_mode\":\"circular\",\"indexer.settings.log_level\":\"info\",\"indexer.settings.persisted_snapshot.interval\":5000,\"indexer.settings.compaction.min_frag\":30,\"indexer.settings.inmemory_snapshot.interval\""...>>]
[ns_server:debug,2020-03-27T19:47:05.664Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{local_changes_count,<<"60a6bc3db77e6c7b91c556140dcfec71">>} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{14,63752557625}}]}]
[ns_server:debug,2020-03-27T19:47:05.664Z,ns_1@127.0.0.1:ns_config_rep<0.335.0>:ns_config_rep:do_push_keys:321]Replicating some config keys ([cluster_name,
                               {local_changes_count,
                                   <<"60a6bc3db77e6c7b91c556140dcfec71">>},
                               {metakv,<<"/indexing/settings/config">>}]..)
[ns_server:debug,2020-03-27T19:47:05.664Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{local_changes_count,<<"60a6bc3db77e6c7b91c556140dcfec71">>} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{15,63752557625}}]}]
[ns_server:debug,2020-03-27T19:47:05.669Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
cluster_name ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557625}}]},
 66,97,115,101,67,108,117,115,116,101,114]
[error_logger:error,2020-03-27T19:47:06.241Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================CRASH REPORT=========================
  crasher:
    initial call: ns_log:'-start_link_crash_consumer/0-fun-0-'/0
    pid: <0.317.0>
    registered_name: []
    exception exit: {{nodedown,'babysitter_of_ns_1@cb.local'},
                     {gen_server,call,
                                 [{ns_crash_log,'babysitter_of_ns_1@cb.local'},
                                  consume,infinity]}}
      in function  gen_server:call/3 (gen_server.erl, line 214)
      in call from ns_log:crash_consumption_loop/0 (src/ns_log.erl, line 62)
      in call from misc:delaying_crash/2 (src/misc.erl, line 1605)
    ancestors: [ns_server_sup,ns_server_nodes_sup,<0.206.0>,
                  ns_server_cluster_sup,root_sup,<0.118.0>]
    message_queue_len: 0
    messages: []
    links: [<0.310.0>]
    dictionary: []
    trap_exit: false
    status: running
    heap_size: 1598
    stack_size: 27
    reductions: 5002
  neighbours:

[error_logger:error,2020-03-27T19:47:06.242Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================SUPERVISOR REPORT=========================
     Supervisor: {local,ns_server_sup}
     Context:    child_terminated
     Reason:     {{nodedown,'babysitter_of_ns_1@cb.local'},
                  {gen_server,call,
                              [{ns_crash_log,'babysitter_of_ns_1@cb.local'},
                               consume,infinity]}}
     Offender:   [{pid,<0.317.0>},
                  {name,ns_crash_log_consumer},
                  {mfargs,{ns_log,start_link_crash_consumer,[]}},
                  {restart_type,{permanent,4}},
                  {shutdown,1000},
                  {child_type,worker}]


[error_logger:info,2020-03-27T19:47:06.242Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.2673.0>},
                       {name,ns_crash_log_consumer},
                       {mfargs,{ns_log,start_link_crash_consumer,[]}},
                       {restart_type,{permanent,4}},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-03-27T19:47:06.637Z,ns_1@127.0.0.1:<0.2640.0>:auto_failover_logic:log_master_activity:177]Transitioned node {'ns_1@127.0.0.1',<<"60a6bc3db77e6c7b91c556140dcfec71">>} state new -> up
[ns_server:debug,2020-03-27T19:47:07.789Z,ns_1@127.0.0.1:compaction_daemon<0.533.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2020-03-27T19:47:07.790Z,ns_1@127.0.0.1:compaction_daemon<0.533.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2020-03-27T19:47:07.790Z,ns_1@127.0.0.1:compaction_daemon<0.533.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2020-03-27T19:47:07.790Z,ns_1@127.0.0.1:compaction_daemon<0.533.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2020-03-27T19:47:37.799Z,ns_1@127.0.0.1:compaction_daemon<0.533.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2020-03-27T19:47:37.799Z,ns_1@127.0.0.1:compaction_daemon<0.533.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2020-03-27T19:47:37.799Z,ns_1@127.0.0.1:compaction_daemon<0.533.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2020-03-27T19:47:37.799Z,ns_1@127.0.0.1:compaction_daemon<0.533.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2020-03-27T19:48:03.176Z,ns_1@127.0.0.1:ldap_auth_cache<0.261.0>:active_cache:cleanup:231]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2020-03-27T19:48:07.808Z,ns_1@127.0.0.1:compaction_daemon<0.533.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2020-03-27T19:48:07.808Z,ns_1@127.0.0.1:compaction_daemon<0.533.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2020-03-27T19:48:07.809Z,ns_1@127.0.0.1:compaction_daemon<0.533.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2020-03-27T19:48:07.809Z,ns_1@127.0.0.1:compaction_daemon<0.533.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2020-03-27T19:48:37.814Z,ns_1@127.0.0.1:compaction_daemon<0.533.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2020-03-27T19:48:37.815Z,ns_1@127.0.0.1:compaction_daemon<0.533.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2020-03-27T19:48:37.815Z,ns_1@127.0.0.1:compaction_daemon<0.533.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2020-03-27T19:48:37.815Z,ns_1@127.0.0.1:compaction_daemon<0.533.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2020-03-27T19:49:07.816Z,ns_1@127.0.0.1:compaction_daemon<0.533.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2020-03-27T19:49:07.816Z,ns_1@127.0.0.1:compaction_daemon<0.533.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2020-03-27T19:49:07.817Z,ns_1@127.0.0.1:compaction_daemon<0.533.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2020-03-27T19:49:07.817Z,ns_1@127.0.0.1:compaction_daemon<0.533.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2020-03-27T19:49:18.184Z,ns_1@127.0.0.1:ldap_auth_cache<0.261.0>:active_cache:cleanup:231]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2020-03-27T19:49:37.819Z,ns_1@127.0.0.1:compaction_daemon<0.533.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2020-03-27T19:49:37.819Z,ns_1@127.0.0.1:compaction_daemon<0.533.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2020-03-27T19:49:37.819Z,ns_1@127.0.0.1:compaction_daemon<0.533.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2020-03-27T19:49:37.819Z,ns_1@127.0.0.1:compaction_daemon<0.533.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2020-03-27T19:50:07.822Z,ns_1@127.0.0.1:compaction_daemon<0.533.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2020-03-27T19:50:07.822Z,ns_1@127.0.0.1:compaction_daemon<0.533.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2020-03-27T19:50:07.822Z,ns_1@127.0.0.1:compaction_daemon<0.533.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2020-03-27T19:50:07.823Z,ns_1@127.0.0.1:compaction_daemon<0.533.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2020-03-27T19:50:33.190Z,ns_1@127.0.0.1:ldap_auth_cache<0.261.0>:active_cache:cleanup:231]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2020-03-27T19:50:37.823Z,ns_1@127.0.0.1:compaction_daemon<0.533.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2020-03-27T19:50:37.823Z,ns_1@127.0.0.1:compaction_daemon<0.533.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2020-03-27T19:50:37.824Z,ns_1@127.0.0.1:compaction_daemon<0.533.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2020-03-27T19:50:37.824Z,ns_1@127.0.0.1:compaction_daemon<0.533.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2020-03-27T19:51:07.826Z,ns_1@127.0.0.1:compaction_daemon<0.533.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2020-03-27T19:51:07.826Z,ns_1@127.0.0.1:compaction_daemon<0.533.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2020-03-27T19:51:07.826Z,ns_1@127.0.0.1:compaction_daemon<0.533.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2020-03-27T19:51:07.826Z,ns_1@127.0.0.1:compaction_daemon<0.533.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2020-03-27T19:51:37.829Z,ns_1@127.0.0.1:compaction_daemon<0.533.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2020-03-27T19:51:37.830Z,ns_1@127.0.0.1:compaction_daemon<0.533.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2020-03-27T19:51:37.830Z,ns_1@127.0.0.1:compaction_daemon<0.533.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2020-03-27T19:51:37.830Z,ns_1@127.0.0.1:compaction_daemon<0.533.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2020-03-27T19:51:48.193Z,ns_1@127.0.0.1:ldap_auth_cache<0.261.0>:active_cache:cleanup:231]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2020-03-27T19:52:07.830Z,ns_1@127.0.0.1:compaction_daemon<0.533.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2020-03-27T19:52:07.830Z,ns_1@127.0.0.1:compaction_daemon<0.533.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2020-03-27T19:52:07.830Z,ns_1@127.0.0.1:compaction_daemon<0.533.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2020-03-27T19:52:07.830Z,ns_1@127.0.0.1:compaction_daemon<0.533.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2020-03-27T19:52:37.832Z,ns_1@127.0.0.1:compaction_daemon<0.533.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2020-03-27T19:52:37.832Z,ns_1@127.0.0.1:compaction_daemon<0.533.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2020-03-27T19:52:37.832Z,ns_1@127.0.0.1:compaction_daemon<0.533.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2020-03-27T19:52:37.832Z,ns_1@127.0.0.1:compaction_daemon<0.533.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:info,2020-03-27T19:53:00.539Z,ns_1@127.0.0.1:netconfig_updater<0.202.0>:netconfig_updater:apply_config_unprotected:158]Node is going to apply the following settings: [{externalListeners,
                                                 [{inet,false},
                                                  {inet6,false}]}]
[ns_server:debug,2020-03-27T19:53:00.568Z,ns_1@127.0.0.1:cb_dist<0.2483.0>:cb_dist:info_msg:754]cb_dist: Updated cb_dist config "/opt/couchbase/var/lib/couchbase/config/dist_cfg": [{external_listeners,
                                                                                      [inet_tcp_dist,
                                                                                       inet6_tcp_dist]},
                                                                                     {preferred_external_proto,
                                                                                      inet_tcp_dist},
                                                                                     {preferred_local_proto,
                                                                                      inet_tcp_dist}]
[ns_server:debug,2020-03-27T19:53:00.573Z,ns_1@127.0.0.1:cb_dist<0.2483.0>:cb_dist:info_msg:754]cb_dist: Reloading configuration: [{external_listeners,
                                       [inet_tcp_dist,inet6_tcp_dist]},
                                   {preferred_external_proto,inet_tcp_dist},
                                   {preferred_local_proto,inet_tcp_dist}]
[ns_server:debug,2020-03-27T19:53:00.574Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{local_changes_count,<<"60a6bc3db77e6c7b91c556140dcfec71">>} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{16,63752557980}}]}]
[ns_server:info,2020-03-27T19:53:00.574Z,ns_1@127.0.0.1:netconfig_updater<0.202.0>:netconfig_updater:apply_config_unprotected:187]Node network settings ([{externalListeners,[{inet,false},{inet6,false}]}]) successfully applied
[ns_server:debug,2020-03-27T19:53:00.574Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',address_family} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|inet]
[ns_server:debug,2020-03-27T19:53:00.574Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{local_changes_count,<<"60a6bc3db77e6c7b91c556140dcfec71">>} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{17,63752557980}}]}]
[ns_server:debug,2020-03-27T19:53:00.574Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',node_encryption} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|false]
[ns_server:debug,2020-03-27T19:53:00.574Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{local_changes_count,<<"60a6bc3db77e6c7b91c556140dcfec71">>} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{18,63752557980}}]}]
[ns_server:debug,2020-03-27T19:53:00.574Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',erl_external_listeners} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]},
 {inet,false},
 {inet6,false}]
[ns_server:debug,2020-03-27T19:53:00.575Z,ns_1@127.0.0.1:ns_config_rep<0.335.0>:ns_config_rep:do_push_keys:321]Replicating some config keys ([{local_changes_count,
                                   <<"60a6bc3db77e6c7b91c556140dcfec71">>},
                               {node,'ns_1@127.0.0.1',address_family},
                               {node,'ns_1@127.0.0.1',erl_external_listeners},
                               {node,'ns_1@127.0.0.1',node_encryption}]..)
[cluster:info,2020-03-27T19:53:00.626Z,ns_1@127.0.0.1:ns_cluster<0.189.0>:ns_cluster:handle_call:355]Changing address to "127.0.0.1" due to client request
[cluster:info,2020-03-27T19:53:00.626Z,ns_1@127.0.0.1:ns_cluster<0.189.0>:ns_cluster:do_change_address:596]Change of address to "127.0.0.1" is requested.
[cluster:debug,2020-03-27T19:53:00.626Z,ns_1@127.0.0.1:<0.9962.0>:ns_cluster:maybe_rename:626]Not renaming node.
[ns_server:debug,2020-03-27T19:53:00.640Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{metakv,<<"/indexing/settings/config">>} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{3,63752557980}}]}|
 <<"{\"indexer.settings.compaction.days_of_week\":\"Sunday,Monday,Tuesday,Wednesday,Thursday,Friday,Saturday\",\"indexer.settings.compaction.interval\":\"00:00,00:00\",\"indexer.settings.persisted_snapshot.interval\":5000,\"indexer.settings.log_level\":\"info\",\"indexer.settings.compaction.compaction_mode\":\"circular\",\"indexer.settings.compaction.min_frag\":30,\"indexer.settings.inmemory_snapshot.interval\""...>>]
[ns_server:debug,2020-03-27T19:53:00.640Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{local_changes_count,<<"60a6bc3db77e6c7b91c556140dcfec71">>} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{19,63752557980}}]}]
[ns_server:debug,2020-03-27T19:53:00.642Z,ns_1@127.0.0.1:ns_config_rep<0.335.0>:ns_config_rep:do_push_keys:321]Replicating some config keys ([{local_changes_count,
                                   <<"60a6bc3db77e6c7b91c556140dcfec71">>},
                               {metakv,<<"/indexing/settings/config">>}]..)
[ns_server:debug,2020-03-27T19:53:00.645Z,ns_1@127.0.0.1:ns_audit<0.479.0>:ns_audit:handle_call:125]Audit modify_index_storage_mode: [{storageMode,<<"plasma">>},
                                  {real_userid,
                                      {[{domain,wrong_token},
                                        {user,<<"<ud></ud>">>}]}},
                                  {sessionid,
                                      <<"3c9673d43ade3c5c4478657f38eb6001">>},
                                  {remote,
                                      {[{ip,<<"192.168.0.1">>},{port,54300}]}},
                                  {timestamp,<<"2020-03-27T19:53:00.643Z">>}]
[ns_server:debug,2020-03-27T19:53:00.663Z,ns_1@127.0.0.1:ns_config_rep<0.335.0>:ns_config_rep:do_push_keys:321]Replicating some config keys ([{local_changes_count,
                                   <<"60a6bc3db77e6c7b91c556140dcfec71">>},
                               {metakv,<<"/indexing/settings/config">>}]..)
[ns_server:debug,2020-03-27T19:53:00.663Z,ns_1@127.0.0.1:ns_config_rep<0.335.0>:ns_config_rep:do_push_keys:321]Replicating some config keys ([cluster_name,
                               {local_changes_count,
                                   <<"60a6bc3db77e6c7b91c556140dcfec71">>}]..)
[ns_server:debug,2020-03-27T19:53:00.665Z,ns_1@127.0.0.1:ns_audit<0.479.0>:ns_audit:handle_call:125]Audit cluster_settings: [{cluster_name,<<"Base">>},
                         {quotas,{[{kv,292},
                                   {index,512},
                                   {fts,256},
                                   {cbas,1024},
                                   {eventing,256}]}},
                         {real_userid,{[{domain,wrong_token},
                                        {user,<<"<ud></ud>">>}]}},
                         {sessionid,<<"3c9673d43ade3c5c4478657f38eb6001">>},
                         {remote,{[{ip,<<"192.168.0.1">>},{port,54300}]}},
                         {timestamp,<<"2020-03-27T19:53:00.663Z">>}]
[ns_server:debug,2020-03-27T19:53:00.664Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{metakv,<<"/indexing/settings/config">>} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{4,63752557980}}]}|
 <<"{\"indexer.settings.compaction.days_of_week\":\"Sunday,Monday,Tuesday,Wednesday,Thursday,Friday,Saturday\",\"indexer.settings.compaction.interval\":\"00:00,00:00\",\"indexer.settings.compaction.compaction_mode\":\"circular\",\"indexer.settings.log_level\":\"info\",\"indexer.settings.persisted_snapshot.interval\":5000,\"indexer.settings.compaction.min_frag\":30,\"indexer.settings.inmemory_snapshot.interval\""...>>]
[ns_server:debug,2020-03-27T19:53:00.665Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{local_changes_count,<<"60a6bc3db77e6c7b91c556140dcfec71">>} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{20,63752557980}}]}]
[ns_server:debug,2020-03-27T19:53:00.665Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{local_changes_count,<<"60a6bc3db77e6c7b91c556140dcfec71">>} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{21,63752557980}}]}]
[ns_server:debug,2020-03-27T19:53:00.665Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
cluster_name ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557980}}]},
 66,97,115,101]
[ns_server:debug,2020-03-27T19:53:03.202Z,ns_1@127.0.0.1:ldap_auth_cache<0.261.0>:active_cache:cleanup:231]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2020-03-27T19:53:03.202Z,ns_1@127.0.0.1:roles_cache<0.272.0>:active_cache:renew:211]Starting roles_cache cache renewal
[ns_server:debug,2020-03-27T19:53:03.202Z,ns_1@127.0.0.1:roles_cache<0.272.0>:active_cache:renew:217]roles_cache cache renewal process originated 0 queries
[ns_server:debug,2020-03-27T19:53:07.835Z,ns_1@127.0.0.1:compaction_daemon<0.533.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2020-03-27T19:53:07.835Z,ns_1@127.0.0.1:compaction_daemon<0.533.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2020-03-27T19:53:07.835Z,ns_1@127.0.0.1:compaction_daemon<0.533.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2020-03-27T19:53:07.835Z,ns_1@127.0.0.1:compaction_daemon<0.533.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:info,2020-03-27T20:03:32.866Z,nonode@nohost:<0.118.0>:ns_server:init_logging:150]Started & configured logging
[ns_server:info,2020-03-27T20:03:32.898Z,nonode@nohost:<0.118.0>:ns_server:log_pending:32]Static config terms:
[{error_logger_mf_dir,"/opt/couchbase/var/lib/couchbase/logs"},
 {path_config_bindir,"/opt/couchbase/bin"},
 {path_config_etcdir,"/opt/couchbase/etc/couchbase"},
 {path_config_libdir,"/opt/couchbase/lib"},
 {path_config_datadir,"/opt/couchbase/var/lib/couchbase"},
 {path_config_tmpdir,"/opt/couchbase/var/lib/couchbase/tmp"},
 {path_config_secdir,"/opt/couchbase/etc/security"},
 {nodefile,"/opt/couchbase/var/lib/couchbase/couchbase-server.node"},
 {loglevel_default,debug},
 {loglevel_couchdb,info},
 {loglevel_ns_server,debug},
 {loglevel_error_logger,debug},
 {loglevel_user,debug},
 {loglevel_menelaus,debug},
 {loglevel_ns_doctor,debug},
 {loglevel_stats,debug},
 {loglevel_rebalance,debug},
 {loglevel_cluster,debug},
 {loglevel_views,debug},
 {loglevel_mapreduce_errors,debug},
 {loglevel_xdcr,debug},
 {loglevel_access,info},
 {loglevel_cbas,debug},
 {disk_sink_opts,[{rotation,[{compress,true},
                             {size,41943040},
                             {num_files,10},
                             {buffer_size_max,52428800}]}]},
 {disk_sink_opts_json_rpc,[{rotation,[{compress,true},
                                      {size,41943040},
                                      {num_files,2},
                                      {buffer_size_max,52428800}]}]},
 {net_kernel_verbosity,10}]
[ns_server:warn,2020-03-27T20:03:32.899Z,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter error_logger_mf_dir, which is given from command line
[ns_server:warn,2020-03-27T20:03:32.899Z,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter path_config_bindir, which is given from command line
[ns_server:warn,2020-03-27T20:03:32.899Z,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter path_config_etcdir, which is given from command line
[ns_server:warn,2020-03-27T20:03:32.899Z,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter path_config_libdir, which is given from command line
[ns_server:warn,2020-03-27T20:03:32.900Z,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter path_config_datadir, which is given from command line
[ns_server:warn,2020-03-27T20:03:32.900Z,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter path_config_tmpdir, which is given from command line
[ns_server:warn,2020-03-27T20:03:32.900Z,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter path_config_secdir, which is given from command line
[ns_server:warn,2020-03-27T20:03:32.900Z,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter nodefile, which is given from command line
[ns_server:warn,2020-03-27T20:03:32.900Z,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_default, which is given from command line
[ns_server:warn,2020-03-27T20:03:32.900Z,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_couchdb, which is given from command line
[ns_server:warn,2020-03-27T20:03:32.900Z,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_ns_server, which is given from command line
[ns_server:warn,2020-03-27T20:03:32.900Z,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_error_logger, which is given from command line
[ns_server:warn,2020-03-27T20:03:32.900Z,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_user, which is given from command line
[ns_server:warn,2020-03-27T20:03:32.900Z,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_menelaus, which is given from command line
[ns_server:warn,2020-03-27T20:03:32.900Z,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_ns_doctor, which is given from command line
[ns_server:warn,2020-03-27T20:03:32.900Z,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_stats, which is given from command line
[ns_server:warn,2020-03-27T20:03:32.900Z,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_rebalance, which is given from command line
[ns_server:warn,2020-03-27T20:03:32.900Z,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_cluster, which is given from command line
[ns_server:warn,2020-03-27T20:03:32.900Z,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_views, which is given from command line
[ns_server:warn,2020-03-27T20:03:32.900Z,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_mapreduce_errors, which is given from command line
[ns_server:warn,2020-03-27T20:03:32.900Z,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_xdcr, which is given from command line
[ns_server:warn,2020-03-27T20:03:32.900Z,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_access, which is given from command line
[ns_server:warn,2020-03-27T20:03:32.900Z,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_cbas, which is given from command line
[ns_server:warn,2020-03-27T20:03:32.900Z,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter disk_sink_opts, which is given from command line
[ns_server:warn,2020-03-27T20:03:32.900Z,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter disk_sink_opts_json_rpc, which is given from command line
[ns_server:warn,2020-03-27T20:03:32.900Z,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter net_kernel_verbosity, which is given from command line
[ns_server:info,2020-03-27T20:03:32.924Z,nonode@nohost:dist_manager<0.166.0>:dist_manager:read_address_config_from_path:99]Reading ip config from "/opt/couchbase/var/lib/couchbase/ip_start"
[ns_server:info,2020-03-27T20:03:32.925Z,nonode@nohost:dist_manager<0.166.0>:dist_manager:read_address_config_from_path:99]Reading ip config from "/opt/couchbase/var/lib/couchbase/ip"
[error_logger:info,2020-03-27T20:03:32.932Z,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,inet_gethost_native_sup}
             started: [{pid,<0.168.0>},{mfa,{inet_gethost_native,init,[[]]}}]

[error_logger:info,2020-03-27T20:03:32.932Z,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,kernel_safe_sup}
             started: [{pid,<0.167.0>},
                       {id,inet_gethost_native_sup},
                       {mfargs,{inet_gethost_native,start_link,[]}},
                       {restart_type,temporary},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:info,2020-03-27T20:03:32.944Z,nonode@nohost:dist_manager<0.166.0>:dist_manager:bringup:249]Attempting to bring up net_kernel with name 'ns_1@127.0.0.1'
[error_logger:info,2020-03-27T20:03:32.964Z,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ssl_dist_admin_sup}
             started: [{pid,<0.172.0>},
                       {id,ssl_pem_cache_dist},
                       {mfargs,{ssl_pem_cache,start_link_dist,[[]]}},
                       {restart_type,permanent},
                       {shutdown,4000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:03:32.964Z,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ssl_dist_admin_sup}
             started: [{pid,<0.173.0>},
                       {id,ssl_dist_manager},
                       {mfargs,{ssl_manager,start_link_dist,[[]]}},
                       {restart_type,permanent},
                       {shutdown,4000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:03:32.964Z,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ssl_dist_sup}
             started: [{pid,<0.171.0>},
                       {id,ssl_dist_admin_sup},
                       {mfargs,{ssl_dist_admin_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,4000},
                       {child_type,supervisor}]

[error_logger:info,2020-03-27T20:03:32.969Z,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ssl_dist_sup}
             started: [{pid,<0.174.0>},
                       {id,ssl_tls_dist_proxy},
                       {mfargs,{ssl_tls_dist_proxy,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,4000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:03:32.972Z,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ssl_dist_connection_sup}
             started: [{pid,<0.176.0>},
                       {id,dist_tls_connection},
                       {mfargs,{tls_connection_sup,start_link_dist,[]}},
                       {restart_type,permanent},
                       {shutdown,4000},
                       {child_type,supervisor}]

[error_logger:info,2020-03-27T20:03:32.972Z,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ssl_dist_connection_sup}
             started: [{pid,<0.177.0>},
                       {id,dist_tls_socket},
                       {mfargs,{ssl_listen_tracker_sup,start_link_dist,[]}},
                       {restart_type,permanent},
                       {shutdown,4000},
                       {child_type,supervisor}]

[error_logger:info,2020-03-27T20:03:32.972Z,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ssl_dist_sup}
             started: [{pid,<0.175.0>},
                       {id,ssl_dist_connection_sup},
                       {mfargs,{ssl_dist_connection_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,4000},
                       {child_type,supervisor}]

[error_logger:info,2020-03-27T20:03:32.973Z,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,net_sup}
             started: [{pid,<0.170.0>},
                       {id,ssl_dist_sup},
                       {mfargs,{ssl_dist_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[ns_server:debug,2020-03-27T20:03:32.979Z,nonode@nohost:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Starting cb_dist with config [{external_listeners,
                                        [inet_tcp_dist,inet6_tcp_dist]},
                                       {preferred_external_proto,
                                        inet_tcp_dist},
                                       {preferred_local_proto,inet_tcp_dist}]
[error_logger:info,2020-03-27T20:03:32.982Z,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,net_sup}
             started: [{pid,<0.178.0>},
                       {id,cb_dist},
                       {mfargs,{cb_dist,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:03:32.982Z,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,net_sup}
             started: [{pid,<0.179.0>},
                       {id,cb_epmd},
                       {mfargs,{cb_epmd,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,2000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:03:32.984Z,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,net_sup}
             started: [{pid,<0.180.0>},
                       {id,auth},
                       {mfargs,{auth,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,2000},
                       {child_type,worker}]

[ns_server:debug,2020-03-27T20:03:32.990Z,nonode@nohost:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Initial protos: [inet_tcp_dist,inet6_tcp_dist], required protos: [inet_tcp_dist]
[ns_server:debug,2020-03-27T20:03:32.990Z,nonode@nohost:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Starting inet_tcp_dist listener on 21100...
[ns_server:debug,2020-03-27T20:03:32.991Z,nonode@nohost:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Starting inet6_tcp_dist listener on 21100...
[ns_server:debug,2020-03-27T20:03:32.996Z,ns_1@127.0.0.1:dist_manager<0.166.0>:dist_manager:configure_net_kernel:293]Set net_kernel vebosity to 10 -> 0
[error_logger:info,2020-03-27T20:03:32.996Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,net_sup}
             started: [{pid,<0.181.0>},
                       {id,net_kernel},
                       {mfargs,
                           {net_kernel,start_link,
                               [['ns_1@127.0.0.1',longnames],false]}},
                       {restart_type,permanent},
                       {shutdown,2000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:03:32.996Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,kernel_sup}
             started: [{pid,<0.169.0>},
                       {id,net_sup_dynamic},
                       {mfargs,
                           {erl_distribution,start_link,
                               [['ns_1@127.0.0.1',longnames],false]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,supervisor}]

[ns_server:info,2020-03-27T20:03:32.998Z,ns_1@127.0.0.1:dist_manager<0.166.0>:dist_manager:save_node:175]saving node to "/opt/couchbase/var/lib/couchbase/couchbase-server.node"
[ns_server:debug,2020-03-27T20:03:33.014Z,ns_1@127.0.0.1:dist_manager<0.166.0>:dist_manager:bringup:263]Attempted to save node name to disk: ok
[ns_server:debug,2020-03-27T20:03:33.014Z,ns_1@127.0.0.1:dist_manager<0.166.0>:dist_manager:wait_for_node:270]Waiting for connection to node 'babysitter_of_ns_1@cb.local' to be established
[ns_server:debug,2020-03-27T20:03:33.014Z,ns_1@127.0.0.1:net_kernel<0.181.0>:cb_dist:info_msg:754]cb_dist: Setting up new connection to 'babysitter_of_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2020-03-27T20:03:33.014Z,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Added connection {con,#Ref<0.1304447726.2365587457.29802>,
                               inet_tcp_dist,undefined,undefined}
[error_logger:info,2020-03-27T20:03:33.015Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{connect,normal,'babysitter_of_ns_1@cb.local'}}
[ns_server:debug,2020-03-27T20:03:33.015Z,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Updated connection: {con,#Ref<0.1304447726.2365587457.29802>,
                                  inet_tcp_dist,<0.185.0>,
                                  #Ref<0.1304447726.2365587458.29057>}
[ns_server:debug,2020-03-27T20:03:33.027Z,ns_1@127.0.0.1:dist_manager<0.166.0>:dist_manager:wait_for_node:282]Observed node 'babysitter_of_ns_1@cb.local' to come up
[ns_server:info,2020-03-27T20:03:33.029Z,ns_1@127.0.0.1:dist_manager<0.166.0>:dist_manager:save_address_config:162]Deleting irrelevant ip file "/opt/couchbase/var/lib/couchbase/ip_start": {error,
                                                                          enoent}
[ns_server:info,2020-03-27T20:03:33.029Z,ns_1@127.0.0.1:dist_manager<0.166.0>:dist_manager:save_address_config:163]saving ip config to "/opt/couchbase/var/lib/couchbase/ip"
[ns_server:info,2020-03-27T20:03:33.059Z,ns_1@127.0.0.1:dist_manager<0.166.0>:dist_manager:save_address_config:166]Persisted the address successfully
[error_logger:info,2020-03-27T20:03:33.059Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,root_sup}
             started: [{pid,<0.166.0>},
                       {id,dist_manager},
                       {mfargs,{dist_manager,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:03:33.072Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.188.0>},
                       {id,local_tasks},
                       {mfargs,{local_tasks,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[ns_server:info,2020-03-27T20:03:33.076Z,ns_1@127.0.0.1:ns_server_cluster_sup<0.187.0>:log_os_info:start_link:25]OS type: {unix,linux} Version: {4,19,76}
Runtime info: [{otp_release,"20"},
               {erl_version,"9.3.3.9"},
               {erl_version_long,
                   "Erlang/OTP 20 [erts-9.3.3.9] [source-d27a01d] [64-bit] [smp:2:2] [ds:2:2:10] [async-threads:16] [kernel-poll:true]\n"},
               {system_arch_raw,"x86_64-unknown-linux-gnu"},
               {system_arch,"x86_64-unknown-linux-gnu"},
               {localtime,{{2020,3,27},{20,3,33}}},
               {memory,
                   [{total,36411208},
                    {processes,9357224},
                    {processes_used,9353688},
                    {system,27053984},
                    {atom,388625},
                    {atom_used,364430},
                    {binary,175720},
                    {code,8250921},
                    {ets,1509640}]},
               {loaded,
                   [ns_info,log_os_info,local_tasks,restartable,
                    ns_server_cluster_sup,ns_cluster,dist_util,ns_node_disco,
                    inet6_tcp,inet6_tcp_dist,re,auth,rand,
                    ssl_dist_connection_sup,ssl_tls_dist_proxy,
                    ssl_dist_admin_sup,ssl_dist_sup,inet_tls_dist,
                    inet_tcp_dist,inet_tcp,gen_tcp,erl_epmd,cb_epmd,gen_udp,
                    inet_hosts,dist_manager,root_sup,path_config,cb_dist,
                    unicode_util,calendar,ale_default_formatter,
                    'ale_logger-metakv','ale_logger-rebalance',
                    'ale_logger-menelaus','ale_logger-stats',
                    'ale_logger-json_rpc','ale_logger-access',
                    'ale_logger-ns_server','ale_logger-user',
                    'ale_logger-ns_doctor','ale_logger-cluster',
                    'ale_logger-xdcr',erl_bits,otp_internal,ns_log_sink,
                    ale_disk_sink,misc,couch_util,ns_server,io_lib_fread,
                    filelib,cpu_sup,memsup,disksup,os_mon,string,io,
                    release_handler,alarm_handler,sasl,timer,tftp_sup,
                    httpd_sup,httpc_handler_sup,httpc_cookie,inets_trace,
                    httpc_manager,httpc,httpc_profile_sup,httpc_sup,ftp_sup,
                    inets_sup,inets_app,ssl,lhttpc_manager,lhttpc_sup,lhttpc,
                    dtls_udp_sup,dtls_connection_sup,ssl_listen_tracker_sup,
                    tls_connection_sup,ssl_connection_sup,ssl_session_cache,
                    ssl_manager,ssl_pkix_db,ssl_pem_cache,ssl_admin_sup,
                    ssl_sup,ssl_app,ale_error_logger_handler,
                    'ale_logger-ale_logger','ale_logger-error_logger',
                    beam_opcodes,maps,beam_dict,beam_asm,beam_validator,
                    beam_z,beam_flatten,beam_trim,beam_record,beam_receive,
                    beam_bsm,beam_peep,beam_dead,beam_split,beam_type,
                    beam_clean,beam_bs,beam_except,beam_block,beam_utils,
                    beam_reorder,beam_jump,beam_a,v3_codegen,v3_life,
                    v3_kernel,sys_core_dsetel,sys_core_bsm,erl_bifs,
                    cerl_clauses,cerl_sets,sys_core_fold,cerl_trees,
                    sys_core_inline,core_lib,cerl,v3_core,erl_expand_records,
                    sofs,erl_internal,sets,ordsets,compile,dynamic_compile,
                    ale_utils,io_lib_pretty,io_lib_format,io_lib,ale_codegen,
                    dict,ale,ale_dynamic_sup,ale_sup,ale_app,ns_bootstrap,
                    child_erlang,orddict,c,erl_signal_handler,kernel_config,
                    user_io,user_sup,supervisor_bridge,standard_error,
                    net_kernel,global_group,erl_distribution,epp,
                    inet_gethost_native,inet_parse,inet,inet_udp,inet_config,
                    inet_db,global,rpc,unicode,os,hipe_unified_loader,
                    gb_trees,gb_sets,binary,erl_anno,proplists,erl_scan,ets,
                    file_server,application_controller,application,
                    application_master,error_logger,file_io_server,filename,
                    code_server,error_handler,kernel,heart,code,gen_event,
                    erl_lint,supervisor,erl_eval,proc_lib,erl_parse,file,
                    gen_server,gen,lists,erts_dirty_process_code_checker,
                    erts_literal_area_collector,erl_tracer,erts_internal,
                    erlang,erl_prim_loader,prim_zip,zlib,prim_file,prim_inet,
                    prim_eval,init,erts_code_purger,otp_ring0]},
               {applications,
                   [{sasl,"SASL  CXC 138 11","3.1.2"},
                    {os_mon,"CPO  CXC 138 46","2.4.4"},
                    {inets,"INETS  CXC 138 49","6.5.2.4"},
                    {crypto,"CRYPTO","4.2.2.2"},
                    {ale,"Another Logger for Erlang","0.0.0"},
                    {lhttpc,"Lightweight HTTP Client","1.3.0"},
                    {stdlib,"ERTS  CXC 138 10","3.4.5.1"},
                    {ssl,"Erlang/OTP SSL application","8.2.6.4"},
                    {kernel,"ERTS  CXC 138 10","5.4.3.2"},
                    {public_key,"Public key infrastructure","1.5.2"},
                    {asn1,"The Erlang ASN1 compiler version 5.0.5.2",
                        "5.0.5.2"},
                    {ns_server,"Couchbase server","6.5.0-4960-enterprise"}]},
               {pre_loaded,
                   [erts_dirty_process_code_checker,
                    erts_literal_area_collector,erl_tracer,erts_internal,
                    erlang,erl_prim_loader,prim_zip,zlib,prim_file,prim_inet,
                    prim_eval,init,erts_code_purger,otp_ring0]},
               {process_count,131},
               {node,'ns_1@127.0.0.1'},
               {nodes,[]},
               {registered,
                   [application_controller,erl_prim_loader,httpd_sup,auth,
                    dtls_udp_sup,cb_dist,dtls_connection_sup,
                    ns_server_cluster_sup,tls_connection_sup,sasl_sup,
                    release_handler,lhttpc_sup,httpc_sup,lhttpc_manager,
                    alarm_handler,httpc_profile_sup,
                    ssl_listen_tracker_supdist,httpc_manager,
                    httpc_handler_sup,ssl_connection_sup_dist,'sink-ns_log',
                    local_tasks,standard_error_sup,ftp_sup,
                    'sink-disk_json_rpc','sink-disk_metakv',inets_sup,
                    'sink-disk_access_int','sink-disk_access',standard_error,
                    'sink-disk_reports',ale_stats_events,'sink-disk_stats',
                    'sink-disk_xdcr',timer_server,'sink-disk_debug',ale_sup,
                    inet_gethost_native,'sink-disk_error',inet_db,
                    'sink-disk_default',ale_dynamic_sup,kernel_safe_sup,
                    ssl_pem_cache_dist,rex,global_group,net_sup,
                    ssl_connection_sup,kernel_sup,ssl_admin_sup,tftp_sup,
                    global_name_server,ssl_sup,root_sup,os_mon_sup,
                    erts_code_purger,file_server_2,error_logger,cpu_sup,
                    memsup,erl_epmd,init,disksup,ale,erl_signal_server,
                    net_kernel,dist_manager,ssl_pem_cache,ssl_manager,
                    ssl_dist_admin_sup,ssl_dist_connection_sup,ssl_dist_sup,
                    ssl_tls_dist_proxy,ssl_manager_dist,user,sasl_safe_sup,
                    ssl_listen_tracker_sup,inet_gethost_native_sup,
                    code_server]},
               {cookie,nocookie},
               {wordsize,8},
               {wall_clock,2}]
[ns_server:info,2020-03-27T20:03:33.088Z,ns_1@127.0.0.1:ns_server_cluster_sup<0.187.0>:log_os_info:start_link:27]Manifest:
["<manifest>",
 "  <remote fetch=\"git://github.com/blevesearch/\" name=\"blevesearch\" />",
 "  <remote fetch=\"git://github.com/couchbase/\" name=\"couchbase\" review=\"review.couchbase.org\" />",
 "  <remote fetch=\"ssh://git@github.com/couchbase/\" name=\"couchbase-priv\" review=\"review.couchbase.org\" />",
 "  <remote fetch=\"git://github.com/couchbasedeps/\" name=\"couchbasedeps\" review=\"review.couchbase.org\" />",
 "  <remote fetch=\"git://github.com/couchbaselabs/\" name=\"couchbaselabs\" review=\"review.couchbase.org\" />",
 "  ","  <default remote=\"couchbase\" revision=\"master\" />","  ",
 "  <project groups=\"kv\" name=\"HdrHistogram_c\" path=\"third_party/HdrHistogram_c\" remote=\"couchbasedeps\" revision=\"bc8aef24ea57884464027f841c1ad7436a42c615\" />",
 "  <project groups=\"notdefault,enterprise,analytics\" name=\"analytics-dcp-client\" path=\"analytics/java-dcp-client\" revision=\"691cec38f47eaab04ad81556cc065d22f1eb8749\" />",
 "  <project groups=\"notdefault,enterprise,analytics\" name=\"asterixdb\" path=\"analytics/asterixdb\" revision=\"672a36b64a0632b72aa4b4df59635ceaa0e340de\" />",
 "  <project groups=\"backup,notdefault,enterprise\" name=\"backup\" path=\"goproj/src/github.com/couchbase/backup\" remote=\"couchbase-priv\" revision=\"cfa0f75f28402d2e1aa254b2a374bead19433526\" upstream=\"mad-hatter\" />",
 "  <project groups=\"kv\" name=\"benchmark\" remote=\"couchbasedeps\" revision=\"74b24058ad4914b837200d0341050657ba154e4a\" />",
 "  <project name=\"bitset\" path=\"godeps/src/github.com/willf/bitset\" remote=\"couchbasedeps\" revision=\"28a4168144bb8ac95454e1f51c84da1933681ad4\" />",
 "  <project name=\"blance\" path=\"godeps/src/github.com/couchbase/blance\" revision=\"5cd1345cca3ed72f1e63d41d622fcda73e63fea8\" upstream=\"master\" />",
 "  <project name=\"bleve\" path=\"godeps/src/github.com/blevesearch/bleve\" remote=\"blevesearch\" revision=\"b7a0cb6a1d4fdbaeb7ab5bdec6a9732b995e39a0\" />",
 "  <project name=\"bleve-mapping-ui\" path=\"godeps/src/github.com/blevesearch/bleve-mapping-ui\" remote=\"blevesearch\" revision=\"7987f3c80047347b1e2c3a5fafae8da56daf97d7\" />",
 "  <project name=\"bolt\" path=\"godeps/src/github.com/boltdb/bolt\" remote=\"couchbasedeps\" revision=\"51f99c862475898df9773747d3accd05a7ca33c1\" />",
 "  <project name=\"buffer\" path=\"godeps/src/github.com/tdewolff/buffer\" remote=\"couchbasedeps\" revision=\"43cef5ba7b6ce99cc410632dad46cf1c6c97026e\" />",
 "  <project groups=\"notdefault,build\" name=\"build\" path=\"cbbuild\" revision=\"f2a16b53bb74146f20d18ba2c0443d5f10a9a550\" upstream=\"master\">",
 "    <annotation name=\"RELEASE\" value=\"mad-hatter\" />",
 "    <annotation name=\"PRODUCT\" value=\"couchbase-server\" />",
 "    <annotation name=\"BLD_NUM\" value=\"4960\" />",
 "    <annotation name=\"VERSION\" value=\"6.5.0\" />","  </project>",
 "  <project groups=\"notdefault,enterprise,analytics\" name=\"cbas\" path=\"goproj/src/github.com/couchbase/cbas\" remote=\"couchbase-priv\" revision=\"e3ec01671ca2f253a5f32cf9e258d3be7fdbfe9a\" />",
 "  <project groups=\"notdefault,enterprise,analytics\" name=\"cbas-core\" path=\"analytics\" remote=\"couchbase-priv\" revision=\"c86a9fc60d074711470b112753c5695dee79dcf7\" />",
 "  <project groups=\"analytics\" name=\"cbas-ui\" revision=\"8744108f25c4520b09009ff277d35223e208fe30\" />",
 "  <project name=\"cbauth\" path=\"godeps/src/github.com/couchbase/cbauth\" revision=\"82614adbe4d480de5675d8eee9b21a180a779222\" upstream=\"master\" />",
 "  <project groups=\"backup\" name=\"cbflag\" path=\"godeps/src/github.com/couchbase/cbflag\" revision=\"9892b6db3537c54be7719f47ad25e0d513333b3e\" upstream=\"master\" />",
 "  <project name=\"cbft\" path=\"goproj/src/github.com/couchbase/cbft\" revision=\"ef487dda0baef8a258bac4f7482af3b761e4a8e0\" upstream=\"mad-hatter\" />",
 "  <project groups=\"notdefault,enterprise\" name=\"cbftx\" path=\"goproj/src/github.com/couchbase/cbftx\" remote=\"couchbase-priv\" revision=\"46dbb7c6edac7dfef017ae889d7a5b7536ce904d\" upstream=\"master\" />",
 "  <project name=\"cbgt\" path=\"goproj/src/github.com/couchbase/cbgt\" revision=\"c78e34377d7a8f017328f57a3376642f37458464\" upstream=\"mad-hatter\" />",
 "  <project name=\"cbsummary\" path=\"goproj/src/github.com/couchbase/cbsummary\" revision=\"31ba0584a81d5b293cedfb236109ab95036aa395\" upstream=\"master\" />",
 "  <project groups=\"backup\" name=\"clog\" path=\"godeps/src/github.com/couchbase/clog\" revision=\"b8e6d5d421bcc34f522e3a9a12fd6e09980995b1\" upstream=\"master\" />",
 "  <project name=\"cobra\" path=\"godeps/src/github.com/spf13/cobra\" remote=\"couchbasedeps\" revision=\"0f056af21f5f368e5b0646079d0094a2c64150f7\" />",
 "  <project name=\"context\" path=\"godeps/src/github.com/gorilla/context\" remote=\"couchbasedeps\" revision=\"215affda49addc4c8ef7e2534915df2c8c35c6cd\" />",
 "  <project groups=\"notdefault,kv_ee,enterprise\" name=\"couch_rocks\" remote=\"couchbase-priv\" revision=\"75f37fa46bfe5e445dee077157303968a3e09126\" upstream=\"master\" />",
 "  <project groups=\"kv\" name=\"couchbase-cli\" revision=\"abb0c1036566f4bd579aaadbaaa4e13466a23ef7\" upstream=\"master\" />",
 "  <project name=\"couchdb\" revision=\"fa3c64b1b85ad3145bb7910d3fe7ee90c060247e\" upstream=\"mad-hatter\" />",
 "  <project groups=\"notdefault,packaging\" name=\"couchdbx-app\" revision=\"b2a111967ba02772dc600d5c15a6514e2dea7d68\" upstream=\"master\" />",
 "  <project groups=\"kv\" name=\"couchstore\" revision=\"fff3e20090414206853b2293f17667279dda0337\" />",
 "  <project groups=\"backup\" name=\"crypto\" path=\"godeps/src/golang.org/x/crypto\" remote=\"couchbasedeps\" revision=\"bd6f299fb381e4c3393d1c4b1f0b94f5e77650c8\" />",
 "  <project name=\"cuckoofilter\" path=\"godeps/src/github.com/seiflotfy/cuckoofilter\" remote=\"couchbasedeps\" revision=\"d04838794ab86926d32b124345777e55e6f43974\" />",
 "  <project name=\"cznic-b\" path=\"godeps/src/github.com/cznic/b\" remote=\"couchbasedeps\" revision=\"b96e30f1b7bd34b0b9d8760798d67eca83d7f09e\" />",
 "  <project name=\"docloader\" path=\"goproj/src/github.com/couchbase/docloader\" revision=\"13cf07af78594aff20d00db4633af27d81fc921d\" upstream=\"master\" />",
 "  <project name=\"dparval\" path=\"godeps/src/github.com/couchbase/dparval\" revision=\"9def03782da875a2477c05bf64985db3f19f59ae\" upstream=\"master\" />",
 "  <project groups=\"backup\" name=\"errors\" path=\"godeps/src/github.com/pkg/errors\" remote=\"couchbasedeps\" revision=\"30136e27e2ac8d167177e8a583aa4c3fea5be833\" />",
 "  <project name=\"etcd-bbolt\" path=\"godeps/src/github.com/etcd-io/bbolt\" remote=\"couchbasedeps\" revision=\"7ee3ded59d4835e10f3e7d0f7603c42aa5e83820\" />",
 "  <project groups=\"notdefault,enterprise\" name=\"eventing\" path=\"goproj/src/github.com/couchbase/eventing\" revision=\"dec7a7d51b71309d43d7aea4803cd45f6ad001da\" upstream=\"mad-hatter\" />",
 "  <project groups=\"notdefault,enterprise\" name=\"eventing-ee\" path=\"goproj/src/github.com/couchbase/eventing-ee\" remote=\"couchbase-priv\" revision=\"398acea25e003c1739d3f45f53121bdec857e485\" upstream=\"mad-hatter\" />",
 "  <project name=\"flatbuffers\" path=\"godeps/src/github.com/google/flatbuffers\" remote=\"couchbasedeps\" revision=\"1a8968225130caeddd16e227678e6f8af1926303\" />",
 "  <project groups=\"backup,kv\" name=\"forestdb\" revision=\"4c3b2f9b1d869b6b71556e461d6ee68f941c1ba5\" upstream=\"cb-master\" />",
 "  <project name=\"fwd\" path=\"godeps/src/github.com/philhofer/fwd\" remote=\"couchbasedeps\" revision=\"bb6d471dc95d4fe11e432687f8b70ff496cf3136\" />",
 "  <project name=\"geocouch\" revision=\"92def13f6b049553da1aa1488ce0bde6b7d0f459\" upstream=\"master\" />",
 "  <project name=\"ghistogram\" path=\"godeps/src/github.com/couchbase/ghistogram\" revision=\"d910dd063dd68fb4d2a1ba344440f834ebb4ef62\" upstream=\"master\" />",
 "  <project name=\"go-bindata-assetfs\" path=\"godeps/src/github.com/elazarl/go-bindata-assetfs\" remote=\"couchbasedeps\" revision=\"57eb5e1fc594ad4b0b1dbea7b286d299e0cb43c2\" />",
 "  <project name=\"go-couchbase\" path=\"godeps/src/github.com/couchbase/go-couchbase\" revision=\"12d479a70a3ef189d8fb2424f5e2eea3632c0c9a\" upstream=\"mad-hatter\" />",
 "  <project name=\"go-curl\" path=\"godeps/src/github.com/andelf/go-curl\" remote=\"couchbasedeps\" revision=\"f0b2afc926ec79be5d7f30393b3485352781a705\" upstream=\"20161221-couchbase\" />",
 "  <project name=\"go-genproto\" path=\"godeps/src/google.golang.org/genproto\" remote=\"couchbasedeps\" revision=\"2b5a72b8730b0b16380010cfe5286c42108d88e7\" />",
 "  <project name=\"go-jsonpointer\" path=\"godeps/src/github.com/dustin/go-jsonpointer\" remote=\"couchbasedeps\" revision=\"75939f54b39e7dafae879e61f65438dadc5f288c\" />",
 "  <project name=\"go-metrics\" path=\"godeps/src/github.com/rcrowley/go-metrics\" remote=\"couchbasedeps\" revision=\"dee209f2455f101a5e4e593dea94872d2c62d85d\" />",
 "  <project name=\"go-porterstemmer\" path=\"godeps/src/github.com/blevesearch/go-porterstemmer\" remote=\"blevesearch\" revision=\"23a2c8e5cf1f380f27722c6d2ae8896431dc7d0e\" />",
 "  <project name=\"go-runewidth\" path=\"godeps/src/github.com/mattn/go-runewidth\" remote=\"couchbasedeps\" revision=\"703b5e6b11ae25aeb2af9ebb5d5fdf8fa2575211\" />",
 "  <project name=\"go-slab\" path=\"godeps/src/github.com/couchbase/go-slab\" revision=\"1f5f7f282713ccfab3f46b1610cb8da34bcf676f\" upstream=\"master\" />",
 "  <project groups=\"backup\" name=\"go-sqlite3\" path=\"godeps/src/github.com/mattn/go-sqlite3\" remote=\"couchbasedeps\" revision=\"ad30583d8387ce8118f8605eaeb3b4f7b4ae0ee1\" />",
 "  <project name=\"go-unsnap-stream\" path=\"godeps/src/github.com/glycerine/go-unsnap-stream\" remote=\"couchbasedeps\" revision=\"62a9a9eb44fd8932157b1a8ace2149eff5971af6\" />",
 "  <project name=\"go-zookeeper\" path=\"godeps/src/github.com/samuel/go-zookeeper\" remote=\"couchbasedeps\" revision=\"fa6674abf3f4580b946a01bf7a1ce4ba8766205b\" />",
 "  <project name=\"go_json\" path=\"godeps/src/github.com/couchbase/go_json\" revision=\"d47ffbbc4863b0020bb85c4e181d4044ea184d40\" upstream=\"mad-hatter\" />",
 "  <project name=\"go_n1ql\" path=\"godeps/src/github.com/couchbase/go_n1ql\" revision=\"6cf4e348b127e21f56e53eb8c3faaea56afdc588\" upstream=\"master\" />",
 "  <project groups=\"backup\" name=\"gocb\" path=\"godeps/src/gopkg.in/couchbase/gocb.v1\" revision=\"01c846cb025ddd50a2ef4c82a27992b40c230dbb\" upstream=\"refs/tags/v1.4.2\" />",
 "  <project groups=\"backup\" name=\"gocbconnstr\" path=\"godeps/src/gopkg.in/couchbaselabs/gocbconnstr.v1\" remote=\"couchbaselabs\" revision=\"083dcfef49cfdcb42a0f5ecf8c0c29b0cbaa640f\" upstream=\"master\" />",
 "  <project groups=\"backup\" name=\"gocbcore\" path=\"godeps/src/gopkg.in/couchbase/gocbcore.v7\" revision=\"441cb91f01ce26932514ec10d9e59e568ee27722\" upstream=\"refs/tags/v7.1.14\" />",
 "  <project name=\"godbc\" path=\"godeps/src/github.com/couchbase/godbc\" revision=\"b2aaaa21900ab3e95d37d38fb5a0f320426cbe56\" upstream=\"mad-hatter\" />",
 "  <project name=\"gofarmhash\" path=\"godeps/src/github.com/leemcloughlin/gofarmhash\" remote=\"couchbasedeps\" revision=\"0a055c5b87a8c55ce83459cbf2776b563822a942\" />",
 "  <project groups=\"backup\" name=\"goforestdb\" path=\"godeps/src/github.com/couchbase/goforestdb\" revision=\"0b501227de0e8c55d99ed14e900eea1a1dbaf899\" upstream=\"master\" />",
 "  <project name=\"gojson\" path=\"godeps/src/github.com/dustin/gojson\" remote=\"couchbasedeps\" revision=\"af16e0e771e2ed110f2785564ae33931de8829e4\" />",
 "  <project name=\"gojsonsm\" path=\"godeps/src/github.com/couchbase/gojsonsm\" remote=\"couchbaselabs\" revision=\"eec4953dcb855282c483b8cd4fe03a8074e2f7a1\" upstream=\"master\" />",
 "  <project name=\"golang-pkg-pcre\" path=\"godeps/src/github.com/glenn-brown/golang-pkg-pcre\" remote=\"couchbasedeps\" revision=\"48bb82a8b8ceea98f4e97825b43870f6ba1970d6\" />",
 "  <project groups=\"backup\" name=\"golang-snappy\" path=\"godeps/src/github.com/golang/snappy\" remote=\"couchbasedeps\" revision=\"723cc1e459b8eea2dea4583200fd60757d40097a\" />",
 "  <project name=\"golang-tools\" path=\"godeps/src/golang.org/x/tools\" remote=\"couchbasedeps\" revision=\"a28dfb48e06b2296b66678872c2cb638f0304f20\" />",
 "  <project name=\"goleveldb\" path=\"godeps/src/github.com/syndtr/goleveldb\" remote=\"couchbasedeps\" revision=\"fa5b5c78794bc5c18f330361059f871ae8c2b9d6\" />",
 "  <project name=\"gomemcached\" path=\"godeps/src/github.com/couchbase/gomemcached\" revision=\"2b4197fedf38f694a33465050d1396e03e97db19\" upstream=\"mad-hatter\" />",
 "  <project name=\"gometa\" path=\"goproj/src/github.com/couchbase/gometa\" revision=\"563cdf343321e2025b73852bcf454860a4880300\" upstream=\"mad-hatter\" />",
 "  <project groups=\"kv\" name=\"googletest\" remote=\"couchbasedeps\" revision=\"f397fa5ec6365329b2e82eb2d8c03a7897bbefb5\" />",
 "  <project name=\"goskiplist\" path=\"godeps/src/github.com/ryszard/goskiplist\" remote=\"couchbasedeps\" revision=\"2dfbae5fcf46374f166f8969cb07e167f1be6273\" />",
 "  <project name=\"gosnappy\" path=\"godeps/src/github.com/syndtr/gosnappy\" remote=\"couchbasedeps\" revision=\"156a073208e131d7d2e212cb749feae7c339e846\" />",
 "  <project groups=\"backup\" name=\"goutils\" path=\"godeps/src/github.com/couchbase/goutils\" revision=\"b49639060d85b267c5bdb7d4e3246d4ccca94e79\" upstream=\"mad-hatter\" />",
 "  <project name=\"goxdcr\" path=\"goproj/src/github.com/couchbase/goxdcr\" revision=\"03e000156faeecd5e77eb79fc45d7c73f26b2899\" upstream=\"mad-hatter\" />",
 "  <project name=\"grpc-go\" path=\"godeps/src/google.golang.org/grpc\" remote=\"couchbasedeps\" revision=\"df014850f6dee74ba2fc94874043a9f3f75fbfd8\" upstream=\"refs/tags/v1.17.0\" />",
 "  <project groups=\"kv\" name=\"gsl-lite\" path=\"third_party/gsl-lite\" remote=\"couchbasedeps\" revision=\"57542c7e7ced375346e9ac55dad85b942cfad556\" upstream=\"refs/tags/v0.25.0\" />",
 "  <project name=\"gtreap\" path=\"godeps/src/github.com/steveyen/gtreap\" remote=\"couchbasedeps\" revision=\"0abe01ef9be25c4aedc174758ec2d917314d6d70\" />",
 "  <project name=\"httprouter\" path=\"godeps/src/github.com/julienschmidt/httprouter\" remote=\"couchbasedeps\" revision=\"975b5c4c7c21c0e3d2764200bf2aa8e34657ae6e\" />",
 "  <project name=\"indexing\" path=\"goproj/src/github.com/couchbase/indexing\" revision=\"fc2e1b715bf9c098bf0991af666388dd446edf9b\" upstream=\"mad-hatter\" />",
 "  <project name=\"json-iterator-go\" path=\"godeps/src/github.com/json-iterator/go\" remote=\"couchbasedeps\" revision=\"f7279a603edee96fe7764d3de9c6ff8cf9970994\" />",
 "  <project name=\"jsonparser\" path=\"godeps/src/github.com/buger/jsonparser\" remote=\"couchbasedeps\" revision=\"bf1c66bbce23153d89b23f8960071a680dbef54b\" />",
 "  <project groups=\"backup\" name=\"jsonx\" path=\"godeps/src/gopkg.in/couchbaselabs/jsonx.v1\" remote=\"couchbaselabs\" revision=\"5b7baa20429a46a5543ee259664cc86502738cad\" upstream=\"master\" />",
 "  <project groups=\"kv\" name=\"kv_engine\" revision=\"2a368c39481ff4d42c6f755bd7d185b9a57554ca\" upstream=\"6.5.0\" />",
 "  <project name=\"levigo\" path=\"godeps/src/github.com/jmhodges/levigo\" remote=\"couchbasedeps\" revision=\"1ddad808d437abb2b8a55a950ec2616caa88969b\" />",
 "  <project groups=\"notdefault,enterprise\" name=\"libcouchbase\" revision=\"152e1a18bbcfd75bbb5a1388ed5ee050cde8a56d\" />",
 "  <project name=\"liner\" path=\"godeps/src/github.com/peterh/liner\" remote=\"couchbasedeps\" revision=\"6f820f8f90ce9482ffbd40bb15f9ea9932f4942d\" />",
 "  <project name=\"liner\" path=\"godeps/src/github.com/sbinet/liner\" remote=\"couchbasedeps\" revision=\"d9335eee40a45a4f5d74524c90040d6fe6013d50\" />",
 "  <project groups=\"notdefault,enterprise,kv_ee\" name=\"magma\" remote=\"couchbase-priv\" revision=\"c8e91e0af8b46d0a0e026d23ebbfab4048f670b6\" />",
 "  <project name=\"minify\" path=\"godeps/src/github.com/tdewolff/minify\" remote=\"couchbasedeps\" revision=\"ede45cc53f43891267b1fe7c689db9c76d4ce0fb\" />",
 "  <project name=\"mmap-go\" path=\"godeps/src/github.com/edsrzf/mmap-go\" remote=\"couchbasedeps\" revision=\"935e0e8a636ca4ba70b713f3e38a19e1b77739e8\" />",
 "  <project name=\"mobile-service\" path=\"goproj/src/github.com/couchbase/mobile-service\" revision=\"4672fde0390f115a25f4f4bfe9d1511836de47a7\" upstream=\"master\" />",
 "  <project name=\"moss\" path=\"godeps/src/github.com/couchbase/moss\" revision=\"a0cae174c4987cb28c071e0796e25b58834108d8\" upstream=\"master\" />",
 "  <project name=\"mossScope\" path=\"godeps/src/github.com/couchbase/mossScope\" revision=\"aa48ddbc0e832bc68dde56c4b69e30c5cb3983eb\" upstream=\"master\" />",
 "  <project name=\"mousetrap\" path=\"godeps/src/github.com/inconshreveable/mousetrap\" remote=\"couchbasedeps\" revision=\"76626ae9c91c4f2a10f34cad8ce83ea42c93bb75\" />",
 "  <project name=\"msgp\" path=\"godeps/src/github.com/tinylib/msgp\" remote=\"couchbasedeps\" revision=\"5bb5e1aed7ba5bcc93307153b020e7ffe79b0509\" />",
 "  <project name=\"mux\" path=\"godeps/src/github.com/gorilla/mux\" remote=\"couchbasedeps\" revision=\"043ee6597c29786140136a5747b6a886364f5282\" />",
 "  <project name=\"n1fty\" path=\"godeps/src/github.com/couchbase/n1fty\" revision=\"f28de9b4e73d7acdf3b07b7f7318bb23973f7dc6\" upstream=\"mad-hatter\" />",
 "  <project groups=\"backup\" name=\"net\" path=\"godeps/src/golang.org/x/net\" remote=\"couchbasedeps\" revision=\"44b7c21cbf19450f38b337eb6b6fe4f6496fb5b3\" />",
 "  <project name=\"nitro\" path=\"goproj/src/github.com/couchbase/nitro\" revision=\"4fc6475fb3352618cdf93fead56271bb29d15571\" upstream=\"mad-hatter\" />",
 "  <project name=\"npipe\" path=\"godeps/src/github.com/natefinch/npipe\" remote=\"couchbasedeps\" revision=\"272c8150302e83f23d32a355364578c9c13ab20f\" />",
 "  <project name=\"ns_server\" revision=\"3fe2759eb53c12478f75bd1613f8998401b0635c\" upstream=\"mad-hatter\" />",
 "  <project groups=\"backup\" name=\"opentracing-go\" path=\"godeps/src/github.com/opentracing/opentracing-go\" remote=\"couchbasedeps\" revision=\"1949ddbfd147afd4d964a9f00b24eb291e0e7c38\" />",
 "  <project name=\"parse\" path=\"godeps/src/github.com/tdewolff/parse\" remote=\"couchbasedeps\" revision=\"0334a869253aca4b3a10c56c3f3139b394aec3a9\" />",
 "  <project name=\"participle\" path=\"godeps/src/github.com/alecthomas/participle\" remote=\"couchbasedeps\" revision=\"bf8340a459bd383e5eb7d44a9a1b3af23b6cf8cd\" />",
 "  <project name=\"pflag\" path=\"godeps/src/github.com/spf13/pflag\" remote=\"couchbasedeps\" revision=\"a232f6d9f87afaaa08bafaff5da685f974b83313\" />",
 "  <project groups=\"kv\" name=\"phosphor\" revision=\"53ca1eeae7bd3deea5b7bf48b3d4188b47e530d1\" upstream=\"master\" />",
 "  <project name=\"pierrec-lz4\" path=\"godeps/src/github.com/pierrec/lz4\" remote=\"couchbasedeps\" revision=\"ed8d4cc3b461464e69798080a0092bd028910298\" />",
 "  <project name=\"pierrec-xxHash\" path=\"godeps/src/github.com/pierrec/xxHash\" remote=\"couchbasedeps\" revision=\"a0006b13c722f7f12368c00a3d3c2ae8a999a0c6\" />",
 "  <project groups=\"notdefault,enterprise\" name=\"plasma\" path=\"goproj/src/github.com/couchbase/plasma\" remote=\"couchbase-priv\" revision=\"4aa86645ce4b4673de08f6829b446b9c00cd3f3d\" upstream=\"mad-hatter\" />",
 "  <project groups=\"kv\" name=\"platform\" revision=\"bec44f963f3c4d73d3735380a8107b7292558749\" upstream=\"mad-hatter\" />",
 "  <project groups=\"kv\" name=\"product-texts\" revision=\"7a3aa547b3f5eb3ea28d279a08384609cd2cea7c\" upstream=\"master\" />",
 "  <project name=\"protobuf\" path=\"godeps/src/github.com/golang/protobuf\" remote=\"couchbasedeps\" revision=\"ddf22928ea3c56eb4292a0adbbf5001b1e8e7d0d\" />",
 "  <project name=\"query\" path=\"goproj/src/github.com/couchbase/query\" revision=\"a1708edce7216cdc4f21b4d4dd0eb4001d38e3c0\" upstream=\"mad-hatter\" />",
 "  <project groups=\"notdefault,enterprise\" name=\"query-ee\" path=\"goproj/src/github.com/couchbase/query-ee\" remote=\"couchbase-priv\" revision=\"3ef4ab89910a53b6acfaba4cc7d96091ab33a346\" upstream=\"mad-hatter\" />",
 "  <project name=\"query-ui\" revision=\"d736c5b2b97eeea0bf8170a40cfa7533e168388e\" upstream=\"master\" />",
 "  <project name=\"retriever\" path=\"godeps/src/github.com/couchbase/retriever\" revision=\"e3419088e4d3b4fe3aad3b364fdbe9a154f85f17\" upstream=\"master\" />",
 "  <project name=\"roaring\" path=\"godeps/src/github.com/RoaringBitmap/roaring\" remote=\"couchbasedeps\" revision=\"d0ce1763c3526f65703c395da50da7a7fb2138d5\" />",
 "  <project name=\"segment\" path=\"godeps/src/github.com/blevesearch/segment\" remote=\"blevesearch\" revision=\"762005e7a34fd909a84586299f1dd457371d36ee\" />",
 "  <project groups=\"kv\" name=\"sigar\" revision=\"c33791d6d5de19d6c5575aa33f8e5dba848414d8\" upstream=\"master\" />",
 "  <project name=\"snowballstem\" path=\"godeps/src/github.com/blevesearch/snowballstem\" remote=\"blevesearch\" revision=\"26b06a2c243d4f8ca5db3486f94409dd5b2a7467\" />",
 "  <project groups=\"kv\" name=\"spdlog\" path=\"third_party/spdlog\" remote=\"couchbasedeps\" revision=\"20967a170429d0d37e09a485bc3cf5b153554924\" upstream=\"v1.1.0-couchbase\" />",
 "  <project name=\"strconv\" path=\"godeps/src/github.com/tdewolff/strconv\" remote=\"couchbasedeps\" revision=\"9b189f5be77f33c46776f24dbddb2a7ab32af214\" />",
 "  <project groups=\"kv\" name=\"subjson\" revision=\"ae63ab4b653870e400855f8563da40dda49f0eb3\" upstream=\"master\" />",
 "  <project groups=\"backup\" name=\"sys\" path=\"godeps/src/golang.org/x/sys\" remote=\"couchbasedeps\" revision=\"7fbe1cd0fcc20051e1fcb87fbabec4a1bacaaeba\" />",
 "  <project name=\"testrunner\" revision=\"ee64d41320d14fabe814a241a5cf4f6a6f6e827a\" upstream=\"mad-hatter\" />",
 "  <project groups=\"backup\" name=\"text\" path=\"godeps/src/golang.org/x/text\" remote=\"couchbasedeps\" revision=\"88f656faf3f37f690df1a32515b479415e1a6769\" />",
 "  <project groups=\"kv\" name=\"tlm\" revision=\"7279de40e2a171aeed67b2566bd499d7157df965\">",
 "    <copyfile dest=\"GNUmakefile\" src=\"GNUmakefile\" />",
 "    <copyfile dest=\"Makefile\" src=\"Makefile\" />",
 "    <copyfile dest=\"CMakeLists.txt\" src=\"CMakeLists.txt\" />",
 "    <copyfile dest=\".clang-format\" src=\"dot-clang-format\" />",
 "    <copyfile dest=\"third_party/CMakeLists.txt\" src=\"third-party-CMakeLists.txt\" />",
 "  </project>",
 "  <project groups=\"backup\" name=\"ts\" path=\"godeps/src/github.com/olekukonko/ts\" remote=\"couchbasedeps\" revision=\"ecf753e7c962639ab5a1fb46f7da627d4c0a04b8\" />",
 "  <project groups=\"backup\" name=\"uuid\" path=\"godeps/src/github.com/google/uuid\" remote=\"couchbasedeps\" revision=\"dec09d789f3dba190787f8b4454c7d3c936fed9e\" />",
 "  <project name=\"vellum\" path=\"godeps/src/github.com/couchbase/vellum\" revision=\"ef2e028c01fdb60c46da4067d2e83745b8d54120\" upstream=\"master\" />",
 "  <project groups=\"notdefault,packaging\" name=\"voltron\" remote=\"couchbase-priv\" revision=\"45188488712448a326c8efad0d8c7b00e8afbefe\" upstream=\"master\" />",
 "  <project name=\"zstd\" path=\"godeps/src/github.com/DataDog/zstd\" remote=\"couchbasedeps\" revision=\"aebefd9fcb99f22cd691ef778a12ed68f0e6a1ab\" />",
 "</manifest>"]

[error_logger:info,2020-03-27T20:03:33.096Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.189.0>},
                       {id,timeout_diag_logger},
                       {mfargs,{timeout_diag_logger,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:03:33.098Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.190.0>},
                       {id,ns_cookie_manager},
                       {mfargs,{ns_cookie_manager,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:03:33.101Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.191.0>},
                       {id,ns_cluster},
                       {mfargs,{ns_cluster,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[ns_server:info,2020-03-27T20:03:33.103Z,ns_1@127.0.0.1:ns_config_sup<0.192.0>:ns_config_sup:init:32]loading static ns_config from "/opt/couchbase/etc/couchbase/config"
[error_logger:info,2020-03-27T20:03:33.103Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_config_sup}
             started: [{pid,<0.193.0>},
                       {id,ns_config_events},
                       {mfargs,
                           {gen_event,start_link,[{local,ns_config_events}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:03:33.103Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_config_sup}
             started: [{pid,<0.194.0>},
                       {id,ns_config_events_local},
                       {mfargs,
                           {gen_event,start_link,
                               [{local,ns_config_events_local}]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[ns_server:info,2020-03-27T20:03:33.159Z,ns_1@127.0.0.1:ns_config<0.195.0>:ns_config:load_config:1106]Loading static config from "/opt/couchbase/etc/couchbase/config"
[ns_server:info,2020-03-27T20:03:33.161Z,ns_1@127.0.0.1:ns_config<0.195.0>:ns_config:load_config:1120]Loading dynamic config from "/opt/couchbase/var/lib/couchbase/config/config.dat"
[ns_server:debug,2020-03-27T20:03:33.188Z,ns_1@127.0.0.1:ns_config<0.195.0>:ns_config:load_config:1128]Here's full dynamic config we loaded:
[[{cluster_name,
   [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557980}}]},
    66,97,115,101]},
  {{metakv,<<"/eventing/settings/config">>},
   [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557537}}]}|
    <<"{\"ram_quota\":256}">>]},
  {cbas_memory_quota,1024},
  {fts_memory_quota,256},
  {{metakv,<<"/indexing/settings/config">>},
   [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{4,63752557980}}]}|
    <<"{\"indexer.settings.compaction.days_of_week\":\"Sunday,Monday,Tuesday,Wednesday,Thursday,Friday,Saturday\",\"indexer.settings.compaction.interval\":\"00:00,00:00\",\"indexer.settings.compaction.compaction_mode\":\"circular\",\"indexer.settings.log_level\":\"info\",\"indexer.settings.persisted_snapshot.interval\":5000,\"indexer.settings.compaction.min_frag\":30,\"indexer.settings.inmemory_snapshot.interval\":200,\"indexer.settings.max_cpu_percent\":0,\"indexer.settings.storage_mode\":\"plasma\",\"indexer.settings.recovery.max_rollbacks\":2,\"indexer.settings.memory_quota\":536870912,\"indexer.settings.compaction.abort_exceed_interval\":false}">>]},
  {memory_quota,292},
  {{node,'ns_1@127.0.0.1',erl_external_listeners},
   [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]},
    {inet,false},
    {inet6,false}]},
  {{node,'ns_1@127.0.0.1',node_encryption},
   [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
    false]},
  {{node,'ns_1@127.0.0.1',address_family},
   [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
    inet]},
  {auto_failover_cfg,
   [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557537}}]},
    {enabled,true},
    {timeout,120},
    {count,0},
    {failover_on_data_disk_issues,[{enabled,false},{timePeriod,120}]},
    {failover_server_group,false},
    {max_count,1},
    {failed_over_server_groups,[]},
    {can_abort_rebalance,true}]},
  {retry_rebalance,
   [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557537}}]},
    {enabled,false},
    {after_time_period,300},
    {max_attempts,1}]},
  {audit_decriptors,
   [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557537}}]},
    {8243,
     [{name,<<"mutate document">>},
      {description,<<"Document was mutated via the REST API">>},
      {enabled,true},
      {module,ns_server}]},
    {8255,
     [{name,<<"read document">>},
      {description,<<"Document was read via the REST API">>},
      {enabled,false},
      {module,ns_server}]},
    {8257,
     [{name,<<"alert email sent">>},
      {description,<<"An alert email was successfully sent">>},
      {enabled,true},
      {module,ns_server}]},
    {20480,
     [{name,<<"opened DCP connection">>},
      {description,<<"opened DCP connection">>},
      {enabled,true},
      {module,memcached}]},
    {20482,
     [{name,<<"external memcached bucket flush">>},
      {description,
       <<"External user flushed the content of a memcached bucket">>},
      {enabled,true},
      {module,memcached}]},
    {20483,
     [{name,<<"invalid packet">>},
      {description,<<"Rejected an invalid packet">>},
      {enabled,true},
      {module,memcached}]},
    {20485,
     [{name,<<"authentication succeeded">>},
      {description,<<"Authentication to the cluster succeeded">>},
      {enabled,false},
      {module,memcached}]},
    {20488,
     [{name,<<"document read">>},
      {description,<<"Document was read">>},
      {enabled,false},
      {module,memcached}]},
    {20489,
     [{name,<<"document locked">>},
      {description,<<"Document was locked">>},
      {enabled,false},
      {module,memcached}]},
    {20490,
     [{name,<<"document modify">>},
      {description,<<"Document was modified">>},
      {enabled,false},
      {module,memcached}]},
    {20491,
     [{name,<<"document delete">>},
      {description,<<"Document was deleted">>},
      {enabled,false},
      {module,memcached}]},
    {20492,
     [{name,<<"select bucket">>},
      {description,<<"The specified bucket was selected">>},
      {enabled,true},
      {module,memcached}]},
    {28672,
     [{name,<<"SELECT statement">>},
      {description,<<"A N1QL SELECT statement was executed">>},
      {enabled,false},
      {module,n1ql}]},
    {28673,
     [{name,<<"EXPLAIN statement">>},
      {description,<<"A N1QL EXPLAIN statement was executed">>},
      {enabled,false},
      {module,n1ql}]},
    {28674,
     [{name,<<"PREPARE statement">>},
      {description,<<"A N1QL PREPARE statement was executed">>},
      {enabled,false},
      {module,n1ql}]},
    {28675,
     [{name,<<"INFER statement">>},
      {description,<<"A N1QL INFER statement was executed">>},
      {enabled,false},
      {module,n1ql}]},
    {28676,
     [{name,<<"INSERT statement">>},
      {description,<<"A N1QL INSERT statement was executed">>},
      {enabled,false},
      {module,n1ql}]},
    {28677,
     [{name,<<"UPSERT statement">>},
      {description,<<"A N1QL UPSERT statement was executed">>},
      {enabled,false},
      {module,n1ql}]},
    {28678,
     [{name,<<"DELETE statement">>},
      {description,<<"A N1QL DELETE statement was executed">>},
      {enabled,false},
      {module,n1ql}]},
    {28679,
     [{name,<<"UPDATE statement">>},
      {description,<<"A N1QL UPDATE statement was executed">>},
      {enabled,false},
      {module,n1ql}]},
    {28680,
     [{name,<<"MERGE statement">>},
      {description,<<"A N1QL MERGE statement was executed">>},
      {enabled,false},
      {module,n1ql}]},
    {28681,
     [{name,<<"CREATE INDEX statement">>},
      {description,<<"A N1QL CREATE INDEX statement was executed">>},
      {enabled,false},
      {module,n1ql}]},
    {28682,
     [{name,<<"DROP INDEX statement">>},
      {description,<<"A N1QL DROP INDEX statement was executed">>},
      {enabled,false},
      {module,n1ql}]},
    {28683,
     [{name,<<"ALTER INDEX statement">>},
      {description,<<"A N1QL ALTER INDEX statement was executed">>},
      {enabled,false},
      {module,n1ql}]},
    {28684,
     [{name,<<"BUILD INDEX statement">>},
      {description,<<"A N1QL BUILD INDEX statement was executed">>},
      {enabled,false},
      {module,n1ql}]},
    {28685,
     [{name,<<"GRANT ROLE statement">>},
      {description,<<"A N1QL GRANT ROLE statement was executed">>},
      {enabled,false},
      {module,n1ql}]},
    {28686,
     [{name,<<"REVOKE ROLE statement">>},
      {description,<<"A N1QL REVOKE ROLE statement was executed">>},
      {enabled,false},
      {module,n1ql}]},
    {28687,
     [{name,<<"UNRECOGNIZED statement">>},
      {description,
       <<"An unrecognized statement was received by the N1QL query engine">>},
      {enabled,false},
      {module,n1ql}]},
    {28688,
     [{name,<<"CREATE PRIMARY INDEX statement">>},
      {description,<<"A N1QL CREATE PRIMARY INDEX statement was executed">>},
      {enabled,false},
      {module,n1ql}]},
    {28689,
     [{name,<<"/admin/stats API request">>},
      {description,<<"An HTTP request was made to the API at /admin/stats.">>},
      {enabled,false},
      {module,n1ql}]},
    {28690,
     [{name,<<"/admin/vitals API request">>},
      {description,
       <<"An HTTP request was made to the API at /admin/vitals.">>},
      {enabled,false},
      {module,n1ql}]},
    {28691,
     [{name,<<"/admin/prepareds API request">>},
      {description,
       <<"An HTTP request was made to the API at /admin/prepareds.">>},
      {enabled,false},
      {module,n1ql}]},
    {28692,
     [{name,<<"/admin/active_requests API request">>},
      {description,
       <<"An HTTP request was made to the API at /admin/active_requests.">>},
      {enabled,false},
      {module,n1ql}]},
    {28693,
     [{name,<<"/admin/indexes/prepareds API request">>},
      {description,
       <<"An HTTP request was made to the API at /admin/indexes/prepareds.">>},
      {enabled,false},
      {module,n1ql}]},
    {28694,
     [{name,<<"/admin/indexes/active_requests API request">>},
      {description,
       <<"An HTTP request was made to the API at /admin/indexes/active_requests.">>},
      {enabled,false},
      {module,n1ql}]},
    {28695,
     [{name,<<"/admin/indexes/completed_requests API request">>},
      {description,
       <<"An HTTP request was made to the API at /admin/indexes/completed_requests.">>},
      {enabled,false},
      {module,n1ql}]},
    {28697,
     [{name,<<"/admin/ping API request">>},
      {description,<<"An HTTP request was made to the API at /admin/ping.">>},
      {enabled,false},
      {module,n1ql}]},
    {28698,
     [{name,<<"/admin/config API request">>},
      {description,
       <<"An HTTP request was made to the API at /admin/config.">>},
      {enabled,false},
      {module,n1ql}]},
    {28699,
     [{name,<<"/admin/ssl_cert API request">>},
      {description,
       <<"An HTTP request was made to the API at /admin/ssl_cert.">>},
      {enabled,false},
      {module,n1ql}]},
    {28700,
     [{name,<<"/admin/settings API request">>},
      {description,
       <<"An HTTP request was made to the API at /admin/settings.">>},
      {enabled,false},
      {module,n1ql}]},
    {28701,
     [{name,<<"/admin/clusters API request">>},
      {description,
       <<"An HTTP request was made to the API at /admin/clusters.">>},
      {enabled,false},
      {module,n1ql}]},
    {28702,
     [{name,<<"/admin/completed_requests API request">>},
      {description,
       <<"An HTTP request was made to the API at /admin/completed_requests.">>},
      {enabled,false},
      {module,n1ql}]},
    {28704,
     [{name,<<"/admin/functions API request">>},
      {description,
       <<"An HTTP request was made to the API at /admin/functions.">>},
      {enabled,false},
      {module,n1ql}]},
    {28705,
     [{name,<<"/admin/indexes/functions API request">>},
      {description,
       <<"An HTTP request was made to the API at /admin/indexes/functions.">>},
      {enabled,false},
      {module,n1ql}]},
    {32768,
     [{name,<<"Create Function">>},
      {description,<<"Eventing function definition was created or updated">>},
      {enabled,true},
      {module,eventing}]},
    {32769,
     [{name,<<"Delete Function">>},
      {description,<<"Eventing function definition was deleted">>},
      {enabled,true},
      {module,eventing}]},
    {32770,
     [{name,<<"Fetch Functions">>},
      {description,<<"Eventing function definition was read">>},
      {enabled,false},
      {module,eventing}]},
    {32771,
     [{name,<<"List Deployed">>},
      {description,<<"Eventing deployed functions list was read">>},
      {enabled,false},
      {module,eventing}]},
    {32772,
     [{name,<<"Fetch Drafts">>},
      {description,<<"Eventing function draft definitions were read">>},
      {enabled,false},
      {module,eventing}]},
    {32773,
     [{name,<<"Delete Drafts">>},
      {description,<<"Eventing function draft definitions were deleted">>},
      {enabled,true},
      {module,eventing}]},
    {32774,
     [{name,<<"Save Draft">>},
      {description,<<"Save a draft definition to the store">>},
      {enabled,true},
      {module,eventing}]},
    {32775,
     [{name,<<"Start Debug">>},
      {description,<<"Start eventing function debugger">>},
      {enabled,true},
      {module,eventing}]},
    {32776,
     [{name,<<"Stop Debug">>},
      {description,<<"Stop eventing function debugger">>},
      {enabled,true},
      {module,eventing}]},
    {32777,
     [{name,<<"Start Tracing">>},
      {description,<<"Start tracing eventing function execution">>},
      {enabled,true},
      {module,eventing}]},
    {32778,
     [{name,<<"Stop Tracing">>},
      {description,<<"Stop tracing eventing function execution">>},
      {enabled,true},
      {module,eventing}]},
    {32779,
     [{name,<<"Set Settings">>},
      {description,<<"Save settings for a given app">>},
      {enabled,true},
      {module,eventing}]},
    {32780,
     [{name,<<"Fetch Config">>},
      {description,<<"Get config for eventing">>},
      {enabled,false},
      {module,eventing}]},
    {32781,
     [{name,<<"Save Config">>},
      {description,<<"Save config for eventing">>},
      {enabled,true},
      {module,eventing}]},
    {32782,
     [{name,<<"Cleanup Eventing">>},
      {description,<<"Clears up app definitions and settings from metakv">>},
      {enabled,true},
      {module,eventing}]},
    {32783,
     [{name,<<"Get Settings">>},
      {description,<<"Get settings for a given app">>},
      {enabled,false},
      {module,eventing}]},
    {32784,
     [{name,<<"Import Functions">>},
      {description,<<"Import a list of functions">>},
      {enabled,false},
      {module,eventing}]},
    {32785,
     [{name,<<"Export Functions">>},
      {description,<<"Export the list of functions">>},
      {enabled,false},
      {module,eventing}]},
    {32786,
     [{name,<<"List Running">>},
      {description,<<"Eventing running function list was read">>},
      {enabled,false},
      {module,eventing}]},
    {36865,
     [{name,<<"Service configuration change">>},
      {description,<<"A successful service configuration change was made.">>},
      {enabled,true},
      {module,analytics}]},
    {36866,
     [{name,<<"Node configuration change">>},
      {description,<<"A successful node configuration change was made.">>},
      {enabled,true},
      {module,analytics}]},
    {40960,
     [{name,<<"Create Design Doc">>},
      {description,<<"Design Doc is Created">>},
      {enabled,true},
      {module,view_engine}]},
    {40961,
     [{name,<<"Delete Design Doc">>},
      {description,<<"Design Doc is Deleted">>},
      {enabled,true},
      {module,view_engine}]},
    {40962,
     [{name,<<"Query DDoc Meta Data">>},
      {description,<<"Design Doc Meta Data Query Request">>},
      {enabled,true},
      {module,view_engine}]},
    {40963,
     [{name,<<"View Query">>},
      {description,<<"View Query Request">>},
      {enabled,false},
      {module,view_engine}]},
    {40964,
     [{name,<<"Update Design Doc">>},
      {description,<<"Design Doc is Updated">>},
      {enabled,true},
      {module,view_engine}]}]},
  {scramsha_fallback_salt,
   [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557537}}]}|
    <<176,211,90,3,15,137,19,29,193,47,34,182>>]},
  {{metakv,<<"/query/settings/config">>},
   [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557537}}]}|
    <<"{\"timeout\":0,\"n1ql-feat-ctrl\":12,\"max-parallelism\":1,\"query.settings.curl_whitelist\":{\"all_access\":false,\"allowed_urls\":[],\"disallowed_urls\":[]},\"query.settings.tmp_space_dir\":\"/opt/couchbase/var/lib/couchbase/tmp\",\"completed-limit\":4000,\"prepared-limit\":16384,\"pipeline-batch\":16,\"pipeline-cap\":512,\"scan-cap\":512,\"loglevel\":\"info\",\"completed-threshold\":1000,\"query.settings.tmp_space_size\":5120}">>]},
  {client_cert_auth,
   [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557537}}]},
    {state,"disable"},
    {prefixes,[]}]},
  {cluster_compat_version,
   [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{5,63752557537}}]},
    6,5]},
  {otp,
   [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557536}}]},
    {cookie,{sanitized,<<"C4PeZIe8LLStUy+d1HIN638gh0wTZW33h1wdSpFkvzc=">>}}]},
  {{node,'ns_1@127.0.0.1',eventing_dir},
   [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]},
    47,111,112,116,47,99,111,117,99,104,98,97,115,101,47,118,97,114,47,108,
    105,98,47,99,111,117,99,104,98,97,115,101,47,100,97,116,97]},
  {{node,'ns_1@127.0.0.1',cbas_dirs},
   [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]},
    "/opt/couchbase/var/lib/couchbase/data"]},
  {cert_and_pkey,
   [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557532}}]}|
    {<<"-----BEGIN CERTIFICATE-----\nMIIDAjCCAeqgAwIBAgIIFgBA0G6s0lwwDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciAzNDhlZDI3MDAeFw0xMzAxMDEwMDAwMDBaFw00\nOTEyMzEyMzU5NTlaMCQxIjAgBgNVBAMTGUNvdWNoYmFzZSBTZXJ2ZXIgMzQ4ZWQy\nNzAwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQDh0efJgvfpArvc3iI0\ncHa/zZhS9WoaoNHksFooYqy3AYiZR+8S3IgPS0nfcZGrIWJUPgokUqBqh6u59abc\nvCuyMwLNkkuLOkT6wzUztrAqoCoOEl4avKFmI0JsiWA/qKDU/kapi0rG9MzOwmWz\nhLUBFUx1SGUYpWzUOP0v5riPMhR5SvfMR/fiYfm/ruKWF5VHtNFW2EgfysURPlxa\ndArURVkUW6MzTV6cDXMXR1P83qZVhKvWp65OPD9O8XPHRrqvv17QBditavjLOEgY\n2xdWQ4kS3avQASnvVP2e5iRO3iF4WmDRJAXmWqMTAiOlUICrBloL7aIjA8lebsm1\nDTGlAgMBAAGjODA2MA4GA1UdDwEB/wQEAwICpDATBgNVHSUEDDAKBggrBgEFBQcD\nATAPBgNVHRMBAf8EBTADAQH/MA0GCSqGSIb3DQEBCwUAA4IBAQAGUUF8Ryt6D6fP\nEPsAnydivOMd12RrwrjQMeoxnRYQcuKuDZaskALu5ggo3Ro3OIR+6ZkYuSlsG+/Y\nUng/DPuFi78HQfWDXCLJ5sQ1PpyedT7cn8Blq4OPkZcrp+Naoa1NmBt8QzGWQ2ah\n3Sum/CJq0EzfSebYoRqEiYGST45Fs8HCL4HP+wTttPc2myjYJE87brwbmdAt0/Wd\nQBPz4YAtfqxhpEE3+uNa1DuZidvDVKttkUBv/W14I8eZ6Hiqs6dusHQK9U8GQp6U\nib99QaSZXrTiU+bx7CdDKAxbLE82QUj5S+vezY5wN+AvfSrrvA6AhizkGaMZWvaq\nSqN1PyyX\n-----END CERTIFICATE-----\n">>,
     <<"*****">>}]},
  {alert_limits,
   [{max_overhead_perc,50},{max_disk_used,90},{max_indexer_ram,75}]},
  {audit,
   [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557537}}]},
    {enabled,[]},
    {disabled_users,[]},
    {auditd_enabled,false},
    {rotate_interval,86400},
    {rotate_size,20971520},
    {disabled,[]},
    {sync,[]},
    {log_path,"/opt/couchbase/var/lib/couchbase/logs"}]},
  {auto_reprovision_cfg,[{enabled,true},{max_nodes,1},{count,0}]},
  {autocompaction,
   [{database_fragmentation_threshold,{30,undefined}},
    {view_fragmentation_threshold,{30,undefined}}]},
  {buckets,
   [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557537}}]},
    {configs,[]}]},
  {drop_request_memory_threshold_mib,undefined},
  {email_alerts,
   [{recipients,["root@localhost"]},
    {sender,"couchbase@localhost"},
    {enabled,false},
    {email_server,
     [{user,[]},{pass,"*****"},{host,"localhost"},{port,25},{encrypt,false}]},
    {alerts,
     [auto_failover_node,auto_failover_maximum_reached,
      auto_failover_other_nodes_down,auto_failover_cluster_too_small,
      auto_failover_disabled,ip,disk,overhead,ep_oom_errors,
      ep_item_commit_failed,audit_dropped_events,indexer_ram_max_usage,
      ep_clock_cas_drift_threshold_exceeded,communication_issue]}]},
  {index_aware_rebalance_disabled,false},
  {log_redaction_default_cfg,[{redact_level,none}]},
  {max_bucket_count,
   [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557537}}]}|
    30]},
  {memcached,[]},
  {nodes_wanted,
   [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557625}}]},
    'ns_1@127.0.0.1']},
  {password_policy,[{min_length,6},{must_present,[]}]},
  {quorum_nodes,
   [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]},
    'ns_1@127.0.0.1']},
  {remote_clusters,[]},
  {replication,[{enabled,true}]},
  {rest,[{port,8091}]},
  {rest_creds,null},
  {secure_headers,[]},
  {server_groups,
   [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557625}}]},
    [{uuid,<<"0">>},{name,<<"Group 1">>},{nodes,['ns_1@127.0.0.1']}]]},
  {set_view_update_daemon,
   [{update_interval,5000},
    {update_min_changes,5000},
    {replica_update_min_changes,5000}]},
  {{couchdb,max_parallel_indexers},4},
  {{couchdb,max_parallel_replica_indexers},2},
  {{request_limit,capi},undefined},
  {{request_limit,rest},undefined},
  {{node,'ns_1@127.0.0.1',audit},
   [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}]},
  {{node,'ns_1@127.0.0.1',capi_port},
   [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
    8092]},
  {{node,'ns_1@127.0.0.1',cbas_admin_port},
   [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
    9110]},
  {{node,'ns_1@127.0.0.1',cbas_cc_client_port},
   [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
    9113]},
  {{node,'ns_1@127.0.0.1',cbas_cc_cluster_port},
   [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
    9112]},
  {{node,'ns_1@127.0.0.1',cbas_cc_http_port},
   [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
    9111]},
  {{node,'ns_1@127.0.0.1',cbas_cluster_port},
   [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
    9115]},
  {{node,'ns_1@127.0.0.1',cbas_console_port},
   [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
    9114]},
  {{node,'ns_1@127.0.0.1',cbas_data_port},
   [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
    9116]},
  {{node,'ns_1@127.0.0.1',cbas_debug_port},
   [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
    -1]},
  {{node,'ns_1@127.0.0.1',cbas_http_port},
   [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
    8095]},
  {{node,'ns_1@127.0.0.1',cbas_messaging_port},
   [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
    9118]},
  {{node,'ns_1@127.0.0.1',cbas_metadata_callback_port},
   [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
    9119]},
  {{node,'ns_1@127.0.0.1',cbas_metadata_port},
   [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
    9121]},
  {{node,'ns_1@127.0.0.1',cbas_parent_port},
   [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
    9122]},
  {{node,'ns_1@127.0.0.1',cbas_replication_port},
   [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
    9120]},
  {{node,'ns_1@127.0.0.1',cbas_result_port},
   [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
    9117]},
  {{node,'ns_1@127.0.0.1',cbas_ssl_port},
   [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
    18095]},
  {{node,'ns_1@127.0.0.1',compaction_daemon},
   [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]},
    {check_interval,30},
    {min_db_file_size,131072},
    {min_view_file_size,20971520}]},
  {{node,'ns_1@127.0.0.1',config_version},
   [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
    {6,5}]},
  {{node,'ns_1@127.0.0.1',eventing_debug_port},
   [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
    9140]},
  {{node,'ns_1@127.0.0.1',eventing_http_port},
   [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
    8096]},
  {{node,'ns_1@127.0.0.1',eventing_https_port},
   [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
    18096]},
  {{node,'ns_1@127.0.0.1',fts_grpc_port},
   [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
    9130]},
  {{node,'ns_1@127.0.0.1',fts_grpc_ssl_port},
   [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
    19130]},
  {{node,'ns_1@127.0.0.1',fts_http_port},
   [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
    8094]},
  {{node,'ns_1@127.0.0.1',fts_ssl_port},
   [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
    18094]},
  {{node,'ns_1@127.0.0.1',indexer_admin_port},
   [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
    9100]},
  {{node,'ns_1@127.0.0.1',indexer_http_port},
   [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
    9102]},
  {{node,'ns_1@127.0.0.1',indexer_https_port},
   [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
    19102]},
  {{node,'ns_1@127.0.0.1',indexer_scan_port},
   [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
    9101]},
  {{node,'ns_1@127.0.0.1',indexer_stcatchup_port},
   [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
    9104]},
  {{node,'ns_1@127.0.0.1',indexer_stinit_port},
   [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
    9103]},
  {{node,'ns_1@127.0.0.1',indexer_stmaint_port},
   [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
    9105]},
  {{node,'ns_1@127.0.0.1',is_enterprise},
   [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
    true]},
  {{node,'ns_1@127.0.0.1',isasl},
   [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]},
    {path,"/opt/couchbase/var/lib/couchbase/isasl.pw"}]},
  {{node,'ns_1@127.0.0.1',membership},
   [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
    active]},
  {{node,'ns_1@127.0.0.1',memcached},
   [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]},
    {port,11210},
    {dedicated_port,11209},
    {dedicated_ssl_port,11206},
    {ssl_port,11207},
    {admin_user,"@ns_server"},
    {other_users,
     ["@cbq-engine","@projector","@goxdcr","@index","@fts","@eventing",
      "@cbas"]},
    {admin_pass,"*****"},
    {engines,
     [{membase,
       [{engine,"/opt/couchbase/lib/memcached/ep.so"},
        {static_config_string,"failpartialwarmup=false"}]},
      {memcached,
       [{engine,"/opt/couchbase/lib/memcached/default_engine.so"},
        {static_config_string,"vb0=true"}]}]},
    {config_path,"/opt/couchbase/var/lib/couchbase/config/memcached.json"},
    {audit_file,"/opt/couchbase/var/lib/couchbase/config/audit.json"},
    {rbac_file,"/opt/couchbase/var/lib/couchbase/config/memcached.rbac"},
    {log_path,"/opt/couchbase/var/lib/couchbase/logs"},
    {log_prefix,"memcached.log"},
    {log_generations,20},
    {log_cyclesize,10485760},
    {log_sleeptime,19},
    {log_rotation_period,39003}]},
  {{node,'ns_1@127.0.0.1',memcached_config},
   [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
    {[{interfaces,
       {memcached_config_mgr,omit_missing_mcd_ports,
        [{[{host,<<"*">>},
           {port,port},
           {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
           {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]},
         {[{host,<<"*">>},
           {port,dedicated_port},
           {system,true},
           {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
           {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]},
         {[{host,<<"*">>},
           {port,ssl_port},
           {ssl,
            {[{key,
               <<"/opt/couchbase/var/lib/couchbase/config/memcached-key.pem">>},
              {cert,
               <<"/opt/couchbase/var/lib/couchbase/config/memcached-cert.pem">>}]}},
           {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
           {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]},
         {[{host,<<"*">>},
           {port,dedicated_ssl_port},
           {system,true},
           {ssl,
            {[{key,
               <<"/opt/couchbase/var/lib/couchbase/config/memcached-key.pem">>},
              {cert,
               <<"/opt/couchbase/var/lib/couchbase/config/memcached-cert.pem">>}]}},
           {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
           {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]}]}},
      {ssl_cipher_list,{memcached_config_mgr,get_ssl_cipher_list,[]}},
      {ssl_cipher_order,{memcached_config_mgr,get_ssl_cipher_order,[]}},
      {client_cert_auth,{memcached_config_mgr,client_cert_auth,[]}},
      {ssl_minimum_protocol,{memcached_config_mgr,ssl_minimum_protocol,[]}},
      {connection_idle_time,connection_idle_time},
      {privilege_debug,privilege_debug},
      {breakpad,
       {[{enabled,breakpad_enabled},
         {minidump_dir,{memcached_config_mgr,get_minidump_dir,[]}}]}},
      {opentracing,
       {[{enabled,opentracing_enabled},
         {module,{"~s",[opentracing_module]}},
         {config,{"~s",[opentracing_config]}}]}},
      {admin,{"~s",[admin_user]}},
      {verbosity,verbosity},
      {audit_file,{"~s",[audit_file]}},
      {rbac_file,{"~s",[rbac_file]}},
      {dedupe_nmvb_maps,dedupe_nmvb_maps},
      {tracing_enabled,tracing_enabled},
      {datatype_snappy,{memcached_config_mgr,is_snappy_enabled,[]}},
      {xattr_enabled,true},
      {scramsha_fallback_salt,{memcached_config_mgr,get_fallback_salt,[]}},
      {collections_enabled,{memcached_config_mgr,collections_enabled,[]}},
      {max_connections,max_connections},
      {system_connections,system_connections},
      {num_reader_threads,num_reader_threads},
      {num_writer_threads,num_writer_threads},
      {logger,
       {[{filename,{"~s/~s",[log_path,log_prefix]}},
         {cyclesize,log_cyclesize},
         {sleeptime,log_sleeptime}]}},
      {external_auth_service,
       {memcached_config_mgr,get_external_auth_service,[]}},
      {active_external_users_push_interval,
       {memcached_config_mgr,get_external_users_push_interval,[]}}]}]},
  {{node,'ns_1@127.0.0.1',memcached_dedicated_ssl_port},
   [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
    11206]},
  {{node,'ns_1@127.0.0.1',memcached_defaults},
   [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]},
    {max_connections,65000},
    {system_connections,5000},
    {connection_idle_time,0},
    {verbosity,0},
    {privilege_debug,false},
    {opentracing_enabled,false},
    {opentracing_module,[]},
    {opentracing_config,[]},
    {breakpad_enabled,true},
    {breakpad_minidump_dir_path,"/opt/couchbase/var/lib/couchbase/crash"},
    {dedupe_nmvb_maps,false},
    {tracing_enabled,true},
    {datatype_snappy,true},
    {num_reader_threads,<<"default">>},
    {num_writer_threads,<<"default">>}]},
  {{node,'ns_1@127.0.0.1',moxi},
   [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]},
    {port,0}]},
  {{node,'ns_1@127.0.0.1',ns_log},
   [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]},
    {filename,"/opt/couchbase/var/lib/couchbase/ns_log"}]},
  {{node,'ns_1@127.0.0.1',port_servers},
   [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}]},
  {{node,'ns_1@127.0.0.1',projector_port},
   [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
    9999]},
  {{node,'ns_1@127.0.0.1',projector_ssl_port},
   [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
    9999]},
  {{node,'ns_1@127.0.0.1',query_port},
   [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
    8093]},
  {{node,'ns_1@127.0.0.1',rest},
   [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]},
    {port,8091},
    {port_meta,global}]},
  {{node,'ns_1@127.0.0.1',saslauthd_enabled},
   [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
    true]},
  {{node,'ns_1@127.0.0.1',ssl_capi_port},
   [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
    18092]},
  {{node,'ns_1@127.0.0.1',ssl_query_port},
   [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
    18093]},
  {{node,'ns_1@127.0.0.1',ssl_rest_port},
   [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
    18091]},
  {{node,'ns_1@127.0.0.1',uuid},
   [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
    <<"60a6bc3db77e6c7b91c556140dcfec71">>]},
  {{node,'ns_1@127.0.0.1',xdcr_rest_port},
   [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
    9998]},
  {{node,'ns_1@127.0.0.1',{project_intact,is_vulnerable}},
   [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
    false]},
  {{local_changes_count,<<"60a6bc3db77e6c7b91c556140dcfec71">>},
   [{'_vclock',
     [{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{21,63752557980}}]}]}]]
[ns_server:info,2020-03-27T20:03:33.197Z,ns_1@127.0.0.1:ns_config<0.195.0>:ns_config:load_config:1149]Here's full dynamic config we loaded + static & default config:
[{{local_changes_count,<<"60a6bc3db77e6c7b91c556140dcfec71">>},
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{21,63752557980}}]}]},
 {{node,'ns_1@127.0.0.1',{project_intact,is_vulnerable}},
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
   false]},
 {{node,'ns_1@127.0.0.1',xdcr_rest_port},
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
   9998]},
 {{node,'ns_1@127.0.0.1',uuid},
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
   <<"60a6bc3db77e6c7b91c556140dcfec71">>]},
 {{node,'ns_1@127.0.0.1',ssl_rest_port},
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
   18091]},
 {{node,'ns_1@127.0.0.1',ssl_query_port},
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
   18093]},
 {{node,'ns_1@127.0.0.1',ssl_capi_port},
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
   18092]},
 {{node,'ns_1@127.0.0.1',saslauthd_enabled},
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
   true]},
 {{node,'ns_1@127.0.0.1',rest},
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]},
   {port,8091},
   {port_meta,global}]},
 {{node,'ns_1@127.0.0.1',query_port},
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
   8093]},
 {{node,'ns_1@127.0.0.1',projector_ssl_port},
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
   9999]},
 {{node,'ns_1@127.0.0.1',projector_port},
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
   9999]},
 {{node,'ns_1@127.0.0.1',port_servers},
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}]},
 {{node,'ns_1@127.0.0.1',ns_log},
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]},
   {filename,"/opt/couchbase/var/lib/couchbase/ns_log"}]},
 {{node,'ns_1@127.0.0.1',moxi},
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]},
   {port,0}]},
 {{node,'ns_1@127.0.0.1',memcached_defaults},
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]},
   {max_connections,65000},
   {system_connections,5000},
   {connection_idle_time,0},
   {verbosity,0},
   {privilege_debug,false},
   {opentracing_enabled,false},
   {opentracing_module,[]},
   {opentracing_config,[]},
   {breakpad_enabled,true},
   {breakpad_minidump_dir_path,"/opt/couchbase/var/lib/couchbase/crash"},
   {dedupe_nmvb_maps,false},
   {tracing_enabled,true},
   {datatype_snappy,true},
   {num_reader_threads,<<"default">>},
   {num_writer_threads,<<"default">>}]},
 {{node,'ns_1@127.0.0.1',memcached_dedicated_ssl_port},
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
   11206]},
 {{node,'ns_1@127.0.0.1',memcached_config},
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
   {[{interfaces,
      {memcached_config_mgr,omit_missing_mcd_ports,
       [{[{host,<<"*">>},
          {port,port},
          {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
          {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]},
        {[{host,<<"*">>},
          {port,dedicated_port},
          {system,true},
          {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
          {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]},
        {[{host,<<"*">>},
          {port,ssl_port},
          {ssl,
           {[{key,
              <<"/opt/couchbase/var/lib/couchbase/config/memcached-key.pem">>},
             {cert,
              <<"/opt/couchbase/var/lib/couchbase/config/memcached-cert.pem">>}]}},
          {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
          {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]},
        {[{host,<<"*">>},
          {port,dedicated_ssl_port},
          {system,true},
          {ssl,
           {[{key,
              <<"/opt/couchbase/var/lib/couchbase/config/memcached-key.pem">>},
             {cert,
              <<"/opt/couchbase/var/lib/couchbase/config/memcached-cert.pem">>}]}},
          {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
          {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]}]}},
     {ssl_cipher_list,{memcached_config_mgr,get_ssl_cipher_list,[]}},
     {ssl_cipher_order,{memcached_config_mgr,get_ssl_cipher_order,[]}},
     {client_cert_auth,{memcached_config_mgr,client_cert_auth,[]}},
     {ssl_minimum_protocol,{memcached_config_mgr,ssl_minimum_protocol,[]}},
     {connection_idle_time,connection_idle_time},
     {privilege_debug,privilege_debug},
     {breakpad,
      {[{enabled,breakpad_enabled},
        {minidump_dir,{memcached_config_mgr,get_minidump_dir,[]}}]}},
     {opentracing,
      {[{enabled,opentracing_enabled},
        {module,{"~s",[opentracing_module]}},
        {config,{"~s",[opentracing_config]}}]}},
     {admin,{"~s",[admin_user]}},
     {verbosity,verbosity},
     {audit_file,{"~s",[audit_file]}},
     {rbac_file,{"~s",[rbac_file]}},
     {dedupe_nmvb_maps,dedupe_nmvb_maps},
     {tracing_enabled,tracing_enabled},
     {datatype_snappy,{memcached_config_mgr,is_snappy_enabled,[]}},
     {xattr_enabled,true},
     {scramsha_fallback_salt,{memcached_config_mgr,get_fallback_salt,[]}},
     {collections_enabled,{memcached_config_mgr,collections_enabled,[]}},
     {max_connections,max_connections},
     {system_connections,system_connections},
     {num_reader_threads,num_reader_threads},
     {num_writer_threads,num_writer_threads},
     {logger,
      {[{filename,{"~s/~s",[log_path,log_prefix]}},
        {cyclesize,log_cyclesize},
        {sleeptime,log_sleeptime}]}},
     {external_auth_service,
      {memcached_config_mgr,get_external_auth_service,[]}},
     {active_external_users_push_interval,
      {memcached_config_mgr,get_external_users_push_interval,[]}}]}]},
 {{node,'ns_1@127.0.0.1',memcached},
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]},
   {port,11210},
   {dedicated_port,11209},
   {dedicated_ssl_port,11206},
   {ssl_port,11207},
   {admin_user,"@ns_server"},
   {other_users,
    ["@cbq-engine","@projector","@goxdcr","@index","@fts","@eventing",
     "@cbas"]},
   {admin_pass,"*****"},
   {engines,
    [{membase,
      [{engine,"/opt/couchbase/lib/memcached/ep.so"},
       {static_config_string,"failpartialwarmup=false"}]},
     {memcached,
      [{engine,"/opt/couchbase/lib/memcached/default_engine.so"},
       {static_config_string,"vb0=true"}]}]},
   {config_path,"/opt/couchbase/var/lib/couchbase/config/memcached.json"},
   {audit_file,"/opt/couchbase/var/lib/couchbase/config/audit.json"},
   {rbac_file,"/opt/couchbase/var/lib/couchbase/config/memcached.rbac"},
   {log_path,"/opt/couchbase/var/lib/couchbase/logs"},
   {log_prefix,"memcached.log"},
   {log_generations,20},
   {log_cyclesize,10485760},
   {log_sleeptime,19},
   {log_rotation_period,39003}]},
 {{node,'ns_1@127.0.0.1',membership},
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
   active]},
 {{node,'ns_1@127.0.0.1',isasl},
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]},
   {path,"/opt/couchbase/var/lib/couchbase/isasl.pw"}]},
 {{node,'ns_1@127.0.0.1',is_enterprise},
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
   true]},
 {{node,'ns_1@127.0.0.1',indexer_stmaint_port},
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
   9105]},
 {{node,'ns_1@127.0.0.1',indexer_stinit_port},
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
   9103]},
 {{node,'ns_1@127.0.0.1',indexer_stcatchup_port},
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
   9104]},
 {{node,'ns_1@127.0.0.1',indexer_scan_port},
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
   9101]},
 {{node,'ns_1@127.0.0.1',indexer_https_port},
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
   19102]},
 {{node,'ns_1@127.0.0.1',indexer_http_port},
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
   9102]},
 {{node,'ns_1@127.0.0.1',indexer_admin_port},
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
   9100]},
 {{node,'ns_1@127.0.0.1',fts_ssl_port},
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
   18094]},
 {{node,'ns_1@127.0.0.1',fts_http_port},
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
   8094]},
 {{node,'ns_1@127.0.0.1',fts_grpc_ssl_port},
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
   19130]},
 {{node,'ns_1@127.0.0.1',fts_grpc_port},
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
   9130]},
 {{node,'ns_1@127.0.0.1',eventing_https_port},
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
   18096]},
 {{node,'ns_1@127.0.0.1',eventing_http_port},
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
   8096]},
 {{node,'ns_1@127.0.0.1',eventing_debug_port},
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
   9140]},
 {{node,'ns_1@127.0.0.1',config_version},
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
   {6,5}]},
 {{node,'ns_1@127.0.0.1',compaction_daemon},
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]},
   {check_interval,30},
   {min_db_file_size,131072},
   {min_view_file_size,20971520}]},
 {{node,'ns_1@127.0.0.1',cbas_ssl_port},
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
   18095]},
 {{node,'ns_1@127.0.0.1',cbas_result_port},
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
   9117]},
 {{node,'ns_1@127.0.0.1',cbas_replication_port},
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
   9120]},
 {{node,'ns_1@127.0.0.1',cbas_parent_port},
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
   9122]},
 {{node,'ns_1@127.0.0.1',cbas_metadata_port},
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
   9121]},
 {{node,'ns_1@127.0.0.1',cbas_metadata_callback_port},
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
   9119]},
 {{node,'ns_1@127.0.0.1',cbas_messaging_port},
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
   9118]},
 {{node,'ns_1@127.0.0.1',cbas_http_port},
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
   8095]},
 {{node,'ns_1@127.0.0.1',cbas_debug_port},
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|-1]},
 {{node,'ns_1@127.0.0.1',cbas_data_port},
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
   9116]},
 {{node,'ns_1@127.0.0.1',cbas_console_port},
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
   9114]},
 {{node,'ns_1@127.0.0.1',cbas_cluster_port},
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
   9115]},
 {{node,'ns_1@127.0.0.1',cbas_cc_http_port},
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
   9111]},
 {{node,'ns_1@127.0.0.1',cbas_cc_cluster_port},
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
   9112]},
 {{node,'ns_1@127.0.0.1',cbas_cc_client_port},
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
   9113]},
 {{node,'ns_1@127.0.0.1',cbas_admin_port},
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
   9110]},
 {{node,'ns_1@127.0.0.1',capi_port},
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
   8092]},
 {{node,'ns_1@127.0.0.1',audit},
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}]},
 {{request_limit,rest},undefined},
 {{request_limit,capi},undefined},
 {{couchdb,max_parallel_replica_indexers},2},
 {{couchdb,max_parallel_indexers},4},
 {set_view_update_daemon,
  [{update_interval,5000},
   {update_min_changes,5000},
   {replica_update_min_changes,5000}]},
 {server_groups,
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557625}}]},
   [{uuid,<<"0">>},{name,<<"Group 1">>},{nodes,['ns_1@127.0.0.1']}]]},
 {secure_headers,[]},
 {rest_creds,null},
 {rest,[{port,8091}]},
 {replication,[{enabled,true}]},
 {remote_clusters,[]},
 {quorum_nodes,
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]},
   'ns_1@127.0.0.1']},
 {password_policy,[{min_length,6},{must_present,[]}]},
 {nodes_wanted,
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557625}}]},
   'ns_1@127.0.0.1']},
 {memcached,[]},
 {max_bucket_count,
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557537}}]}|30]},
 {log_redaction_default_cfg,[{redact_level,none}]},
 {index_aware_rebalance_disabled,false},
 {email_alerts,
  [{recipients,["root@localhost"]},
   {sender,"couchbase@localhost"},
   {enabled,false},
   {email_server,
    [{user,[]},{pass,"*****"},{host,"localhost"},{port,25},{encrypt,false}]},
   {alerts,
    [auto_failover_node,auto_failover_maximum_reached,
     auto_failover_other_nodes_down,auto_failover_cluster_too_small,
     auto_failover_disabled,ip,disk,overhead,ep_oom_errors,
     ep_item_commit_failed,audit_dropped_events,indexer_ram_max_usage,
     ep_clock_cas_drift_threshold_exceeded,communication_issue]}]},
 {drop_request_memory_threshold_mib,undefined},
 {buckets,
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557537}}]},
   {configs,[]}]},
 {autocompaction,
  [{database_fragmentation_threshold,{30,undefined}},
   {view_fragmentation_threshold,{30,undefined}}]},
 {auto_reprovision_cfg,[{enabled,true},{max_nodes,1},{count,0}]},
 {audit,
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557537}}]},
   {enabled,[]},
   {disabled_users,[]},
   {auditd_enabled,false},
   {rotate_interval,86400},
   {rotate_size,20971520},
   {disabled,[]},
   {sync,[]},
   {log_path,"/opt/couchbase/var/lib/couchbase/logs"}]},
 {alert_limits,
  [{max_overhead_perc,50},{max_disk_used,90},{max_indexer_ram,75}]},
 {cert_and_pkey,
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557532}}]}|
   {<<"-----BEGIN CERTIFICATE-----\nMIIDAjCCAeqgAwIBAgIIFgBA0G6s0lwwDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciAzNDhlZDI3MDAeFw0xMzAxMDEwMDAwMDBaFw00\nOTEyMzEyMzU5NTlaMCQxIjAgBgNVBAMTGUNvdWNoYmFzZSBTZXJ2ZXIgMzQ4ZWQy\nNzAwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQDh0efJgvfpArvc3iI0\ncHa/zZhS9WoaoNHksFooYqy3AYiZR+8S3IgPS0nfcZGrIWJUPgokUqBqh6u59abc\nvCuyMwLNkkuLOkT6wzUztrAqoCoOEl4avKFmI0JsiWA/qKDU/kapi0rG9MzOwmWz\nhLUBFUx1SGUYpWzUOP0v5riPMhR5SvfMR/fiYfm/ruKWF5VHtNFW2EgfysURPlxa\ndArURVkUW6MzTV6cDXMXR1P83qZVhKvWp65OPD9O8XPHRrqvv17QBditavjLOEgY\n2xdWQ4kS3avQASnvVP2e5iRO3iF4WmDRJAXmWqMTAiOlUICrBloL7aIjA8lebsm1\nDTGlAgMBAAGjODA2MA4GA1UdDwEB/wQEAwICpDATBgNVHSUEDDAKBggrBgEFBQcD\nATAPBgNVHRMBAf8EBTADAQH/MA0GCSqGSIb3DQEBCwUAA4IBAQAGUUF8Ryt6D6fP\nEPsAnydivOMd12RrwrjQMeoxnRYQcuKuDZaskALu5ggo3Ro3OIR+6ZkYuSlsG+/Y\nUng/DPuFi78HQfWDXCLJ5sQ1PpyedT7cn8Blq4OPkZcrp+Naoa1NmBt8QzGWQ2ah\n3Sum/CJq0EzfSebYoRqEiYGST45Fs8HCL4HP+wTttPc2myjYJE87brwbmdAt0/Wd\nQBPz4YAtfqxhpEE3+uNa1DuZidvDVKttkUBv/W14I8eZ6Hiqs6dusHQK9U8GQp6U\nib99QaSZXrTiU+bx7CdDKAxbLE82QUj5S+vezY5wN+AvfSrrvA6AhizkGaMZWvaq\nSqN1PyyX\n-----END CERTIFICATE-----\n">>,
    <<"*****">>}]},
 {{node,'ns_1@127.0.0.1',cbas_dirs},
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]},
   "/opt/couchbase/var/lib/couchbase/data"]},
 {{node,'ns_1@127.0.0.1',eventing_dir},
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]},
   47,111,112,116,47,99,111,117,99,104,98,97,115,101,47,118,97,114,47,108,105,
   98,47,99,111,117,99,104,98,97,115,101,47,100,97,116,97]},
 {otp,
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557536}}]},
   {cookie,{sanitized,<<"C4PeZIe8LLStUy+d1HIN638gh0wTZW33h1wdSpFkvzc=">>}}]},
 {cluster_compat_version,
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{5,63752557537}}]},
   6,5]},
 {client_cert_auth,
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557537}}]},
   {state,"disable"},
   {prefixes,[]}]},
 {{metakv,<<"/query/settings/config">>},
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557537}}]}|
   <<"{\"timeout\":0,\"n1ql-feat-ctrl\":12,\"max-parallelism\":1,\"query.settings.curl_whitelist\":{\"all_access\":false,\"allowed_urls\":[],\"disallowed_urls\":[]},\"query.settings.tmp_space_dir\":\"/opt/couchbase/var/lib/couchbase/tmp\",\"completed-limit\":4000,\"prepared-limit\":16384,\"pipeline-batch\":16,\"pipeline-cap\":512,\"scan-cap\":512,\"loglevel\":\"info\",\"completed-threshold\":1000,\"query.settings.tmp_space_size\":5120}">>]},
 {scramsha_fallback_salt,
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557537}}]}|
   <<176,211,90,3,15,137,19,29,193,47,34,182>>]},
 {audit_decriptors,
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557537}}]},
   {8243,
    [{name,<<"mutate document">>},
     {description,<<"Document was mutated via the REST API">>},
     {enabled,true},
     {module,ns_server}]},
   {8255,
    [{name,<<"read document">>},
     {description,<<"Document was read via the REST API">>},
     {enabled,false},
     {module,ns_server}]},
   {8257,
    [{name,<<"alert email sent">>},
     {description,<<"An alert email was successfully sent">>},
     {enabled,true},
     {module,ns_server}]},
   {20480,
    [{name,<<"opened DCP connection">>},
     {description,<<"opened DCP connection">>},
     {enabled,true},
     {module,memcached}]},
   {20482,
    [{name,<<"external memcached bucket flush">>},
     {description,
      <<"External user flushed the content of a memcached bucket">>},
     {enabled,true},
     {module,memcached}]},
   {20483,
    [{name,<<"invalid packet">>},
     {description,<<"Rejected an invalid packet">>},
     {enabled,true},
     {module,memcached}]},
   {20485,
    [{name,<<"authentication succeeded">>},
     {description,<<"Authentication to the cluster succeeded">>},
     {enabled,false},
     {module,memcached}]},
   {20488,
    [{name,<<"document read">>},
     {description,<<"Document was read">>},
     {enabled,false},
     {module,memcached}]},
   {20489,
    [{name,<<"document locked">>},
     {description,<<"Document was locked">>},
     {enabled,false},
     {module,memcached}]},
   {20490,
    [{name,<<"document modify">>},
     {description,<<"Document was modified">>},
     {enabled,false},
     {module,memcached}]},
   {20491,
    [{name,<<"document delete">>},
     {description,<<"Document was deleted">>},
     {enabled,false},
     {module,memcached}]},
   {20492,
    [{name,<<"select bucket">>},
     {description,<<"The specified bucket was selected">>},
     {enabled,true},
     {module,memcached}]},
   {28672,
    [{name,<<"SELECT statement">>},
     {description,<<"A N1QL SELECT statement was executed">>},
     {enabled,false},
     {module,n1ql}]},
   {28673,
    [{name,<<"EXPLAIN statement">>},
     {description,<<"A N1QL EXPLAIN statement was executed">>},
     {enabled,false},
     {module,n1ql}]},
   {28674,
    [{name,<<"PREPARE statement">>},
     {description,<<"A N1QL PREPARE statement was executed">>},
     {enabled,false},
     {module,n1ql}]},
   {28675,
    [{name,<<"INFER statement">>},
     {description,<<"A N1QL INFER statement was executed">>},
     {enabled,false},
     {module,n1ql}]},
   {28676,
    [{name,<<"INSERT statement">>},
     {description,<<"A N1QL INSERT statement was executed">>},
     {enabled,false},
     {module,n1ql}]},
   {28677,
    [{name,<<"UPSERT statement">>},
     {description,<<"A N1QL UPSERT statement was executed">>},
     {enabled,false},
     {module,n1ql}]},
   {28678,
    [{name,<<"DELETE statement">>},
     {description,<<"A N1QL DELETE statement was executed">>},
     {enabled,false},
     {module,n1ql}]},
   {28679,
    [{name,<<"UPDATE statement">>},
     {description,<<"A N1QL UPDATE statement was executed">>},
     {enabled,false},
     {module,n1ql}]},
   {28680,
    [{name,<<"MERGE statement">>},
     {description,<<"A N1QL MERGE statement was executed">>},
     {enabled,false},
     {module,n1ql}]},
   {28681,
    [{name,<<"CREATE INDEX statement">>},
     {description,<<"A N1QL CREATE INDEX statement was executed">>},
     {enabled,false},
     {module,n1ql}]},
   {28682,
    [{name,<<"DROP INDEX statement">>},
     {description,<<"A N1QL DROP INDEX statement was executed">>},
     {enabled,false},
     {module,n1ql}]},
   {28683,
    [{name,<<"ALTER INDEX statement">>},
     {description,<<"A N1QL ALTER INDEX statement was executed">>},
     {enabled,false},
     {module,n1ql}]},
   {28684,
    [{name,<<"BUILD INDEX statement">>},
     {description,<<"A N1QL BUILD INDEX statement was executed">>},
     {enabled,false},
     {module,n1ql}]},
   {28685,
    [{name,<<"GRANT ROLE statement">>},
     {description,<<"A N1QL GRANT ROLE statement was executed">>},
     {enabled,false},
     {module,n1ql}]},
   {28686,
    [{name,<<"REVOKE ROLE statement">>},
     {description,<<"A N1QL REVOKE ROLE statement was executed">>},
     {enabled,false},
     {module,n1ql}]},
   {28687,
    [{name,<<"UNRECOGNIZED statement">>},
     {description,
      <<"An unrecognized statement was received by the N1QL query engine">>},
     {enabled,false},
     {module,n1ql}]},
   {28688,
    [{name,<<"CREATE PRIMARY INDEX statement">>},
     {description,<<"A N1QL CREATE PRIMARY INDEX statement was executed">>},
     {enabled,false},
     {module,n1ql}]},
   {28689,
    [{name,<<"/admin/stats API request">>},
     {description,<<"An HTTP request was made to the API at /admin/stats.">>},
     {enabled,false},
     {module,n1ql}]},
   {28690,
    [{name,<<"/admin/vitals API request">>},
     {description,<<"An HTTP request was made to the API at /admin/vitals.">>},
     {enabled,false},
     {module,n1ql}]},
   {28691,
    [{name,<<"/admin/prepareds API request">>},
     {description,
      <<"An HTTP request was made to the API at /admin/prepareds.">>},
     {enabled,false},
     {module,n1ql}]},
   {28692,
    [{name,<<"/admin/active_requests API request">>},
     {description,
      <<"An HTTP request was made to the API at /admin/active_requests.">>},
     {enabled,false},
     {module,n1ql}]},
   {28693,
    [{name,<<"/admin/indexes/prepareds API request">>},
     {description,
      <<"An HTTP request was made to the API at /admin/indexes/prepareds.">>},
     {enabled,false},
     {module,n1ql}]},
   {28694,
    [{name,<<"/admin/indexes/active_requests API request">>},
     {description,
      <<"An HTTP request was made to the API at /admin/indexes/active_requests.">>},
     {enabled,false},
     {module,n1ql}]},
   {28695,
    [{name,<<"/admin/indexes/completed_requests API request">>},
     {description,
      <<"An HTTP request was made to the API at /admin/indexes/completed_requests.">>},
     {enabled,false},
     {module,n1ql}]},
   {28697,
    [{name,<<"/admin/ping API request">>},
     {description,<<"An HTTP request was made to the API at /admin/ping.">>},
     {enabled,false},
     {module,n1ql}]},
   {28698,
    [{name,<<"/admin/config API request">>},
     {description,<<"An HTTP request was made to the API at /admin/config.">>},
     {enabled,false},
     {module,n1ql}]},
   {28699,
    [{name,<<"/admin/ssl_cert API request">>},
     {description,
      <<"An HTTP request was made to the API at /admin/ssl_cert.">>},
     {enabled,false},
     {module,n1ql}]},
   {28700,
    [{name,<<"/admin/settings API request">>},
     {description,
      <<"An HTTP request was made to the API at /admin/settings.">>},
     {enabled,false},
     {module,n1ql}]},
   {28701,
    [{name,<<"/admin/clusters API request">>},
     {description,
      <<"An HTTP request was made to the API at /admin/clusters.">>},
     {enabled,false},
     {module,n1ql}]},
   {28702,
    [{name,<<"/admin/completed_requests API request">>},
     {description,
      <<"An HTTP request was made to the API at /admin/completed_requests.">>},
     {enabled,false},
     {module,n1ql}]},
   {28704,
    [{name,<<"/admin/functions API request">>},
     {description,
      <<"An HTTP request was made to the API at /admin/functions.">>},
     {enabled,false},
     {module,n1ql}]},
   {28705,
    [{name,<<"/admin/indexes/functions API request">>},
     {description,
      <<"An HTTP request was made to the API at /admin/indexes/functions.">>},
     {enabled,false},
     {module,n1ql}]},
   {32768,
    [{name,<<"Create Function">>},
     {description,<<"Eventing function definition was created or updated">>},
     {enabled,true},
     {module,eventing}]},
   {32769,
    [{name,<<"Delete Function">>},
     {description,<<"Eventing function definition was deleted">>},
     {enabled,true},
     {module,eventing}]},
   {32770,
    [{name,<<"Fetch Functions">>},
     {description,<<"Eventing function definition was read">>},
     {enabled,false},
     {module,eventing}]},
   {32771,
    [{name,<<"List Deployed">>},
     {description,<<"Eventing deployed functions list was read">>},
     {enabled,false},
     {module,eventing}]},
   {32772,
    [{name,<<"Fetch Drafts">>},
     {description,<<"Eventing function draft definitions were read">>},
     {enabled,false},
     {module,eventing}]},
   {32773,
    [{name,<<"Delete Drafts">>},
     {description,<<"Eventing function draft definitions were deleted">>},
     {enabled,true},
     {module,eventing}]},
   {32774,
    [{name,<<"Save Draft">>},
     {description,<<"Save a draft definition to the store">>},
     {enabled,true},
     {module,eventing}]},
   {32775,
    [{name,<<"Start Debug">>},
     {description,<<"Start eventing function debugger">>},
     {enabled,true},
     {module,eventing}]},
   {32776,
    [{name,<<"Stop Debug">>},
     {description,<<"Stop eventing function debugger">>},
     {enabled,true},
     {module,eventing}]},
   {32777,
    [{name,<<"Start Tracing">>},
     {description,<<"Start tracing eventing function execution">>},
     {enabled,true},
     {module,eventing}]},
   {32778,
    [{name,<<"Stop Tracing">>},
     {description,<<"Stop tracing eventing function execution">>},
     {enabled,true},
     {module,eventing}]},
   {32779,
    [{name,<<"Set Settings">>},
     {description,<<"Save settings for a given app">>},
     {enabled,true},
     {module,eventing}]},
   {32780,
    [{name,<<"Fetch Config">>},
     {description,<<"Get config for eventing">>},
     {enabled,false},
     {module,eventing}]},
   {32781,
    [{name,<<"Save Config">>},
     {description,<<"Save config for eventing">>},
     {enabled,true},
     {module,eventing}]},
   {32782,
    [{name,<<"Cleanup Eventing">>},
     {description,<<"Clears up app definitions and settings from metakv">>},
     {enabled,true},
     {module,eventing}]},
   {32783,
    [{name,<<"Get Settings">>},
     {description,<<"Get settings for a given app">>},
     {enabled,false},
     {module,eventing}]},
   {32784,
    [{name,<<"Import Functions">>},
     {description,<<"Import a list of functions">>},
     {enabled,false},
     {module,eventing}]},
   {32785,
    [{name,<<"Export Functions">>},
     {description,<<"Export the list of functions">>},
     {enabled,false},
     {module,eventing}]},
   {32786,
    [{name,<<"List Running">>},
     {description,<<"Eventing running function list was read">>},
     {enabled,false},
     {module,eventing}]},
   {36865,
    [{name,<<"Service configuration change">>},
     {description,<<"A successful service configuration change was made.">>},
     {enabled,true},
     {module,analytics}]},
   {36866,
    [{name,<<"Node configuration change">>},
     {description,<<"A successful node configuration change was made.">>},
     {enabled,true},
     {module,analytics}]},
   {40960,
    [{name,<<"Create Design Doc">>},
     {description,<<"Design Doc is Created">>},
     {enabled,true},
     {module,view_engine}]},
   {40961,
    [{name,<<"Delete Design Doc">>},
     {description,<<"Design Doc is Deleted">>},
     {enabled,true},
     {module,view_engine}]},
   {40962,
    [{name,<<"Query DDoc Meta Data">>},
     {description,<<"Design Doc Meta Data Query Request">>},
     {enabled,true},
     {module,view_engine}]},
   {40963,
    [{name,<<"View Query">>},
     {description,<<"View Query Request">>},
     {enabled,false},
     {module,view_engine}]},
   {40964,
    [{name,<<"Update Design Doc">>},
     {description,<<"Design Doc is Updated">>},
     {enabled,true},
     {module,view_engine}]}]},
 {retry_rebalance,
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557537}}]},
   {enabled,false},
   {after_time_period,300},
   {max_attempts,1}]},
 {auto_failover_cfg,
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557537}}]},
   {enabled,true},
   {timeout,120},
   {count,0},
   {failover_on_data_disk_issues,[{enabled,false},{timePeriod,120}]},
   {failover_server_group,false},
   {max_count,1},
   {failed_over_server_groups,[]},
   {can_abort_rebalance,true}]},
 {{node,'ns_1@127.0.0.1',address_family},
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
   inet]},
 {{node,'ns_1@127.0.0.1',node_encryption},
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
   false]},
 {{node,'ns_1@127.0.0.1',erl_external_listeners},
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]},
   {inet,false},
   {inet6,false}]},
 {memory_quota,292},
 {{metakv,<<"/indexing/settings/config">>},
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{4,63752557980}}]}|
   <<"{\"indexer.settings.compaction.days_of_week\":\"Sunday,Monday,Tuesday,Wednesday,Thursday,Friday,Saturday\",\"indexer.settings.compaction.interval\":\"00:00,00:00\",\"indexer.settings.compaction.compaction_mode\":\"circular\",\"indexer.settings.log_level\":\"info\",\"indexer.settings.persisted_snapshot.interval\":5000,\"indexer.settings.compaction.min_frag\":30,\"indexer.settings.inmemory_snapshot.interval\":200,\"indexer.settings.max_cpu_percent\":0,\"indexer.settings.storage_mode\":\"plasma\",\"indexer.settings.recovery.max_rollbacks\":2,\"indexer.settings.memory_quota\":536870912,\"indexer.settings.compaction.abort_exceed_interval\":false}">>]},
 {fts_memory_quota,256},
 {cbas_memory_quota,1024},
 {{metakv,<<"/eventing/settings/config">>},
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557537}}]}|
   <<"{\"ram_quota\":256}">>]},
 {cluster_name,
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557980}}]},
   66,97,115,101]}]
[error_logger:info,2020-03-27T20:03:33.203Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_config_sup}
             started: [{pid,<0.195.0>},
                       {id,ns_config},
                       {mfargs,
                           {ns_config,start_link,
                               ["/opt/couchbase/etc/couchbase/config",
                                ns_config_default]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:03:33.205Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_config_sup}
             started: [{pid,<0.201.0>},
                       {id,ns_config_remote},
                       {mfargs,{ns_config_replica,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:03:33.207Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_config_sup}
             started: [{pid,<0.202.0>},
                       {id,ns_config_log},
                       {mfargs,{ns_config_log,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:03:33.207Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.192.0>},
                       {id,ns_config_sup},
                       {mfargs,{ns_config_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[ns_server:debug,2020-03-27T20:03:33.210Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{local_changes_count,<<"60a6bc3db77e6c7b91c556140dcfec71">>} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{22,63752558613}}]}]
[error_logger:info,2020-03-27T20:03:33.210Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.204.0>},
                       {id,netconfig_updater},
                       {mfargs,{netconfig_updater,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-03-27T20:03:33.217Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.207.0>},
                       {id,json_rpc_connection_sup},
                       {mfargs,{json_rpc_connection_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-03-27T20:03:33.265Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.210.0>},
                       {name,remote_monitors},
                       {mfargs,{remote_monitors,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-03-27T20:03:33.282Z,ns_1@127.0.0.1:menelaus_barrier<0.211.0>:one_shot_barrier:barrier_body:58]Barrier menelaus_barrier has started
[error_logger:info,2020-03-27T20:03:33.282Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.211.0>},
                       {name,menelaus_barrier},
                       {mfargs,{menelaus_sup,barrier_start_link,[]}},
                       {restart_type,temporary},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:03:33.283Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.212.0>},
                       {name,rest_lhttpc_pool},
                       {mfargs,
                           {lhttpc_manager,start_link,
                               [[{name,rest_lhttpc_pool},
                                 {connection_timeout,120000},
                                 {pool_size,20}]]}},
                       {restart_type,{permanent,1}},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:03:33.289Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.213.0>},
                       {name,memcached_refresh},
                       {mfargs,{memcached_refresh,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:03:33.298Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_ssl_services_sup}
             started: [{pid,<0.215.0>},
                       {id,ssl_service_events},
                       {mfargs,
                           {gen_event,start_link,
                               [{local,ssl_service_events}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-03-27T20:03:33.402Z,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Restarting tls distribution protocols (if any)
[ns_server:debug,2020-03-27T20:03:33.402Z,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: ignoring closing of inet6_tls_dist because listener is not started
[ns_server:debug,2020-03-27T20:03:33.402Z,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: ignoring closing of inet_tls_dist because listener is not started
[ns_server:info,2020-03-27T20:03:33.437Z,ns_1@127.0.0.1:ns_ssl_services_setup<0.216.0>:ns_ssl_services_setup:init:462]Used ssl options:
[{keyfile,"/opt/couchbase/var/lib/couchbase/config/ssl-cert-key.pem"},
 {certfile,"/opt/couchbase/var/lib/couchbase/config/ssl-cert-key.pem"},
 {versions,['tlsv1.1','tlsv1.2']},
 {cacerts,[<<48,130,3,2,48,130,1,234,160,3,2,1,2,2,8,22,0,64,208,110,172,210,
             92,48,13,6,9,42,134,72,134,247,13,1,1,11,5,0,48,36,49,34,48,32,
             6,3,85,4,3,19,25,67,111,117,99,104,98,97,115,101,32,83,101,114,
             118,101,114,32,51,52,56,101,100,50,55,48,48,30,23,13,49,51,48,
             49,48,49,48,48,48,48,48,48,90,23,13,52,57,49,50,51,49,50,51,53,
             57,53,57,90,48,36,49,34,48,32,6,3,85,4,3,19,25,67,111,117,99,
             104,98,97,115,101,32,83,101,114,118,101,114,32,51,52,56,101,100,
             50,55,48,48,130,1,34,48,13,6,9,42,134,72,134,247,13,1,1,1,5,0,3,
             130,1,15,0,48,130,1,10,2,130,1,1,0,225,209,231,201,130,247,233,
             2,187,220,222,34,52,112,118,191,205,152,82,245,106,26,160,209,
             228,176,90,40,98,172,183,1,136,153,71,239,18,220,136,15,75,73,
             223,113,145,171,33,98,84,62,10,36,82,160,106,135,171,185,245,
             166,220,188,43,178,51,2,205,146,75,139,58,68,250,195,53,51,182,
             176,42,160,42,14,18,94,26,188,161,102,35,66,108,137,96,63,168,
             160,212,254,70,169,139,74,198,244,204,206,194,101,179,132,181,1,
             21,76,117,72,101,24,165,108,212,56,253,47,230,184,143,50,20,121,
             74,247,204,71,247,226,97,249,191,174,226,150,23,149,71,180,209,
             86,216,72,31,202,197,17,62,92,90,116,10,212,69,89,20,91,163,51,
             77,94,156,13,115,23,71,83,252,222,166,85,132,171,214,167,174,78,
             60,63,78,241,115,199,70,186,175,191,94,208,5,216,173,106,248,
             203,56,72,24,219,23,86,67,137,18,221,171,208,1,41,239,84,253,
             158,230,36,78,222,33,120,90,96,209,36,5,230,90,163,19,2,35,165,
             80,128,171,6,90,11,237,162,35,3,201,94,110,201,181,13,49,165,2,
             3,1,0,1,163,56,48,54,48,14,6,3,85,29,15,1,1,255,4,4,3,2,2,164,
             48,19,6,3,85,29,37,4,12,48,10,6,8,43,6,1,5,5,7,3,1,48,15,6,3,85,
             29,19,1,1,255,4,5,48,3,1,1,255,48,13,6,9,42,134,72,134,247,13,1,
             1,11,5,0,3,130,1,1,0,6,81,65,124,71,43,122,15,167,207,16,251,0,
             159,39,98,188,227,29,215,100,107,194,184,208,49,234,49,157,22,
             16,114,226,174,13,150,172,144,2,238,230,8,40,221,26,55,56,132,
             126,233,153,24,185,41,108,27,239,216,82,120,63,12,251,133,139,
             191,7,65,245,131,92,34,201,230,196,53,62,156,158,117,62,220,159,
             192,101,171,131,143,145,151,43,167,227,90,161,173,77,152,27,124,
             67,49,150,67,102,161,221,43,166,252,34,106,208,76,223,73,230,
             216,161,26,132,137,129,146,79,142,69,179,193,194,47,129,207,251,
             4,237,180,247,54,155,40,216,36,79,59,110,188,27,153,208,45,211,
             245,157,64,19,243,225,128,45,126,172,97,164,65,55,250,227,90,
             212,59,153,137,219,195,84,171,109,145,64,111,253,109,120,35,199,
             153,232,120,170,179,167,110,176,116,10,245,79,6,66,158,148,137,
             191,125,65,164,153,94,180,226,83,230,241,236,39,67,40,12,91,44,
             79,54,65,72,249,75,235,222,205,142,112,55,224,47,125,42,235,188,
             14,128,134,44,228,25,163,25,90,246,170,74,163,117,63,44,151>>]},
 {dh,<<48,130,1,8,2,130,1,1,0,152,202,99,248,92,201,35,238,246,5,77,93,120,10,
       118,129,36,52,111,193,167,220,49,229,106,105,152,133,121,157,73,158,
       232,153,197,197,21,171,140,30,207,52,165,45,8,221,162,21,199,183,66,
       211,247,51,224,102,214,190,130,96,253,218,193,35,43,139,145,89,200,250,
       145,92,50,80,134,135,188,205,254,148,122,136,237,220,186,147,187,104,
       159,36,147,217,117,74,35,163,145,249,175,242,18,221,124,54,140,16,246,
       169,84,252,45,47,99,136,30,60,189,203,61,86,225,117,255,4,91,46,110,
       167,173,106,51,65,10,248,94,225,223,73,40,232,140,26,11,67,170,118,190,
       67,31,127,233,39,68,88,132,171,224,62,187,207,160,189,209,101,74,8,205,
       174,146,173,80,105,144,246,25,153,86,36,24,178,163,64,202,221,95,184,
       110,244,32,226,217,34,55,188,230,55,16,216,247,173,246,139,76,187,66,
       211,159,17,46,20,18,48,80,27,250,96,189,29,214,234,241,34,69,254,147,
       103,220,133,40,164,84,8,44,241,61,164,151,9,135,41,60,75,4,202,133,173,
       72,6,69,167,89,112,174,40,229,171,2,1,2>>},
 {ciphers,[{ecdhe_ecdsa,aes_256_gcm,aead,sha384},
           {ecdhe_rsa,aes_256_gcm,aead,sha384},
           {ecdhe_ecdsa,aes_256_cbc,sha384,sha384},
           {ecdhe_rsa,aes_256_cbc,sha384,sha384},
           {ecdh_ecdsa,aes_256_gcm,aead,sha384},
           {ecdh_rsa,aes_256_gcm,aead,sha384},
           {ecdh_ecdsa,aes_256_cbc,sha384,sha384},
           {ecdh_rsa,aes_256_cbc,sha384,sha384},
           {ecdhe_ecdsa,chacha20_poly1305,aead,sha256},
           {ecdhe_rsa,chacha20_poly1305,aead,sha256},
           {dhe_rsa,chacha20_poly1305,aead,sha256},
           {dhe_rsa,aes_256_gcm,aead,sha384},
           {dhe_dss,aes_256_gcm,aead,sha384},
           {dhe_rsa,aes_256_cbc,sha256},
           {dhe_dss,aes_256_cbc,sha256},
           {rsa,aes_256_gcm,aead,sha384},
           {rsa,aes_256_cbc,sha256},
           {ecdhe_ecdsa,aes_128_gcm,aead,sha256},
           {ecdhe_rsa,aes_128_gcm,aead,sha256},
           {ecdhe_ecdsa,aes_128_cbc,sha256,sha256},
           {ecdhe_rsa,aes_128_cbc,sha256,sha256},
           {ecdh_ecdsa,aes_128_gcm,aead,sha256},
           {ecdh_rsa,aes_128_gcm,aead,sha256},
           {ecdh_ecdsa,aes_128_cbc,sha256,sha256},
           {ecdh_rsa,aes_128_cbc,sha256,sha256},
           {dhe_rsa,aes_128_gcm,aead,sha256},
           {dhe_dss,aes_128_gcm,aead,sha256},
           {dhe_rsa,aes_128_cbc,sha256},
           {dhe_dss,aes_128_cbc,sha256},
           {rsa,aes_128_gcm,aead,sha256},
           {rsa,aes_128_cbc,sha256},
           {ecdhe_ecdsa,aes_256_cbc,sha},
           {ecdhe_rsa,aes_256_cbc,sha},
           {dhe_rsa,aes_256_cbc,sha},
           {dhe_dss,aes_256_cbc,sha},
           {ecdh_ecdsa,aes_256_cbc,sha},
           {ecdh_rsa,aes_256_cbc,sha},
           {rsa,aes_256_cbc,sha},
           {ecdhe_ecdsa,aes_128_cbc,sha},
           {ecdhe_rsa,aes_128_cbc,sha},
           {dhe_rsa,aes_128_cbc,sha},
           {dhe_dss,aes_128_cbc,sha},
           {ecdh_ecdsa,aes_128_cbc,sha},
           {ecdh_rsa,aes_128_cbc,sha},
           {rsa,aes_128_cbc,sha},
           {ecdhe_ecdsa,'3des_ede_cbc',sha},
           {ecdhe_rsa,'3des_ede_cbc',sha},
           {dhe_rsa,'3des_ede_cbc',sha},
           {dhe_dss,'3des_ede_cbc',sha},
           {ecdh_ecdsa,'3des_ede_cbc',sha},
           {ecdh_rsa,'3des_ede_cbc',sha},
           {rsa,'3des_ede_cbc',sha}]},
 {honor_cipher_order,true},
 {secure_renegotiate,true},
 {client_renegotiation,false}]
[error_logger:info,2020-03-27T20:03:33.439Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_ssl_services_sup}
             started: [{pid,<0.216.0>},
                       {id,ns_ssl_services_setup},
                       {mfargs,{ns_ssl_services_setup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:info,2020-03-27T20:03:33.460Z,ns_1@127.0.0.1:<0.219.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for cbas
[ns_server:info,2020-03-27T20:03:33.460Z,ns_1@127.0.0.1:<0.219.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for eventing
[ns_server:info,2020-03-27T20:03:33.461Z,ns_1@127.0.0.1:<0.219.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for fts
[ns_server:info,2020-03-27T20:03:33.461Z,ns_1@127.0.0.1:<0.219.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for n1ql
[error_logger:info,2020-03-27T20:03:33.505Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {<0.219.0>,menelaus_web}
             started: [{pid,<0.220.0>},
                       {id,menelaus_web_ipv4},
                       {mfargs,
                        {menelaus_web,http_server,
                         [[{ip,"0.0.0.0"},
                           {name,menelaus_web_ssl_ipv4},
                           {ssl,true},
                           {ssl_opts,
                            [{keyfile,
                              "/opt/couchbase/var/lib/couchbase/config/ssl-cert-key.pem"},
                             {certfile,
                              "/opt/couchbase/var/lib/couchbase/config/ssl-cert-key.pem"},
                             {versions,['tlsv1.1','tlsv1.2']},
                             {cacerts,
                              [<<48,130,3,2,48,130,1,234,160,3,2,1,2,2,8,22,
                                 0,64,208,110,172,210,92,48,13,6,9,42,134,72,
                                 134,247,13,1,1,11,5,0,48,36,49,34,48,32,6,3,
                                 85,4,3,19,25,67,111,117,99,104,98,97,115,
                                 101,32,83,101,114,118,101,114,32,51,52,56,
                                 101,100,50,55,48,48,30,23,13,49,51,48,49,48,
                                 49,48,48,48,48,48,48,90,23,13,52,57,49,50,
                                 51,49,50,51,53,57,53,57,90,48,36,49,34,48,
                                 32,6,3,85,4,3,19,25,67,111,117,99,104,98,97,
                                 115,101,32,83,101,114,118,101,114,32,51,52,
                                 56,101,100,50,55,48,48,130,1,34,48,13,6,9,
                                 42,134,72,134,247,13,1,1,1,5,0,3,130,1,15,0,
                                 48,130,1,10,2,130,1,1,0,225,209,231,201,130,
                                 247,233,2,187,220,222,34,52,112,118,191,205,
                                 152,82,245,106,26,160,209,228,176,90,40,98,
                                 172,183,1,136,153,71,239,18,220,136,15,75,
                                 73,223,113,145,171,33,98,84,62,10,36,82,160,
                                 106,135,171,185,245,166,220,188,43,178,51,2,
                                 205,146,75,139,58,68,250,195,53,51,182,176,
                                 42,160,42,14,18,94,26,188,161,102,35,66,108,
                                 137,96,63,168,160,212,254,70,169,139,74,198,
                                 244,204,206,194,101,179,132,181,1,21,76,117,
                                 72,101,24,165,108,212,56,253,47,230,184,143,
                                 50,20,121,74,247,204,71,247,226,97,249,191,
                                 174,226,150,23,149,71,180,209,86,216,72,31,
                                 202,197,17,62,92,90,116,10,212,69,89,20,91,
                                 163,51,77,94,156,13,115,23,71,83,252,222,
                                 166,85,132,171,214,167,174,78,60,63,78,241,
                                 115,199,70,186,175,191,94,208,5,216,173,106,
                                 248,203,56,72,24,219,23,86,67,137,18,221,
                                 171,208,1,41,239,84,253,158,230,36,78,222,
                                 33,120,90,96,209,36,5,230,90,163,19,2,35,
                                 165,80,128,171,6,90,11,237,162,35,3,201,94,
                                 110,201,181,13,49,165,2,3,1,0,1,163,56,48,
                                 54,48,14,6,3,85,29,15,1,1,255,4,4,3,2,2,164,
                                 48,19,6,3,85,29,37,4,12,48,10,6,8,43,6,1,5,
                                 5,7,3,1,48,15,6,3,85,29,19,1,1,255,4,5,48,3,
                                 1,1,255,48,13,6,9,42,134,72,134,247,13,1,1,
                                 11,5,0,3,130,1,1,0,6,81,65,124,71,43,122,15,
                                 167,207,16,251,0,159,39,98,188,227,29,215,
                                 100,107,194,184,208,49,234,49,157,22,16,114,
                                 226,174,13,150,172,144,2,238,230,8,40,221,
                                 26,55,56,132,126,233,153,24,185,41,108,27,
                                 239,216,82,120,63,12,251,133,139,191,7,65,
                                 245,131,92,34,201,230,196,53,62,156,158,117,
                                 62,220,159,192,101,171,131,143,145,151,43,
                                 167,227,90,161,173,77,152,27,124,67,49,150,
                                 67,102,161,221,43,166,252,34,106,208,76,223,
                                 73,230,216,161,26,132,137,129,146,79,142,69,
                                 179,193,194,47,129,207,251,4,237,180,247,54,
                                 155,40,216,36,79,59,110,188,27,153,208,45,
                                 211,245,157,64,19,243,225,128,45,126,172,97,
                                 164,65,55,250,227,90,212,59,153,137,219,195,
                                 84,171,109,145,64,111,253,109,120,35,199,
                                 153,232,120,170,179,167,110,176,116,10,245,
                                 79,6,66,158,148,137,191,125,65,164,153,94,
                                 180,226,83,230,241,236,39,67,40,12,91,44,79,
                                 54,65,72,249,75,235,222,205,142,112,55,224,
                                 47,125,42,235,188,14,128,134,44,228,25,163,
                                 25,90,246,170,74,163,117,63,44,151>>]},
                             {dh,
                              <<48,130,1,8,2,130,1,1,0,152,202,99,248,92,201,
                                35,238,246,5,77,93,120,10,118,129,36,52,111,
                                193,167,220,49,229,106,105,152,133,121,157,73,
                                158,232,153,197,197,21,171,140,30,207,52,165,
                                45,8,221,162,21,199,183,66,211,247,51,224,102,
                                214,190,130,96,253,218,193,35,43,139,145,89,
                                200,250,145,92,50,80,134,135,188,205,254,148,
                                122,136,237,220,186,147,187,104,159,36,147,
                                217,117,74,35,163,145,249,175,242,18,221,124,
                                54,140,16,246,169,84,252,45,47,99,136,30,60,
                                189,203,61,86,225,117,255,4,91,46,110,167,173,
                                106,51,65,10,248,94,225,223,73,40,232,140,26,
                                11,67,170,118,190,67,31,127,233,39,68,88,132,
                                171,224,62,187,207,160,189,209,101,74,8,205,
                                174,146,173,80,105,144,246,25,153,86,36,24,
                                178,163,64,202,221,95,184,110,244,32,226,217,
                                34,55,188,230,55,16,216,247,173,246,139,76,
                                187,66,211,159,17,46,20,18,48,80,27,250,96,
                                189,29,214,234,241,34,69,254,147,103,220,133,
                                40,164,84,8,44,241,61,164,151,9,135,41,60,75,
                                4,202,133,173,72,6,69,167,89,112,174,40,229,
                                171,2,1,2>>},
                             {ciphers,
                              [{ecdhe_ecdsa,aes_256_gcm,aead,sha384},
                               {ecdhe_rsa,aes_256_gcm,aead,sha384},
                               {ecdhe_ecdsa,aes_256_cbc,sha384,sha384},
                               {ecdhe_rsa,aes_256_cbc,sha384,sha384},
                               {ecdh_ecdsa,aes_256_gcm,aead,sha384},
                               {ecdh_rsa,aes_256_gcm,aead,sha384},
                               {ecdh_ecdsa,aes_256_cbc,sha384,sha384},
                               {ecdh_rsa,aes_256_cbc,sha384,sha384},
                               {ecdhe_ecdsa,chacha20_poly1305,aead,sha256},
                               {ecdhe_rsa,chacha20_poly1305,aead,sha256},
                               {dhe_rsa,chacha20_poly1305,aead,sha256},
                               {dhe_rsa,aes_256_gcm,aead,sha384},
                               {dhe_dss,aes_256_gcm,aead,sha384},
                               {dhe_rsa,aes_256_cbc,sha256},
                               {dhe_dss,aes_256_cbc,sha256},
                               {rsa,aes_256_gcm,aead,sha384},
                               {rsa,aes_256_cbc,sha256},
                               {ecdhe_ecdsa,aes_128_gcm,aead,sha256},
                               {ecdhe_rsa,aes_128_gcm,aead,sha256},
                               {ecdhe_ecdsa,aes_128_cbc,sha256,sha256},
                               {ecdhe_rsa,aes_128_cbc,sha256,sha256},
                               {ecdh_ecdsa,aes_128_gcm,aead,sha256},
                               {ecdh_rsa,aes_128_gcm,aead,sha256},
                               {ecdh_ecdsa,aes_128_cbc,sha256,sha256},
                               {ecdh_rsa,aes_128_cbc,sha256,sha256},
                               {dhe_rsa,aes_128_gcm,aead,sha256},
                               {dhe_dss,aes_128_gcm,aead,sha256},
                               {dhe_rsa,aes_128_cbc,sha256},
                               {dhe_dss,aes_128_cbc,sha256},
                               {rsa,aes_128_gcm,aead,sha256},
                               {rsa,aes_128_cbc,sha256},
                               {ecdhe_ecdsa,aes_256_cbc,sha},
                               {ecdhe_rsa,aes_256_cbc,sha},
                               {dhe_rsa,aes_256_cbc,sha},
                               {dhe_dss,aes_256_cbc,sha},
                               {ecdh_ecdsa,aes_256_cbc,sha},
                               {ecdh_rsa,aes_256_cbc,sha},
                               {rsa,aes_256_cbc,sha},
                               {ecdhe_ecdsa,aes_128_cbc,sha},
                               {ecdhe_rsa,aes_128_cbc,sha},
                               {dhe_rsa,aes_128_cbc,sha},
                               {dhe_dss,aes_128_cbc,sha},
                               {ecdh_ecdsa,aes_128_cbc,sha},
                               {ecdh_rsa,aes_128_cbc,sha},
                               {rsa,aes_128_cbc,sha},
                               {ecdhe_ecdsa,'3des_ede_cbc',sha},
                               {ecdhe_rsa,'3des_ede_cbc',sha},
                               {dhe_rsa,'3des_ede_cbc',sha},
                               {dhe_dss,'3des_ede_cbc',sha},
                               {ecdh_ecdsa,'3des_ede_cbc',sha},
                               {ecdh_rsa,'3des_ede_cbc',sha},
                               {rsa,'3des_ede_cbc',sha}]},
                             {honor_cipher_order,true},
                             {secure_renegotiate,true},
                             {client_renegotiation,false}]},
                           {port,18091}]]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[ns_server:info,2020-03-27T20:03:33.506Z,ns_1@127.0.0.1:<0.219.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for cbas
[ns_server:info,2020-03-27T20:03:33.506Z,ns_1@127.0.0.1:<0.219.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for eventing
[ns_server:info,2020-03-27T20:03:33.507Z,ns_1@127.0.0.1:<0.219.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for fts
[ns_server:info,2020-03-27T20:03:33.507Z,ns_1@127.0.0.1:<0.219.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for n1ql
[error_logger:info,2020-03-27T20:03:33.507Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {<0.219.0>,menelaus_web}
             started: [{pid,<0.238.0>},
                       {id,menelaus_web_ipv6},
                       {mfargs,
                        {menelaus_web,http_server,
                         [[{ip,"::"},
                           {name,menelaus_web_ssl_ipv6},
                           {ssl,true},
                           {ssl_opts,
                            [{keyfile,
                              "/opt/couchbase/var/lib/couchbase/config/ssl-cert-key.pem"},
                             {certfile,
                              "/opt/couchbase/var/lib/couchbase/config/ssl-cert-key.pem"},
                             {versions,['tlsv1.1','tlsv1.2']},
                             {cacerts,
                              [<<48,130,3,2,48,130,1,234,160,3,2,1,2,2,8,22,
                                 0,64,208,110,172,210,92,48,13,6,9,42,134,72,
                                 134,247,13,1,1,11,5,0,48,36,49,34,48,32,6,3,
                                 85,4,3,19,25,67,111,117,99,104,98,97,115,
                                 101,32,83,101,114,118,101,114,32,51,52,56,
                                 101,100,50,55,48,48,30,23,13,49,51,48,49,48,
                                 49,48,48,48,48,48,48,90,23,13,52,57,49,50,
                                 51,49,50,51,53,57,53,57,90,48,36,49,34,48,
                                 32,6,3,85,4,3,19,25,67,111,117,99,104,98,97,
                                 115,101,32,83,101,114,118,101,114,32,51,52,
                                 56,101,100,50,55,48,48,130,1,34,48,13,6,9,
                                 42,134,72,134,247,13,1,1,1,5,0,3,130,1,15,0,
                                 48,130,1,10,2,130,1,1,0,225,209,231,201,130,
                                 247,233,2,187,220,222,34,52,112,118,191,205,
                                 152,82,245,106,26,160,209,228,176,90,40,98,
                                 172,183,1,136,153,71,239,18,220,136,15,75,
                                 73,223,113,145,171,33,98,84,62,10,36,82,160,
                                 106,135,171,185,245,166,220,188,43,178,51,2,
                                 205,146,75,139,58,68,250,195,53,51,182,176,
                                 42,160,42,14,18,94,26,188,161,102,35,66,108,
                                 137,96,63,168,160,212,254,70,169,139,74,198,
                                 244,204,206,194,101,179,132,181,1,21,76,117,
                                 72,101,24,165,108,212,56,253,47,230,184,143,
                                 50,20,121,74,247,204,71,247,226,97,249,191,
                                 174,226,150,23,149,71,180,209,86,216,72,31,
                                 202,197,17,62,92,90,116,10,212,69,89,20,91,
                                 163,51,77,94,156,13,115,23,71,83,252,222,
                                 166,85,132,171,214,167,174,78,60,63,78,241,
                                 115,199,70,186,175,191,94,208,5,216,173,106,
                                 248,203,56,72,24,219,23,86,67,137,18,221,
                                 171,208,1,41,239,84,253,158,230,36,78,222,
                                 33,120,90,96,209,36,5,230,90,163,19,2,35,
                                 165,80,128,171,6,90,11,237,162,35,3,201,94,
                                 110,201,181,13,49,165,2,3,1,0,1,163,56,48,
                                 54,48,14,6,3,85,29,15,1,1,255,4,4,3,2,2,164,
                                 48,19,6,3,85,29,37,4,12,48,10,6,8,43,6,1,5,
                                 5,7,3,1,48,15,6,3,85,29,19,1,1,255,4,5,48,3,
                                 1,1,255,48,13,6,9,42,134,72,134,247,13,1,1,
                                 11,5,0,3,130,1,1,0,6,81,65,124,71,43,122,15,
                                 167,207,16,251,0,159,39,98,188,227,29,215,
                                 100,107,194,184,208,49,234,49,157,22,16,114,
                                 226,174,13,150,172,144,2,238,230,8,40,221,
                                 26,55,56,132,126,233,153,24,185,41,108,27,
                                 239,216,82,120,63,12,251,133,139,191,7,65,
                                 245,131,92,34,201,230,196,53,62,156,158,117,
                                 62,220,159,192,101,171,131,143,145,151,43,
                                 167,227,90,161,173,77,152,27,124,67,49,150,
                                 67,102,161,221,43,166,252,34,106,208,76,223,
                                 73,230,216,161,26,132,137,129,146,79,142,69,
                                 179,193,194,47,129,207,251,4,237,180,247,54,
                                 155,40,216,36,79,59,110,188,27,153,208,45,
                                 211,245,157,64,19,243,225,128,45,126,172,97,
                                 164,65,55,250,227,90,212,59,153,137,219,195,
                                 84,171,109,145,64,111,253,109,120,35,199,
                                 153,232,120,170,179,167,110,176,116,10,245,
                                 79,6,66,158,148,137,191,125,65,164,153,94,
                                 180,226,83,230,241,236,39,67,40,12,91,44,79,
                                 54,65,72,249,75,235,222,205,142,112,55,224,
                                 47,125,42,235,188,14,128,134,44,228,25,163,
                                 25,90,246,170,74,163,117,63,44,151>>]},
                             {dh,
                              <<48,130,1,8,2,130,1,1,0,152,202,99,248,92,201,
                                35,238,246,5,77,93,120,10,118,129,36,52,111,
                                193,167,220,49,229,106,105,152,133,121,157,73,
                                158,232,153,197,197,21,171,140,30,207,52,165,
                                45,8,221,162,21,199,183,66,211,247,51,224,102,
                                214,190,130,96,253,218,193,35,43,139,145,89,
                                200,250,145,92,50,80,134,135,188,205,254,148,
                                122,136,237,220,186,147,187,104,159,36,147,
                                217,117,74,35,163,145,249,175,242,18,221,124,
                                54,140,16,246,169,84,252,45,47,99,136,30,60,
                                189,203,61,86,225,117,255,4,91,46,110,167,173,
                                106,51,65,10,248,94,225,223,73,40,232,140,26,
                                11,67,170,118,190,67,31,127,233,39,68,88,132,
                                171,224,62,187,207,160,189,209,101,74,8,205,
                                174,146,173,80,105,144,246,25,153,86,36,24,
                                178,163,64,202,221,95,184,110,244,32,226,217,
                                34,55,188,230,55,16,216,247,173,246,139,76,
                                187,66,211,159,17,46,20,18,48,80,27,250,96,
                                189,29,214,234,241,34,69,254,147,103,220,133,
                                40,164,84,8,44,241,61,164,151,9,135,41,60,75,
                                4,202,133,173,72,6,69,167,89,112,174,40,229,
                                171,2,1,2>>},
                             {ciphers,
                              [{ecdhe_ecdsa,aes_256_gcm,aead,sha384},
                               {ecdhe_rsa,aes_256_gcm,aead,sha384},
                               {ecdhe_ecdsa,aes_256_cbc,sha384,sha384},
                               {ecdhe_rsa,aes_256_cbc,sha384,sha384},
                               {ecdh_ecdsa,aes_256_gcm,aead,sha384},
                               {ecdh_rsa,aes_256_gcm,aead,sha384},
                               {ecdh_ecdsa,aes_256_cbc,sha384,sha384},
                               {ecdh_rsa,aes_256_cbc,sha384,sha384},
                               {ecdhe_ecdsa,chacha20_poly1305,aead,sha256},
                               {ecdhe_rsa,chacha20_poly1305,aead,sha256},
                               {dhe_rsa,chacha20_poly1305,aead,sha256},
                               {dhe_rsa,aes_256_gcm,aead,sha384},
                               {dhe_dss,aes_256_gcm,aead,sha384},
                               {dhe_rsa,aes_256_cbc,sha256},
                               {dhe_dss,aes_256_cbc,sha256},
                               {rsa,aes_256_gcm,aead,sha384},
                               {rsa,aes_256_cbc,sha256},
                               {ecdhe_ecdsa,aes_128_gcm,aead,sha256},
                               {ecdhe_rsa,aes_128_gcm,aead,sha256},
                               {ecdhe_ecdsa,aes_128_cbc,sha256,sha256},
                               {ecdhe_rsa,aes_128_cbc,sha256,sha256},
                               {ecdh_ecdsa,aes_128_gcm,aead,sha256},
                               {ecdh_rsa,aes_128_gcm,aead,sha256},
                               {ecdh_ecdsa,aes_128_cbc,sha256,sha256},
                               {ecdh_rsa,aes_128_cbc,sha256,sha256},
                               {dhe_rsa,aes_128_gcm,aead,sha256},
                               {dhe_dss,aes_128_gcm,aead,sha256},
                               {dhe_rsa,aes_128_cbc,sha256},
                               {dhe_dss,aes_128_cbc,sha256},
                               {rsa,aes_128_gcm,aead,sha256},
                               {rsa,aes_128_cbc,sha256},
                               {ecdhe_ecdsa,aes_256_cbc,sha},
                               {ecdhe_rsa,aes_256_cbc,sha},
                               {dhe_rsa,aes_256_cbc,sha},
                               {dhe_dss,aes_256_cbc,sha},
                               {ecdh_ecdsa,aes_256_cbc,sha},
                               {ecdh_rsa,aes_256_cbc,sha},
                               {rsa,aes_256_cbc,sha},
                               {ecdhe_ecdsa,aes_128_cbc,sha},
                               {ecdhe_rsa,aes_128_cbc,sha},
                               {dhe_rsa,aes_128_cbc,sha},
                               {dhe_dss,aes_128_cbc,sha},
                               {ecdh_ecdsa,aes_128_cbc,sha},
                               {ecdh_rsa,aes_128_cbc,sha},
                               {rsa,aes_128_cbc,sha},
                               {ecdhe_ecdsa,'3des_ede_cbc',sha},
                               {ecdhe_rsa,'3des_ede_cbc',sha},
                               {dhe_rsa,'3des_ede_cbc',sha},
                               {dhe_dss,'3des_ede_cbc',sha},
                               {ecdh_ecdsa,'3des_ede_cbc',sha},
                               {ecdh_rsa,'3des_ede_cbc',sha},
                               {rsa,'3des_ede_cbc',sha}]},
                             {honor_cipher_order,true},
                             {secure_renegotiate,true},
                             {client_renegotiation,false}]},
                           {port,18091}]]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[ns_server:debug,2020-03-27T20:03:33.508Z,ns_1@127.0.0.1:<0.218.0>:restartable:start_child:98]Started child process <0.219.0>
  MFA: {ns_ssl_services_setup,start_link_rest_service,[]}
[error_logger:info,2020-03-27T20:03:33.508Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_ssl_services_sup}
             started: [{pid,<0.218.0>},
                       {id,ns_rest_ssl_service},
                       {mfargs,
                           {restartable,start_link,
                               [{ns_ssl_services_setup,
                                    start_link_rest_service,[]},
                                1000]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:03:33.508Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.214.0>},
                       {name,ns_ssl_services_sup},
                       {mfargs,{ns_ssl_services_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-03-27T20:03:33.519Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.256.0>},
                       {name,ldap_auth_cache},
                       {mfargs,{ldap_auth_cache,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:03:33.521Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,users_sup}
             started: [{pid,<0.259.0>},
                       {id,user_storage_events},
                       {mfargs,
                           {gen_event,start_link,
                               [{local,user_storage_events}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:03:33.528Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,users_storage_sup}
             started: [{pid,<0.261.0>},
                       {id,users_replicator},
                       {mfargs,{menelaus_users,start_replicator,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-03-27T20:03:33.529Z,ns_1@127.0.0.1:users_replicator<0.261.0>:replicated_storage:wait_for_startup:54]Start waiting for startup
[ns_server:debug,2020-03-27T20:03:33.531Z,ns_1@127.0.0.1:users_storage<0.262.0>:replicated_storage:anounce_startup:68]Announce my startup to <0.261.0>
[ns_server:debug,2020-03-27T20:03:33.531Z,ns_1@127.0.0.1:users_replicator<0.261.0>:replicated_storage:wait_for_startup:57]Received replicated storage registration from <0.262.0>
[error_logger:info,2020-03-27T20:03:33.534Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,users_storage_sup}
             started: [{pid,<0.262.0>},
                       {id,users_storage},
                       {mfargs,{menelaus_users,start_storage,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:03:33.534Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,users_sup}
             started: [{pid,<0.260.0>},
                       {id,users_storage_sup},
                       {mfargs,{users_storage_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[ns_server:debug,2020-03-27T20:03:33.534Z,ns_1@127.0.0.1:users_storage<0.262.0>:replicated_dets:open:177]Opening file "/opt/couchbase/var/lib/couchbase/config/users.dets"
[ns_server:debug,2020-03-27T20:03:33.536Z,ns_1@127.0.0.1:compiled_roles_cache<0.264.0>:versioned_cache:init:47]Starting versioned cache compiled_roles_cache
[error_logger:info,2020-03-27T20:03:33.536Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,users_sup}
             started: [{pid,<0.264.0>},
                       {id,compiled_roles_cache},
                       {mfargs,{menelaus_roles,start_compiled_roles_cache,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:03:33.542Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,users_sup}
             started: [{pid,<0.267.0>},
                       {id,roles_cache},
                       {mfargs,{roles_cache,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:03:33.542Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.258.0>},
                       {name,users_sup},
                       {mfargs,{users_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-03-27T20:03:33.548Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,kernel_safe_sup}
             started: [{pid,<0.271.0>},
                       {id,dets_sup},
                       {mfargs,{dets_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,supervisor}]

[error_logger:info,2020-03-27T20:03:33.548Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,kernel_safe_sup}
             started: [{pid,<0.272.0>},
                       {id,dets},
                       {mfargs,{dets_server,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,2000},
                       {child_type,worker}]

[ns_server:info,2020-03-27T20:03:33.566Z,ns_1@127.0.0.1:users_storage<0.262.0>:replicated_dets:convert_docs_to_55_in_dets:209]Checking for pre 5.5 records in dets: users_storage
[ns_server:debug,2020-03-27T20:03:33.568Z,ns_1@127.0.0.1:wait_link_to_couchdb_node<0.275.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:152]Waiting for ns_couchdb node to start
[ns_server:debug,2020-03-27T20:03:33.568Z,ns_1@127.0.0.1:users_storage<0.262.0>:replicated_dets:init_after_ack:170]Loading 0 items, 300 words took 34ms
[ns_server:debug,2020-03-27T20:03:33.568Z,ns_1@127.0.0.1:net_kernel<0.181.0>:cb_dist:info_msg:754]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2020-03-27T20:03:33.568Z,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Added connection {con,#Ref<0.1304447726.2365587457.30110>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2020-03-27T20:03:33.568Z,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Updated connection: {con,#Ref<0.1304447726.2365587457.30110>,
                                  inet_tcp_dist,<0.278.0>,
                                  #Ref<0.1304447726.2365587457.30114>}
[error_logger:info,2020-03-27T20:03:33.568Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.270.0>},
                       {name,start_couchdb_node},
                       {mfargs,{ns_server_nodes_sup,start_couchdb_node,[]}},
                       {restart_type,{permanent,5}},
                       {shutdown,86400000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:03:33.569Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{connect,normal,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2020-03-27T20:03:33.569Z,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Connection down: {con,#Ref<0.1304447726.2365587457.30110>,
                               inet_tcp_dist,<0.278.0>,
                               #Ref<0.1304447726.2365587457.30114>}
[error_logger:info,2020-03-27T20:03:33.569Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{'EXIT',<0.278.0>,shutdown}}
[error_logger:info,2020-03-27T20:03:33.569Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{net_kernel,913,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2020-03-27T20:03:33.569Z,ns_1@127.0.0.1:<0.276.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:166]ns_couchdb is not ready: {badrpc,nodedown}
[ns_server:debug,2020-03-27T20:03:33.575Z,ns_1@127.0.0.1:users_replicator<0.261.0>:doc_replicator:loop:60]doing replicate_newnodes_docs
[error_logger:info,2020-03-27T20:03:33.770Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{connect,normal,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2020-03-27T20:03:33.770Z,ns_1@127.0.0.1:net_kernel<0.181.0>:cb_dist:info_msg:754]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2020-03-27T20:03:33.770Z,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Added connection {con,#Ref<0.1304447726.2365587457.30126>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2020-03-27T20:03:33.770Z,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Updated connection: {con,#Ref<0.1304447726.2365587457.30126>,
                                  inet_tcp_dist,<0.281.0>,
                                  #Ref<0.1304447726.2365587457.30130>}
[ns_server:debug,2020-03-27T20:03:33.771Z,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Connection down: {con,#Ref<0.1304447726.2365587457.30126>,
                               inet_tcp_dist,<0.281.0>,
                               #Ref<0.1304447726.2365587457.30130>}
[error_logger:info,2020-03-27T20:03:33.771Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{'EXIT',<0.281.0>,shutdown}}
[error_logger:info,2020-03-27T20:03:33.771Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{net_kernel,913,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2020-03-27T20:03:33.771Z,ns_1@127.0.0.1:<0.276.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:166]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2020-03-27T20:03:33.971Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{connect,normal,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2020-03-27T20:03:33.971Z,ns_1@127.0.0.1:net_kernel<0.181.0>:cb_dist:info_msg:754]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2020-03-27T20:03:33.971Z,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Added connection {con,#Ref<0.1304447726.2365587458.29238>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2020-03-27T20:03:33.971Z,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Updated connection: {con,#Ref<0.1304447726.2365587458.29238>,
                                  inet_tcp_dist,<0.284.0>,
                                  #Ref<0.1304447726.2365587458.29242>}
[ns_server:debug,2020-03-27T20:03:33.972Z,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Connection down: {con,#Ref<0.1304447726.2365587458.29238>,
                               inet_tcp_dist,<0.284.0>,
                               #Ref<0.1304447726.2365587458.29242>}
[error_logger:info,2020-03-27T20:03:33.972Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{'EXIT',<0.284.0>,shutdown}}
[error_logger:info,2020-03-27T20:03:33.972Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{net_kernel,913,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2020-03-27T20:03:33.972Z,ns_1@127.0.0.1:<0.276.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:166]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2020-03-27T20:03:34.181Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{connect,normal,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2020-03-27T20:03:34.181Z,ns_1@127.0.0.1:net_kernel<0.181.0>:cb_dist:info_msg:754]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2020-03-27T20:03:34.181Z,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Added connection {con,#Ref<0.1304447726.2365587458.29253>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2020-03-27T20:03:34.181Z,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Updated connection: {con,#Ref<0.1304447726.2365587458.29253>,
                                  inet_tcp_dist,<0.287.0>,
                                  #Ref<0.1304447726.2365587458.29257>}
[ns_server:debug,2020-03-27T20:03:34.181Z,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Connection down: {con,#Ref<0.1304447726.2365587458.29253>,
                               inet_tcp_dist,<0.287.0>,
                               #Ref<0.1304447726.2365587458.29257>}
[error_logger:info,2020-03-27T20:03:34.181Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{'EXIT',<0.287.0>,shutdown}}
[error_logger:info,2020-03-27T20:03:34.181Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{net_kernel,913,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2020-03-27T20:03:34.181Z,ns_1@127.0.0.1:<0.276.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:166]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2020-03-27T20:03:34.382Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{connect,normal,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2020-03-27T20:03:34.382Z,ns_1@127.0.0.1:net_kernel<0.181.0>:cb_dist:info_msg:754]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2020-03-27T20:03:34.382Z,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Added connection {con,#Ref<0.1304447726.2365587458.29268>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2020-03-27T20:03:34.382Z,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Updated connection: {con,#Ref<0.1304447726.2365587458.29268>,
                                  inet_tcp_dist,<0.290.0>,
                                  #Ref<0.1304447726.2365587458.29272>}
[ns_server:debug,2020-03-27T20:03:34.382Z,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Connection down: {con,#Ref<0.1304447726.2365587458.29268>,
                               inet_tcp_dist,<0.290.0>,
                               #Ref<0.1304447726.2365587458.29272>}
[error_logger:info,2020-03-27T20:03:34.383Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{'EXIT',<0.290.0>,shutdown}}
[error_logger:info,2020-03-27T20:03:34.383Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{net_kernel,913,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2020-03-27T20:03:34.383Z,ns_1@127.0.0.1:<0.276.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:166]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2020-03-27T20:03:34.583Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{connect,normal,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2020-03-27T20:03:34.583Z,ns_1@127.0.0.1:net_kernel<0.181.0>:cb_dist:info_msg:754]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2020-03-27T20:03:34.583Z,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Added connection {con,#Ref<0.1304447726.2365587458.29283>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2020-03-27T20:03:34.583Z,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Updated connection: {con,#Ref<0.1304447726.2365587458.29283>,
                                  inet_tcp_dist,<0.293.0>,
                                  #Ref<0.1304447726.2365587457.30134>}
[ns_server:debug,2020-03-27T20:03:34.628Z,ns_1@127.0.0.1:<0.276.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:166]ns_couchdb is not ready: false
[ns_server:debug,2020-03-27T20:03:34.829Z,ns_1@127.0.0.1:<0.276.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:166]ns_couchdb is not ready: false
[ns_server:debug,2020-03-27T20:03:35.031Z,ns_1@127.0.0.1:<0.276.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:166]ns_couchdb is not ready: false
[error_logger:info,2020-03-27T20:03:35.669Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,kernel_safe_sup}
             started: [{pid,<0.298.0>},
                       {id,timer2_server},
                       {mfargs,{timer2,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:info,2020-03-27T20:03:35.869Z,ns_1@127.0.0.1:ns_couchdb_port<0.270.0>:ns_port_server:log:224]ns_couchdb<0.270.0>: Apache CouchDB  (LogLevel=info) is starting.

[ns_server:info,2020-03-27T20:03:36.219Z,ns_1@127.0.0.1:ns_couchdb_port<0.270.0>:ns_port_server:log:224]ns_couchdb<0.270.0>: Apache CouchDB has started. Time to relax.

[error_logger:info,2020-03-27T20:03:36.272Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.275.0>},
                       {name,wait_for_couchdb_node},
                       {mfargs,
                           {erlang,apply,
                               [#Fun<ns_server_nodes_sup.0.58023840>,[]]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-03-27T20:03:36.291Z,ns_1@127.0.0.1:ns_server_nodes_sup<0.209.0>:ns_storage_conf:setup_db_and_ix_paths:64]Initialize db_and_ix_paths variable with [{db_path,
                                           "/opt/couchbase/var/lib/couchbase/data"},
                                          {index_path,
                                           "/opt/couchbase/var/lib/couchbase/data"}]
[error_logger:info,2020-03-27T20:03:36.305Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.301.0>},
                       {name,ns_disksup},
                       {mfargs,{ns_disksup,start_link,[]}},
                       {restart_type,{permanent,4}},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:03:36.311Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.302.0>},
                       {name,diag_handler_worker},
                       {mfargs,{work_queue,start_link,[diag_handler_worker]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:info,2020-03-27T20:03:36.315Z,ns_1@127.0.0.1:ns_server_sup<0.300.0>:dir_size:start_link:39]Starting quick version of dir_size with program name: godu
[error_logger:info,2020-03-27T20:03:36.316Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.303.0>},
                       {name,dir_size},
                       {mfargs,{dir_size,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:03:36.325Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.304.0>},
                       {name,request_throttler},
                       {mfargs,{request_throttler,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:03:36.335Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.305.0>},
                       {name,ns_log},
                       {mfargs,{ns_log,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:03:36.335Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.306.0>},
                       {name,ns_crash_log_consumer},
                       {mfargs,{ns_log,start_link_crash_consumer,[]}},
                       {restart_type,{permanent,4}},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-03-27T20:03:36.343Z,ns_1@127.0.0.1:memcached_passwords<0.307.0>:memcached_cfg:init:62]Init config writer for memcached_passwords, "/opt/couchbase/var/lib/couchbase/isasl.pw"
[ns_server:debug,2020-03-27T20:03:36.345Z,ns_1@127.0.0.1:memcached_passwords<0.307.0>:memcached_cfg:write_cfg:118]Writing config file for: "/opt/couchbase/var/lib/couchbase/isasl.pw"
[ns_server:debug,2020-03-27T20:03:36.409Z,ns_1@127.0.0.1:users_storage<0.262.0>:replicated_dets:handle_call:302]Suspended by process <0.307.0>
[ns_server:debug,2020-03-27T20:03:36.409Z,ns_1@127.0.0.1:memcached_passwords<0.307.0>:replicated_dets:select_from_dets_locked:350]Starting select with {users_storage,[{{docv2,{auth,{'_',local}},'_','_'},
                                      [],
                                      ['$_']}],
                                    100}
[ns_server:debug,2020-03-27T20:03:36.409Z,ns_1@127.0.0.1:users_storage<0.262.0>:replicated_dets:handle_call:309]Released by process <0.307.0>
[ns_server:debug,2020-03-27T20:03:36.421Z,ns_1@127.0.0.1:memcached_refresh<0.213.0>:memcached_refresh:handle_cast:55]Refresh of isasl requested
[error_logger:info,2020-03-27T20:03:36.422Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.307.0>},
                       {name,memcached_passwords},
                       {mfargs,{memcached_passwords,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:warn,2020-03-27T20:03:36.431Z,ns_1@127.0.0.1:memcached_refresh<0.213.0>:ns_memcached:connect:1101]Unable to connect: {error,{badmatch,{error,econnrefused}}}.
[ns_server:debug,2020-03-27T20:03:36.431Z,ns_1@127.0.0.1:memcached_refresh<0.213.0>:memcached_refresh:handle_info:93]Refresh of [isasl] failed. Retry in 1000 ms.
[ns_server:debug,2020-03-27T20:03:36.434Z,ns_1@127.0.0.1:memcached_permissions<0.310.0>:memcached_cfg:init:62]Init config writer for memcached_permissions, "/opt/couchbase/var/lib/couchbase/config/memcached.rbac"
[ns_server:debug,2020-03-27T20:03:36.446Z,ns_1@127.0.0.1:memcached_permissions<0.310.0>:memcached_cfg:write_cfg:118]Writing config file for: "/opt/couchbase/var/lib/couchbase/config/memcached.rbac"
[ns_server:debug,2020-03-27T20:03:36.454Z,ns_1@127.0.0.1:users_storage<0.262.0>:replicated_dets:handle_call:302]Suspended by process <0.310.0>
[ns_server:debug,2020-03-27T20:03:36.454Z,ns_1@127.0.0.1:memcached_permissions<0.310.0>:replicated_dets:select_from_dets_locked:350]Starting select with {users_storage,[{{docv2,{user,{'_',local}},'_','_'},
                                      [],
                                      ['$_']}],
                                    100}
[ns_server:debug,2020-03-27T20:03:36.454Z,ns_1@127.0.0.1:users_storage<0.262.0>:replicated_dets:handle_call:309]Released by process <0.310.0>
[ns_server:info,2020-03-27T20:03:36.472Z,ns_1@127.0.0.1:ns_couchdb_port<0.270.0>:ns_port_server:log:224]ns_couchdb<0.270.0>: 187: Booted. Waiting for shutdown request
ns_couchdb<0.270.0>: working as port

[ns_server:debug,2020-03-27T20:03:36.476Z,ns_1@127.0.0.1:memcached_refresh<0.213.0>:memcached_refresh:handle_cast:55]Refresh of rbac requested
[error_logger:info,2020-03-27T20:03:36.477Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.310.0>},
                       {name,memcached_permissions},
                       {mfargs,{memcached_permissions,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:warn,2020-03-27T20:03:36.478Z,ns_1@127.0.0.1:memcached_refresh<0.213.0>:ns_memcached:connect:1101]Unable to connect: {error,{badmatch,{error,econnrefused}}}.
[ns_server:debug,2020-03-27T20:03:36.478Z,ns_1@127.0.0.1:memcached_refresh<0.213.0>:memcached_refresh:handle_info:93]Refresh of [rbac,isasl] failed. Retry in 1000 ms.
[error_logger:info,2020-03-27T20:03:36.485Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.313.0>},
                       {name,ns_email_alert},
                       {mfargs,{ns_email_alert,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:03:36.490Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_node_disco_sup}
             started: [{pid,<0.315.0>},
                       {id,ns_node_disco_events},
                       {mfargs,
                           {gen_event,start_link,
                               [{local,ns_node_disco_events}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-03-27T20:03:36.490Z,ns_1@127.0.0.1:ns_node_disco<0.316.0>:ns_node_disco:init:128]Initting ns_node_disco with []
[ns_server:debug,2020-03-27T20:03:36.490Z,ns_1@127.0.0.1:ns_cookie_manager<0.190.0>:ns_cookie_manager:do_cookie_sync:107]ns_cookie_manager do_cookie_sync
[user:info,2020-03-27T20:03:36.491Z,ns_1@127.0.0.1:ns_cookie_manager<0.190.0>:ns_cookie_manager:do_cookie_sync:128]Node 'ns_1@127.0.0.1' synchronized otp cookie {sanitized,
                                               <<"C4PeZIe8LLStUy+d1HIN638gh0wTZW33h1wdSpFkvzc=">>} from cluster
[ns_server:debug,2020-03-27T20:03:36.497Z,ns_1@127.0.0.1:<0.317.0>:ns_node_disco:do_nodes_wanted_updated_fun:214]ns_node_disco: nodes_wanted updated: ['ns_1@127.0.0.1'], with cookie: {sanitized,
                                                                       <<"C4PeZIe8LLStUy+d1HIN638gh0wTZW33h1wdSpFkvzc=">>}
[ns_server:debug,2020-03-27T20:03:36.508Z,ns_1@127.0.0.1:<0.317.0>:ns_node_disco:do_nodes_wanted_updated_fun:220]ns_node_disco: nodes_wanted pong: ['ns_1@127.0.0.1'], with cookie: {sanitized,
                                                                    <<"C4PeZIe8LLStUy+d1HIN638gh0wTZW33h1wdSpFkvzc=">>}
[error_logger:info,2020-03-27T20:03:36.508Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_node_disco_sup}
             started: [{pid,<0.316.0>},
                       {id,ns_node_disco},
                       {mfargs,{ns_node_disco,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:03:36.515Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_node_disco_sup}
             started: [{pid,<0.318.0>},
                       {id,ns_node_disco_log},
                       {mfargs,{ns_node_disco_log,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:03:36.521Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_node_disco_sup}
             started: [{pid,<0.319.0>},
                       {id,ns_node_disco_conf_events},
                       {mfargs,{ns_node_disco_conf_events,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:03:36.523Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_node_disco_sup}
             started: [{pid,<0.320.0>},
                       {id,ns_config_rep_merger},
                       {mfargs,{ns_config_rep,start_link_merger,[]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[ns_server:debug,2020-03-27T20:03:36.523Z,ns_1@127.0.0.1:ns_config_rep<0.321.0>:ns_config_rep:init:71]init pulling
[ns_server:debug,2020-03-27T20:03:36.523Z,ns_1@127.0.0.1:ns_config_rep<0.321.0>:ns_config_rep:init:73]init pushing
[ns_server:debug,2020-03-27T20:03:36.529Z,ns_1@127.0.0.1:ns_config_rep<0.321.0>:ns_config_rep:init:77]init reannouncing
[ns_server:debug,2020-03-27T20:03:36.530Z,ns_1@127.0.0.1:compiled_roles_cache<0.264.0>:versioned_cache:handle_info:92]Flushing cache compiled_roles_cache due to version change from undefined to {[6,
                                                                              5],
                                                                             {0,
                                                                              1500707660},
                                                                             {0,
                                                                              1500707660},
                                                                             false,
                                                                             []}
[ns_server:debug,2020-03-27T20:03:36.531Z,ns_1@127.0.0.1:ns_config_events<0.193.0>:ns_node_disco_conf_events:handle_event:44]ns_node_disco_conf_events config on nodes_wanted
[ns_server:debug,2020-03-27T20:03:36.531Z,ns_1@127.0.0.1:ns_cookie_manager<0.190.0>:ns_cookie_manager:do_cookie_sync:107]ns_cookie_manager do_cookie_sync
[ns_server:debug,2020-03-27T20:03:36.531Z,ns_1@127.0.0.1:<0.327.0>:ns_node_disco:do_nodes_wanted_updated_fun:214]ns_node_disco: nodes_wanted updated: ['ns_1@127.0.0.1'], with cookie: {sanitized,
                                                                       <<"C4PeZIe8LLStUy+d1HIN638gh0wTZW33h1wdSpFkvzc=">>}
[ns_server:debug,2020-03-27T20:03:36.531Z,ns_1@127.0.0.1:<0.327.0>:ns_node_disco:do_nodes_wanted_updated_fun:220]ns_node_disco: nodes_wanted pong: ['ns_1@127.0.0.1'], with cookie: {sanitized,
                                                                    <<"C4PeZIe8LLStUy+d1HIN638gh0wTZW33h1wdSpFkvzc=">>}
[ns_server:debug,2020-03-27T20:03:36.531Z,ns_1@127.0.0.1:ns_config_events<0.193.0>:ns_node_disco_conf_events:handle_event:50]ns_node_disco_conf_events config on otp
[ns_server:debug,2020-03-27T20:03:36.532Z,ns_1@127.0.0.1:ns_cookie_manager<0.190.0>:ns_cookie_manager:do_cookie_sync:107]ns_cookie_manager do_cookie_sync
[ns_server:debug,2020-03-27T20:03:36.534Z,ns_1@127.0.0.1:<0.328.0>:ns_node_disco:do_nodes_wanted_updated_fun:214]ns_node_disco: nodes_wanted updated: ['ns_1@127.0.0.1'], with cookie: {sanitized,
                                                                       <<"C4PeZIe8LLStUy+d1HIN638gh0wTZW33h1wdSpFkvzc=">>}
[ns_server:debug,2020-03-27T20:03:36.534Z,ns_1@127.0.0.1:<0.328.0>:ns_node_disco:do_nodes_wanted_updated_fun:220]ns_node_disco: nodes_wanted pong: ['ns_1@127.0.0.1'], with cookie: {sanitized,
                                                                    <<"C4PeZIe8LLStUy+d1HIN638gh0wTZW33h1wdSpFkvzc=">>}
[ns_server:debug,2020-03-27T20:03:36.538Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
alert_limits ->
[{max_overhead_perc,50},{max_disk_used,90},{max_indexer_ram,75}]
[error_logger:info,2020-03-27T20:03:36.539Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_node_disco_sup}
             started: [{pid,<0.321.0>},
                       {id,ns_config_rep},
                       {mfargs,{ns_config_rep,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-03-27T20:03:36.538Z,ns_1@127.0.0.1:ns_config_rep<0.321.0>:ns_config_rep:do_push_keys:321]Replicating some config keys ([alert_limits,audit,audit_decriptors,
                               auto_failover_cfg,auto_reprovision_cfg,
                               autocompaction,buckets,cbas_memory_quota,
                               cert_and_pkey,client_cert_auth,
                               cluster_compat_version,cluster_name,
                               drop_request_memory_threshold_mib,email_alerts,
                               fts_memory_quota,
                               index_aware_rebalance_disabled,
                               log_redaction_default_cfg,max_bucket_count,
                               memcached,memory_quota,nodes_wanted,otp,
                               password_policy,quorum_nodes,remote_clusters,
                               replication,rest,rest_creds,retry_rebalance,
                               scramsha_fallback_salt,secure_headers,
                               server_groups,set_view_update_daemon,
                               {couchdb,max_parallel_indexers},
                               {couchdb,max_parallel_replica_indexers},
                               {local_changes_count,
                                   <<"60a6bc3db77e6c7b91c556140dcfec71">>},
                               {metakv,<<"/eventing/settings/config">>},
                               {metakv,<<"/indexing/settings/config">>},
                               {metakv,<<"/query/settings/config">>},
                               {request_limit,capi},
                               {request_limit,rest},
                               {node,'ns_1@127.0.0.1',address_family},
                               {node,'ns_1@127.0.0.1',audit},
                               {node,'ns_1@127.0.0.1',capi_port},
                               {node,'ns_1@127.0.0.1',cbas_admin_port},
                               {node,'ns_1@127.0.0.1',cbas_cc_client_port},
                               {node,'ns_1@127.0.0.1',cbas_cc_cluster_port},
                               {node,'ns_1@127.0.0.1',cbas_cc_http_port},
                               {node,'ns_1@127.0.0.1',cbas_cluster_port},
                               {node,'ns_1@127.0.0.1',cbas_console_port},
                               {node,'ns_1@127.0.0.1',cbas_data_port},
                               {node,'ns_1@127.0.0.1',cbas_debug_port},
                               {node,'ns_1@127.0.0.1',cbas_dirs},
                               {node,'ns_1@127.0.0.1',cbas_http_port},
                               {node,'ns_1@127.0.0.1',cbas_messaging_port},
                               {node,'ns_1@127.0.0.1',
                                   cbas_metadata_callback_port},
                               {node,'ns_1@127.0.0.1',cbas_metadata_port},
                               {node,'ns_1@127.0.0.1',cbas_parent_port},
                               {node,'ns_1@127.0.0.1',cbas_replication_port},
                               {node,'ns_1@127.0.0.1',cbas_result_port},
                               {node,'ns_1@127.0.0.1',cbas_ssl_port},
                               {node,'ns_1@127.0.0.1',compaction_daemon},
                               {node,'ns_1@127.0.0.1',config_version},
                               {node,'ns_1@127.0.0.1',erl_external_listeners}]..)
[error_logger:info,2020-03-27T20:03:36.540Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.314.0>},
                       {name,ns_node_disco_sup},
                       {mfargs,{ns_node_disco_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[ns_server:debug,2020-03-27T20:03:36.540Z,ns_1@127.0.0.1:memcached_passwords<0.307.0>:memcached_cfg:write_cfg:118]Writing config file for: "/opt/couchbase/var/lib/couchbase/isasl.pw"
[ns_server:debug,2020-03-27T20:03:36.540Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
audit ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557537}}]},
 {enabled,[]},
 {disabled_users,[]},
 {auditd_enabled,false},
 {rotate_interval,86400},
 {rotate_size,20971520},
 {disabled,[]},
 {sync,[]},
 {log_path,"/opt/couchbase/var/lib/couchbase/logs"}]
[ns_server:debug,2020-03-27T20:03:36.544Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
audit_decriptors ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557537}}]},
 {8243,
  [{name,<<"mutate document">>},
   {description,<<"Document was mutated via the REST API">>},
   {enabled,true},
   {module,ns_server}]},
 {8255,
  [{name,<<"read document">>},
   {description,<<"Document was read via the REST API">>},
   {enabled,false},
   {module,ns_server}]},
 {8257,
  [{name,<<"alert email sent">>},
   {description,<<"An alert email was successfully sent">>},
   {enabled,true},
   {module,ns_server}]},
 {20480,
  [{name,<<"opened DCP connection">>},
   {description,<<"opened DCP connection">>},
   {enabled,true},
   {module,memcached}]},
 {20482,
  [{name,<<"external memcached bucket flush">>},
   {description,<<"External user flushed the content of a memcached bucket">>},
   {enabled,true},
   {module,memcached}]},
 {20483,
  [{name,<<"invalid packet">>},
   {description,<<"Rejected an invalid packet">>},
   {enabled,true},
   {module,memcached}]},
 {20485,
  [{name,<<"authentication succeeded">>},
   {description,<<"Authentication to the cluster succeeded">>},
   {enabled,false},
   {module,memcached}]},
 {20488,
  [{name,<<"document read">>},
   {description,<<"Document was read">>},
   {enabled,false},
   {module,memcached}]},
 {20489,
  [{name,<<"document locked">>},
   {description,<<"Document was locked">>},
   {enabled,false},
   {module,memcached}]},
 {20490,
  [{name,<<"document modify">>},
   {description,<<"Document was modified">>},
   {enabled,false},
   {module,memcached}]},
 {20491,
  [{name,<<"document delete">>},
   {description,<<"Document was deleted">>},
   {enabled,false},
   {module,memcached}]},
 {20492,
  [{name,<<"select bucket">>},
   {description,<<"The specified bucket was selected">>},
   {enabled,true},
   {module,memcached}]},
 {28672,
  [{name,<<"SELECT statement">>},
   {description,<<"A N1QL SELECT statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28673,
  [{name,<<"EXPLAIN statement">>},
   {description,<<"A N1QL EXPLAIN statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28674,
  [{name,<<"PREPARE statement">>},
   {description,<<"A N1QL PREPARE statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28675,
  [{name,<<"INFER statement">>},
   {description,<<"A N1QL INFER statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28676,
  [{name,<<"INSERT statement">>},
   {description,<<"A N1QL INSERT statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28677,
  [{name,<<"UPSERT statement">>},
   {description,<<"A N1QL UPSERT statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28678,
  [{name,<<"DELETE statement">>},
   {description,<<"A N1QL DELETE statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28679,
  [{name,<<"UPDATE statement">>},
   {description,<<"A N1QL UPDATE statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28680,
  [{name,<<"MERGE statement">>},
   {description,<<"A N1QL MERGE statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28681,
  [{name,<<"CREATE INDEX statement">>},
   {description,<<"A N1QL CREATE INDEX statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28682,
  [{name,<<"DROP INDEX statement">>},
   {description,<<"A N1QL DROP INDEX statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28683,
  [{name,<<"ALTER INDEX statement">>},
   {description,<<"A N1QL ALTER INDEX statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28684,
  [{name,<<"BUILD INDEX statement">>},
   {description,<<"A N1QL BUILD INDEX statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28685,
  [{name,<<"GRANT ROLE statement">>},
   {description,<<"A N1QL GRANT ROLE statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28686,
  [{name,<<"REVOKE ROLE statement">>},
   {description,<<"A N1QL REVOKE ROLE statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28687,
  [{name,<<"UNRECOGNIZED statement">>},
   {description,<<"An unrecognized statement was received by the N1QL query engine">>},
   {enabled,false},
   {module,n1ql}]},
 {28688,
  [{name,<<"CREATE PRIMARY INDEX statement">>},
   {description,<<"A N1QL CREATE PRIMARY INDEX statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28689,
  [{name,<<"/admin/stats API request">>},
   {description,<<"An HTTP request was made to the API at /admin/stats.">>},
   {enabled,false},
   {module,n1ql}]},
 {28690,
  [{name,<<"/admin/vitals API request">>},
   {description,<<"An HTTP request was made to the API at /admin/vitals.">>},
   {enabled,false},
   {module,n1ql}]},
 {28691,
  [{name,<<"/admin/prepareds API request">>},
   {description,<<"An HTTP request was made to the API at /admin/prepareds.">>},
   {enabled,false},
   {module,n1ql}]},
 {28692,
  [{name,<<"/admin/active_requests API request">>},
   {description,<<"An HTTP request was made to the API at /admin/active_requests.">>},
   {enabled,false},
   {module,n1ql}]},
 {28693,
  [{name,<<"/admin/indexes/prepareds API request">>},
   {description,<<"An HTTP request was made to the API at /admin/indexes/prepareds.">>},
   {enabled,false},
   {module,n1ql}]},
 {28694,
  [{name,<<"/admin/indexes/active_requests API request">>},
   {description,<<"An HTTP request was made to the API at /admin/indexes/active_requests.">>},
   {enabled,false},
   {module,n1ql}]},
 {28695,
  [{name,<<"/admin/indexes/completed_requests API request">>},
   {description,<<"An HTTP request was made to the API at /admin/indexes/completed_requests.">>},
   {enabled,false},
   {module,n1ql}]},
 {28697,
  [{name,<<"/admin/ping API request">>},
   {description,<<"An HTTP request was made to the API at /admin/ping.">>},
   {enabled,false},
   {module,n1ql}]},
 {28698,
  [{name,<<"/admin/config API request">>},
   {description,<<"An HTTP request was made to the API at /admin/config.">>},
   {enabled,false},
   {module,n1ql}]},
 {28699,
  [{name,<<"/admin/ssl_cert API request">>},
   {description,<<"An HTTP request was made to the API at /admin/ssl_cert.">>},
   {enabled,false},
   {module,n1ql}]},
 {28700,
  [{name,<<"/admin/settings API request">>},
   {description,<<"An HTTP request was made to the API at /admin/settings.">>},
   {enabled,false},
   {module,n1ql}]},
 {28701,
  [{name,<<"/admin/clusters API request">>},
   {description,<<"An HTTP request was made to the API at /admin/clusters.">>},
   {enabled,false},
   {module,n1ql}]},
 {28702,
  [{name,<<"/admin/completed_requests API request">>},
   {description,<<"An HTTP request was made to the API at /admin/completed_requests.">>},
   {enabled,false},
   {module,n1ql}]},
 {28704,
  [{name,<<"/admin/functions API request">>},
   {description,<<"An HTTP request was made to the API at /admin/functions.">>},
   {enabled,false},
   {module,n1ql}]},
 {28705,
  [{name,<<"/admin/indexes/functions API request">>},
   {description,<<"An HTTP request was made to the API at /admin/indexes/functions.">>},
   {enabled,false},
   {module,n1ql}]},
 {32768,
  [{name,<<"Create Function">>},
   {description,<<"Eventing function definition was created or updated">>},
   {enabled,true},
   {module,eventing}]},
 {32769,
  [{name,<<"Delete Function">>},
   {description,<<"Eventing function definition was deleted">>},
   {enabled,true},
   {module,eventing}]},
 {32770,
  [{name,<<"Fetch Functions">>},
   {description,<<"Eventing function definition was read">>},
   {enabled,false},
   {module,eventing}]},
 {32771,
  [{name,<<"List Deployed">>},
   {description,<<"Eventing deployed functions list was read">>},
   {enabled,false},
   {module,eventing}]},
 {32772,
  [{name,<<"Fetch Drafts">>},
   {description,<<"Eventing function draft definitions were read">>},
   {enabled,false},
   {module,eventing}]},
 {32773,
  [{name,<<"Delete Drafts">>},
   {description,<<"Eventing function draft definitions were deleted">>},
   {enabled,true},
   {module,eventing}]},
 {32774,
  [{name,<<"Save Draft">>},
   {description,<<"Save a draft definition to the store">>},
   {enabled,true},
   {module,eventing}]},
 {32775,
  [{name,<<"Start Debug">>},
   {description,<<"Start eventing function debugger">>},
   {enabled,true},
   {module,eventing}]},
 {32776,
  [{name,<<"Stop Debug">>},
   {description,<<"Stop eventing function debugger">>},
   {enabled,true},
   {module,eventing}]},
 {32777,
  [{name,<<"Start Tracing">>},
   {description,<<"Start tracing eventing function execution">>},
   {enabled,true},
   {module,eventing}]},
 {32778,
  [{name,<<"Stop Tracing">>},
   {description,<<"Stop tracing eventing function execution">>},
   {enabled,true},
   {module,eventing}]},
 {32779,
  [{name,<<"Set Settings">>},
   {description,<<"Save settings for a given app">>},
   {enabled,true},
   {module,eventing}]},
 {32780,
  [{name,<<"Fetch Config">>},
   {description,<<"Get config for eventing">>},
   {enabled,false},
   {module,eventing}]},
 {32781,
  [{name,<<"Save Config">>},
   {description,<<"Save config for eventing">>},
   {enabled,true},
   {module,eventing}]},
 {32782,
  [{name,<<"Cleanup Eventing">>},
   {description,<<"Clears up app definitions and settings from metakv">>},
   {enabled,true},
   {module,eventing}]},
 {32783,
  [{name,<<"Get Settings">>},
   {description,<<"Get settings for a given app">>},
   {enabled,false},
   {module,eventing}]},
 {32784,
  [{name,<<"Import Functions">>},
   {description,<<"Import a list of functions">>},
   {enabled,false},
   {module,eventing}]},
 {32785,
  [{name,<<"Export Functions">>},
   {description,<<"Export the list of functions">>},
   {enabled,false},
   {module,eventing}]},
 {32786,
  [{name,<<"List Running">>},
   {description,<<"Eventing running function list was read">>},
   {enabled,false},
   {module,eventing}]},
 {36865,
  [{name,<<"Service configuration change">>},
   {description,<<"A successful service configuration change was made.">>},
   {enabled,true},
   {module,analytics}]},
 {36866,
  [{name,<<"Node configuration change">>},
   {description,<<"A successful node configuration change was made.">>},
   {enabled,true},
   {module,analytics}]},
 {40960,
  [{name,<<"Create Design Doc">>},
   {description,<<"Design Doc is Created">>},
   {enabled,true},
   {module,view_engine}]},
 {40961,
  [{name,<<"Delete Design Doc">>},
   {description,<<"Design Doc is Deleted">>},
   {enabled,true},
   {module,view_engine}]},
 {40962,
  [{name,<<"Query DDoc Meta Data">>},
   {description,<<"Design Doc Meta Data Query Request">>},
   {enabled,true},
   {module,view_engine}]},
 {40963,
  [{name,<<"View Query">>},
   {description,<<"View Query Request">>},
   {enabled,false},
   {module,view_engine}]},
 {40964,
  [{name,<<"Update Design Doc">>},
   {description,<<"Design Doc is Updated">>},
   {enabled,true},
   {module,view_engine}]}]
[ns_server:debug,2020-03-27T20:03:36.548Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
auto_failover_cfg ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557537}}]},
 {enabled,true},
 {timeout,120},
 {count,0},
 {failover_on_data_disk_issues,[{enabled,false},{timePeriod,120}]},
 {failover_server_group,false},
 {max_count,1},
 {failed_over_server_groups,[]},
 {can_abort_rebalance,true}]
[ns_server:debug,2020-03-27T20:03:36.549Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
auto_reprovision_cfg ->
[{enabled,true},{max_nodes,1},{count,0}]
[ns_server:debug,2020-03-27T20:03:36.549Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
autocompaction ->
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}}]
[ns_server:debug,2020-03-27T20:03:36.549Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
buckets ->
[[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557537}}],{configs,[]}]
[ns_server:debug,2020-03-27T20:03:36.549Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
cbas_memory_quota ->
1024
[ns_server:debug,2020-03-27T20:03:36.549Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
cert_and_pkey ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557532}}]}|
 {<<"-----BEGIN CERTIFICATE-----\nMIIDAjCCAeqgAwIBAgIIFgBA0G6s0lwwDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciAzNDhlZDI3MDAeFw0xMzAxMDEwMDAwMDBaFw00\nOTEyMzEyMzU5NTlaMCQxIjAgBgNVBAMTGUNvdWNoYmFzZSBTZXJ2ZXIgMzQ4ZWQy\nNzAwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQDh0efJgvfpArvc3iI0\ncHa/zZhS9WoaoNHksFooYqy3AYiZR+8S3IgPS0nfcZGrIWJUPgokUqBqh6u59abc\nvCuyMwLNkkuLOkT6wzUztrAqoCoOEl4"...>>,
  <<"*****">>}]
[ns_server:debug,2020-03-27T20:03:36.549Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
client_cert_auth ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557537}}]},
 {state,"disable"},
 {prefixes,[]}]
[ns_server:debug,2020-03-27T20:03:36.549Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
cluster_compat_version ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{5,63752557537}}]},6,5]
[ns_server:debug,2020-03-27T20:03:36.549Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
cluster_name ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557980}}]},
 66,97,115,101]
[ns_server:debug,2020-03-27T20:03:36.549Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
drop_request_memory_threshold_mib ->
undefined
[ns_server:debug,2020-03-27T20:03:36.549Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
email_alerts ->
[{recipients,["root@localhost"]},
 {sender,"couchbase@localhost"},
 {enabled,false},
 {email_server,[{user,[]},
                {pass,"*****"},
                {host,"localhost"},
                {port,25},
                {encrypt,false}]},
 {alerts,[auto_failover_node,auto_failover_maximum_reached,
          auto_failover_other_nodes_down,auto_failover_cluster_too_small,
          auto_failover_disabled,ip,disk,overhead,ep_oom_errors,
          ep_item_commit_failed,audit_dropped_events,indexer_ram_max_usage,
          ep_clock_cas_drift_threshold_exceeded,communication_issue]}]
[ns_server:debug,2020-03-27T20:03:36.549Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
fts_memory_quota ->
256
[ns_server:debug,2020-03-27T20:03:36.549Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
index_aware_rebalance_disabled ->
false
[ns_server:debug,2020-03-27T20:03:36.549Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
log_redaction_default_cfg ->
[{redact_level,none}]
[ns_server:debug,2020-03-27T20:03:36.549Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
max_bucket_count ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557537}}]}|30]
[ns_server:debug,2020-03-27T20:03:36.549Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
memcached ->
[]
[ns_server:debug,2020-03-27T20:03:36.550Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
memory_quota ->
292
[ns_server:debug,2020-03-27T20:03:36.550Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
nodes_wanted ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557625}}]},
 'ns_1@127.0.0.1']
[ns_server:debug,2020-03-27T20:03:36.550Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
otp ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557536}}]},
 {cookie,{sanitized,<<"C4PeZIe8LLStUy+d1HIN638gh0wTZW33h1wdSpFkvzc=">>}}]
[ns_server:debug,2020-03-27T20:03:36.550Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
password_policy ->
[{min_length,6},{must_present,[]}]
[ns_server:debug,2020-03-27T20:03:36.550Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
quorum_nodes ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]},
 'ns_1@127.0.0.1']
[ns_server:debug,2020-03-27T20:03:36.550Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
remote_clusters ->
[]
[ns_server:debug,2020-03-27T20:03:36.550Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
replication ->
[{enabled,true}]
[ns_server:debug,2020-03-27T20:03:36.550Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
rest ->
[{port,8091}]
[ns_server:debug,2020-03-27T20:03:36.550Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
rest_creds ->
null
[ns_server:debug,2020-03-27T20:03:36.550Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
retry_rebalance ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557537}}]},
 {enabled,false},
 {after_time_period,300},
 {max_attempts,1}]
[ns_server:debug,2020-03-27T20:03:36.550Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
scramsha_fallback_salt ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557537}}]}|
 <<176,211,90,3,15,137,19,29,193,47,34,182>>]
[ns_server:debug,2020-03-27T20:03:36.550Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
secure_headers ->
[]
[ns_server:debug,2020-03-27T20:03:36.550Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
server_groups ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557625}}]},
 [{uuid,<<"0">>},{name,<<"Group 1">>},{nodes,['ns_1@127.0.0.1']}]]
[ns_server:debug,2020-03-27T20:03:36.550Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
set_view_update_daemon ->
[{update_interval,5000},
 {update_min_changes,5000},
 {replica_update_min_changes,5000}]
[ns_server:debug,2020-03-27T20:03:36.550Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{couchdb,max_parallel_indexers} ->
4
[ns_server:debug,2020-03-27T20:03:36.550Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{couchdb,max_parallel_replica_indexers} ->
2
[ns_server:debug,2020-03-27T20:03:36.550Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{local_changes_count,<<"60a6bc3db77e6c7b91c556140dcfec71">>} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{22,63752558613}}]}]
[ns_server:debug,2020-03-27T20:03:36.550Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{metakv,<<"/eventing/settings/config">>} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557537}}]}|
 <<"{\"ram_quota\":256}">>]
[ns_server:debug,2020-03-27T20:03:36.551Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{metakv,<<"/indexing/settings/config">>} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{4,63752557980}}]}|
 <<"{\"indexer.settings.compaction.days_of_week\":\"Sunday,Monday,Tuesday,Wednesday,Thursday,Friday,Saturday\",\"indexer.settings.compaction.interval\":\"00:00,00:00\",\"indexer.settings.compaction.compaction_mode\":\"circular\",\"indexer.settings.log_level\":\"info\",\"indexer.settings.persisted_snapshot.interval\":5000,\"indexer.settings.compaction.min_frag\":30,\"indexer.settings.inmemory_snapshot.interval\""...>>]
[ns_server:debug,2020-03-27T20:03:36.551Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{metakv,<<"/query/settings/config">>} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557537}}]}|
 <<"{\"timeout\":0,\"n1ql-feat-ctrl\":12,\"max-parallelism\":1,\"query.settings.curl_whitelist\":{\"all_access\":false,\"allowed_urls\":[],\"disallowed_urls\":[]},\"query.settings.tmp_space_dir\":\"/opt/couchbase/var/lib/couchbase/tmp\",\"completed-limit\":4000,\"prepared-limit\":16384,\"pipeline-batch\":16,\"pipeline-cap\":512,\"scan-cap\":512,\"loglevel\":\"info\",\"completed-threshold\":1000,\"query.settings.tmp_space_si"...>>]
[ns_server:debug,2020-03-27T20:03:36.551Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{request_limit,capi} ->
undefined
[ns_server:debug,2020-03-27T20:03:36.551Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{request_limit,rest} ->
undefined
[ns_server:debug,2020-03-27T20:03:36.551Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',address_family} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|inet]
[ns_server:debug,2020-03-27T20:03:36.551Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',audit} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}]
[ns_server:debug,2020-03-27T20:03:36.551Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',capi_port} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|8092]
[ns_server:debug,2020-03-27T20:03:36.551Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',cbas_admin_port} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|9110]
[ns_server:debug,2020-03-27T20:03:36.551Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',cbas_cc_client_port} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|9113]
[ns_server:debug,2020-03-27T20:03:36.551Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',cbas_cc_cluster_port} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|9112]
[ns_server:debug,2020-03-27T20:03:36.551Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',cbas_cc_http_port} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|9111]
[ns_server:debug,2020-03-27T20:03:36.551Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',cbas_cluster_port} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|9115]
[ns_server:debug,2020-03-27T20:03:36.551Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',cbas_console_port} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|9114]
[ns_server:debug,2020-03-27T20:03:36.551Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',cbas_data_port} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|9116]
[ns_server:debug,2020-03-27T20:03:36.551Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',cbas_debug_port} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|-1]
[ns_server:debug,2020-03-27T20:03:36.551Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',cbas_dirs} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]},
 "/opt/couchbase/var/lib/couchbase/data"]
[ns_server:debug,2020-03-27T20:03:36.551Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',cbas_http_port} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|8095]
[ns_server:debug,2020-03-27T20:03:36.551Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',cbas_messaging_port} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|9118]
[ns_server:debug,2020-03-27T20:03:36.552Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',cbas_metadata_callback_port} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|9119]
[ns_server:debug,2020-03-27T20:03:36.552Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',cbas_metadata_port} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|9121]
[ns_server:debug,2020-03-27T20:03:36.552Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',cbas_parent_port} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|9122]
[ns_server:debug,2020-03-27T20:03:36.552Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',cbas_replication_port} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|9120]
[ns_server:debug,2020-03-27T20:03:36.552Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',cbas_result_port} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|9117]
[ns_server:debug,2020-03-27T20:03:36.552Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',cbas_ssl_port} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|18095]
[ns_server:debug,2020-03-27T20:03:36.552Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',compaction_daemon} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]},
 {check_interval,30},
 {min_db_file_size,131072},
 {min_view_file_size,20971520}]
[ns_server:debug,2020-03-27T20:03:36.552Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',config_version} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|{6,5}]
[ns_server:debug,2020-03-27T20:03:36.552Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',erl_external_listeners} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]},
 {inet,false},
 {inet6,false}]
[ns_server:debug,2020-03-27T20:03:36.552Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',eventing_debug_port} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|9140]
[ns_server:debug,2020-03-27T20:03:36.552Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',eventing_dir} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]},
 47,111,112,116,47,99,111,117,99,104,98,97,115,101,47,118,97,114,47,108,105,
 98,47,99,111,117,99,104,98,97,115,101,47,100,97,116,97]
[ns_server:debug,2020-03-27T20:03:36.552Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',eventing_http_port} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|8096]
[ns_server:debug,2020-03-27T20:03:36.552Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',eventing_https_port} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|18096]
[ns_server:debug,2020-03-27T20:03:36.552Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',fts_grpc_port} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|9130]
[ns_server:debug,2020-03-27T20:03:36.552Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',fts_grpc_ssl_port} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|19130]
[ns_server:debug,2020-03-27T20:03:36.552Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',fts_http_port} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|8094]
[ns_server:debug,2020-03-27T20:03:36.552Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',fts_ssl_port} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|18094]
[ns_server:debug,2020-03-27T20:03:36.553Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',indexer_admin_port} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|9100]
[ns_server:debug,2020-03-27T20:03:36.553Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',indexer_http_port} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|9102]
[ns_server:debug,2020-03-27T20:03:36.553Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',indexer_https_port} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|19102]
[ns_server:debug,2020-03-27T20:03:36.553Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',indexer_scan_port} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|9101]
[ns_server:debug,2020-03-27T20:03:36.553Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',indexer_stcatchup_port} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|9104]
[ns_server:debug,2020-03-27T20:03:36.553Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',indexer_stinit_port} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|9103]
[ns_server:debug,2020-03-27T20:03:36.553Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',indexer_stmaint_port} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|9105]
[ns_server:debug,2020-03-27T20:03:36.553Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',is_enterprise} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|true]
[ns_server:debug,2020-03-27T20:03:36.553Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',isasl} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]},
 {path,"/opt/couchbase/var/lib/couchbase/isasl.pw"}]
[ns_server:debug,2020-03-27T20:03:36.553Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',membership} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
 active]
[ns_server:debug,2020-03-27T20:03:36.553Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',memcached} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]},
 {port,11210},
 {dedicated_port,11209},
 {dedicated_ssl_port,11206},
 {ssl_port,11207},
 {admin_user,"@ns_server"},
 {other_users,["@cbq-engine","@projector","@goxdcr","@index","@fts",
               "@eventing","@cbas"]},
 {admin_pass,"*****"},
 {engines,[{membase,[{engine,"/opt/couchbase/lib/memcached/ep.so"},
                     {static_config_string,"failpartialwarmup=false"}]},
           {memcached,[{engine,"/opt/couchbase/lib/memcached/default_engine.so"},
                       {static_config_string,"vb0=true"}]}]},
 {config_path,"/opt/couchbase/var/lib/couchbase/config/memcached.json"},
 {audit_file,"/opt/couchbase/var/lib/couchbase/config/audit.json"},
 {rbac_file,"/opt/couchbase/var/lib/couchbase/config/memcached.rbac"},
 {log_path,"/opt/couchbase/var/lib/couchbase/logs"},
 {log_prefix,"memcached.log"},
 {log_generations,20},
 {log_cyclesize,10485760},
 {log_sleeptime,19},
 {log_rotation_period,39003}]
[ns_server:debug,2020-03-27T20:03:36.554Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',memcached_config} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
 {[{interfaces,
    {memcached_config_mgr,omit_missing_mcd_ports,
     [{[{host,<<"*">>},
        {port,port},
        {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
        {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]},
      {[{host,<<"*">>},
        {port,dedicated_port},
        {system,true},
        {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
        {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]},
      {[{host,<<"*">>},
        {port,ssl_port},
        {ssl,
         {[{key,
            <<"/opt/couchbase/var/lib/couchbase/config/memcached-key.pem">>},
           {cert,
            <<"/opt/couchbase/var/lib/couchbase/config/memcached-cert.pem">>}]}},
        {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
        {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]},
      {[{host,<<"*">>},
        {port,dedicated_ssl_port},
        {system,true},
        {ssl,
         {[{key,
            <<"/opt/couchbase/var/lib/couchbase/config/memcached-key.pem">>},
           {cert,
            <<"/opt/couchbase/var/lib/couchbase/config/memcached-cert.pem">>}]}},
        {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
        {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]}]}},
   {ssl_cipher_list,{memcached_config_mgr,get_ssl_cipher_list,[]}},
   {ssl_cipher_order,{memcached_config_mgr,get_ssl_cipher_order,[]}},
   {client_cert_auth,{memcached_config_mgr,client_cert_auth,[]}},
   {ssl_minimum_protocol,{memcached_config_mgr,ssl_minimum_protocol,[]}},
   {connection_idle_time,connection_idle_time},
   {privilege_debug,privilege_debug},
   {breakpad,
    {[{enabled,breakpad_enabled},
      {minidump_dir,{memcached_config_mgr,get_minidump_dir,[]}}]}},
   {opentracing,
    {[{enabled,opentracing_enabled},
      {module,{"~s",[opentracing_module]}},
      {config,{"~s",[opentracing_config]}}]}},
   {admin,{"~s",[admin_user]}},
   {verbosity,verbosity},
   {audit_file,{"~s",[audit_file]}},
   {rbac_file,{"~s",[rbac_file]}},
   {dedupe_nmvb_maps,dedupe_nmvb_maps},
   {tracing_enabled,tracing_enabled},
   {datatype_snappy,{memcached_config_mgr,is_snappy_enabled,[]}},
   {xattr_enabled,true},
   {scramsha_fallback_salt,{memcached_config_mgr,get_fallback_salt,[]}},
   {collections_enabled,{memcached_config_mgr,collections_enabled,[]}},
   {max_connections,max_connections},
   {system_connections,system_connections},
   {num_reader_threads,num_reader_threads},
   {num_writer_threads,num_writer_threads},
   {logger,
    {[{filename,{"~s/~s",[log_path,log_prefix]}},
      {cyclesize,log_cyclesize},
      {sleeptime,log_sleeptime}]}},
   {external_auth_service,{memcached_config_mgr,get_external_auth_service,[]}},
   {active_external_users_push_interval,
    {memcached_config_mgr,get_external_users_push_interval,[]}}]}]
[ns_server:debug,2020-03-27T20:03:36.554Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',memcached_dedicated_ssl_port} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|11206]
[ns_server:debug,2020-03-27T20:03:36.554Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',memcached_defaults} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]},
 {max_connections,65000},
 {system_connections,5000},
 {connection_idle_time,0},
 {verbosity,0},
 {privilege_debug,false},
 {opentracing_enabled,false},
 {opentracing_module,[]},
 {opentracing_config,[]},
 {breakpad_enabled,true},
 {breakpad_minidump_dir_path,"/opt/couchbase/var/lib/couchbase/crash"},
 {dedupe_nmvb_maps,false},
 {tracing_enabled,true},
 {datatype_snappy,true},
 {num_reader_threads,<<"default">>},
 {num_writer_threads,<<"default">>}]
[ns_server:debug,2020-03-27T20:03:36.554Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',moxi} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]},
 {port,0}]
[ns_server:debug,2020-03-27T20:03:36.555Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',node_encryption} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|false]
[ns_server:debug,2020-03-27T20:03:36.555Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',ns_log} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]},
 {filename,"/opt/couchbase/var/lib/couchbase/ns_log"}]
[ns_server:debug,2020-03-27T20:03:36.555Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',port_servers} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}]
[ns_server:debug,2020-03-27T20:03:36.555Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',projector_port} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|9999]
[ns_server:debug,2020-03-27T20:03:36.555Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',projector_ssl_port} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|9999]
[ns_server:debug,2020-03-27T20:03:36.555Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',query_port} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|8093]
[ns_server:debug,2020-03-27T20:03:36.555Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',rest} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]},
 {port,8091},
 {port_meta,global}]
[ns_server:debug,2020-03-27T20:03:36.555Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',saslauthd_enabled} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|true]
[ns_server:debug,2020-03-27T20:03:36.555Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',ssl_capi_port} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|18092]
[ns_server:debug,2020-03-27T20:03:36.555Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',ssl_query_port} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|18093]
[ns_server:debug,2020-03-27T20:03:36.555Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',ssl_rest_port} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|18091]
[ns_server:debug,2020-03-27T20:03:36.555Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',uuid} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
 <<"60a6bc3db77e6c7b91c556140dcfec71">>]
[ns_server:debug,2020-03-27T20:03:36.555Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',xdcr_rest_port} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|9998]
[ns_server:debug,2020-03-27T20:03:36.555Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',{project_intact,is_vulnerable}} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|false]
[error_logger:info,2020-03-27T20:03:36.595Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.333.0>},
                       {name,vbucket_map_mirror},
                       {mfargs,{vbucket_map_mirror,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[ns_server:debug,2020-03-27T20:03:36.603Z,ns_1@127.0.0.1:ns_ssl_services_setup<0.216.0>:ns_ssl_services_setup:trigger_ssl_reload:594]Notify services [capi_ssl_service] about secure_headers_changed change
[ns_server:debug,2020-03-27T20:03:36.603Z,ns_1@127.0.0.1:ns_ssl_services_setup<0.216.0>:ns_ssl_services_setup:notify_services:740]Going to notify following services: [capi_ssl_service]
[ns_server:info,2020-03-27T20:03:36.624Z,ns_1@127.0.0.1:<0.338.0>:ns_ssl_services_setup:notify_service:772]Successfully notified service capi_ssl_service
[ns_server:info,2020-03-27T20:03:36.625Z,ns_1@127.0.0.1:ns_ssl_services_setup<0.216.0>:ns_ssl_services_setup:notify_services:756]Succesfully notified services [capi_ssl_service]
[error_logger:info,2020-03-27T20:03:36.629Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.339.0>},
                       {name,bucket_info_cache},
                       {mfargs,{bucket_info_cache,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:03:36.629Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.342.0>},
                       {name,ns_tick_event},
                       {mfargs,{gen_event,start_link,[{local,ns_tick_event}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:03:36.630Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.343.0>},
                       {name,buckets_events},
                       {mfargs,
                           {gen_event,start_link,[{local,buckets_events}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:03:36.630Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.344.0>},
                       {name,ns_stats_event},
                       {mfargs,
                           {gen_event,start_link,[{local,ns_stats_event}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-03-27T20:03:36.645Z,ns_1@127.0.0.1:users_storage<0.262.0>:replicated_dets:handle_call:302]Suspended by process <0.307.0>
[ns_server:debug,2020-03-27T20:03:36.645Z,ns_1@127.0.0.1:memcached_passwords<0.307.0>:replicated_dets:select_from_dets_locked:350]Starting select with {users_storage,[{{docv2,{auth,{'_',local}},'_','_'},
                                      [],
                                      ['$_']}],
                                    100}
[ns_server:debug,2020-03-27T20:03:36.645Z,ns_1@127.0.0.1:users_storage<0.262.0>:replicated_dets:handle_call:309]Released by process <0.307.0>
[error_logger:info,2020-03-27T20:03:36.657Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.345.0>},
                       {name,samples_loader_tasks},
                       {mfargs,{samples_loader_tasks,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-03-27T20:03:36.676Z,ns_1@127.0.0.1:memcached_refresh<0.213.0>:memcached_refresh:handle_cast:55]Refresh of isasl requested
[ns_server:warn,2020-03-27T20:03:36.679Z,ns_1@127.0.0.1:memcached_refresh<0.213.0>:ns_memcached:connect:1101]Unable to connect: {error,{badmatch,{error,econnrefused}}}.
[ns_server:debug,2020-03-27T20:03:36.679Z,ns_1@127.0.0.1:memcached_refresh<0.213.0>:memcached_refresh:handle_info:93]Refresh of [rbac,isasl] failed. Retry in 1000 ms.
[error_logger:info,2020-03-27T20:03:36.696Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_heart_sup}
             started: [{pid,<0.347.0>},
                       {id,ns_heart},
                       {mfargs,{ns_heart,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:03:36.696Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_heart_sup}
             started: [{pid,<0.350.0>},
                       {id,ns_heart_slow_updater},
                       {mfargs,{ns_heart,start_link_slow_updater,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:03:36.701Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.346.0>},
                       {name,ns_heart_sup},
                       {mfargs,{ns_heart_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-03-27T20:03:36.702Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_doctor_sup}
             started: [{pid,<0.354.0>},
                       {id,ns_doctor_events},
                       {mfargs,
                           {gen_event,start_link,[{local,ns_doctor_events}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-03-27T20:03:36.712Z,ns_1@127.0.0.1:ns_heart<0.347.0>:ns_heart:grab_latest_stats:263]Ignoring failure to grab "@system" stats:
{'EXIT',{badarg,[{ets,last,['stats_archiver-@system-minute'],[]},
                 {stats_archiver,latest_sample,2,
                                 [{file,"src/stats_archiver.erl"},{line,120}]},
                 {ns_heart,grab_latest_stats,1,
                           [{file,"src/ns_heart.erl"},{line,259}]},
                 {ns_heart,'-current_status_slow_inner/0-lc$^0/1-0-',1,
                           [{file,"src/ns_heart.erl"},{line,282}]},
                 {ns_heart,current_status_slow_inner,0,
                           [{file,"src/ns_heart.erl"},{line,282}]},
                 {ns_heart,current_status_slow,1,
                           [{file,"src/ns_heart.erl"},{line,250}]},
                 {ns_heart,update_current_status,1,
                           [{file,"src/ns_heart.erl"},{line,187}]},
                 {ns_heart,handle_info,2,
                           [{file,"src/ns_heart.erl"},{line,118}]}]}}

[ns_server:debug,2020-03-27T20:03:36.713Z,ns_1@127.0.0.1:ns_heart<0.347.0>:ns_heart:grab_latest_stats:263]Ignoring failure to grab "@system-processes" stats:
{'EXIT',{badarg,[{ets,last,['stats_archiver-@system-processes-minute'],[]},
                 {stats_archiver,latest_sample,2,
                                 [{file,"src/stats_archiver.erl"},{line,120}]},
                 {ns_heart,grab_latest_stats,1,
                           [{file,"src/ns_heart.erl"},{line,259}]},
                 {ns_heart,'-current_status_slow_inner/0-lc$^0/1-0-',1,
                           [{file,"src/ns_heart.erl"},{line,282}]},
                 {ns_heart,'-current_status_slow_inner/0-lc$^0/1-0-',1,
                           [{file,"src/ns_heart.erl"},{line,282}]},
                 {ns_heart,current_status_slow_inner,0,
                           [{file,"src/ns_heart.erl"},{line,282}]},
                 {ns_heart,current_status_slow,1,
                           [{file,"src/ns_heart.erl"},{line,250}]},
                 {ns_heart,update_current_status,1,
                           [{file,"src/ns_heart.erl"},{line,187}]}]}}

[ns_server:debug,2020-03-27T20:03:36.727Z,ns_1@127.0.0.1:<0.352.0>:restartable:start_child:98]Started child process <0.353.0>
  MFA: {ns_doctor_sup,start_link,[]}
[error_logger:info,2020-03-27T20:03:36.726Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_doctor_sup}
             started: [{pid,<0.356.0>},
                       {id,ns_doctor},
                       {mfargs,{ns_doctor,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:03:36.730Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.352.0>},
                       {name,ns_doctor_sup},
                       {mfargs,
                           {restartable,start_link,
                               [{ns_doctor_sup,start_link,[]},infinity]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-03-27T20:03:36.730Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.359.0>},
                       {name,master_activity_events},
                       {mfargs,
                           {gen_event,start_link,
                               [{local,master_activity_events}]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:03:36.759Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.360.0>},
                       {name,xdcr_ckpt_store},
                       {mfargs,{simple_store,start_link,[xdcr_ckpt_data]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:03:36.760Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.361.0>},
                       {name,metakv_worker},
                       {mfargs,{work_queue,start_link,[metakv_worker]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:03:36.760Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.362.0>},
                       {name,index_events},
                       {mfargs,{gen_event,start_link,[{local,index_events}]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:03:36.760Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.363.0>},
                       {name,index_settings_manager},
                       {mfargs,{index_settings_manager,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:03:36.767Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.365.0>},
                       {name,query_settings_manager},
                       {mfargs,{query_settings_manager,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:03:36.780Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.367.0>},
                       {name,eventing_settings_manager},
                       {mfargs,{eventing_settings_manager,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:03:36.780Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.369.0>},
                       {name,audit_events},
                       {mfargs,{gen_event,start_link,[{local,audit_events}]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:03:36.811Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,menelaus_sup}
             started: [{pid,<0.372.0>},
                       {id,menelaus_ui_auth},
                       {mfargs,{menelaus_ui_auth,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:03:36.812Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,menelaus_sup}
             started: [{pid,<0.374.0>},
                       {id,scram_sha},
                       {mfargs,{scram_sha,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[ns_server:debug,2020-03-27T20:03:36.814Z,ns_1@127.0.0.1:ns_heart<0.347.0>:goxdcr_rest:get_from_goxdcr:140]Goxdcr is temporary not available. Return empty list.
[error_logger:info,2020-03-27T20:03:36.816Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,menelaus_sup}
             started: [{pid,<0.375.0>},
                       {id,menelaus_local_auth},
                       {mfargs,{menelaus_local_auth,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[ns_server:debug,2020-03-27T20:03:36.827Z,ns_1@127.0.0.1:ns_heart<0.347.0>:cluster_logs_collection_task:maybe_build_cluster_logs_task:46]Ignoring exception trying to read cluster_logs_collection_task_status table: error:badarg
[error_logger:info,2020-03-27T20:03:36.835Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,menelaus_sup}
             started: [{pid,<0.376.0>},
                       {id,menelaus_web_cache},
                       {mfargs,{menelaus_web_cache,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:03:36.843Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,menelaus_sup}
             started: [{pid,<0.377.0>},
                       {id,menelaus_stats_gatherer},
                       {mfargs,{menelaus_stats_gatherer,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:03:36.844Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,menelaus_sup}
             started: [{pid,<0.378.0>},
                       {id,json_rpc_events},
                       {mfargs,
                           {gen_event,start_link,[{local,json_rpc_events}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:info,2020-03-27T20:03:36.850Z,ns_1@127.0.0.1:<0.380.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for cbas
[ns_server:info,2020-03-27T20:03:36.850Z,ns_1@127.0.0.1:<0.380.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for eventing
[ns_server:info,2020-03-27T20:03:36.850Z,ns_1@127.0.0.1:<0.380.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for fts
[ns_server:info,2020-03-27T20:03:36.852Z,ns_1@127.0.0.1:<0.380.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for n1ql
[error_logger:info,2020-03-27T20:03:36.853Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {<0.380.0>,menelaus_web}
             started: [{pid,<0.381.0>},
                       {id,menelaus_web_ipv4},
                       {mfargs,
                           {menelaus_web,http_server,
                               [[{ip,"0.0.0.0"},{name,menelaus_web_ipv4}]]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[ns_server:info,2020-03-27T20:03:36.856Z,ns_1@127.0.0.1:<0.380.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for cbas
[ns_server:info,2020-03-27T20:03:36.857Z,ns_1@127.0.0.1:<0.380.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for eventing
[ns_server:info,2020-03-27T20:03:36.857Z,ns_1@127.0.0.1:<0.380.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for fts
[ns_server:info,2020-03-27T20:03:36.858Z,ns_1@127.0.0.1:<0.380.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for n1ql
[error_logger:info,2020-03-27T20:03:36.861Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {<0.380.0>,menelaus_web}
             started: [{pid,<0.403.0>},
                       {id,menelaus_web_ipv6},
                       {mfargs,
                           {menelaus_web,http_server,
                               [[{ip,"::"},{name,menelaus_web_ipv6}]]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[ns_server:debug,2020-03-27T20:03:36.860Z,ns_1@127.0.0.1:<0.379.0>:restartable:start_child:98]Started child process <0.380.0>
  MFA: {menelaus_web,start_link,[]}
[error_logger:info,2020-03-27T20:03:36.861Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,menelaus_sup}
             started: [{pid,<0.379.0>},
                       {id,menelaus_web},
                       {mfargs,
                           {restartable,start_link,
                               [{menelaus_web,start_link,[]},infinity]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-03-27T20:03:36.871Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,menelaus_sup}
             started: [{pid,<0.422.0>},
                       {id,menelaus_event},
                       {mfargs,{menelaus_event,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:03:36.887Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,menelaus_sup}
             started: [{pid,<0.424.0>},
                       {id,hot_keys_keeper},
                       {mfargs,{hot_keys_keeper,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[ns_server:debug,2020-03-27T20:03:36.891Z,ns_1@127.0.0.1:ns_heart_slow_status_updater<0.350.0>:ns_heart:grab_latest_stats:263]Ignoring failure to grab "@system" stats:
{'EXIT',{badarg,[{ets,last,['stats_archiver-@system-minute'],[]},
                 {stats_archiver,latest_sample,2,
                                 [{file,"src/stats_archiver.erl"},{line,120}]},
                 {ns_heart,grab_latest_stats,1,
                           [{file,"src/ns_heart.erl"},{line,259}]},
                 {ns_heart,'-current_status_slow_inner/0-lc$^0/1-0-',1,
                           [{file,"src/ns_heart.erl"},{line,282}]},
                 {ns_heart,current_status_slow_inner,0,
                           [{file,"src/ns_heart.erl"},{line,282}]},
                 {ns_heart,current_status_slow,1,
                           [{file,"src/ns_heart.erl"},{line,250}]},
                 {ns_heart,slow_updater_loop,0,
                           [{file,"src/ns_heart.erl"},{line,244}]},
                 {proc_lib,init_p_do_apply,3,
                           [{file,"proc_lib.erl"},{line,247}]}]}}

[ns_server:debug,2020-03-27T20:03:36.891Z,ns_1@127.0.0.1:ns_heart_slow_status_updater<0.350.0>:ns_heart:grab_latest_stats:263]Ignoring failure to grab "@system-processes" stats:
{'EXIT',{badarg,[{ets,last,['stats_archiver-@system-processes-minute'],[]},
                 {stats_archiver,latest_sample,2,
                                 [{file,"src/stats_archiver.erl"},{line,120}]},
                 {ns_heart,grab_latest_stats,1,
                           [{file,"src/ns_heart.erl"},{line,259}]},
                 {ns_heart,'-current_status_slow_inner/0-lc$^0/1-0-',1,
                           [{file,"src/ns_heart.erl"},{line,282}]},
                 {ns_heart,'-current_status_slow_inner/0-lc$^0/1-0-',1,
                           [{file,"src/ns_heart.erl"},{line,282}]},
                 {ns_heart,current_status_slow_inner,0,
                           [{file,"src/ns_heart.erl"},{line,282}]},
                 {ns_heart,current_status_slow,1,
                           [{file,"src/ns_heart.erl"},{line,250}]},
                 {ns_heart,slow_updater_loop,0,
                           [{file,"src/ns_heart.erl"},{line,244}]}]}}

[ns_server:debug,2020-03-27T20:03:36.892Z,ns_1@127.0.0.1:ns_heart_slow_status_updater<0.350.0>:goxdcr_rest:get_from_goxdcr:140]Goxdcr is temporary not available. Return empty list.
[ns_server:debug,2020-03-27T20:03:36.892Z,ns_1@127.0.0.1:ns_heart_slow_status_updater<0.350.0>:cluster_logs_collection_task:maybe_build_cluster_logs_task:46]Ignoring exception trying to read cluster_logs_collection_task_status table: error:badarg
[error_logger:info,2020-03-27T20:03:36.893Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,menelaus_sup}
             started: [{pid,<0.434.0>},
                       {id,menelaus_web_alerts_srv},
                       {mfargs,{menelaus_web_alerts_srv,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:03:36.905Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,menelaus_sup}
             started: [{pid,<0.444.0>},
                       {id,menelaus_cbauth},
                       {mfargs,{menelaus_cbauth,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[user:info,2020-03-27T20:03:36.917Z,ns_1@127.0.0.1:ns_server_sup<0.300.0>:menelaus_sup:start_link:48]Couchbase Server has started on web port 8091 on node 'ns_1@127.0.0.1'. Version: "6.5.0-4960-enterprise".
[error_logger:info,2020-03-27T20:03:36.917Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.370.0>},
                       {name,menelaus},
                       {mfargs,{menelaus_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-03-27T20:03:36.917Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.450.0>},
                       {name,ns_ports_setup},
                       {mfargs,{ns_ports_setup,start,[]}},
                       {restart_type,{permanent,4}},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:03:36.925Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,service_agent_sup}
             started: [{pid,<0.454.0>},
                       {id,service_agent_children_sup},
                       {mfargs,
                           {supervisor,start_link,
                               [{local,service_agent_children_sup},
                                service_agent_sup,child]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-03-27T20:03:36.926Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,service_agent_sup}
             started: [{pid,<0.455.0>},
                       {id,service_agent_worker},
                       {mfargs,
                           {erlang,apply,
                               [#Fun<service_agent_sup.0.107373856>,[]]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:03:36.926Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.453.0>},
                       {name,service_agent_sup},
                       {mfargs,{service_agent_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[ns_server:debug,2020-03-27T20:03:36.954Z,ns_1@127.0.0.1:ns_ports_setup<0.450.0>:ns_ports_manager:set_dynamic_children:54]Setting children [memcached,saslauthd_port,goxdcr]
[error_logger:info,2020-03-27T20:03:36.975Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.457.0>},
                       {name,ns_memcached_sockets_pool},
                       {mfargs,{ns_memcached_sockets_pool,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-03-27T20:03:37.048Z,ns_1@127.0.0.1:memcached_auth_server<0.458.0>:memcached_auth_server:reconnect:233]Skipping creation of 'Auth provider' connection because external users are disabled
[error_logger:info,2020-03-27T20:03:37.048Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.458.0>},
                       {name,memcached_auth_server},
                       {mfargs,{memcached_auth_server,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-03-27T20:03:37.049Z,ns_1@127.0.0.1:ns_audit_cfg<0.461.0>:ns_audit_cfg:write_audit_json:259]Writing new content to "/opt/couchbase/var/lib/couchbase/config/audit.json", Params [{descriptors_path,
                                                                                      "/opt/couchbase/etc/security"},
                                                                                     {version,
                                                                                      2},
                                                                                     {uuid,
                                                                                      "48537283"},
                                                                                     {event_states,
                                                                                      {[]}},
                                                                                     {filtering_enabled,
                                                                                      true},
                                                                                     {disabled_userids,
                                                                                      []},
                                                                                     {auditd_enabled,
                                                                                      false},
                                                                                     {log_path,
                                                                                      "/opt/couchbase/var/lib/couchbase/logs"},
                                                                                     {rotate_interval,
                                                                                      86400},
                                                                                     {rotate_size,
                                                                                      20971520},
                                                                                     {sync,
                                                                                      []}]
[ns_server:debug,2020-03-27T20:03:37.108Z,ns_1@127.0.0.1:ns_audit_cfg<0.461.0>:ns_audit_cfg:notify_memcached:170]Instruct memcached to reload audit config
[error_logger:info,2020-03-27T20:03:37.108Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.461.0>},
                       {name,ns_audit_cfg},
                       {mfargs,{ns_audit_cfg,start_link,[]}},
                       {restart_type,{permanent,4}},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:warn,2020-03-27T20:03:37.111Z,ns_1@127.0.0.1:<0.464.0>:ns_memcached:connect:1104]Unable to connect: {error,{badmatch,{error,econnrefused}}}, retrying.
[ns_server:debug,2020-03-27T20:03:37.138Z,ns_1@127.0.0.1:memcached_config_mgr<0.466.0>:memcached_config_mgr:init:49]waiting for completion of initial ns_ports_setup round
[error_logger:info,2020-03-27T20:03:37.138Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.465.0>},
                       {name,ns_audit},
                       {mfargs,{ns_audit,start_link,[]}},
                       {restart_type,{permanent,4}},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:03:37.139Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.466.0>},
                       {name,memcached_config_mgr},
                       {mfargs,{memcached_config_mgr,start_link,[]}},
                       {restart_type,{permanent,4}},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:info,2020-03-27T20:03:37.153Z,ns_1@127.0.0.1:<0.467.0>:ns_memcached_log_rotator:init:42]Starting log rotator on "/opt/couchbase/var/lib/couchbase/logs"/"memcached.log"* with an initial period of 39003ms
[error_logger:info,2020-03-27T20:03:37.153Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.467.0>},
                       {name,ns_memcached_log_rotator},
                       {mfargs,{ns_memcached_log_rotator,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:03:37.158Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.468.0>},
                       {name,testconditions_store},
                       {mfargs,{simple_store,start_link,[testconditions]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:03:37.166Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.469.0>},
                       {name,terse_cluster_info_uploader},
                       {mfargs,{terse_cluster_info_uploader,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-03-27T20:03:37.178Z,ns_1@127.0.0.1:ns_ports_setup<0.450.0>:ns_ports_setup:set_children:85]Monitor ns_child_ports_sup <12939.109.0>
[ns_server:debug,2020-03-27T20:03:37.178Z,ns_1@127.0.0.1:memcached_config_mgr<0.466.0>:memcached_config_mgr:init:51]ns_ports_setup seems to be ready
[ns_server:debug,2020-03-27T20:03:37.189Z,ns_1@127.0.0.1:terse_cluster_info_uploader<0.469.0>:terse_cluster_info_uploader:handle_info:48]Refreshing terse cluster info with <<"{\"rev\":22,\"nodesExt\":[{\"services\":{\"mgmt\":8091,\"mgmtSSL\":18091,\"kv\":11210,\"kvSSL\":11207,\"capi\":8092,\"capiSSL\":18092,\"projector\":9999,\"projector\":9999},\"thisNode\":true}],\"clusterCapabilitiesVer\":[1,0],\"clusterCapabilities\":{\"n1ql\":[\"enhancedPreparedStatements\"]}}">>
[ns_server:warn,2020-03-27T20:03:37.190Z,ns_1@127.0.0.1:<0.474.0>:ns_memcached:connect:1104]Unable to connect: {error,{badmatch,{error,econnrefused}}}, retrying.
[error_logger:info,2020-03-27T20:03:37.191Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_bucket_worker_sup}
             started: [{pid,<0.475.0>},
                       {id,ns_bucket_sup},
                       {mfargs,{ns_bucket_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[ns_server:debug,2020-03-27T20:03:37.194Z,ns_1@127.0.0.1:memcached_config_mgr<0.466.0>:memcached_config_mgr:find_port_pid_loop:137]Found memcached port <12939.116.0>
[error_logger:info,2020-03-27T20:03:37.207Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_bucket_worker_sup}
             started: [{pid,<0.477.0>},
                       {id,ns_bucket_worker},
                       {mfargs,{ns_bucket_worker,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:03:37.207Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.471.0>},
                       {name,ns_bucket_worker_sup},
                       {mfargs,{ns_bucket_worker_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-03-27T20:03:37.214Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.480.0>},
                       {name,system_stats_collector},
                       {mfargs,{system_stats_collector,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:03:37.215Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.484.0>},
                       {name,{stats_archiver,"@system"}},
                       {mfargs,{stats_archiver,start_link,["@system"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:03:37.235Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.486.0>},
                       {name,{stats_reader,"@system"}},
                       {mfargs,{stats_reader,start_link,["@system"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:03:37.236Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.487.0>},
                       {name,{stats_archiver,"@system-processes"}},
                       {mfargs,
                           {stats_archiver,start_link,["@system-processes"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:03:37.236Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.489.0>},
                       {name,{stats_reader,"@system-processes"}},
                       {mfargs,
                           {stats_reader,start_link,["@system-processes"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:03:37.237Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.490.0>},
                       {name,{stats_archiver,"@query"}},
                       {mfargs,{stats_archiver,start_link,["@query"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:03:37.237Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.492.0>},
                       {name,{stats_reader,"@query"}},
                       {mfargs,{stats_reader,start_link,["@query"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:03:37.270Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.493.0>},
                       {name,query_stats_collector},
                       {mfargs,{query_stats_collector,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:03:37.284Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.495.0>},
                       {name,{stats_archiver,"@global"}},
                       {mfargs,{stats_archiver,start_link,["@global"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:03:37.285Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.497.0>},
                       {name,{stats_reader,"@global"}},
                       {mfargs,{stats_reader,start_link,["@global"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-03-27T20:03:37.288Z,ns_1@127.0.0.1:memcached_config_mgr<0.466.0>:memcached_config_mgr:init:82]wrote memcached config to /opt/couchbase/var/lib/couchbase/config/memcached.json. Will activate memcached port server
[ns_server:debug,2020-03-27T20:03:37.297Z,ns_1@127.0.0.1:memcached_config_mgr<0.466.0>:memcached_config_mgr:init:86]activated memcached port server
[error_logger:info,2020-03-27T20:03:37.305Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.498.0>},
                       {name,global_stats_collector},
                       {mfargs,{global_stats_collector,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:03:37.324Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.501.0>},
                       {name,goxdcr_status_keeper},
                       {mfargs,{goxdcr_status_keeper,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-03-27T20:03:37.328Z,ns_1@127.0.0.1:goxdcr_status_keeper<0.501.0>:goxdcr_rest:get_from_goxdcr:140]Goxdcr is temporary not available. Return empty list.
[ns_server:debug,2020-03-27T20:03:37.346Z,ns_1@127.0.0.1:goxdcr_status_keeper<0.501.0>:goxdcr_rest:get_from_goxdcr:140]Goxdcr is temporary not available. Return empty list.
[error_logger:info,2020-03-27T20:03:37.347Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,services_stats_sup}
             started: [{pid,<0.505.0>},
                       {id,service_stats_children_sup},
                       {mfargs,
                           {supervisor,start_link,
                               [{local,service_stats_children_sup},
                                services_stats_sup,child]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-03-27T20:03:37.357Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,service_status_keeper_sup}
             started: [{pid,<0.507.0>},
                       {id,service_status_keeper_worker},
                       {mfargs,
                           {work_queue,start_link,
                               [service_status_keeper_worker]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:03:37.383Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,service_status_keeper_sup}
             started: [{pid,<0.508.0>},
                       {id,service_status_keeper_index},
                       {mfargs,{service_index,start_keeper,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:03:37.402Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,service_status_keeper_sup}
             started: [{pid,<0.511.0>},
                       {id,service_status_keeper_fts},
                       {mfargs,{service_fts,start_keeper,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:03:37.407Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,service_status_keeper_sup}
             started: [{pid,<0.514.0>},
                       {id,service_status_keeper_eventing},
                       {mfargs,{service_eventing,start_keeper,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:03:37.408Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,services_stats_sup}
             started: [{pid,<0.506.0>},
                       {id,service_status_keeper_sup},
                       {mfargs,{service_status_keeper_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-03-27T20:03:37.408Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,services_stats_sup}
             started: [{pid,<0.517.0>},
                       {id,service_stats_worker},
                       {mfargs,
                           {erlang,apply,
                               [#Fun<services_stats_sup.0.108537742>,[]]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:03:37.408Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.504.0>},
                       {name,services_stats_sup},
                       {mfargs,{services_stats_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[ns_server:debug,2020-03-27T20:03:37.438Z,ns_1@127.0.0.1:<0.521.0>:new_concurrency_throttle:init:115]init concurrent throttle process, pid: <0.521.0>, type: kv_throttle# of available token: 1
[ns_server:warn,2020-03-27T20:03:37.446Z,ns_1@127.0.0.1:memcached_refresh<0.213.0>:ns_memcached:connect:1101]Unable to connect: {error,{badmatch,{error,econnrefused}}}.
[ns_server:debug,2020-03-27T20:03:37.447Z,ns_1@127.0.0.1:memcached_refresh<0.213.0>:memcached_refresh:handle_info:93]Refresh of [rbac,isasl] failed. Retry in 1000 ms.
[ns_server:debug,2020-03-27T20:03:37.451Z,ns_1@127.0.0.1:compaction_daemon<0.519.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2020-03-27T20:03:37.451Z,ns_1@127.0.0.1:compaction_daemon<0.519.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2020-03-27T20:03:37.451Z,ns_1@127.0.0.1:compaction_daemon<0.519.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2020-03-27T20:03:37.451Z,ns_1@127.0.0.1:compaction_daemon<0.519.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2020-03-27T20:03:37.451Z,ns_1@127.0.0.1:compaction_daemon<0.519.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_master. Rescheduling compaction.
[ns_server:debug,2020-03-27T20:03:37.451Z,ns_1@127.0.0.1:compaction_daemon<0.519.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_master too soon. Next run will be in 3600s
[error_logger:info,2020-03-27T20:03:37.451Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.519.0>},
                       {name,compaction_daemon},
                       {mfargs,{compaction_daemon,start_link,[]}},
                       {restart_type,{permanent,4}},
                       {shutdown,86400000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:03:37.454Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,cluster_logs_sup}
             started: [{pid,<0.523.0>},
                       {id,ets_holder},
                       {mfargs,
                           {cluster_logs_collection_task,
                               start_link_ets_holder,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:03:37.456Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.522.0>},
                       {name,cluster_logs_sup},
                       {mfargs,{cluster_logs_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-03-27T20:03:37.456Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.524.0>},
                       {name,leader_events},
                       {mfargs,{gen_event,start_link,[{local,leader_events}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:03:37.483Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,leader_leases_sup}
             started: [{pid,<0.528.0>},
                       {id,leader_activities},
                       {mfargs,{leader_activities,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,10000},
                       {child_type,worker}]

[ns_server:warn,2020-03-27T20:03:37.531Z,ns_1@127.0.0.1:leader_lease_agent<0.529.0>:leader_lease_agent:maybe_recover_persisted_lease:399]Found persisted lease [{node,'ns_1@127.0.0.1'},
                       {uuid,<<"989da350031a7b3cec93d0674a7161ac">>},
                       {time_left,15000},
                       {status,active}]
[error_logger:info,2020-03-27T20:03:37.532Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,leader_leases_sup}
             started: [{pid,<0.529.0>},
                       {id,leader_lease_agent},
                       {mfargs,{leader_lease_agent,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:03:37.532Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,leader_services_sup}
             started: [{pid,<0.527.0>},
                       {id,leader_leases_sup},
                       {mfargs,{leader_leases_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-03-27T20:03:37.542Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,leader_registry_sup}
             started: [{pid,<0.531.0>},
                       {id,leader_registry_server},
                       {mfargs,{leader_registry_server,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-03-27T20:03:37.554Z,ns_1@127.0.0.1:memcached_refresh<0.213.0>:memcached_refresh:handle_info:89]Refresh of [rbac,isasl] succeeded
[ns_server:debug,2020-03-27T20:03:37.554Z,ns_1@127.0.0.1:leader_registry_sup<0.530.0>:mb_master:check_master_takeover_needed:283]Sending master node question to the following nodes: []
[ns_server:debug,2020-03-27T20:03:37.554Z,ns_1@127.0.0.1:leader_registry_sup<0.530.0>:mb_master:check_master_takeover_needed:285]Got replies: []
[ns_server:debug,2020-03-27T20:03:37.554Z,ns_1@127.0.0.1:leader_registry_sup<0.530.0>:mb_master:check_master_takeover_needed:291]Was unable to discover master, not going to force mastership takeover
[user:info,2020-03-27T20:03:37.569Z,ns_1@127.0.0.1:mb_master<0.534.0>:mb_master:init:103]I'm the only node, so I'm the master.
[ns_server:debug,2020-03-27T20:03:37.569Z,ns_1@127.0.0.1:leader_registry<0.531.0>:leader_registry_server:handle_new_leader:241]New leader is 'ns_1@127.0.0.1'. Invalidating name cache.
[ns_server:debug,2020-03-27T20:03:37.598Z,ns_1@127.0.0.1:mb_master<0.534.0>:master_activity_events:submit_cast:82]Failed to send master activity event: {error,badarg}
[error_logger:info,2020-03-27T20:03:37.606Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,mb_master_sup}
             started: [{pid,<0.537.0>},
                       {id,leader_lease_acquirer},
                       {mfargs,{leader_lease_acquirer,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,10000},
                       {child_type,worker}]

[ns_server:debug,2020-03-27T20:03:37.609Z,ns_1@127.0.0.1:leader_quorum_nodes_manager<0.539.0>:leader_quorum_nodes_manager:pull_config:114]Attempting to pull config from nodes:
[]
[error_logger:info,2020-03-27T20:03:37.610Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,mb_master_sup}
             started: [{pid,<0.539.0>},
                       {id,leader_quorum_nodes_manager},
                       {mfargs,{leader_quorum_nodes_manager,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-03-27T20:03:37.611Z,ns_1@127.0.0.1:leader_quorum_nodes_manager<0.539.0>:leader_quorum_nodes_manager:pull_config:119]Pulled config successfully.
[ns_server:warn,2020-03-27T20:03:37.617Z,ns_1@127.0.0.1:<0.543.0>:leader_lease_acquire_worker:handle_lease_already_acquired:232]Failed to acquire lease from 'ns_1@127.0.0.1' because its already taken by {'ns_1@127.0.0.1',
                                                                            <<"989da350031a7b3cec93d0674a7161ac">>} (valid for 14914ms)
[ns_server:info,2020-03-27T20:03:37.619Z,ns_1@127.0.0.1:mb_master_sup<0.536.0>:misc:start_singleton:857]start_singleton(gen_server, start_link, [{via,leader_registry,ns_tick},
                                         ns_tick,[],[]]): started as <0.546.0> on 'ns_1@127.0.0.1'

[error_logger:info,2020-03-27T20:03:37.619Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,mb_master_sup}
             started: [{pid,<0.546.0>},
                       {id,ns_tick},
                       {mfargs,{ns_tick,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,10},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:03:37.621Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_orchestrator_sup}
             started: [{pid,<0.548.0>},
                       {id,compat_mode_events},
                       {mfargs,
                           {gen_event,start_link,
                               [{local,compat_mode_events}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:03:37.622Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_orchestrator_sup}
             started: [{pid,<0.549.0>},
                       {id,compat_mode_manager},
                       {mfargs,{compat_mode_manager,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:03:37.626Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_orchestrator_child_sup}
             started: [{pid,<0.551.0>},
                       {id,ns_janitor_server},
                       {mfargs,{ns_janitor_server,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:info,2020-03-27T20:03:37.629Z,ns_1@127.0.0.1:ns_orchestrator_child_sup<0.550.0>:misc:start_singleton:857]start_singleton(gen_server, start_link, [{via,leader_registry,
                                          auto_reprovision},
                                         auto_reprovision,[],[]]): started as <0.552.0> on 'ns_1@127.0.0.1'

[error_logger:info,2020-03-27T20:03:37.629Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_orchestrator_child_sup}
             started: [{pid,<0.552.0>},
                       {id,auto_reprovision},
                       {mfargs,{auto_reprovision,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:info,2020-03-27T20:03:37.633Z,ns_1@127.0.0.1:ns_orchestrator_child_sup<0.550.0>:misc:start_singleton:857]start_singleton(gen_server, start_link, [{via,leader_registry,auto_rebalance},
                                         auto_rebalance,[],[]]): started as <0.553.0> on 'ns_1@127.0.0.1'

[error_logger:info,2020-03-27T20:03:37.633Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_orchestrator_child_sup}
             started: [{pid,<0.553.0>},
                       {id,auto_rebalance},
                       {mfargs,{auto_rebalance,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:info,2020-03-27T20:03:37.634Z,ns_1@127.0.0.1:ns_orchestrator_child_sup<0.550.0>:misc:start_singleton:857]start_singleton(gen_statem, start_link, [{via,leader_registry,ns_orchestrator},
                                         ns_orchestrator,[],[]]): started as <0.554.0> on 'ns_1@127.0.0.1'

[error_logger:info,2020-03-27T20:03:37.634Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_orchestrator_child_sup}
             started: [{pid,<0.554.0>},
                       {id,ns_orchestrator},
                       {mfargs,{ns_orchestrator,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:03:37.634Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_orchestrator_sup}
             started: [{pid,<0.550.0>},
                       {id,ns_orchestrator_child_sup},
                       {mfargs,{ns_orchestrator_child_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[ns_server:debug,2020-03-27T20:03:37.648Z,ns_1@127.0.0.1:<0.556.0>:auto_failover:init:185]init auto_failover.
[user:info,2020-03-27T20:03:37.648Z,ns_1@127.0.0.1:<0.556.0>:auto_failover:handle_call:216]Enabled auto-failover with timeout 120 and max count 1
[ns_server:debug,2020-03-27T20:03:37.655Z,ns_1@127.0.0.1:ns_config_rep<0.321.0>:ns_config_rep:do_push_keys:321]Replicating some config keys ([auto_failover_cfg,
                               {local_changes_count,
                                   <<"60a6bc3db77e6c7b91c556140dcfec71">>}]..)
[ns_server:info,2020-03-27T20:03:37.655Z,ns_1@127.0.0.1:ns_orchestrator_sup<0.547.0>:misc:start_singleton:857]start_singleton(gen_server, start_link, [{via,leader_registry,auto_failover},
                                         auto_failover,[],[]]): started as <0.556.0> on 'ns_1@127.0.0.1'

[ns_server:debug,2020-03-27T20:03:37.656Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{local_changes_count,<<"60a6bc3db77e6c7b91c556140dcfec71">>} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{23,63752558617}}]}]
[error_logger:info,2020-03-27T20:03:37.656Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_orchestrator_sup}
             started: [{pid,<0.556.0>},
                       {id,auto_failover},
                       {mfargs,{auto_failover,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-03-27T20:03:37.658Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
auto_failover_cfg ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557537}}]},
 {enabled,true},
 {timeout,120},
 {count,0},
 {failover_on_data_disk_issues,[{enabled,false},{timePeriod,120}]},
 {failover_server_group,false},
 {max_count,1},
 {failed_over_server_groups,[]},
 {can_abort_rebalance,true}]
[error_logger:info,2020-03-27T20:03:37.658Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,mb_master_sup}
             started: [{pid,<0.547.0>},
                       {id,ns_orchestrator_sup},
                       {mfargs,{ns_orchestrator_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[ns_server:info,2020-03-27T20:03:37.662Z,ns_1@127.0.0.1:mb_master_sup<0.536.0>:misc:start_singleton:857]start_singleton(work_queue, start_link, [{via,leader_registry,collections}]): started as <0.563.0> on 'ns_1@127.0.0.1'

[error_logger:info,2020-03-27T20:03:37.664Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,mb_master_sup}
             started: [{pid,<0.563.0>},
                       {id,collections},
                       {mfargs,{collections,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-03-27T20:03:37.666Z,ns_1@127.0.0.1:<0.564.0>:license_reporting:init:66]Starting license_reporting server
[ns_server:info,2020-03-27T20:03:37.666Z,ns_1@127.0.0.1:mb_master_sup<0.536.0>:misc:start_singleton:857]start_singleton(gen_server, start_link, [{via,leader_registry,
                                          license_reporting},
                                         license_reporting,[],[]]): started as <0.564.0> on 'ns_1@127.0.0.1'

[error_logger:info,2020-03-27T20:03:37.667Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,mb_master_sup}
             started: [{pid,<0.564.0>},
                       {id,license_reporting},
                       {mfargs,{license_reporting,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-03-27T20:03:37.667Z,ns_1@127.0.0.1:<0.525.0>:restartable:start_child:98]Started child process <0.526.0>
  MFA: {leader_services_sup,start_link,[]}
[error_logger:info,2020-03-27T20:03:37.667Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,leader_registry_sup}
             started: [{pid,<0.534.0>},
                       {id,mb_master},
                       {mfargs,{mb_master,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-03-27T20:03:37.667Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,leader_services_sup}
             started: [{pid,<0.530.0>},
                       {id,leader_registry_sup},
                       {mfargs,{leader_registry_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-03-27T20:03:37.667Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.525.0>},
                       {name,leader_services_sup},
                       {mfargs,
                           {restartable,start_link,
                               [{leader_services_sup,start_link,[]},
                                infinity]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-03-27T20:03:37.671Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.566.0>},
                       {name,ns_tick_agent},
                       {mfargs,{ns_tick_agent,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:03:37.671Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.568.0>},
                       {name,master_activity_events_ingress},
                       {mfargs,
                           {gen_event,start_link,
                               [{local,master_activity_events_ingress}]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:03:37.672Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.569.0>},
                       {name,master_activity_events_timestamper},
                       {mfargs,
                           {master_activity_events,start_link_timestamper,[]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:03:37.684Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.570.0>},
                       {name,master_activity_events_pids_watcher},
                       {mfargs,
                           {master_activity_events_pids_watcher,start_link,
                               []}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:03:37.697Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.571.0>},
                       {name,master_activity_events_keeper},
                       {mfargs,{master_activity_events_keeper,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:03:37.705Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,health_monitor_sup}
             started: [{pid,<0.574.0>},
                       {id,ns_server_monitor},
                       {mfargs,{ns_server_monitor,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:03:37.707Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,health_monitor_sup}
             started: [{pid,<0.576.0>},
                       {id,service_monitor_children_sup},
                       {mfargs,
                           {supervisor,start_link,
                               [{local,service_monitor_children_sup},
                                health_monitor_sup,child]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-03-27T20:03:37.707Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,health_monitor_sup}
             started: [{pid,<0.577.0>},
                       {id,service_monitor_worker},
                       {mfargs,
                           {erlang,apply,
                               [#Fun<health_monitor_sup.0.112499759>,[]]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:03:37.716Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,health_monitor_sup}
             started: [{pid,<0.583.0>},
                       {id,node_monitor},
                       {mfargs,{node_monitor,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:03:37.721Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,health_monitor_sup}
             started: [{pid,<0.589.0>},
                       {id,node_status_analyzer},
                       {mfargs,{node_status_analyzer,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:03:37.721Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.573.0>},
                       {name,health_monitor_sup},
                       {mfargs,{health_monitor_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-03-27T20:03:37.727Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.591.0>},
                       {name,rebalance_agent},
                       {mfargs,{rebalance_agent,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[ns_server:debug,2020-03-27T20:03:37.770Z,ns_1@127.0.0.1:ns_server_nodes_sup<0.209.0>:one_shot_barrier:notify:27]Notifying on barrier menelaus_barrier
[error_logger:info,2020-03-27T20:03:37.770Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.592.0>},
                       {name,ns_rebalance_report_manager},
                       {mfargs,{ns_rebalance_report_manager,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:03:37.770Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.300.0>},
                       {name,ns_server_sup},
                       {mfargs,{ns_server_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[ns_server:debug,2020-03-27T20:03:37.770Z,ns_1@127.0.0.1:menelaus_barrier<0.211.0>:one_shot_barrier:barrier_body:62]Barrier menelaus_barrier got notification from <0.209.0>
[ns_server:debug,2020-03-27T20:03:37.771Z,ns_1@127.0.0.1:ns_server_nodes_sup<0.209.0>:one_shot_barrier:notify:32]Successfuly notified on barrier menelaus_barrier
[ns_server:debug,2020-03-27T20:03:37.773Z,ns_1@127.0.0.1:<0.208.0>:restartable:start_child:98]Started child process <0.209.0>
  MFA: {ns_server_nodes_sup,start_link,[]}
[error_logger:info,2020-03-27T20:03:37.776Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.208.0>},
                       {id,ns_server_nodes_sup},
                       {mfargs,
                           {restartable,start_link,
                               [{ns_server_nodes_sup,start_link,[]},
                                infinity]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[ns_server:debug,2020-03-27T20:03:37.781Z,ns_1@127.0.0.1:compiled_roles_cache<0.264.0>:menelaus_roles:build_compiled_roles:753]Compile roles for user {"@",admin}
[error_logger:info,2020-03-27T20:03:37.792Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.594.0>},
                       {id,remote_api},
                       {mfargs,{remote_api,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-03-27T20:03:37.792Z,ns_1@127.0.0.1:<0.5.0>:child_erlang:child_loop:130]141: Entered child_loop
[error_logger:info,2020-03-27T20:03:37.792Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,root_sup}
             started: [{pid,<0.187.0>},
                       {id,ns_server_cluster_sup},
                       {mfargs,{ns_server_cluster_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-03-27T20:03:37.792Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
         application: ns_server
          started_at: 'ns_1@127.0.0.1'

[ns_server:debug,2020-03-27T20:03:37.829Z,ns_1@127.0.0.1:json_rpc_connection-saslauthd-saslauthd-port<0.595.0>:json_rpc_connection:init:73]Observed revrpc connection: label "saslauthd-saslauthd-port", handling process <0.595.0>
[ns_server:debug,2020-03-27T20:03:37.829Z,ns_1@127.0.0.1:json_rpc_connection-goxdcr-cbauth<0.596.0>:json_rpc_connection:init:73]Observed revrpc connection: label "goxdcr-cbauth", handling process <0.596.0>
[ns_server:debug,2020-03-27T20:03:37.829Z,ns_1@127.0.0.1:menelaus_cbauth<0.444.0>:menelaus_cbauth:handle_cast:107]Observed json rpc process {"goxdcr-cbauth",<0.596.0>} started
[ns_server:debug,2020-03-27T20:03:37.865Z,ns_1@127.0.0.1:compiled_roles_cache<0.264.0>:menelaus_roles:build_compiled_roles:753]Compile roles for user {"@goxdcr-cbauth",admin}
[ns_server:debug,2020-03-27T20:03:38.650Z,ns_1@127.0.0.1:<0.556.0>:auto_failover_logic:log_master_activity:177]Transitioned node {'ns_1@127.0.0.1',<<"60a6bc3db77e6c7b91c556140dcfec71">>} state new -> up
[ns_server:debug,2020-03-27T20:03:41.697Z,ns_1@127.0.0.1:ns_heart_slow_status_updater<0.350.0>:goxdcr_rest:get_from_goxdcr:140]Goxdcr is temporary not available. Return empty list.
[ns_server:debug,2020-03-27T20:03:52.532Z,ns_1@127.0.0.1:leader_lease_agent<0.529.0>:leader_lease_agent:handle_lease_expired:286]Lease held by {lease_holder,<<"989da350031a7b3cec93d0674a7161ac">>,
                            'ns_1@127.0.0.1'} expired. Starting expirer.
[ns_server:debug,2020-03-27T20:03:52.534Z,ns_1@127.0.0.1:leader_lease_agent<0.529.0>:leader_lease_agent:do_handle_acquire_lease:149]Granting lease to {lease_holder,<<"d28c599b6a432dc5d7503c01390563df">>,
                                'ns_1@127.0.0.1'} for 15000ms
[ns_server:info,2020-03-27T20:03:52.547Z,ns_1@127.0.0.1:<0.543.0>:leader_lease_acquire_worker:handle_fresh_lease_acquired:302]Acquired lease from node 'ns_1@127.0.0.1' (lease uuid: <<"d28c599b6a432dc5d7503c01390563df">>)
[ns_server:debug,2020-03-27T20:04:07.456Z,ns_1@127.0.0.1:compaction_daemon<0.519.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2020-03-27T20:04:07.456Z,ns_1@127.0.0.1:compaction_daemon<0.519.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2020-03-27T20:04:07.456Z,ns_1@127.0.0.1:compaction_daemon<0.519.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2020-03-27T20:04:07.456Z,ns_1@127.0.0.1:compaction_daemon<0.519.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2020-03-27T20:04:20.969Z,ns_1@127.0.0.1:compiled_roles_cache<0.264.0>:menelaus_roles:build_compiled_roles:753]Compile roles for user {[],wrong_token}
[ns_server:debug,2020-03-27T20:04:37.457Z,ns_1@127.0.0.1:compaction_daemon<0.519.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2020-03-27T20:04:37.457Z,ns_1@127.0.0.1:compaction_daemon<0.519.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2020-03-27T20:04:37.457Z,ns_1@127.0.0.1:compaction_daemon<0.519.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2020-03-27T20:04:37.457Z,ns_1@127.0.0.1:compaction_daemon<0.519.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2020-03-27T20:04:48.523Z,ns_1@127.0.0.1:ldap_auth_cache<0.256.0>:active_cache:cleanup:231]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:info,2020-03-27T20:04:57.522Z,ns_1@127.0.0.1:netconfig_updater<0.204.0>:netconfig_updater:apply_config_unprotected:158]Node is going to apply the following settings: [{externalListeners,
                                                 [{inet,false},
                                                  {inet6,false}]}]
[ns_server:debug,2020-03-27T20:04:57.539Z,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Updated cb_dist config "/opt/couchbase/var/lib/couchbase/config/dist_cfg": [{external_listeners,
                                                                                      [inet_tcp_dist,
                                                                                       inet6_tcp_dist]},
                                                                                     {preferred_external_proto,
                                                                                      inet_tcp_dist},
                                                                                     {preferred_local_proto,
                                                                                      inet_tcp_dist}]
[ns_server:debug,2020-03-27T20:04:57.547Z,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Reloading configuration: [{external_listeners,
                                       [inet_tcp_dist,inet6_tcp_dist]},
                                   {preferred_external_proto,inet_tcp_dist},
                                   {preferred_local_proto,inet_tcp_dist}]
[ns_server:debug,2020-03-27T20:04:57.548Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{local_changes_count,<<"60a6bc3db77e6c7b91c556140dcfec71">>} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{24,63752558697}}]}]
[ns_server:debug,2020-03-27T20:04:57.548Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',address_family} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|inet]
[ns_server:debug,2020-03-27T20:04:57.548Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{local_changes_count,<<"60a6bc3db77e6c7b91c556140dcfec71">>} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{25,63752558697}}]}]
[ns_server:info,2020-03-27T20:04:57.548Z,ns_1@127.0.0.1:netconfig_updater<0.204.0>:netconfig_updater:apply_config_unprotected:187]Node network settings ([{externalListeners,[{inet,false},{inet6,false}]}]) successfully applied
[ns_server:debug,2020-03-27T20:04:57.548Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',node_encryption} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|false]
[ns_server:debug,2020-03-27T20:04:57.548Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{local_changes_count,<<"60a6bc3db77e6c7b91c556140dcfec71">>} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{26,63752558697}}]}]
[ns_server:debug,2020-03-27T20:04:57.549Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',erl_external_listeners} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]},
 {inet,false},
 {inet6,false}]
[ns_server:debug,2020-03-27T20:04:57.549Z,ns_1@127.0.0.1:ns_config_rep<0.321.0>:ns_config_rep:do_push_keys:321]Replicating some config keys ([{local_changes_count,
                                   <<"60a6bc3db77e6c7b91c556140dcfec71">>},
                               {node,'ns_1@127.0.0.1',address_family},
                               {node,'ns_1@127.0.0.1',erl_external_listeners},
                               {node,'ns_1@127.0.0.1',node_encryption}]..)
[cluster:info,2020-03-27T20:04:57.641Z,ns_1@127.0.0.1:ns_cluster<0.191.0>:ns_cluster:handle_call:355]Changing address to "127.0.0.1" due to client request
[cluster:info,2020-03-27T20:04:57.641Z,ns_1@127.0.0.1:ns_cluster<0.191.0>:ns_cluster:do_change_address:596]Change of address to "127.0.0.1" is requested.
[cluster:debug,2020-03-27T20:04:57.641Z,ns_1@127.0.0.1:<0.2247.0>:ns_cluster:maybe_rename:626]Not renaming node.
[ns_server:debug,2020-03-27T20:04:57.654Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{metakv,<<"/indexing/settings/config">>} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{5,63752558697}}]}|
 <<"{\"indexer.settings.compaction.days_of_week\":\"Sunday,Monday,Tuesday,Wednesday,Thursday,Friday,Saturday\",\"indexer.settings.compaction.interval\":\"00:00,00:00\",\"indexer.settings.persisted_snapshot.interval\":5000,\"indexer.settings.log_level\":\"info\",\"indexer.settings.compaction.compaction_mode\":\"circular\",\"indexer.settings.compaction.min_frag\":30,\"indexer.settings.inmemory_snapshot.interval\""...>>]
[ns_server:debug,2020-03-27T20:04:57.654Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{local_changes_count,<<"60a6bc3db77e6c7b91c556140dcfec71">>} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{27,63752558697}}]}]
[ns_server:debug,2020-03-27T20:04:57.654Z,ns_1@127.0.0.1:ns_audit<0.465.0>:ns_audit:handle_call:125]Audit modify_index_storage_mode: [{storageMode,<<"plasma">>},
                                  {real_userid,
                                      {[{domain,wrong_token},
                                        {user,<<"<ud></ud>">>}]}},
                                  {sessionid,
                                      <<"3c9673d43ade3c5c4478657f38eb6001">>},
                                  {remote,
                                      {[{ip,<<"192.168.48.1">>},
                                        {port,50422}]}},
                                  {timestamp,<<"2020-03-27T20:04:57.654Z">>}]
[ns_server:debug,2020-03-27T20:04:57.656Z,ns_1@127.0.0.1:ns_config_rep<0.321.0>:ns_config_rep:do_push_keys:321]Replicating some config keys ([{local_changes_count,
                                   <<"60a6bc3db77e6c7b91c556140dcfec71">>},
                               {metakv,<<"/indexing/settings/config">>}]..)
[ns_server:debug,2020-03-27T20:04:57.665Z,ns_1@127.0.0.1:ns_audit<0.465.0>:ns_audit:handle_call:125]Audit cluster_settings: [{cluster_name,<<"Base">>},
                         {quotas,{[{kv,292},
                                   {index,512},
                                   {fts,256},
                                   {cbas,1024},
                                   {eventing,256}]}},
                         {real_userid,{[{domain,wrong_token},
                                        {user,<<"<ud></ud>">>}]}},
                         {sessionid,<<"3c9673d43ade3c5c4478657f38eb6001">>},
                         {remote,{[{ip,<<"192.168.48.1">>},{port,50422}]}},
                         {timestamp,<<"2020-03-27T20:04:57.664Z">>}]
[ns_server:debug,2020-03-27T20:04:57.665Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{metakv,<<"/indexing/settings/config">>} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{6,63752558697}}]}|
 <<"{\"indexer.settings.compaction.days_of_week\":\"Sunday,Monday,Tuesday,Wednesday,Thursday,Friday,Saturday\",\"indexer.settings.compaction.interval\":\"00:00,00:00\",\"indexer.settings.compaction.compaction_mode\":\"circular\",\"indexer.settings.log_level\":\"info\",\"indexer.settings.persisted_snapshot.interval\":5000,\"indexer.settings.compaction.min_frag\":30,\"indexer.settings.inmemory_snapshot.interval\""...>>]
[ns_server:debug,2020-03-27T20:04:57.665Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{local_changes_count,<<"60a6bc3db77e6c7b91c556140dcfec71">>} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{28,63752558697}}]}]
[ns_server:debug,2020-03-27T20:04:57.665Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{local_changes_count,<<"60a6bc3db77e6c7b91c556140dcfec71">>} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{29,63752558697}}]}]
[ns_server:debug,2020-03-27T20:04:57.666Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
cluster_name ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557980}}]},
 66,97,115,101]
[ns_server:debug,2020-03-27T20:04:57.666Z,ns_1@127.0.0.1:ns_config_rep<0.321.0>:ns_config_rep:do_push_keys:321]Replicating some config keys ([cluster_name,
                               {local_changes_count,
                                   <<"60a6bc3db77e6c7b91c556140dcfec71">>},
                               {metakv,<<"/indexing/settings/config">>}]..)
[ns_server:debug,2020-03-27T20:05:07.458Z,ns_1@127.0.0.1:compaction_daemon<0.519.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2020-03-27T20:05:07.458Z,ns_1@127.0.0.1:compaction_daemon<0.519.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2020-03-27T20:05:07.458Z,ns_1@127.0.0.1:compaction_daemon<0.519.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2020-03-27T20:05:07.458Z,ns_1@127.0.0.1:compaction_daemon<0.519.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2020-03-27T20:05:37.460Z,ns_1@127.0.0.1:compaction_daemon<0.519.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2020-03-27T20:05:37.460Z,ns_1@127.0.0.1:compaction_daemon<0.519.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2020-03-27T20:05:37.460Z,ns_1@127.0.0.1:compaction_daemon<0.519.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2020-03-27T20:05:37.460Z,ns_1@127.0.0.1:compaction_daemon<0.519.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2020-03-27T20:06:03.526Z,ns_1@127.0.0.1:ldap_auth_cache<0.256.0>:active_cache:cleanup:231]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2020-03-27T20:06:07.461Z,ns_1@127.0.0.1:compaction_daemon<0.519.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2020-03-27T20:06:07.461Z,ns_1@127.0.0.1:compaction_daemon<0.519.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2020-03-27T20:06:07.461Z,ns_1@127.0.0.1:compaction_daemon<0.519.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2020-03-27T20:06:07.461Z,ns_1@127.0.0.1:compaction_daemon<0.519.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2020-03-27T20:06:37.462Z,ns_1@127.0.0.1:compaction_daemon<0.519.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2020-03-27T20:06:37.462Z,ns_1@127.0.0.1:compaction_daemon<0.519.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2020-03-27T20:06:37.462Z,ns_1@127.0.0.1:compaction_daemon<0.519.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2020-03-27T20:06:37.462Z,ns_1@127.0.0.1:compaction_daemon<0.519.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2020-03-27T20:07:07.464Z,ns_1@127.0.0.1:compaction_daemon<0.519.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2020-03-27T20:07:07.464Z,ns_1@127.0.0.1:compaction_daemon<0.519.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2020-03-27T20:07:07.464Z,ns_1@127.0.0.1:compaction_daemon<0.519.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2020-03-27T20:07:07.464Z,ns_1@127.0.0.1:compaction_daemon<0.519.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2020-03-27T20:07:18.528Z,ns_1@127.0.0.1:ldap_auth_cache<0.256.0>:active_cache:cleanup:231]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2020-03-27T20:07:37.464Z,ns_1@127.0.0.1:compaction_daemon<0.519.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2020-03-27T20:07:37.464Z,ns_1@127.0.0.1:compaction_daemon<0.519.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2020-03-27T20:07:37.464Z,ns_1@127.0.0.1:compaction_daemon<0.519.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2020-03-27T20:07:37.464Z,ns_1@127.0.0.1:compaction_daemon<0.519.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2020-03-27T20:08:07.467Z,ns_1@127.0.0.1:compaction_daemon<0.519.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2020-03-27T20:08:07.467Z,ns_1@127.0.0.1:compaction_daemon<0.519.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2020-03-27T20:08:07.467Z,ns_1@127.0.0.1:compaction_daemon<0.519.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2020-03-27T20:08:07.467Z,ns_1@127.0.0.1:compaction_daemon<0.519.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2020-03-27T20:08:33.531Z,ns_1@127.0.0.1:ldap_auth_cache<0.256.0>:active_cache:cleanup:231]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2020-03-27T20:08:37.468Z,ns_1@127.0.0.1:compaction_daemon<0.519.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2020-03-27T20:08:37.468Z,ns_1@127.0.0.1:compaction_daemon<0.519.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2020-03-27T20:08:37.468Z,ns_1@127.0.0.1:compaction_daemon<0.519.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2020-03-27T20:08:37.468Z,ns_1@127.0.0.1:compaction_daemon<0.519.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:info,2020-03-27T20:08:54.820Z,ns_1@127.0.0.1:netconfig_updater<0.204.0>:netconfig_updater:apply_config_unprotected:158]Node is going to apply the following settings: [{externalListeners,
                                                 [{inet,false},
                                                  {inet6,false}]}]
[ns_server:debug,2020-03-27T20:08:54.859Z,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Updated cb_dist config "/opt/couchbase/var/lib/couchbase/config/dist_cfg": [{external_listeners,
                                                                                      [inet_tcp_dist,
                                                                                       inet6_tcp_dist]},
                                                                                     {preferred_external_proto,
                                                                                      inet_tcp_dist},
                                                                                     {preferred_local_proto,
                                                                                      inet_tcp_dist}]
[ns_server:debug,2020-03-27T20:08:54.869Z,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Reloading configuration: [{external_listeners,
                                       [inet_tcp_dist,inet6_tcp_dist]},
                                   {preferred_external_proto,inet_tcp_dist},
                                   {preferred_local_proto,inet_tcp_dist}]
[ns_server:debug,2020-03-27T20:08:54.869Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{local_changes_count,<<"60a6bc3db77e6c7b91c556140dcfec71">>} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{30,63752558934}}]}]
[ns_server:debug,2020-03-27T20:08:54.869Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',address_family} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|inet]
[ns_server:info,2020-03-27T20:08:54.870Z,ns_1@127.0.0.1:netconfig_updater<0.204.0>:netconfig_updater:apply_config_unprotected:187]Node network settings ([{externalListeners,[{inet,false},{inet6,false}]}]) successfully applied
[ns_server:debug,2020-03-27T20:08:54.870Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{local_changes_count,<<"60a6bc3db77e6c7b91c556140dcfec71">>} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{31,63752558934}}]}]
[ns_server:debug,2020-03-27T20:08:54.870Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',node_encryption} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|false]
[ns_server:debug,2020-03-27T20:08:54.871Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{local_changes_count,<<"60a6bc3db77e6c7b91c556140dcfec71">>} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{32,63752558934}}]}]
[ns_server:debug,2020-03-27T20:08:54.871Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',erl_external_listeners} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]},
 {inet,false},
 {inet6,false}]
[ns_server:debug,2020-03-27T20:08:54.873Z,ns_1@127.0.0.1:ns_config_rep<0.321.0>:ns_config_rep:do_push_keys:321]Replicating some config keys ([{local_changes_count,
                                   <<"60a6bc3db77e6c7b91c556140dcfec71">>},
                               {node,'ns_1@127.0.0.1',address_family},
                               {node,'ns_1@127.0.0.1',erl_external_listeners},
                               {node,'ns_1@127.0.0.1',node_encryption}]..)
[cluster:info,2020-03-27T20:08:54.939Z,ns_1@127.0.0.1:ns_cluster<0.191.0>:ns_cluster:handle_call:355]Changing address to "127.0.0.1" due to client request
[cluster:info,2020-03-27T20:08:54.939Z,ns_1@127.0.0.1:ns_cluster<0.191.0>:ns_cluster:do_change_address:596]Change of address to "127.0.0.1" is requested.
[cluster:debug,2020-03-27T20:08:54.940Z,ns_1@127.0.0.1:<0.7138.0>:ns_cluster:maybe_rename:626]Not renaming node.
[ns_server:debug,2020-03-27T20:08:54.951Z,ns_1@127.0.0.1:ns_config_rep<0.321.0>:ns_config_rep:do_push_keys:321]Replicating some config keys ([{local_changes_count,
                                   <<"60a6bc3db77e6c7b91c556140dcfec71">>},
                               {metakv,<<"/indexing/settings/config">>}]..)
[ns_server:debug,2020-03-27T20:08:54.953Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{metakv,<<"/indexing/settings/config">>} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{7,63752558934}}]}|
 <<"{\"indexer.settings.compaction.days_of_week\":\"Sunday,Monday,Tuesday,Wednesday,Thursday,Friday,Saturday\",\"indexer.settings.compaction.interval\":\"00:00,00:00\",\"indexer.settings.persisted_snapshot.interval\":5000,\"indexer.settings.log_level\":\"info\",\"indexer.settings.compaction.compaction_mode\":\"circular\",\"indexer.settings.compaction.min_frag\":30,\"indexer.settings.inmemory_snapshot.interval\""...>>]
[ns_server:debug,2020-03-27T20:08:54.953Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{local_changes_count,<<"60a6bc3db77e6c7b91c556140dcfec71">>} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{33,63752558934}}]}]
[ns_server:debug,2020-03-27T20:08:54.955Z,ns_1@127.0.0.1:ns_audit<0.465.0>:ns_audit:handle_call:125]Audit modify_index_storage_mode: [{storageMode,<<"plasma">>},
                                  {real_userid,
                                      {[{domain,wrong_token},
                                        {user,<<"<ud></ud>">>}]}},
                                  {sessionid,
                                      <<"3c9673d43ade3c5c4478657f38eb6001">>},
                                  {remote,
                                      {[{ip,<<"192.168.48.1">>},
                                        {port,50442}]}},
                                  {timestamp,<<"2020-03-27T20:08:54.955Z">>}]
[ns_server:debug,2020-03-27T20:08:54.968Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{metakv,<<"/indexing/settings/config">>} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{8,63752558934}}]}|
 <<"{\"indexer.settings.compaction.days_of_week\":\"Sunday,Monday,Tuesday,Wednesday,Thursday,Friday,Saturday\",\"indexer.settings.compaction.interval\":\"00:00,00:00\",\"indexer.settings.compaction.compaction_mode\":\"circular\",\"indexer.settings.log_level\":\"info\",\"indexer.settings.persisted_snapshot.interval\":5000,\"indexer.settings.compaction.min_frag\":30,\"indexer.settings.inmemory_snapshot.interval\""...>>]
[ns_server:debug,2020-03-27T20:08:54.969Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{local_changes_count,<<"60a6bc3db77e6c7b91c556140dcfec71">>} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{34,63752558934}}]}]
[ns_server:debug,2020-03-27T20:08:54.970Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{local_changes_count,<<"60a6bc3db77e6c7b91c556140dcfec71">>} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{35,63752558934}}]}]
[ns_server:debug,2020-03-27T20:08:54.969Z,ns_1@127.0.0.1:ns_audit<0.465.0>:ns_audit:handle_call:125]Audit cluster_settings: [{cluster_name,<<"First">>},
                         {quotas,{[{kv,292},
                                   {index,512},
                                   {fts,256},
                                   {cbas,1024},
                                   {eventing,256}]}},
                         {real_userid,{[{domain,wrong_token},
                                        {user,<<"<ud></ud>">>}]}},
                         {sessionid,<<"3c9673d43ade3c5c4478657f38eb6001">>},
                         {remote,{[{ip,<<"192.168.48.1">>},{port,50442}]}},
                         {timestamp,<<"2020-03-27T20:08:54.969Z">>}]
[ns_server:debug,2020-03-27T20:08:54.970Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
cluster_name ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{3,63752558934}}]},
 70,105,114,115,116]
[ns_server:debug,2020-03-27T20:08:54.971Z,ns_1@127.0.0.1:ns_config_rep<0.321.0>:ns_config_rep:do_push_keys:321]Replicating some config keys ([cluster_name,
                               {local_changes_count,
                                   <<"60a6bc3db77e6c7b91c556140dcfec71">>},
                               {metakv,<<"/indexing/settings/config">>}]..)
[ns_server:debug,2020-03-27T20:09:07.468Z,ns_1@127.0.0.1:compaction_daemon<0.519.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2020-03-27T20:09:07.469Z,ns_1@127.0.0.1:compaction_daemon<0.519.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2020-03-27T20:09:07.469Z,ns_1@127.0.0.1:compaction_daemon<0.519.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2020-03-27T20:09:07.469Z,ns_1@127.0.0.1:compaction_daemon<0.519.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2020-03-27T20:09:37.469Z,ns_1@127.0.0.1:compaction_daemon<0.519.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2020-03-27T20:09:37.469Z,ns_1@127.0.0.1:compaction_daemon<0.519.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2020-03-27T20:09:37.469Z,ns_1@127.0.0.1:compaction_daemon<0.519.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2020-03-27T20:09:37.469Z,ns_1@127.0.0.1:compaction_daemon<0.519.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2020-03-27T20:09:48.536Z,ns_1@127.0.0.1:ldap_auth_cache<0.256.0>:active_cache:cleanup:231]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:info,2020-03-27T20:37:38.761Z,nonode@nohost:<0.118.0>:ns_server:init_logging:150]Started & configured logging
[ns_server:info,2020-03-27T20:37:38.794Z,nonode@nohost:<0.118.0>:ns_server:log_pending:32]Static config terms:
[{error_logger_mf_dir,"/opt/couchbase/var/lib/couchbase/logs"},
 {path_config_bindir,"/opt/couchbase/bin"},
 {path_config_etcdir,"/opt/couchbase/etc/couchbase"},
 {path_config_libdir,"/opt/couchbase/lib"},
 {path_config_datadir,"/opt/couchbase/var/lib/couchbase"},
 {path_config_tmpdir,"/opt/couchbase/var/lib/couchbase/tmp"},
 {path_config_secdir,"/opt/couchbase/etc/security"},
 {nodefile,"/opt/couchbase/var/lib/couchbase/couchbase-server.node"},
 {loglevel_default,debug},
 {loglevel_couchdb,info},
 {loglevel_ns_server,debug},
 {loglevel_error_logger,debug},
 {loglevel_user,debug},
 {loglevel_menelaus,debug},
 {loglevel_ns_doctor,debug},
 {loglevel_stats,debug},
 {loglevel_rebalance,debug},
 {loglevel_cluster,debug},
 {loglevel_views,debug},
 {loglevel_mapreduce_errors,debug},
 {loglevel_xdcr,debug},
 {loglevel_access,info},
 {loglevel_cbas,debug},
 {disk_sink_opts,[{rotation,[{compress,true},
                             {size,41943040},
                             {num_files,10},
                             {buffer_size_max,52428800}]}]},
 {disk_sink_opts_json_rpc,[{rotation,[{compress,true},
                                      {size,41943040},
                                      {num_files,2},
                                      {buffer_size_max,52428800}]}]},
 {net_kernel_verbosity,10}]
[ns_server:warn,2020-03-27T20:37:38.795Z,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter error_logger_mf_dir, which is given from command line
[ns_server:warn,2020-03-27T20:37:38.796Z,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter path_config_bindir, which is given from command line
[ns_server:warn,2020-03-27T20:37:38.796Z,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter path_config_etcdir, which is given from command line
[ns_server:warn,2020-03-27T20:37:38.796Z,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter path_config_libdir, which is given from command line
[ns_server:warn,2020-03-27T20:37:38.796Z,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter path_config_datadir, which is given from command line
[ns_server:warn,2020-03-27T20:37:38.796Z,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter path_config_tmpdir, which is given from command line
[ns_server:warn,2020-03-27T20:37:38.796Z,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter path_config_secdir, which is given from command line
[ns_server:warn,2020-03-27T20:37:38.796Z,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter nodefile, which is given from command line
[ns_server:warn,2020-03-27T20:37:38.796Z,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_default, which is given from command line
[ns_server:warn,2020-03-27T20:37:38.796Z,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_couchdb, which is given from command line
[ns_server:warn,2020-03-27T20:37:38.796Z,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_ns_server, which is given from command line
[ns_server:warn,2020-03-27T20:37:38.796Z,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_error_logger, which is given from command line
[ns_server:warn,2020-03-27T20:37:38.796Z,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_user, which is given from command line
[ns_server:warn,2020-03-27T20:37:38.796Z,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_menelaus, which is given from command line
[ns_server:warn,2020-03-27T20:37:38.796Z,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_ns_doctor, which is given from command line
[ns_server:warn,2020-03-27T20:37:38.796Z,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_stats, which is given from command line
[ns_server:warn,2020-03-27T20:37:38.796Z,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_rebalance, which is given from command line
[ns_server:warn,2020-03-27T20:37:38.796Z,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_cluster, which is given from command line
[ns_server:warn,2020-03-27T20:37:38.796Z,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_views, which is given from command line
[ns_server:warn,2020-03-27T20:37:38.796Z,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_mapreduce_errors, which is given from command line
[ns_server:warn,2020-03-27T20:37:38.796Z,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_xdcr, which is given from command line
[ns_server:warn,2020-03-27T20:37:38.796Z,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_access, which is given from command line
[ns_server:warn,2020-03-27T20:37:38.796Z,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_cbas, which is given from command line
[ns_server:warn,2020-03-27T20:37:38.797Z,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter disk_sink_opts, which is given from command line
[ns_server:warn,2020-03-27T20:37:38.797Z,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter disk_sink_opts_json_rpc, which is given from command line
[ns_server:warn,2020-03-27T20:37:38.797Z,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter net_kernel_verbosity, which is given from command line
[ns_server:info,2020-03-27T20:37:38.822Z,nonode@nohost:dist_manager<0.166.0>:dist_manager:read_address_config_from_path:99]Reading ip config from "/opt/couchbase/var/lib/couchbase/ip_start"
[ns_server:info,2020-03-27T20:37:38.822Z,nonode@nohost:dist_manager<0.166.0>:dist_manager:read_address_config_from_path:99]Reading ip config from "/opt/couchbase/var/lib/couchbase/ip"
[error_logger:info,2020-03-27T20:37:38.829Z,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,inet_gethost_native_sup}
             started: [{pid,<0.168.0>},{mfa,{inet_gethost_native,init,[[]]}}]

[error_logger:info,2020-03-27T20:37:38.829Z,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,kernel_safe_sup}
             started: [{pid,<0.167.0>},
                       {id,inet_gethost_native_sup},
                       {mfargs,{inet_gethost_native,start_link,[]}},
                       {restart_type,temporary},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:info,2020-03-27T20:37:38.852Z,nonode@nohost:dist_manager<0.166.0>:dist_manager:bringup:249]Attempting to bring up net_kernel with name 'ns_1@127.0.0.1'
[error_logger:info,2020-03-27T20:37:38.878Z,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ssl_dist_admin_sup}
             started: [{pid,<0.172.0>},
                       {id,ssl_pem_cache_dist},
                       {mfargs,{ssl_pem_cache,start_link_dist,[[]]}},
                       {restart_type,permanent},
                       {shutdown,4000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:37:38.879Z,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ssl_dist_admin_sup}
             started: [{pid,<0.173.0>},
                       {id,ssl_dist_manager},
                       {mfargs,{ssl_manager,start_link_dist,[[]]}},
                       {restart_type,permanent},
                       {shutdown,4000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:37:38.879Z,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ssl_dist_sup}
             started: [{pid,<0.171.0>},
                       {id,ssl_dist_admin_sup},
                       {mfargs,{ssl_dist_admin_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,4000},
                       {child_type,supervisor}]

[error_logger:info,2020-03-27T20:37:38.882Z,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ssl_dist_sup}
             started: [{pid,<0.174.0>},
                       {id,ssl_tls_dist_proxy},
                       {mfargs,{ssl_tls_dist_proxy,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,4000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:37:38.884Z,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ssl_dist_connection_sup}
             started: [{pid,<0.176.0>},
                       {id,dist_tls_connection},
                       {mfargs,{tls_connection_sup,start_link_dist,[]}},
                       {restart_type,permanent},
                       {shutdown,4000},
                       {child_type,supervisor}]

[error_logger:info,2020-03-27T20:37:38.884Z,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ssl_dist_connection_sup}
             started: [{pid,<0.177.0>},
                       {id,dist_tls_socket},
                       {mfargs,{ssl_listen_tracker_sup,start_link_dist,[]}},
                       {restart_type,permanent},
                       {shutdown,4000},
                       {child_type,supervisor}]

[error_logger:info,2020-03-27T20:37:38.884Z,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ssl_dist_sup}
             started: [{pid,<0.175.0>},
                       {id,ssl_dist_connection_sup},
                       {mfargs,{ssl_dist_connection_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,4000},
                       {child_type,supervisor}]

[error_logger:info,2020-03-27T20:37:38.884Z,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,net_sup}
             started: [{pid,<0.170.0>},
                       {id,ssl_dist_sup},
                       {mfargs,{ssl_dist_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[ns_server:debug,2020-03-27T20:37:38.886Z,nonode@nohost:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Starting cb_dist with config [{external_listeners,
                                        [inet_tcp_dist,inet6_tcp_dist]},
                                       {preferred_external_proto,
                                        inet_tcp_dist},
                                       {preferred_local_proto,inet_tcp_dist}]
[error_logger:info,2020-03-27T20:37:38.888Z,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,net_sup}
             started: [{pid,<0.178.0>},
                       {id,cb_dist},
                       {mfargs,{cb_dist,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:37:38.890Z,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,net_sup}
             started: [{pid,<0.179.0>},
                       {id,cb_epmd},
                       {mfargs,{cb_epmd,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,2000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:37:38.891Z,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,net_sup}
             started: [{pid,<0.180.0>},
                       {id,auth},
                       {mfargs,{auth,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,2000},
                       {child_type,worker}]

[ns_server:debug,2020-03-27T20:37:38.893Z,nonode@nohost:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Initial protos: [inet_tcp_dist,inet6_tcp_dist], required protos: [inet_tcp_dist]
[ns_server:debug,2020-03-27T20:37:38.893Z,nonode@nohost:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Starting inet_tcp_dist listener on 21100...
[ns_server:debug,2020-03-27T20:37:38.894Z,nonode@nohost:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Starting inet6_tcp_dist listener on 21100...
[ns_server:debug,2020-03-27T20:37:38.897Z,ns_1@127.0.0.1:dist_manager<0.166.0>:dist_manager:configure_net_kernel:293]Set net_kernel vebosity to 10 -> 0
[error_logger:info,2020-03-27T20:37:38.897Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,net_sup}
             started: [{pid,<0.181.0>},
                       {id,net_kernel},
                       {mfargs,
                           {net_kernel,start_link,
                               [['ns_1@127.0.0.1',longnames],false]}},
                       {restart_type,permanent},
                       {shutdown,2000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:37:38.897Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,kernel_sup}
             started: [{pid,<0.169.0>},
                       {id,net_sup_dynamic},
                       {mfargs,
                           {erl_distribution,start_link,
                               [['ns_1@127.0.0.1',longnames],false]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,supervisor}]

[ns_server:info,2020-03-27T20:37:38.899Z,ns_1@127.0.0.1:dist_manager<0.166.0>:dist_manager:save_node:175]saving node to "/opt/couchbase/var/lib/couchbase/couchbase-server.node"
[ns_server:debug,2020-03-27T20:37:38.913Z,ns_1@127.0.0.1:dist_manager<0.166.0>:dist_manager:bringup:263]Attempted to save node name to disk: ok
[ns_server:debug,2020-03-27T20:37:38.913Z,ns_1@127.0.0.1:dist_manager<0.166.0>:dist_manager:wait_for_node:270]Waiting for connection to node 'babysitter_of_ns_1@cb.local' to be established
[error_logger:info,2020-03-27T20:37:38.913Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{connect,normal,'babysitter_of_ns_1@cb.local'}}
[ns_server:debug,2020-03-27T20:37:38.913Z,ns_1@127.0.0.1:net_kernel<0.181.0>:cb_dist:info_msg:754]cb_dist: Setting up new connection to 'babysitter_of_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2020-03-27T20:37:38.913Z,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Added connection {con,#Ref<0.2968477961.2101608449.153887>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2020-03-27T20:37:38.913Z,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Updated connection: {con,#Ref<0.2968477961.2101608449.153887>,
                                  inet_tcp_dist,<0.185.0>,
                                  #Ref<0.2968477961.2101608450.153314>}
[ns_server:debug,2020-03-27T20:37:38.918Z,ns_1@127.0.0.1:dist_manager<0.166.0>:dist_manager:wait_for_node:282]Observed node 'babysitter_of_ns_1@cb.local' to come up
[ns_server:info,2020-03-27T20:37:38.920Z,ns_1@127.0.0.1:dist_manager<0.166.0>:dist_manager:save_address_config:162]Deleting irrelevant ip file "/opt/couchbase/var/lib/couchbase/ip_start": {error,
                                                                          enoent}
[ns_server:info,2020-03-27T20:37:38.920Z,ns_1@127.0.0.1:dist_manager<0.166.0>:dist_manager:save_address_config:163]saving ip config to "/opt/couchbase/var/lib/couchbase/ip"
[ns_server:info,2020-03-27T20:37:38.936Z,ns_1@127.0.0.1:dist_manager<0.166.0>:dist_manager:save_address_config:166]Persisted the address successfully
[error_logger:info,2020-03-27T20:37:38.939Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,root_sup}
             started: [{pid,<0.166.0>},
                       {id,dist_manager},
                       {mfargs,{dist_manager,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:37:38.952Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.188.0>},
                       {id,local_tasks},
                       {mfargs,{local_tasks,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[ns_server:info,2020-03-27T20:37:38.963Z,ns_1@127.0.0.1:ns_server_cluster_sup<0.187.0>:log_os_info:start_link:25]OS type: {unix,linux} Version: {4,19,76}
Runtime info: [{otp_release,"20"},
               {erl_version,"9.3.3.9"},
               {erl_version_long,
                   "Erlang/OTP 20 [erts-9.3.3.9] [source-d27a01d] [64-bit] [smp:2:2] [ds:2:2:10] [async-threads:16] [kernel-poll:true]\n"},
               {system_arch_raw,"x86_64-unknown-linux-gnu"},
               {system_arch,"x86_64-unknown-linux-gnu"},
               {localtime,{{2020,3,27},{20,37,38}}},
               {memory,
                   [{total,36950248},
                    {processes,9939696},
                    {processes_used,9935784},
                    {system,27010552},
                    {atom,388625},
                    {atom_used,364430},
                    {binary,132208},
                    {code,8250921},
                    {ets,1509680}]},
               {loaded,
                   [ns_info,log_os_info,local_tasks,restartable,
                    ns_server_cluster_sup,ns_cluster,dist_util,ns_node_disco,
                    inet6_tcp,inet6_tcp_dist,re,auth,rand,
                    ssl_dist_connection_sup,ssl_tls_dist_proxy,
                    ssl_dist_admin_sup,ssl_dist_sup,inet_tls_dist,
                    inet_tcp_dist,inet_tcp,gen_tcp,erl_epmd,cb_epmd,gen_udp,
                    inet_hosts,dist_manager,root_sup,path_config,cb_dist,
                    unicode_util,calendar,ale_default_formatter,
                    'ale_logger-metakv','ale_logger-rebalance',
                    'ale_logger-menelaus','ale_logger-stats',
                    'ale_logger-json_rpc','ale_logger-access',
                    'ale_logger-ns_server','ale_logger-user',
                    'ale_logger-ns_doctor','ale_logger-cluster',
                    'ale_logger-xdcr',erl_bits,otp_internal,ns_log_sink,
                    ale_disk_sink,misc,couch_util,ns_server,io_lib_fread,
                    filelib,cpu_sup,memsup,disksup,os_mon,string,io,
                    release_handler,alarm_handler,sasl,timer,tftp_sup,
                    httpd_sup,httpc_handler_sup,httpc_cookie,inets_trace,
                    httpc_manager,httpc,httpc_profile_sup,httpc_sup,ftp_sup,
                    inets_sup,inets_app,ssl,lhttpc_manager,lhttpc_sup,lhttpc,
                    dtls_udp_sup,dtls_connection_sup,ssl_listen_tracker_sup,
                    tls_connection_sup,ssl_connection_sup,ssl_session_cache,
                    ssl_manager,ssl_pkix_db,ssl_pem_cache,ssl_admin_sup,
                    ssl_sup,ssl_app,ale_error_logger_handler,
                    'ale_logger-ale_logger','ale_logger-error_logger',
                    beam_opcodes,maps,beam_dict,beam_asm,beam_validator,
                    beam_z,beam_flatten,beam_trim,beam_record,beam_receive,
                    beam_bsm,beam_peep,beam_dead,beam_split,beam_type,
                    beam_clean,beam_bs,beam_except,beam_block,beam_utils,
                    beam_reorder,beam_jump,beam_a,v3_codegen,v3_life,
                    v3_kernel,sys_core_dsetel,sys_core_bsm,erl_bifs,
                    cerl_clauses,cerl_sets,sys_core_fold,cerl_trees,
                    sys_core_inline,core_lib,cerl,v3_core,erl_expand_records,
                    sofs,erl_internal,sets,ordsets,compile,dynamic_compile,
                    ale_utils,io_lib_pretty,io_lib_format,io_lib,ale_codegen,
                    dict,ale,ale_dynamic_sup,ale_sup,ale_app,ns_bootstrap,
                    child_erlang,orddict,c,erl_signal_handler,kernel_config,
                    user_io,user_sup,supervisor_bridge,standard_error,
                    net_kernel,global_group,erl_distribution,epp,
                    inet_gethost_native,inet_parse,inet,inet_udp,inet_config,
                    inet_db,global,rpc,unicode,os,hipe_unified_loader,
                    gb_trees,gb_sets,binary,erl_anno,proplists,erl_scan,
                    error_handler,file,file_server,file_io_server,gen,ets,
                    gen_event,filename,application_controller,heart,
                    code_server,application_master,kernel,application,
                    error_logger,erl_eval,lists,supervisor,erl_lint,erl_parse,
                    proc_lib,code,gen_server,erts_dirty_process_code_checker,
                    erts_literal_area_collector,erl_tracer,erts_internal,
                    erlang,erl_prim_loader,prim_zip,zlib,prim_file,prim_inet,
                    prim_eval,init,erts_code_purger,otp_ring0]},
               {applications,
                   [{sasl,"SASL  CXC 138 11","3.1.2"},
                    {os_mon,"CPO  CXC 138 46","2.4.4"},
                    {inets,"INETS  CXC 138 49","6.5.2.4"},
                    {crypto,"CRYPTO","4.2.2.2"},
                    {ale,"Another Logger for Erlang","0.0.0"},
                    {lhttpc,"Lightweight HTTP Client","1.3.0"},
                    {stdlib,"ERTS  CXC 138 10","3.4.5.1"},
                    {ssl,"Erlang/OTP SSL application","8.2.6.4"},
                    {kernel,"ERTS  CXC 138 10","5.4.3.2"},
                    {public_key,"Public key infrastructure","1.5.2"},
                    {asn1,"The Erlang ASN1 compiler version 5.0.5.2",
                        "5.0.5.2"},
                    {ns_server,"Couchbase server","6.5.0-4960-enterprise"}]},
               {pre_loaded,
                   [erts_dirty_process_code_checker,
                    erts_literal_area_collector,erl_tracer,erts_internal,
                    erlang,erl_prim_loader,prim_zip,zlib,prim_file,prim_inet,
                    prim_eval,init,erts_code_purger,otp_ring0]},
               {process_count,131},
               {node,'ns_1@127.0.0.1'},
               {nodes,[]},
               {registered,
                   [application_controller,erl_prim_loader,httpd_sup,auth,
                    dtls_udp_sup,cb_dist,dtls_connection_sup,
                    ns_server_cluster_sup,tls_connection_sup,sasl_sup,
                    kernel_safe_sup,release_handler,lhttpc_sup,httpc_sup,
                    lhttpc_manager,alarm_handler,httpc_profile_sup,
                    ssl_listen_tracker_supdist,httpc_manager,
                    httpc_handler_sup,ssl_connection_sup_dist,'sink-ns_log',
                    local_tasks,standard_error_sup,ftp_sup,
                    'sink-disk_json_rpc','sink-disk_metakv',inets_sup,
                    'sink-disk_access_int','sink-disk_access',standard_error,
                    'sink-disk_reports',ale_stats_events,'sink-disk_stats',
                    'sink-disk_xdcr',timer_server,'sink-disk_debug',ale_sup,
                    inet_gethost_native,'sink-disk_error',inet_db,
                    'sink-disk_default',ale_dynamic_sup,ssl_pem_cache_dist,
                    rex,global_group,net_sup,ssl_connection_sup,kernel_sup,
                    ssl_admin_sup,tftp_sup,global_name_server,ssl_sup,
                    root_sup,os_mon_sup,erts_code_purger,file_server_2,
                    error_logger,cpu_sup,memsup,erl_epmd,init,disksup,ale,
                    erl_signal_server,net_kernel,dist_manager,ssl_pem_cache,
                    ssl_manager,ssl_dist_admin_sup,ssl_dist_connection_sup,
                    ssl_dist_sup,ssl_tls_dist_proxy,ssl_manager_dist,user,
                    sasl_safe_sup,ssl_listen_tracker_sup,
                    inet_gethost_native_sup,code_server]},
               {cookie,nocookie},
               {wordsize,8},
               {wall_clock,1}]
[ns_server:info,2020-03-27T20:37:38.971Z,ns_1@127.0.0.1:ns_server_cluster_sup<0.187.0>:log_os_info:start_link:27]Manifest:
["<manifest>",
 "  <remote fetch=\"git://github.com/blevesearch/\" name=\"blevesearch\" />",
 "  <remote fetch=\"git://github.com/couchbase/\" name=\"couchbase\" review=\"review.couchbase.org\" />",
 "  <remote fetch=\"ssh://git@github.com/couchbase/\" name=\"couchbase-priv\" review=\"review.couchbase.org\" />",
 "  <remote fetch=\"git://github.com/couchbasedeps/\" name=\"couchbasedeps\" review=\"review.couchbase.org\" />",
 "  <remote fetch=\"git://github.com/couchbaselabs/\" name=\"couchbaselabs\" review=\"review.couchbase.org\" />",
 "  ","  <default remote=\"couchbase\" revision=\"master\" />","  ",
 "  <project groups=\"kv\" name=\"HdrHistogram_c\" path=\"third_party/HdrHistogram_c\" remote=\"couchbasedeps\" revision=\"bc8aef24ea57884464027f841c1ad7436a42c615\" />",
 "  <project groups=\"notdefault,enterprise,analytics\" name=\"analytics-dcp-client\" path=\"analytics/java-dcp-client\" revision=\"691cec38f47eaab04ad81556cc065d22f1eb8749\" />",
 "  <project groups=\"notdefault,enterprise,analytics\" name=\"asterixdb\" path=\"analytics/asterixdb\" revision=\"672a36b64a0632b72aa4b4df59635ceaa0e340de\" />",
 "  <project groups=\"backup,notdefault,enterprise\" name=\"backup\" path=\"goproj/src/github.com/couchbase/backup\" remote=\"couchbase-priv\" revision=\"cfa0f75f28402d2e1aa254b2a374bead19433526\" upstream=\"mad-hatter\" />",
 "  <project groups=\"kv\" name=\"benchmark\" remote=\"couchbasedeps\" revision=\"74b24058ad4914b837200d0341050657ba154e4a\" />",
 "  <project name=\"bitset\" path=\"godeps/src/github.com/willf/bitset\" remote=\"couchbasedeps\" revision=\"28a4168144bb8ac95454e1f51c84da1933681ad4\" />",
 "  <project name=\"blance\" path=\"godeps/src/github.com/couchbase/blance\" revision=\"5cd1345cca3ed72f1e63d41d622fcda73e63fea8\" upstream=\"master\" />",
 "  <project name=\"bleve\" path=\"godeps/src/github.com/blevesearch/bleve\" remote=\"blevesearch\" revision=\"b7a0cb6a1d4fdbaeb7ab5bdec6a9732b995e39a0\" />",
 "  <project name=\"bleve-mapping-ui\" path=\"godeps/src/github.com/blevesearch/bleve-mapping-ui\" remote=\"blevesearch\" revision=\"7987f3c80047347b1e2c3a5fafae8da56daf97d7\" />",
 "  <project name=\"bolt\" path=\"godeps/src/github.com/boltdb/bolt\" remote=\"couchbasedeps\" revision=\"51f99c862475898df9773747d3accd05a7ca33c1\" />",
 "  <project name=\"buffer\" path=\"godeps/src/github.com/tdewolff/buffer\" remote=\"couchbasedeps\" revision=\"43cef5ba7b6ce99cc410632dad46cf1c6c97026e\" />",
 "  <project groups=\"notdefault,build\" name=\"build\" path=\"cbbuild\" revision=\"f2a16b53bb74146f20d18ba2c0443d5f10a9a550\" upstream=\"master\">",
 "    <annotation name=\"RELEASE\" value=\"mad-hatter\" />",
 "    <annotation name=\"PRODUCT\" value=\"couchbase-server\" />",
 "    <annotation name=\"BLD_NUM\" value=\"4960\" />",
 "    <annotation name=\"VERSION\" value=\"6.5.0\" />","  </project>",
 "  <project groups=\"notdefault,enterprise,analytics\" name=\"cbas\" path=\"goproj/src/github.com/couchbase/cbas\" remote=\"couchbase-priv\" revision=\"e3ec01671ca2f253a5f32cf9e258d3be7fdbfe9a\" />",
 "  <project groups=\"notdefault,enterprise,analytics\" name=\"cbas-core\" path=\"analytics\" remote=\"couchbase-priv\" revision=\"c86a9fc60d074711470b112753c5695dee79dcf7\" />",
 "  <project groups=\"analytics\" name=\"cbas-ui\" revision=\"8744108f25c4520b09009ff277d35223e208fe30\" />",
 "  <project name=\"cbauth\" path=\"godeps/src/github.com/couchbase/cbauth\" revision=\"82614adbe4d480de5675d8eee9b21a180a779222\" upstream=\"master\" />",
 "  <project groups=\"backup\" name=\"cbflag\" path=\"godeps/src/github.com/couchbase/cbflag\" revision=\"9892b6db3537c54be7719f47ad25e0d513333b3e\" upstream=\"master\" />",
 "  <project name=\"cbft\" path=\"goproj/src/github.com/couchbase/cbft\" revision=\"ef487dda0baef8a258bac4f7482af3b761e4a8e0\" upstream=\"mad-hatter\" />",
 "  <project groups=\"notdefault,enterprise\" name=\"cbftx\" path=\"goproj/src/github.com/couchbase/cbftx\" remote=\"couchbase-priv\" revision=\"46dbb7c6edac7dfef017ae889d7a5b7536ce904d\" upstream=\"master\" />",
 "  <project name=\"cbgt\" path=\"goproj/src/github.com/couchbase/cbgt\" revision=\"c78e34377d7a8f017328f57a3376642f37458464\" upstream=\"mad-hatter\" />",
 "  <project name=\"cbsummary\" path=\"goproj/src/github.com/couchbase/cbsummary\" revision=\"31ba0584a81d5b293cedfb236109ab95036aa395\" upstream=\"master\" />",
 "  <project groups=\"backup\" name=\"clog\" path=\"godeps/src/github.com/couchbase/clog\" revision=\"b8e6d5d421bcc34f522e3a9a12fd6e09980995b1\" upstream=\"master\" />",
 "  <project name=\"cobra\" path=\"godeps/src/github.com/spf13/cobra\" remote=\"couchbasedeps\" revision=\"0f056af21f5f368e5b0646079d0094a2c64150f7\" />",
 "  <project name=\"context\" path=\"godeps/src/github.com/gorilla/context\" remote=\"couchbasedeps\" revision=\"215affda49addc4c8ef7e2534915df2c8c35c6cd\" />",
 "  <project groups=\"notdefault,kv_ee,enterprise\" name=\"couch_rocks\" remote=\"couchbase-priv\" revision=\"75f37fa46bfe5e445dee077157303968a3e09126\" upstream=\"master\" />",
 "  <project groups=\"kv\" name=\"couchbase-cli\" revision=\"abb0c1036566f4bd579aaadbaaa4e13466a23ef7\" upstream=\"master\" />",
 "  <project name=\"couchdb\" revision=\"fa3c64b1b85ad3145bb7910d3fe7ee90c060247e\" upstream=\"mad-hatter\" />",
 "  <project groups=\"notdefault,packaging\" name=\"couchdbx-app\" revision=\"b2a111967ba02772dc600d5c15a6514e2dea7d68\" upstream=\"master\" />",
 "  <project groups=\"kv\" name=\"couchstore\" revision=\"fff3e20090414206853b2293f17667279dda0337\" />",
 "  <project groups=\"backup\" name=\"crypto\" path=\"godeps/src/golang.org/x/crypto\" remote=\"couchbasedeps\" revision=\"bd6f299fb381e4c3393d1c4b1f0b94f5e77650c8\" />",
 "  <project name=\"cuckoofilter\" path=\"godeps/src/github.com/seiflotfy/cuckoofilter\" remote=\"couchbasedeps\" revision=\"d04838794ab86926d32b124345777e55e6f43974\" />",
 "  <project name=\"cznic-b\" path=\"godeps/src/github.com/cznic/b\" remote=\"couchbasedeps\" revision=\"b96e30f1b7bd34b0b9d8760798d67eca83d7f09e\" />",
 "  <project name=\"docloader\" path=\"goproj/src/github.com/couchbase/docloader\" revision=\"13cf07af78594aff20d00db4633af27d81fc921d\" upstream=\"master\" />",
 "  <project name=\"dparval\" path=\"godeps/src/github.com/couchbase/dparval\" revision=\"9def03782da875a2477c05bf64985db3f19f59ae\" upstream=\"master\" />",
 "  <project groups=\"backup\" name=\"errors\" path=\"godeps/src/github.com/pkg/errors\" remote=\"couchbasedeps\" revision=\"30136e27e2ac8d167177e8a583aa4c3fea5be833\" />",
 "  <project name=\"etcd-bbolt\" path=\"godeps/src/github.com/etcd-io/bbolt\" remote=\"couchbasedeps\" revision=\"7ee3ded59d4835e10f3e7d0f7603c42aa5e83820\" />",
 "  <project groups=\"notdefault,enterprise\" name=\"eventing\" path=\"goproj/src/github.com/couchbase/eventing\" revision=\"dec7a7d51b71309d43d7aea4803cd45f6ad001da\" upstream=\"mad-hatter\" />",
 "  <project groups=\"notdefault,enterprise\" name=\"eventing-ee\" path=\"goproj/src/github.com/couchbase/eventing-ee\" remote=\"couchbase-priv\" revision=\"398acea25e003c1739d3f45f53121bdec857e485\" upstream=\"mad-hatter\" />",
 "  <project name=\"flatbuffers\" path=\"godeps/src/github.com/google/flatbuffers\" remote=\"couchbasedeps\" revision=\"1a8968225130caeddd16e227678e6f8af1926303\" />",
 "  <project groups=\"backup,kv\" name=\"forestdb\" revision=\"4c3b2f9b1d869b6b71556e461d6ee68f941c1ba5\" upstream=\"cb-master\" />",
 "  <project name=\"fwd\" path=\"godeps/src/github.com/philhofer/fwd\" remote=\"couchbasedeps\" revision=\"bb6d471dc95d4fe11e432687f8b70ff496cf3136\" />",
 "  <project name=\"geocouch\" revision=\"92def13f6b049553da1aa1488ce0bde6b7d0f459\" upstream=\"master\" />",
 "  <project name=\"ghistogram\" path=\"godeps/src/github.com/couchbase/ghistogram\" revision=\"d910dd063dd68fb4d2a1ba344440f834ebb4ef62\" upstream=\"master\" />",
 "  <project name=\"go-bindata-assetfs\" path=\"godeps/src/github.com/elazarl/go-bindata-assetfs\" remote=\"couchbasedeps\" revision=\"57eb5e1fc594ad4b0b1dbea7b286d299e0cb43c2\" />",
 "  <project name=\"go-couchbase\" path=\"godeps/src/github.com/couchbase/go-couchbase\" revision=\"12d479a70a3ef189d8fb2424f5e2eea3632c0c9a\" upstream=\"mad-hatter\" />",
 "  <project name=\"go-curl\" path=\"godeps/src/github.com/andelf/go-curl\" remote=\"couchbasedeps\" revision=\"f0b2afc926ec79be5d7f30393b3485352781a705\" upstream=\"20161221-couchbase\" />",
 "  <project name=\"go-genproto\" path=\"godeps/src/google.golang.org/genproto\" remote=\"couchbasedeps\" revision=\"2b5a72b8730b0b16380010cfe5286c42108d88e7\" />",
 "  <project name=\"go-jsonpointer\" path=\"godeps/src/github.com/dustin/go-jsonpointer\" remote=\"couchbasedeps\" revision=\"75939f54b39e7dafae879e61f65438dadc5f288c\" />",
 "  <project name=\"go-metrics\" path=\"godeps/src/github.com/rcrowley/go-metrics\" remote=\"couchbasedeps\" revision=\"dee209f2455f101a5e4e593dea94872d2c62d85d\" />",
 "  <project name=\"go-porterstemmer\" path=\"godeps/src/github.com/blevesearch/go-porterstemmer\" remote=\"blevesearch\" revision=\"23a2c8e5cf1f380f27722c6d2ae8896431dc7d0e\" />",
 "  <project name=\"go-runewidth\" path=\"godeps/src/github.com/mattn/go-runewidth\" remote=\"couchbasedeps\" revision=\"703b5e6b11ae25aeb2af9ebb5d5fdf8fa2575211\" />",
 "  <project name=\"go-slab\" path=\"godeps/src/github.com/couchbase/go-slab\" revision=\"1f5f7f282713ccfab3f46b1610cb8da34bcf676f\" upstream=\"master\" />",
 "  <project groups=\"backup\" name=\"go-sqlite3\" path=\"godeps/src/github.com/mattn/go-sqlite3\" remote=\"couchbasedeps\" revision=\"ad30583d8387ce8118f8605eaeb3b4f7b4ae0ee1\" />",
 "  <project name=\"go-unsnap-stream\" path=\"godeps/src/github.com/glycerine/go-unsnap-stream\" remote=\"couchbasedeps\" revision=\"62a9a9eb44fd8932157b1a8ace2149eff5971af6\" />",
 "  <project name=\"go-zookeeper\" path=\"godeps/src/github.com/samuel/go-zookeeper\" remote=\"couchbasedeps\" revision=\"fa6674abf3f4580b946a01bf7a1ce4ba8766205b\" />",
 "  <project name=\"go_json\" path=\"godeps/src/github.com/couchbase/go_json\" revision=\"d47ffbbc4863b0020bb85c4e181d4044ea184d40\" upstream=\"mad-hatter\" />",
 "  <project name=\"go_n1ql\" path=\"godeps/src/github.com/couchbase/go_n1ql\" revision=\"6cf4e348b127e21f56e53eb8c3faaea56afdc588\" upstream=\"master\" />",
 "  <project groups=\"backup\" name=\"gocb\" path=\"godeps/src/gopkg.in/couchbase/gocb.v1\" revision=\"01c846cb025ddd50a2ef4c82a27992b40c230dbb\" upstream=\"refs/tags/v1.4.2\" />",
 "  <project groups=\"backup\" name=\"gocbconnstr\" path=\"godeps/src/gopkg.in/couchbaselabs/gocbconnstr.v1\" remote=\"couchbaselabs\" revision=\"083dcfef49cfdcb42a0f5ecf8c0c29b0cbaa640f\" upstream=\"master\" />",
 "  <project groups=\"backup\" name=\"gocbcore\" path=\"godeps/src/gopkg.in/couchbase/gocbcore.v7\" revision=\"441cb91f01ce26932514ec10d9e59e568ee27722\" upstream=\"refs/tags/v7.1.14\" />",
 "  <project name=\"godbc\" path=\"godeps/src/github.com/couchbase/godbc\" revision=\"b2aaaa21900ab3e95d37d38fb5a0f320426cbe56\" upstream=\"mad-hatter\" />",
 "  <project name=\"gofarmhash\" path=\"godeps/src/github.com/leemcloughlin/gofarmhash\" remote=\"couchbasedeps\" revision=\"0a055c5b87a8c55ce83459cbf2776b563822a942\" />",
 "  <project groups=\"backup\" name=\"goforestdb\" path=\"godeps/src/github.com/couchbase/goforestdb\" revision=\"0b501227de0e8c55d99ed14e900eea1a1dbaf899\" upstream=\"master\" />",
 "  <project name=\"gojson\" path=\"godeps/src/github.com/dustin/gojson\" remote=\"couchbasedeps\" revision=\"af16e0e771e2ed110f2785564ae33931de8829e4\" />",
 "  <project name=\"gojsonsm\" path=\"godeps/src/github.com/couchbase/gojsonsm\" remote=\"couchbaselabs\" revision=\"eec4953dcb855282c483b8cd4fe03a8074e2f7a1\" upstream=\"master\" />",
 "  <project name=\"golang-pkg-pcre\" path=\"godeps/src/github.com/glenn-brown/golang-pkg-pcre\" remote=\"couchbasedeps\" revision=\"48bb82a8b8ceea98f4e97825b43870f6ba1970d6\" />",
 "  <project groups=\"backup\" name=\"golang-snappy\" path=\"godeps/src/github.com/golang/snappy\" remote=\"couchbasedeps\" revision=\"723cc1e459b8eea2dea4583200fd60757d40097a\" />",
 "  <project name=\"golang-tools\" path=\"godeps/src/golang.org/x/tools\" remote=\"couchbasedeps\" revision=\"a28dfb48e06b2296b66678872c2cb638f0304f20\" />",
 "  <project name=\"goleveldb\" path=\"godeps/src/github.com/syndtr/goleveldb\" remote=\"couchbasedeps\" revision=\"fa5b5c78794bc5c18f330361059f871ae8c2b9d6\" />",
 "  <project name=\"gomemcached\" path=\"godeps/src/github.com/couchbase/gomemcached\" revision=\"2b4197fedf38f694a33465050d1396e03e97db19\" upstream=\"mad-hatter\" />",
 "  <project name=\"gometa\" path=\"goproj/src/github.com/couchbase/gometa\" revision=\"563cdf343321e2025b73852bcf454860a4880300\" upstream=\"mad-hatter\" />",
 "  <project groups=\"kv\" name=\"googletest\" remote=\"couchbasedeps\" revision=\"f397fa5ec6365329b2e82eb2d8c03a7897bbefb5\" />",
 "  <project name=\"goskiplist\" path=\"godeps/src/github.com/ryszard/goskiplist\" remote=\"couchbasedeps\" revision=\"2dfbae5fcf46374f166f8969cb07e167f1be6273\" />",
 "  <project name=\"gosnappy\" path=\"godeps/src/github.com/syndtr/gosnappy\" remote=\"couchbasedeps\" revision=\"156a073208e131d7d2e212cb749feae7c339e846\" />",
 "  <project groups=\"backup\" name=\"goutils\" path=\"godeps/src/github.com/couchbase/goutils\" revision=\"b49639060d85b267c5bdb7d4e3246d4ccca94e79\" upstream=\"mad-hatter\" />",
 "  <project name=\"goxdcr\" path=\"goproj/src/github.com/couchbase/goxdcr\" revision=\"03e000156faeecd5e77eb79fc45d7c73f26b2899\" upstream=\"mad-hatter\" />",
 "  <project name=\"grpc-go\" path=\"godeps/src/google.golang.org/grpc\" remote=\"couchbasedeps\" revision=\"df014850f6dee74ba2fc94874043a9f3f75fbfd8\" upstream=\"refs/tags/v1.17.0\" />",
 "  <project groups=\"kv\" name=\"gsl-lite\" path=\"third_party/gsl-lite\" remote=\"couchbasedeps\" revision=\"57542c7e7ced375346e9ac55dad85b942cfad556\" upstream=\"refs/tags/v0.25.0\" />",
 "  <project name=\"gtreap\" path=\"godeps/src/github.com/steveyen/gtreap\" remote=\"couchbasedeps\" revision=\"0abe01ef9be25c4aedc174758ec2d917314d6d70\" />",
 "  <project name=\"httprouter\" path=\"godeps/src/github.com/julienschmidt/httprouter\" remote=\"couchbasedeps\" revision=\"975b5c4c7c21c0e3d2764200bf2aa8e34657ae6e\" />",
 "  <project name=\"indexing\" path=\"goproj/src/github.com/couchbase/indexing\" revision=\"fc2e1b715bf9c098bf0991af666388dd446edf9b\" upstream=\"mad-hatter\" />",
 "  <project name=\"json-iterator-go\" path=\"godeps/src/github.com/json-iterator/go\" remote=\"couchbasedeps\" revision=\"f7279a603edee96fe7764d3de9c6ff8cf9970994\" />",
 "  <project name=\"jsonparser\" path=\"godeps/src/github.com/buger/jsonparser\" remote=\"couchbasedeps\" revision=\"bf1c66bbce23153d89b23f8960071a680dbef54b\" />",
 "  <project groups=\"backup\" name=\"jsonx\" path=\"godeps/src/gopkg.in/couchbaselabs/jsonx.v1\" remote=\"couchbaselabs\" revision=\"5b7baa20429a46a5543ee259664cc86502738cad\" upstream=\"master\" />",
 "  <project groups=\"kv\" name=\"kv_engine\" revision=\"2a368c39481ff4d42c6f755bd7d185b9a57554ca\" upstream=\"6.5.0\" />",
 "  <project name=\"levigo\" path=\"godeps/src/github.com/jmhodges/levigo\" remote=\"couchbasedeps\" revision=\"1ddad808d437abb2b8a55a950ec2616caa88969b\" />",
 "  <project groups=\"notdefault,enterprise\" name=\"libcouchbase\" revision=\"152e1a18bbcfd75bbb5a1388ed5ee050cde8a56d\" />",
 "  <project name=\"liner\" path=\"godeps/src/github.com/peterh/liner\" remote=\"couchbasedeps\" revision=\"6f820f8f90ce9482ffbd40bb15f9ea9932f4942d\" />",
 "  <project name=\"liner\" path=\"godeps/src/github.com/sbinet/liner\" remote=\"couchbasedeps\" revision=\"d9335eee40a45a4f5d74524c90040d6fe6013d50\" />",
 "  <project groups=\"notdefault,enterprise,kv_ee\" name=\"magma\" remote=\"couchbase-priv\" revision=\"c8e91e0af8b46d0a0e026d23ebbfab4048f670b6\" />",
 "  <project name=\"minify\" path=\"godeps/src/github.com/tdewolff/minify\" remote=\"couchbasedeps\" revision=\"ede45cc53f43891267b1fe7c689db9c76d4ce0fb\" />",
 "  <project name=\"mmap-go\" path=\"godeps/src/github.com/edsrzf/mmap-go\" remote=\"couchbasedeps\" revision=\"935e0e8a636ca4ba70b713f3e38a19e1b77739e8\" />",
 "  <project name=\"mobile-service\" path=\"goproj/src/github.com/couchbase/mobile-service\" revision=\"4672fde0390f115a25f4f4bfe9d1511836de47a7\" upstream=\"master\" />",
 "  <project name=\"moss\" path=\"godeps/src/github.com/couchbase/moss\" revision=\"a0cae174c4987cb28c071e0796e25b58834108d8\" upstream=\"master\" />",
 "  <project name=\"mossScope\" path=\"godeps/src/github.com/couchbase/mossScope\" revision=\"aa48ddbc0e832bc68dde56c4b69e30c5cb3983eb\" upstream=\"master\" />",
 "  <project name=\"mousetrap\" path=\"godeps/src/github.com/inconshreveable/mousetrap\" remote=\"couchbasedeps\" revision=\"76626ae9c91c4f2a10f34cad8ce83ea42c93bb75\" />",
 "  <project name=\"msgp\" path=\"godeps/src/github.com/tinylib/msgp\" remote=\"couchbasedeps\" revision=\"5bb5e1aed7ba5bcc93307153b020e7ffe79b0509\" />",
 "  <project name=\"mux\" path=\"godeps/src/github.com/gorilla/mux\" remote=\"couchbasedeps\" revision=\"043ee6597c29786140136a5747b6a886364f5282\" />",
 "  <project name=\"n1fty\" path=\"godeps/src/github.com/couchbase/n1fty\" revision=\"f28de9b4e73d7acdf3b07b7f7318bb23973f7dc6\" upstream=\"mad-hatter\" />",
 "  <project groups=\"backup\" name=\"net\" path=\"godeps/src/golang.org/x/net\" remote=\"couchbasedeps\" revision=\"44b7c21cbf19450f38b337eb6b6fe4f6496fb5b3\" />",
 "  <project name=\"nitro\" path=\"goproj/src/github.com/couchbase/nitro\" revision=\"4fc6475fb3352618cdf93fead56271bb29d15571\" upstream=\"mad-hatter\" />",
 "  <project name=\"npipe\" path=\"godeps/src/github.com/natefinch/npipe\" remote=\"couchbasedeps\" revision=\"272c8150302e83f23d32a355364578c9c13ab20f\" />",
 "  <project name=\"ns_server\" revision=\"3fe2759eb53c12478f75bd1613f8998401b0635c\" upstream=\"mad-hatter\" />",
 "  <project groups=\"backup\" name=\"opentracing-go\" path=\"godeps/src/github.com/opentracing/opentracing-go\" remote=\"couchbasedeps\" revision=\"1949ddbfd147afd4d964a9f00b24eb291e0e7c38\" />",
 "  <project name=\"parse\" path=\"godeps/src/github.com/tdewolff/parse\" remote=\"couchbasedeps\" revision=\"0334a869253aca4b3a10c56c3f3139b394aec3a9\" />",
 "  <project name=\"participle\" path=\"godeps/src/github.com/alecthomas/participle\" remote=\"couchbasedeps\" revision=\"bf8340a459bd383e5eb7d44a9a1b3af23b6cf8cd\" />",
 "  <project name=\"pflag\" path=\"godeps/src/github.com/spf13/pflag\" remote=\"couchbasedeps\" revision=\"a232f6d9f87afaaa08bafaff5da685f974b83313\" />",
 "  <project groups=\"kv\" name=\"phosphor\" revision=\"53ca1eeae7bd3deea5b7bf48b3d4188b47e530d1\" upstream=\"master\" />",
 "  <project name=\"pierrec-lz4\" path=\"godeps/src/github.com/pierrec/lz4\" remote=\"couchbasedeps\" revision=\"ed8d4cc3b461464e69798080a0092bd028910298\" />",
 "  <project name=\"pierrec-xxHash\" path=\"godeps/src/github.com/pierrec/xxHash\" remote=\"couchbasedeps\" revision=\"a0006b13c722f7f12368c00a3d3c2ae8a999a0c6\" />",
 "  <project groups=\"notdefault,enterprise\" name=\"plasma\" path=\"goproj/src/github.com/couchbase/plasma\" remote=\"couchbase-priv\" revision=\"4aa86645ce4b4673de08f6829b446b9c00cd3f3d\" upstream=\"mad-hatter\" />",
 "  <project groups=\"kv\" name=\"platform\" revision=\"bec44f963f3c4d73d3735380a8107b7292558749\" upstream=\"mad-hatter\" />",
 "  <project groups=\"kv\" name=\"product-texts\" revision=\"7a3aa547b3f5eb3ea28d279a08384609cd2cea7c\" upstream=\"master\" />",
 "  <project name=\"protobuf\" path=\"godeps/src/github.com/golang/protobuf\" remote=\"couchbasedeps\" revision=\"ddf22928ea3c56eb4292a0adbbf5001b1e8e7d0d\" />",
 "  <project name=\"query\" path=\"goproj/src/github.com/couchbase/query\" revision=\"a1708edce7216cdc4f21b4d4dd0eb4001d38e3c0\" upstream=\"mad-hatter\" />",
 "  <project groups=\"notdefault,enterprise\" name=\"query-ee\" path=\"goproj/src/github.com/couchbase/query-ee\" remote=\"couchbase-priv\" revision=\"3ef4ab89910a53b6acfaba4cc7d96091ab33a346\" upstream=\"mad-hatter\" />",
 "  <project name=\"query-ui\" revision=\"d736c5b2b97eeea0bf8170a40cfa7533e168388e\" upstream=\"master\" />",
 "  <project name=\"retriever\" path=\"godeps/src/github.com/couchbase/retriever\" revision=\"e3419088e4d3b4fe3aad3b364fdbe9a154f85f17\" upstream=\"master\" />",
 "  <project name=\"roaring\" path=\"godeps/src/github.com/RoaringBitmap/roaring\" remote=\"couchbasedeps\" revision=\"d0ce1763c3526f65703c395da50da7a7fb2138d5\" />",
 "  <project name=\"segment\" path=\"godeps/src/github.com/blevesearch/segment\" remote=\"blevesearch\" revision=\"762005e7a34fd909a84586299f1dd457371d36ee\" />",
 "  <project groups=\"kv\" name=\"sigar\" revision=\"c33791d6d5de19d6c5575aa33f8e5dba848414d8\" upstream=\"master\" />",
 "  <project name=\"snowballstem\" path=\"godeps/src/github.com/blevesearch/snowballstem\" remote=\"blevesearch\" revision=\"26b06a2c243d4f8ca5db3486f94409dd5b2a7467\" />",
 "  <project groups=\"kv\" name=\"spdlog\" path=\"third_party/spdlog\" remote=\"couchbasedeps\" revision=\"20967a170429d0d37e09a485bc3cf5b153554924\" upstream=\"v1.1.0-couchbase\" />",
 "  <project name=\"strconv\" path=\"godeps/src/github.com/tdewolff/strconv\" remote=\"couchbasedeps\" revision=\"9b189f5be77f33c46776f24dbddb2a7ab32af214\" />",
 "  <project groups=\"kv\" name=\"subjson\" revision=\"ae63ab4b653870e400855f8563da40dda49f0eb3\" upstream=\"master\" />",
 "  <project groups=\"backup\" name=\"sys\" path=\"godeps/src/golang.org/x/sys\" remote=\"couchbasedeps\" revision=\"7fbe1cd0fcc20051e1fcb87fbabec4a1bacaaeba\" />",
 "  <project name=\"testrunner\" revision=\"ee64d41320d14fabe814a241a5cf4f6a6f6e827a\" upstream=\"mad-hatter\" />",
 "  <project groups=\"backup\" name=\"text\" path=\"godeps/src/golang.org/x/text\" remote=\"couchbasedeps\" revision=\"88f656faf3f37f690df1a32515b479415e1a6769\" />",
 "  <project groups=\"kv\" name=\"tlm\" revision=\"7279de40e2a171aeed67b2566bd499d7157df965\">",
 "    <copyfile dest=\"GNUmakefile\" src=\"GNUmakefile\" />",
 "    <copyfile dest=\"Makefile\" src=\"Makefile\" />",
 "    <copyfile dest=\"CMakeLists.txt\" src=\"CMakeLists.txt\" />",
 "    <copyfile dest=\".clang-format\" src=\"dot-clang-format\" />",
 "    <copyfile dest=\"third_party/CMakeLists.txt\" src=\"third-party-CMakeLists.txt\" />",
 "  </project>",
 "  <project groups=\"backup\" name=\"ts\" path=\"godeps/src/github.com/olekukonko/ts\" remote=\"couchbasedeps\" revision=\"ecf753e7c962639ab5a1fb46f7da627d4c0a04b8\" />",
 "  <project groups=\"backup\" name=\"uuid\" path=\"godeps/src/github.com/google/uuid\" remote=\"couchbasedeps\" revision=\"dec09d789f3dba190787f8b4454c7d3c936fed9e\" />",
 "  <project name=\"vellum\" path=\"godeps/src/github.com/couchbase/vellum\" revision=\"ef2e028c01fdb60c46da4067d2e83745b8d54120\" upstream=\"master\" />",
 "  <project groups=\"notdefault,packaging\" name=\"voltron\" remote=\"couchbase-priv\" revision=\"45188488712448a326c8efad0d8c7b00e8afbefe\" upstream=\"master\" />",
 "  <project name=\"zstd\" path=\"godeps/src/github.com/DataDog/zstd\" remote=\"couchbasedeps\" revision=\"aebefd9fcb99f22cd691ef778a12ed68f0e6a1ab\" />",
 "</manifest>"]

[error_logger:info,2020-03-27T20:37:38.980Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.189.0>},
                       {id,timeout_diag_logger},
                       {mfargs,{timeout_diag_logger,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:37:38.982Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.190.0>},
                       {id,ns_cookie_manager},
                       {mfargs,{ns_cookie_manager,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:37:38.984Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.191.0>},
                       {id,ns_cluster},
                       {mfargs,{ns_cluster,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[ns_server:info,2020-03-27T20:37:38.985Z,ns_1@127.0.0.1:ns_config_sup<0.192.0>:ns_config_sup:init:32]loading static ns_config from "/opt/couchbase/etc/couchbase/config"
[error_logger:info,2020-03-27T20:37:38.985Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_config_sup}
             started: [{pid,<0.193.0>},
                       {id,ns_config_events},
                       {mfargs,
                           {gen_event,start_link,[{local,ns_config_events}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:37:38.985Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_config_sup}
             started: [{pid,<0.194.0>},
                       {id,ns_config_events_local},
                       {mfargs,
                           {gen_event,start_link,
                               [{local,ns_config_events_local}]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[ns_server:info,2020-03-27T20:37:39.014Z,ns_1@127.0.0.1:ns_config<0.195.0>:ns_config:load_config:1106]Loading static config from "/opt/couchbase/etc/couchbase/config"
[ns_server:info,2020-03-27T20:37:39.018Z,ns_1@127.0.0.1:ns_config<0.195.0>:ns_config:load_config:1120]Loading dynamic config from "/opt/couchbase/var/lib/couchbase/config/config.dat"
[ns_server:debug,2020-03-27T20:37:39.038Z,ns_1@127.0.0.1:ns_config<0.195.0>:ns_config:load_config:1128]Here's full dynamic config we loaded:
[[{cluster_name,
   [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{3,63752558934}}]},
    70,105,114,115,116]},
  {{metakv,<<"/eventing/settings/config">>},
   [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557537}}]}|
    <<"{\"ram_quota\":256}">>]},
  {cbas_memory_quota,1024},
  {fts_memory_quota,256},
  {{metakv,<<"/indexing/settings/config">>},
   [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{8,63752558934}}]}|
    <<"{\"indexer.settings.compaction.days_of_week\":\"Sunday,Monday,Tuesday,Wednesday,Thursday,Friday,Saturday\",\"indexer.settings.compaction.interval\":\"00:00,00:00\",\"indexer.settings.compaction.compaction_mode\":\"circular\",\"indexer.settings.log_level\":\"info\",\"indexer.settings.persisted_snapshot.interval\":5000,\"indexer.settings.compaction.min_frag\":30,\"indexer.settings.inmemory_snapshot.interval\":200,\"indexer.settings.max_cpu_percent\":0,\"indexer.settings.storage_mode\":\"plasma\",\"indexer.settings.recovery.max_rollbacks\":2,\"indexer.settings.memory_quota\":536870912,\"indexer.settings.compaction.abort_exceed_interval\":false}">>]},
  {memory_quota,292},
  {{node,'ns_1@127.0.0.1',erl_external_listeners},
   [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]},
    {inet,false},
    {inet6,false}]},
  {{node,'ns_1@127.0.0.1',node_encryption},
   [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
    false]},
  {{node,'ns_1@127.0.0.1',address_family},
   [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
    inet]},
  {auto_failover_cfg,
   [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557537}}]},
    {enabled,true},
    {timeout,120},
    {count,0},
    {failover_on_data_disk_issues,[{enabled,false},{timePeriod,120}]},
    {failover_server_group,false},
    {max_count,1},
    {failed_over_server_groups,[]},
    {can_abort_rebalance,true}]},
  {alert_limits,
   [{max_overhead_perc,50},{max_disk_used,90},{max_indexer_ram,75}]},
  {audit,
   [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557537}}]},
    {enabled,[]},
    {disabled_users,[]},
    {auditd_enabled,false},
    {rotate_interval,86400},
    {rotate_size,20971520},
    {disabled,[]},
    {sync,[]},
    {log_path,"/opt/couchbase/var/lib/couchbase/logs"}]},
  {audit_decriptors,
   [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557537}}]},
    {8243,
     [{name,<<"mutate document">>},
      {description,<<"Document was mutated via the REST API">>},
      {enabled,true},
      {module,ns_server}]},
    {8255,
     [{name,<<"read document">>},
      {description,<<"Document was read via the REST API">>},
      {enabled,false},
      {module,ns_server}]},
    {8257,
     [{name,<<"alert email sent">>},
      {description,<<"An alert email was successfully sent">>},
      {enabled,true},
      {module,ns_server}]},
    {20480,
     [{name,<<"opened DCP connection">>},
      {description,<<"opened DCP connection">>},
      {enabled,true},
      {module,memcached}]},
    {20482,
     [{name,<<"external memcached bucket flush">>},
      {description,
       <<"External user flushed the content of a memcached bucket">>},
      {enabled,true},
      {module,memcached}]},
    {20483,
     [{name,<<"invalid packet">>},
      {description,<<"Rejected an invalid packet">>},
      {enabled,true},
      {module,memcached}]},
    {20485,
     [{name,<<"authentication succeeded">>},
      {description,<<"Authentication to the cluster succeeded">>},
      {enabled,false},
      {module,memcached}]},
    {20488,
     [{name,<<"document read">>},
      {description,<<"Document was read">>},
      {enabled,false},
      {module,memcached}]},
    {20489,
     [{name,<<"document locked">>},
      {description,<<"Document was locked">>},
      {enabled,false},
      {module,memcached}]},
    {20490,
     [{name,<<"document modify">>},
      {description,<<"Document was modified">>},
      {enabled,false},
      {module,memcached}]},
    {20491,
     [{name,<<"document delete">>},
      {description,<<"Document was deleted">>},
      {enabled,false},
      {module,memcached}]},
    {20492,
     [{name,<<"select bucket">>},
      {description,<<"The specified bucket was selected">>},
      {enabled,true},
      {module,memcached}]},
    {28672,
     [{name,<<"SELECT statement">>},
      {description,<<"A N1QL SELECT statement was executed">>},
      {enabled,false},
      {module,n1ql}]},
    {28673,
     [{name,<<"EXPLAIN statement">>},
      {description,<<"A N1QL EXPLAIN statement was executed">>},
      {enabled,false},
      {module,n1ql}]},
    {28674,
     [{name,<<"PREPARE statement">>},
      {description,<<"A N1QL PREPARE statement was executed">>},
      {enabled,false},
      {module,n1ql}]},
    {28675,
     [{name,<<"INFER statement">>},
      {description,<<"A N1QL INFER statement was executed">>},
      {enabled,false},
      {module,n1ql}]},
    {28676,
     [{name,<<"INSERT statement">>},
      {description,<<"A N1QL INSERT statement was executed">>},
      {enabled,false},
      {module,n1ql}]},
    {28677,
     [{name,<<"UPSERT statement">>},
      {description,<<"A N1QL UPSERT statement was executed">>},
      {enabled,false},
      {module,n1ql}]},
    {28678,
     [{name,<<"DELETE statement">>},
      {description,<<"A N1QL DELETE statement was executed">>},
      {enabled,false},
      {module,n1ql}]},
    {28679,
     [{name,<<"UPDATE statement">>},
      {description,<<"A N1QL UPDATE statement was executed">>},
      {enabled,false},
      {module,n1ql}]},
    {28680,
     [{name,<<"MERGE statement">>},
      {description,<<"A N1QL MERGE statement was executed">>},
      {enabled,false},
      {module,n1ql}]},
    {28681,
     [{name,<<"CREATE INDEX statement">>},
      {description,<<"A N1QL CREATE INDEX statement was executed">>},
      {enabled,false},
      {module,n1ql}]},
    {28682,
     [{name,<<"DROP INDEX statement">>},
      {description,<<"A N1QL DROP INDEX statement was executed">>},
      {enabled,false},
      {module,n1ql}]},
    {28683,
     [{name,<<"ALTER INDEX statement">>},
      {description,<<"A N1QL ALTER INDEX statement was executed">>},
      {enabled,false},
      {module,n1ql}]},
    {28684,
     [{name,<<"BUILD INDEX statement">>},
      {description,<<"A N1QL BUILD INDEX statement was executed">>},
      {enabled,false},
      {module,n1ql}]},
    {28685,
     [{name,<<"GRANT ROLE statement">>},
      {description,<<"A N1QL GRANT ROLE statement was executed">>},
      {enabled,false},
      {module,n1ql}]},
    {28686,
     [{name,<<"REVOKE ROLE statement">>},
      {description,<<"A N1QL REVOKE ROLE statement was executed">>},
      {enabled,false},
      {module,n1ql}]},
    {28687,
     [{name,<<"UNRECOGNIZED statement">>},
      {description,
       <<"An unrecognized statement was received by the N1QL query engine">>},
      {enabled,false},
      {module,n1ql}]},
    {28688,
     [{name,<<"CREATE PRIMARY INDEX statement">>},
      {description,<<"A N1QL CREATE PRIMARY INDEX statement was executed">>},
      {enabled,false},
      {module,n1ql}]},
    {28689,
     [{name,<<"/admin/stats API request">>},
      {description,<<"An HTTP request was made to the API at /admin/stats.">>},
      {enabled,false},
      {module,n1ql}]},
    {28690,
     [{name,<<"/admin/vitals API request">>},
      {description,
       <<"An HTTP request was made to the API at /admin/vitals.">>},
      {enabled,false},
      {module,n1ql}]},
    {28691,
     [{name,<<"/admin/prepareds API request">>},
      {description,
       <<"An HTTP request was made to the API at /admin/prepareds.">>},
      {enabled,false},
      {module,n1ql}]},
    {28692,
     [{name,<<"/admin/active_requests API request">>},
      {description,
       <<"An HTTP request was made to the API at /admin/active_requests.">>},
      {enabled,false},
      {module,n1ql}]},
    {28693,
     [{name,<<"/admin/indexes/prepareds API request">>},
      {description,
       <<"An HTTP request was made to the API at /admin/indexes/prepareds.">>},
      {enabled,false},
      {module,n1ql}]},
    {28694,
     [{name,<<"/admin/indexes/active_requests API request">>},
      {description,
       <<"An HTTP request was made to the API at /admin/indexes/active_requests.">>},
      {enabled,false},
      {module,n1ql}]},
    {28695,
     [{name,<<"/admin/indexes/completed_requests API request">>},
      {description,
       <<"An HTTP request was made to the API at /admin/indexes/completed_requests.">>},
      {enabled,false},
      {module,n1ql}]},
    {28697,
     [{name,<<"/admin/ping API request">>},
      {description,<<"An HTTP request was made to the API at /admin/ping.">>},
      {enabled,false},
      {module,n1ql}]},
    {28698,
     [{name,<<"/admin/config API request">>},
      {description,
       <<"An HTTP request was made to the API at /admin/config.">>},
      {enabled,false},
      {module,n1ql}]},
    {28699,
     [{name,<<"/admin/ssl_cert API request">>},
      {description,
       <<"An HTTP request was made to the API at /admin/ssl_cert.">>},
      {enabled,false},
      {module,n1ql}]},
    {28700,
     [{name,<<"/admin/settings API request">>},
      {description,
       <<"An HTTP request was made to the API at /admin/settings.">>},
      {enabled,false},
      {module,n1ql}]},
    {28701,
     [{name,<<"/admin/clusters API request">>},
      {description,
       <<"An HTTP request was made to the API at /admin/clusters.">>},
      {enabled,false},
      {module,n1ql}]},
    {28702,
     [{name,<<"/admin/completed_requests API request">>},
      {description,
       <<"An HTTP request was made to the API at /admin/completed_requests.">>},
      {enabled,false},
      {module,n1ql}]},
    {28704,
     [{name,<<"/admin/functions API request">>},
      {description,
       <<"An HTTP request was made to the API at /admin/functions.">>},
      {enabled,false},
      {module,n1ql}]},
    {28705,
     [{name,<<"/admin/indexes/functions API request">>},
      {description,
       <<"An HTTP request was made to the API at /admin/indexes/functions.">>},
      {enabled,false},
      {module,n1ql}]},
    {32768,
     [{name,<<"Create Function">>},
      {description,<<"Eventing function definition was created or updated">>},
      {enabled,true},
      {module,eventing}]},
    {32769,
     [{name,<<"Delete Function">>},
      {description,<<"Eventing function definition was deleted">>},
      {enabled,true},
      {module,eventing}]},
    {32770,
     [{name,<<"Fetch Functions">>},
      {description,<<"Eventing function definition was read">>},
      {enabled,false},
      {module,eventing}]},
    {32771,
     [{name,<<"List Deployed">>},
      {description,<<"Eventing deployed functions list was read">>},
      {enabled,false},
      {module,eventing}]},
    {32772,
     [{name,<<"Fetch Drafts">>},
      {description,<<"Eventing function draft definitions were read">>},
      {enabled,false},
      {module,eventing}]},
    {32773,
     [{name,<<"Delete Drafts">>},
      {description,<<"Eventing function draft definitions were deleted">>},
      {enabled,true},
      {module,eventing}]},
    {32774,
     [{name,<<"Save Draft">>},
      {description,<<"Save a draft definition to the store">>},
      {enabled,true},
      {module,eventing}]},
    {32775,
     [{name,<<"Start Debug">>},
      {description,<<"Start eventing function debugger">>},
      {enabled,true},
      {module,eventing}]},
    {32776,
     [{name,<<"Stop Debug">>},
      {description,<<"Stop eventing function debugger">>},
      {enabled,true},
      {module,eventing}]},
    {32777,
     [{name,<<"Start Tracing">>},
      {description,<<"Start tracing eventing function execution">>},
      {enabled,true},
      {module,eventing}]},
    {32778,
     [{name,<<"Stop Tracing">>},
      {description,<<"Stop tracing eventing function execution">>},
      {enabled,true},
      {module,eventing}]},
    {32779,
     [{name,<<"Set Settings">>},
      {description,<<"Save settings for a given app">>},
      {enabled,true},
      {module,eventing}]},
    {32780,
     [{name,<<"Fetch Config">>},
      {description,<<"Get config for eventing">>},
      {enabled,false},
      {module,eventing}]},
    {32781,
     [{name,<<"Save Config">>},
      {description,<<"Save config for eventing">>},
      {enabled,true},
      {module,eventing}]},
    {32782,
     [{name,<<"Cleanup Eventing">>},
      {description,<<"Clears up app definitions and settings from metakv">>},
      {enabled,true},
      {module,eventing}]},
    {32783,
     [{name,<<"Get Settings">>},
      {description,<<"Get settings for a given app">>},
      {enabled,false},
      {module,eventing}]},
    {32784,
     [{name,<<"Import Functions">>},
      {description,<<"Import a list of functions">>},
      {enabled,false},
      {module,eventing}]},
    {32785,
     [{name,<<"Export Functions">>},
      {description,<<"Export the list of functions">>},
      {enabled,false},
      {module,eventing}]},
    {32786,
     [{name,<<"List Running">>},
      {description,<<"Eventing running function list was read">>},
      {enabled,false},
      {module,eventing}]},
    {36865,
     [{name,<<"Service configuration change">>},
      {description,<<"A successful service configuration change was made.">>},
      {enabled,true},
      {module,analytics}]},
    {36866,
     [{name,<<"Node configuration change">>},
      {description,<<"A successful node configuration change was made.">>},
      {enabled,true},
      {module,analytics}]},
    {40960,
     [{name,<<"Create Design Doc">>},
      {description,<<"Design Doc is Created">>},
      {enabled,true},
      {module,view_engine}]},
    {40961,
     [{name,<<"Delete Design Doc">>},
      {description,<<"Design Doc is Deleted">>},
      {enabled,true},
      {module,view_engine}]},
    {40962,
     [{name,<<"Query DDoc Meta Data">>},
      {description,<<"Design Doc Meta Data Query Request">>},
      {enabled,true},
      {module,view_engine}]},
    {40963,
     [{name,<<"View Query">>},
      {description,<<"View Query Request">>},
      {enabled,false},
      {module,view_engine}]},
    {40964,
     [{name,<<"Update Design Doc">>},
      {description,<<"Design Doc is Updated">>},
      {enabled,true},
      {module,view_engine}]}]},
  {auto_reprovision_cfg,[{enabled,true},{max_nodes,1},{count,0}]},
  {autocompaction,
   [{database_fragmentation_threshold,{30,undefined}},
    {view_fragmentation_threshold,{30,undefined}}]},
  {buckets,
   [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557537}}]},
    {configs,[]}]},
  {cert_and_pkey,
   [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557532}}]}|
    {<<"-----BEGIN CERTIFICATE-----\nMIIDAjCCAeqgAwIBAgIIFgBA0G6s0lwwDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciAzNDhlZDI3MDAeFw0xMzAxMDEwMDAwMDBaFw00\nOTEyMzEyMzU5NTlaMCQxIjAgBgNVBAMTGUNvdWNoYmFzZSBTZXJ2ZXIgMzQ4ZWQy\nNzAwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQDh0efJgvfpArvc3iI0\ncHa/zZhS9WoaoNHksFooYqy3AYiZR+8S3IgPS0nfcZGrIWJUPgokUqBqh6u59abc\nvCuyMwLNkkuLOkT6wzUztrAqoCoOEl4avKFmI0JsiWA/qKDU/kapi0rG9MzOwmWz\nhLUBFUx1SGUYpWzUOP0v5riPMhR5SvfMR/fiYfm/ruKWF5VHtNFW2EgfysURPlxa\ndArURVkUW6MzTV6cDXMXR1P83qZVhKvWp65OPD9O8XPHRrqvv17QBditavjLOEgY\n2xdWQ4kS3avQASnvVP2e5iRO3iF4WmDRJAXmWqMTAiOlUICrBloL7aIjA8lebsm1\nDTGlAgMBAAGjODA2MA4GA1UdDwEB/wQEAwICpDATBgNVHSUEDDAKBggrBgEFBQcD\nATAPBgNVHRMBAf8EBTADAQH/MA0GCSqGSIb3DQEBCwUAA4IBAQAGUUF8Ryt6D6fP\nEPsAnydivOMd12RrwrjQMeoxnRYQcuKuDZaskALu5ggo3Ro3OIR+6ZkYuSlsG+/Y\nUng/DPuFi78HQfWDXCLJ5sQ1PpyedT7cn8Blq4OPkZcrp+Naoa1NmBt8QzGWQ2ah\n3Sum/CJq0EzfSebYoRqEiYGST45Fs8HCL4HP+wTttPc2myjYJE87brwbmdAt0/Wd\nQBPz4YAtfqxhpEE3+uNa1DuZidvDVKttkUBv/W14I8eZ6Hiqs6dusHQK9U8GQp6U\nib99QaSZXrTiU+bx7CdDKAxbLE82QUj5S+vezY5wN+AvfSrrvA6AhizkGaMZWvaq\nSqN1PyyX\n-----END CERTIFICATE-----\n">>,
     <<"*****">>}]},
  {client_cert_auth,
   [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557537}}]},
    {state,"disable"},
    {prefixes,[]}]},
  {cluster_compat_version,
   [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{5,63752557537}}]},
    6,5]},
  {drop_request_memory_threshold_mib,undefined},
  {email_alerts,
   [{recipients,["root@localhost"]},
    {sender,"couchbase@localhost"},
    {enabled,false},
    {email_server,
     [{user,[]},{pass,"*****"},{host,"localhost"},{port,25},{encrypt,false}]},
    {alerts,
     [auto_failover_node,auto_failover_maximum_reached,
      auto_failover_other_nodes_down,auto_failover_cluster_too_small,
      auto_failover_disabled,ip,disk,overhead,ep_oom_errors,
      ep_item_commit_failed,audit_dropped_events,indexer_ram_max_usage,
      ep_clock_cas_drift_threshold_exceeded,communication_issue]}]},
  {index_aware_rebalance_disabled,false},
  {log_redaction_default_cfg,[{redact_level,none}]},
  {max_bucket_count,
   [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557537}}]}|
    30]},
  {memcached,[]},
  {nodes_wanted,
   [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557625}}]},
    'ns_1@127.0.0.1']},
  {otp,
   [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557536}}]},
    {cookie,{sanitized,<<"C4PeZIe8LLStUy+d1HIN638gh0wTZW33h1wdSpFkvzc=">>}}]},
  {password_policy,[{min_length,6},{must_present,[]}]},
  {quorum_nodes,
   [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]},
    'ns_1@127.0.0.1']},
  {remote_clusters,[]},
  {replication,[{enabled,true}]},
  {rest,[{port,8091}]},
  {rest_creds,null},
  {retry_rebalance,
   [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557537}}]},
    {enabled,false},
    {after_time_period,300},
    {max_attempts,1}]},
  {scramsha_fallback_salt,
   [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557537}}]}|
    <<176,211,90,3,15,137,19,29,193,47,34,182>>]},
  {secure_headers,[]},
  {server_groups,
   [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557625}}]},
    [{uuid,<<"0">>},{name,<<"Group 1">>},{nodes,['ns_1@127.0.0.1']}]]},
  {set_view_update_daemon,
   [{update_interval,5000},
    {update_min_changes,5000},
    {replica_update_min_changes,5000}]},
  {{couchdb,max_parallel_indexers},4},
  {{couchdb,max_parallel_replica_indexers},2},
  {{local_changes_count,<<"60a6bc3db77e6c7b91c556140dcfec71">>},
   [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{35,63752558934}}]}]},
  {{metakv,<<"/query/settings/config">>},
   [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557537}}]}|
    <<"{\"timeout\":0,\"n1ql-feat-ctrl\":12,\"max-parallelism\":1,\"query.settings.curl_whitelist\":{\"all_access\":false,\"allowed_urls\":[],\"disallowed_urls\":[]},\"query.settings.tmp_space_dir\":\"/opt/couchbase/var/lib/couchbase/tmp\",\"completed-limit\":4000,\"prepared-limit\":16384,\"pipeline-batch\":16,\"pipeline-cap\":512,\"scan-cap\":512,\"loglevel\":\"info\",\"completed-threshold\":1000,\"query.settings.tmp_space_size\":5120}">>]},
  {{request_limit,capi},undefined},
  {{request_limit,rest},undefined},
  {{node,'ns_1@127.0.0.1',audit},
   [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}]},
  {{node,'ns_1@127.0.0.1',capi_port},
   [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
    8092]},
  {{node,'ns_1@127.0.0.1',cbas_admin_port},
   [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
    9110]},
  {{node,'ns_1@127.0.0.1',cbas_cc_client_port},
   [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
    9113]},
  {{node,'ns_1@127.0.0.1',cbas_cc_cluster_port},
   [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
    9112]},
  {{node,'ns_1@127.0.0.1',cbas_cc_http_port},
   [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
    9111]},
  {{node,'ns_1@127.0.0.1',cbas_cluster_port},
   [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
    9115]},
  {{node,'ns_1@127.0.0.1',cbas_console_port},
   [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
    9114]},
  {{node,'ns_1@127.0.0.1',cbas_data_port},
   [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
    9116]},
  {{node,'ns_1@127.0.0.1',cbas_debug_port},
   [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
    -1]},
  {{node,'ns_1@127.0.0.1',cbas_dirs},
   [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]},
    "/opt/couchbase/var/lib/couchbase/data"]},
  {{node,'ns_1@127.0.0.1',cbas_http_port},
   [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
    8095]},
  {{node,'ns_1@127.0.0.1',cbas_messaging_port},
   [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
    9118]},
  {{node,'ns_1@127.0.0.1',cbas_metadata_callback_port},
   [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
    9119]},
  {{node,'ns_1@127.0.0.1',cbas_metadata_port},
   [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
    9121]},
  {{node,'ns_1@127.0.0.1',cbas_parent_port},
   [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
    9122]},
  {{node,'ns_1@127.0.0.1',cbas_replication_port},
   [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
    9120]},
  {{node,'ns_1@127.0.0.1',cbas_result_port},
   [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
    9117]},
  {{node,'ns_1@127.0.0.1',cbas_ssl_port},
   [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
    18095]},
  {{node,'ns_1@127.0.0.1',compaction_daemon},
   [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]},
    {check_interval,30},
    {min_db_file_size,131072},
    {min_view_file_size,20971520}]},
  {{node,'ns_1@127.0.0.1',config_version},
   [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
    {6,5}]},
  {{node,'ns_1@127.0.0.1',eventing_debug_port},
   [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
    9140]},
  {{node,'ns_1@127.0.0.1',eventing_dir},
   [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]},
    47,111,112,116,47,99,111,117,99,104,98,97,115,101,47,118,97,114,47,108,
    105,98,47,99,111,117,99,104,98,97,115,101,47,100,97,116,97]},
  {{node,'ns_1@127.0.0.1',eventing_http_port},
   [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
    8096]},
  {{node,'ns_1@127.0.0.1',eventing_https_port},
   [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
    18096]},
  {{node,'ns_1@127.0.0.1',fts_grpc_port},
   [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
    9130]},
  {{node,'ns_1@127.0.0.1',fts_grpc_ssl_port},
   [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
    19130]},
  {{node,'ns_1@127.0.0.1',fts_http_port},
   [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
    8094]},
  {{node,'ns_1@127.0.0.1',fts_ssl_port},
   [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
    18094]},
  {{node,'ns_1@127.0.0.1',indexer_admin_port},
   [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
    9100]},
  {{node,'ns_1@127.0.0.1',indexer_http_port},
   [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
    9102]},
  {{node,'ns_1@127.0.0.1',indexer_https_port},
   [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
    19102]},
  {{node,'ns_1@127.0.0.1',indexer_scan_port},
   [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
    9101]},
  {{node,'ns_1@127.0.0.1',indexer_stcatchup_port},
   [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
    9104]},
  {{node,'ns_1@127.0.0.1',indexer_stinit_port},
   [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
    9103]},
  {{node,'ns_1@127.0.0.1',indexer_stmaint_port},
   [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
    9105]},
  {{node,'ns_1@127.0.0.1',is_enterprise},
   [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
    true]},
  {{node,'ns_1@127.0.0.1',isasl},
   [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]},
    {path,"/opt/couchbase/var/lib/couchbase/isasl.pw"}]},
  {{node,'ns_1@127.0.0.1',membership},
   [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
    active]},
  {{node,'ns_1@127.0.0.1',memcached},
   [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]},
    {port,11210},
    {dedicated_port,11209},
    {dedicated_ssl_port,11206},
    {ssl_port,11207},
    {admin_user,"@ns_server"},
    {other_users,
     ["@cbq-engine","@projector","@goxdcr","@index","@fts","@eventing",
      "@cbas"]},
    {admin_pass,"*****"},
    {engines,
     [{membase,
       [{engine,"/opt/couchbase/lib/memcached/ep.so"},
        {static_config_string,"failpartialwarmup=false"}]},
      {memcached,
       [{engine,"/opt/couchbase/lib/memcached/default_engine.so"},
        {static_config_string,"vb0=true"}]}]},
    {config_path,"/opt/couchbase/var/lib/couchbase/config/memcached.json"},
    {audit_file,"/opt/couchbase/var/lib/couchbase/config/audit.json"},
    {rbac_file,"/opt/couchbase/var/lib/couchbase/config/memcached.rbac"},
    {log_path,"/opt/couchbase/var/lib/couchbase/logs"},
    {log_prefix,"memcached.log"},
    {log_generations,20},
    {log_cyclesize,10485760},
    {log_sleeptime,19},
    {log_rotation_period,39003}]},
  {{node,'ns_1@127.0.0.1',memcached_config},
   [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
    {[{interfaces,
       {memcached_config_mgr,omit_missing_mcd_ports,
        [{[{host,<<"*">>},
           {port,port},
           {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
           {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]},
         {[{host,<<"*">>},
           {port,dedicated_port},
           {system,true},
           {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
           {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]},
         {[{host,<<"*">>},
           {port,ssl_port},
           {ssl,
            {[{key,
               <<"/opt/couchbase/var/lib/couchbase/config/memcached-key.pem">>},
              {cert,
               <<"/opt/couchbase/var/lib/couchbase/config/memcached-cert.pem">>}]}},
           {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
           {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]},
         {[{host,<<"*">>},
           {port,dedicated_ssl_port},
           {system,true},
           {ssl,
            {[{key,
               <<"/opt/couchbase/var/lib/couchbase/config/memcached-key.pem">>},
              {cert,
               <<"/opt/couchbase/var/lib/couchbase/config/memcached-cert.pem">>}]}},
           {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
           {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]}]}},
      {ssl_cipher_list,{memcached_config_mgr,get_ssl_cipher_list,[]}},
      {ssl_cipher_order,{memcached_config_mgr,get_ssl_cipher_order,[]}},
      {client_cert_auth,{memcached_config_mgr,client_cert_auth,[]}},
      {ssl_minimum_protocol,{memcached_config_mgr,ssl_minimum_protocol,[]}},
      {connection_idle_time,connection_idle_time},
      {privilege_debug,privilege_debug},
      {breakpad,
       {[{enabled,breakpad_enabled},
         {minidump_dir,{memcached_config_mgr,get_minidump_dir,[]}}]}},
      {opentracing,
       {[{enabled,opentracing_enabled},
         {module,{"~s",[opentracing_module]}},
         {config,{"~s",[opentracing_config]}}]}},
      {admin,{"~s",[admin_user]}},
      {verbosity,verbosity},
      {audit_file,{"~s",[audit_file]}},
      {rbac_file,{"~s",[rbac_file]}},
      {dedupe_nmvb_maps,dedupe_nmvb_maps},
      {tracing_enabled,tracing_enabled},
      {datatype_snappy,{memcached_config_mgr,is_snappy_enabled,[]}},
      {xattr_enabled,true},
      {scramsha_fallback_salt,{memcached_config_mgr,get_fallback_salt,[]}},
      {collections_enabled,{memcached_config_mgr,collections_enabled,[]}},
      {max_connections,max_connections},
      {system_connections,system_connections},
      {num_reader_threads,num_reader_threads},
      {num_writer_threads,num_writer_threads},
      {logger,
       {[{filename,{"~s/~s",[log_path,log_prefix]}},
         {cyclesize,log_cyclesize},
         {sleeptime,log_sleeptime}]}},
      {external_auth_service,
       {memcached_config_mgr,get_external_auth_service,[]}},
      {active_external_users_push_interval,
       {memcached_config_mgr,get_external_users_push_interval,[]}}]}]},
  {{node,'ns_1@127.0.0.1',memcached_dedicated_ssl_port},
   [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
    11206]},
  {{node,'ns_1@127.0.0.1',memcached_defaults},
   [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]},
    {max_connections,65000},
    {system_connections,5000},
    {connection_idle_time,0},
    {verbosity,0},
    {privilege_debug,false},
    {opentracing_enabled,false},
    {opentracing_module,[]},
    {opentracing_config,[]},
    {breakpad_enabled,true},
    {breakpad_minidump_dir_path,"/opt/couchbase/var/lib/couchbase/crash"},
    {dedupe_nmvb_maps,false},
    {tracing_enabled,true},
    {datatype_snappy,true},
    {num_reader_threads,<<"default">>},
    {num_writer_threads,<<"default">>}]},
  {{node,'ns_1@127.0.0.1',moxi},
   [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]},
    {port,0}]},
  {{node,'ns_1@127.0.0.1',ns_log},
   [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]},
    {filename,"/opt/couchbase/var/lib/couchbase/ns_log"}]},
  {{node,'ns_1@127.0.0.1',port_servers},
   [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}]},
  {{node,'ns_1@127.0.0.1',projector_port},
   [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
    9999]},
  {{node,'ns_1@127.0.0.1',projector_ssl_port},
   [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
    9999]},
  {{node,'ns_1@127.0.0.1',query_port},
   [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
    8093]},
  {{node,'ns_1@127.0.0.1',rest},
   [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]},
    {port,8091},
    {port_meta,global}]},
  {{node,'ns_1@127.0.0.1',saslauthd_enabled},
   [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
    true]},
  {{node,'ns_1@127.0.0.1',ssl_capi_port},
   [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
    18092]},
  {{node,'ns_1@127.0.0.1',ssl_query_port},
   [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
    18093]},
  {{node,'ns_1@127.0.0.1',ssl_rest_port},
   [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
    18091]},
  {{node,'ns_1@127.0.0.1',uuid},
   [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
    <<"60a6bc3db77e6c7b91c556140dcfec71">>]},
  {{node,'ns_1@127.0.0.1',xdcr_rest_port},
   [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
    9998]},
  {{node,'ns_1@127.0.0.1',{project_intact,is_vulnerable}},
   [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
    false]}]]
[ns_server:info,2020-03-27T20:37:39.045Z,ns_1@127.0.0.1:ns_config<0.195.0>:ns_config:load_config:1149]Here's full dynamic config we loaded + static & default config:
[{{node,'ns_1@127.0.0.1',{project_intact,is_vulnerable}},
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
   false]},
 {{node,'ns_1@127.0.0.1',xdcr_rest_port},
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
   9998]},
 {{node,'ns_1@127.0.0.1',uuid},
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
   <<"60a6bc3db77e6c7b91c556140dcfec71">>]},
 {{node,'ns_1@127.0.0.1',ssl_rest_port},
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
   18091]},
 {{node,'ns_1@127.0.0.1',ssl_query_port},
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
   18093]},
 {{node,'ns_1@127.0.0.1',ssl_capi_port},
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
   18092]},
 {{node,'ns_1@127.0.0.1',saslauthd_enabled},
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
   true]},
 {{node,'ns_1@127.0.0.1',rest},
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]},
   {port,8091},
   {port_meta,global}]},
 {{node,'ns_1@127.0.0.1',query_port},
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
   8093]},
 {{node,'ns_1@127.0.0.1',projector_ssl_port},
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
   9999]},
 {{node,'ns_1@127.0.0.1',projector_port},
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
   9999]},
 {{node,'ns_1@127.0.0.1',port_servers},
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}]},
 {{node,'ns_1@127.0.0.1',ns_log},
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]},
   {filename,"/opt/couchbase/var/lib/couchbase/ns_log"}]},
 {{node,'ns_1@127.0.0.1',moxi},
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]},
   {port,0}]},
 {{node,'ns_1@127.0.0.1',memcached_defaults},
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]},
   {max_connections,65000},
   {system_connections,5000},
   {connection_idle_time,0},
   {verbosity,0},
   {privilege_debug,false},
   {opentracing_enabled,false},
   {opentracing_module,[]},
   {opentracing_config,[]},
   {breakpad_enabled,true},
   {breakpad_minidump_dir_path,"/opt/couchbase/var/lib/couchbase/crash"},
   {dedupe_nmvb_maps,false},
   {tracing_enabled,true},
   {datatype_snappy,true},
   {num_reader_threads,<<"default">>},
   {num_writer_threads,<<"default">>}]},
 {{node,'ns_1@127.0.0.1',memcached_dedicated_ssl_port},
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
   11206]},
 {{node,'ns_1@127.0.0.1',memcached_config},
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
   {[{interfaces,
      {memcached_config_mgr,omit_missing_mcd_ports,
       [{[{host,<<"*">>},
          {port,port},
          {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
          {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]},
        {[{host,<<"*">>},
          {port,dedicated_port},
          {system,true},
          {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
          {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]},
        {[{host,<<"*">>},
          {port,ssl_port},
          {ssl,
           {[{key,
              <<"/opt/couchbase/var/lib/couchbase/config/memcached-key.pem">>},
             {cert,
              <<"/opt/couchbase/var/lib/couchbase/config/memcached-cert.pem">>}]}},
          {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
          {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]},
        {[{host,<<"*">>},
          {port,dedicated_ssl_port},
          {system,true},
          {ssl,
           {[{key,
              <<"/opt/couchbase/var/lib/couchbase/config/memcached-key.pem">>},
             {cert,
              <<"/opt/couchbase/var/lib/couchbase/config/memcached-cert.pem">>}]}},
          {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
          {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]}]}},
     {ssl_cipher_list,{memcached_config_mgr,get_ssl_cipher_list,[]}},
     {ssl_cipher_order,{memcached_config_mgr,get_ssl_cipher_order,[]}},
     {client_cert_auth,{memcached_config_mgr,client_cert_auth,[]}},
     {ssl_minimum_protocol,{memcached_config_mgr,ssl_minimum_protocol,[]}},
     {connection_idle_time,connection_idle_time},
     {privilege_debug,privilege_debug},
     {breakpad,
      {[{enabled,breakpad_enabled},
        {minidump_dir,{memcached_config_mgr,get_minidump_dir,[]}}]}},
     {opentracing,
      {[{enabled,opentracing_enabled},
        {module,{"~s",[opentracing_module]}},
        {config,{"~s",[opentracing_config]}}]}},
     {admin,{"~s",[admin_user]}},
     {verbosity,verbosity},
     {audit_file,{"~s",[audit_file]}},
     {rbac_file,{"~s",[rbac_file]}},
     {dedupe_nmvb_maps,dedupe_nmvb_maps},
     {tracing_enabled,tracing_enabled},
     {datatype_snappy,{memcached_config_mgr,is_snappy_enabled,[]}},
     {xattr_enabled,true},
     {scramsha_fallback_salt,{memcached_config_mgr,get_fallback_salt,[]}},
     {collections_enabled,{memcached_config_mgr,collections_enabled,[]}},
     {max_connections,max_connections},
     {system_connections,system_connections},
     {num_reader_threads,num_reader_threads},
     {num_writer_threads,num_writer_threads},
     {logger,
      {[{filename,{"~s/~s",[log_path,log_prefix]}},
        {cyclesize,log_cyclesize},
        {sleeptime,log_sleeptime}]}},
     {external_auth_service,
      {memcached_config_mgr,get_external_auth_service,[]}},
     {active_external_users_push_interval,
      {memcached_config_mgr,get_external_users_push_interval,[]}}]}]},
 {{node,'ns_1@127.0.0.1',memcached},
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]},
   {port,11210},
   {dedicated_port,11209},
   {dedicated_ssl_port,11206},
   {ssl_port,11207},
   {admin_user,"@ns_server"},
   {other_users,
    ["@cbq-engine","@projector","@goxdcr","@index","@fts","@eventing",
     "@cbas"]},
   {admin_pass,"*****"},
   {engines,
    [{membase,
      [{engine,"/opt/couchbase/lib/memcached/ep.so"},
       {static_config_string,"failpartialwarmup=false"}]},
     {memcached,
      [{engine,"/opt/couchbase/lib/memcached/default_engine.so"},
       {static_config_string,"vb0=true"}]}]},
   {config_path,"/opt/couchbase/var/lib/couchbase/config/memcached.json"},
   {audit_file,"/opt/couchbase/var/lib/couchbase/config/audit.json"},
   {rbac_file,"/opt/couchbase/var/lib/couchbase/config/memcached.rbac"},
   {log_path,"/opt/couchbase/var/lib/couchbase/logs"},
   {log_prefix,"memcached.log"},
   {log_generations,20},
   {log_cyclesize,10485760},
   {log_sleeptime,19},
   {log_rotation_period,39003}]},
 {{node,'ns_1@127.0.0.1',membership},
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
   active]},
 {{node,'ns_1@127.0.0.1',isasl},
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]},
   {path,"/opt/couchbase/var/lib/couchbase/isasl.pw"}]},
 {{node,'ns_1@127.0.0.1',is_enterprise},
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
   true]},
 {{node,'ns_1@127.0.0.1',indexer_stmaint_port},
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
   9105]},
 {{node,'ns_1@127.0.0.1',indexer_stinit_port},
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
   9103]},
 {{node,'ns_1@127.0.0.1',indexer_stcatchup_port},
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
   9104]},
 {{node,'ns_1@127.0.0.1',indexer_scan_port},
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
   9101]},
 {{node,'ns_1@127.0.0.1',indexer_https_port},
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
   19102]},
 {{node,'ns_1@127.0.0.1',indexer_http_port},
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
   9102]},
 {{node,'ns_1@127.0.0.1',indexer_admin_port},
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
   9100]},
 {{node,'ns_1@127.0.0.1',fts_ssl_port},
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
   18094]},
 {{node,'ns_1@127.0.0.1',fts_http_port},
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
   8094]},
 {{node,'ns_1@127.0.0.1',fts_grpc_ssl_port},
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
   19130]},
 {{node,'ns_1@127.0.0.1',fts_grpc_port},
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
   9130]},
 {{node,'ns_1@127.0.0.1',eventing_https_port},
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
   18096]},
 {{node,'ns_1@127.0.0.1',eventing_http_port},
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
   8096]},
 {{node,'ns_1@127.0.0.1',eventing_dir},
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]},
   47,111,112,116,47,99,111,117,99,104,98,97,115,101,47,118,97,114,47,108,105,
   98,47,99,111,117,99,104,98,97,115,101,47,100,97,116,97]},
 {{node,'ns_1@127.0.0.1',eventing_debug_port},
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
   9140]},
 {{node,'ns_1@127.0.0.1',config_version},
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
   {6,5}]},
 {{node,'ns_1@127.0.0.1',compaction_daemon},
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]},
   {check_interval,30},
   {min_db_file_size,131072},
   {min_view_file_size,20971520}]},
 {{node,'ns_1@127.0.0.1',cbas_ssl_port},
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
   18095]},
 {{node,'ns_1@127.0.0.1',cbas_result_port},
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
   9117]},
 {{node,'ns_1@127.0.0.1',cbas_replication_port},
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
   9120]},
 {{node,'ns_1@127.0.0.1',cbas_parent_port},
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
   9122]},
 {{node,'ns_1@127.0.0.1',cbas_metadata_port},
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
   9121]},
 {{node,'ns_1@127.0.0.1',cbas_metadata_callback_port},
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
   9119]},
 {{node,'ns_1@127.0.0.1',cbas_messaging_port},
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
   9118]},
 {{node,'ns_1@127.0.0.1',cbas_http_port},
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
   8095]},
 {{node,'ns_1@127.0.0.1',cbas_dirs},
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]},
   "/opt/couchbase/var/lib/couchbase/data"]},
 {{node,'ns_1@127.0.0.1',cbas_debug_port},
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|-1]},
 {{node,'ns_1@127.0.0.1',cbas_data_port},
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
   9116]},
 {{node,'ns_1@127.0.0.1',cbas_console_port},
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
   9114]},
 {{node,'ns_1@127.0.0.1',cbas_cluster_port},
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
   9115]},
 {{node,'ns_1@127.0.0.1',cbas_cc_http_port},
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
   9111]},
 {{node,'ns_1@127.0.0.1',cbas_cc_cluster_port},
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
   9112]},
 {{node,'ns_1@127.0.0.1',cbas_cc_client_port},
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
   9113]},
 {{node,'ns_1@127.0.0.1',cbas_admin_port},
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
   9110]},
 {{node,'ns_1@127.0.0.1',capi_port},
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
   8092]},
 {{node,'ns_1@127.0.0.1',audit},
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}]},
 {{request_limit,rest},undefined},
 {{request_limit,capi},undefined},
 {{metakv,<<"/query/settings/config">>},
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557537}}]}|
   <<"{\"timeout\":0,\"n1ql-feat-ctrl\":12,\"max-parallelism\":1,\"query.settings.curl_whitelist\":{\"all_access\":false,\"allowed_urls\":[],\"disallowed_urls\":[]},\"query.settings.tmp_space_dir\":\"/opt/couchbase/var/lib/couchbase/tmp\",\"completed-limit\":4000,\"prepared-limit\":16384,\"pipeline-batch\":16,\"pipeline-cap\":512,\"scan-cap\":512,\"loglevel\":\"info\",\"completed-threshold\":1000,\"query.settings.tmp_space_size\":5120}">>]},
 {{local_changes_count,<<"60a6bc3db77e6c7b91c556140dcfec71">>},
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{35,63752558934}}]}]},
 {{couchdb,max_parallel_replica_indexers},2},
 {{couchdb,max_parallel_indexers},4},
 {set_view_update_daemon,
  [{update_interval,5000},
   {update_min_changes,5000},
   {replica_update_min_changes,5000}]},
 {server_groups,
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557625}}]},
   [{uuid,<<"0">>},{name,<<"Group 1">>},{nodes,['ns_1@127.0.0.1']}]]},
 {secure_headers,[]},
 {scramsha_fallback_salt,
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557537}}]}|
   <<176,211,90,3,15,137,19,29,193,47,34,182>>]},
 {retry_rebalance,
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557537}}]},
   {enabled,false},
   {after_time_period,300},
   {max_attempts,1}]},
 {rest_creds,null},
 {rest,[{port,8091}]},
 {replication,[{enabled,true}]},
 {remote_clusters,[]},
 {quorum_nodes,
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]},
   'ns_1@127.0.0.1']},
 {password_policy,[{min_length,6},{must_present,[]}]},
 {otp,
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557536}}]},
   {cookie,{sanitized,<<"C4PeZIe8LLStUy+d1HIN638gh0wTZW33h1wdSpFkvzc=">>}}]},
 {nodes_wanted,
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557625}}]},
   'ns_1@127.0.0.1']},
 {memcached,[]},
 {max_bucket_count,
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557537}}]}|30]},
 {log_redaction_default_cfg,[{redact_level,none}]},
 {index_aware_rebalance_disabled,false},
 {email_alerts,
  [{recipients,["root@localhost"]},
   {sender,"couchbase@localhost"},
   {enabled,false},
   {email_server,
    [{user,[]},{pass,"*****"},{host,"localhost"},{port,25},{encrypt,false}]},
   {alerts,
    [auto_failover_node,auto_failover_maximum_reached,
     auto_failover_other_nodes_down,auto_failover_cluster_too_small,
     auto_failover_disabled,ip,disk,overhead,ep_oom_errors,
     ep_item_commit_failed,audit_dropped_events,indexer_ram_max_usage,
     ep_clock_cas_drift_threshold_exceeded,communication_issue]}]},
 {drop_request_memory_threshold_mib,undefined},
 {cluster_compat_version,
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{5,63752557537}}]},
   6,5]},
 {client_cert_auth,
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557537}}]},
   {state,"disable"},
   {prefixes,[]}]},
 {cert_and_pkey,
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557532}}]}|
   {<<"-----BEGIN CERTIFICATE-----\nMIIDAjCCAeqgAwIBAgIIFgBA0G6s0lwwDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciAzNDhlZDI3MDAeFw0xMzAxMDEwMDAwMDBaFw00\nOTEyMzEyMzU5NTlaMCQxIjAgBgNVBAMTGUNvdWNoYmFzZSBTZXJ2ZXIgMzQ4ZWQy\nNzAwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQDh0efJgvfpArvc3iI0\ncHa/zZhS9WoaoNHksFooYqy3AYiZR+8S3IgPS0nfcZGrIWJUPgokUqBqh6u59abc\nvCuyMwLNkkuLOkT6wzUztrAqoCoOEl4avKFmI0JsiWA/qKDU/kapi0rG9MzOwmWz\nhLUBFUx1SGUYpWzUOP0v5riPMhR5SvfMR/fiYfm/ruKWF5VHtNFW2EgfysURPlxa\ndArURVkUW6MzTV6cDXMXR1P83qZVhKvWp65OPD9O8XPHRrqvv17QBditavjLOEgY\n2xdWQ4kS3avQASnvVP2e5iRO3iF4WmDRJAXmWqMTAiOlUICrBloL7aIjA8lebsm1\nDTGlAgMBAAGjODA2MA4GA1UdDwEB/wQEAwICpDATBgNVHSUEDDAKBggrBgEFBQcD\nATAPBgNVHRMBAf8EBTADAQH/MA0GCSqGSIb3DQEBCwUAA4IBAQAGUUF8Ryt6D6fP\nEPsAnydivOMd12RrwrjQMeoxnRYQcuKuDZaskALu5ggo3Ro3OIR+6ZkYuSlsG+/Y\nUng/DPuFi78HQfWDXCLJ5sQ1PpyedT7cn8Blq4OPkZcrp+Naoa1NmBt8QzGWQ2ah\n3Sum/CJq0EzfSebYoRqEiYGST45Fs8HCL4HP+wTttPc2myjYJE87brwbmdAt0/Wd\nQBPz4YAtfqxhpEE3+uNa1DuZidvDVKttkUBv/W14I8eZ6Hiqs6dusHQK9U8GQp6U\nib99QaSZXrTiU+bx7CdDKAxbLE82QUj5S+vezY5wN+AvfSrrvA6AhizkGaMZWvaq\nSqN1PyyX\n-----END CERTIFICATE-----\n">>,
    <<"*****">>}]},
 {buckets,
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557537}}]},
   {configs,[]}]},
 {autocompaction,
  [{database_fragmentation_threshold,{30,undefined}},
   {view_fragmentation_threshold,{30,undefined}}]},
 {auto_reprovision_cfg,[{enabled,true},{max_nodes,1},{count,0}]},
 {audit_decriptors,
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557537}}]},
   {8243,
    [{name,<<"mutate document">>},
     {description,<<"Document was mutated via the REST API">>},
     {enabled,true},
     {module,ns_server}]},
   {8255,
    [{name,<<"read document">>},
     {description,<<"Document was read via the REST API">>},
     {enabled,false},
     {module,ns_server}]},
   {8257,
    [{name,<<"alert email sent">>},
     {description,<<"An alert email was successfully sent">>},
     {enabled,true},
     {module,ns_server}]},
   {20480,
    [{name,<<"opened DCP connection">>},
     {description,<<"opened DCP connection">>},
     {enabled,true},
     {module,memcached}]},
   {20482,
    [{name,<<"external memcached bucket flush">>},
     {description,
      <<"External user flushed the content of a memcached bucket">>},
     {enabled,true},
     {module,memcached}]},
   {20483,
    [{name,<<"invalid packet">>},
     {description,<<"Rejected an invalid packet">>},
     {enabled,true},
     {module,memcached}]},
   {20485,
    [{name,<<"authentication succeeded">>},
     {description,<<"Authentication to the cluster succeeded">>},
     {enabled,false},
     {module,memcached}]},
   {20488,
    [{name,<<"document read">>},
     {description,<<"Document was read">>},
     {enabled,false},
     {module,memcached}]},
   {20489,
    [{name,<<"document locked">>},
     {description,<<"Document was locked">>},
     {enabled,false},
     {module,memcached}]},
   {20490,
    [{name,<<"document modify">>},
     {description,<<"Document was modified">>},
     {enabled,false},
     {module,memcached}]},
   {20491,
    [{name,<<"document delete">>},
     {description,<<"Document was deleted">>},
     {enabled,false},
     {module,memcached}]},
   {20492,
    [{name,<<"select bucket">>},
     {description,<<"The specified bucket was selected">>},
     {enabled,true},
     {module,memcached}]},
   {28672,
    [{name,<<"SELECT statement">>},
     {description,<<"A N1QL SELECT statement was executed">>},
     {enabled,false},
     {module,n1ql}]},
   {28673,
    [{name,<<"EXPLAIN statement">>},
     {description,<<"A N1QL EXPLAIN statement was executed">>},
     {enabled,false},
     {module,n1ql}]},
   {28674,
    [{name,<<"PREPARE statement">>},
     {description,<<"A N1QL PREPARE statement was executed">>},
     {enabled,false},
     {module,n1ql}]},
   {28675,
    [{name,<<"INFER statement">>},
     {description,<<"A N1QL INFER statement was executed">>},
     {enabled,false},
     {module,n1ql}]},
   {28676,
    [{name,<<"INSERT statement">>},
     {description,<<"A N1QL INSERT statement was executed">>},
     {enabled,false},
     {module,n1ql}]},
   {28677,
    [{name,<<"UPSERT statement">>},
     {description,<<"A N1QL UPSERT statement was executed">>},
     {enabled,false},
     {module,n1ql}]},
   {28678,
    [{name,<<"DELETE statement">>},
     {description,<<"A N1QL DELETE statement was executed">>},
     {enabled,false},
     {module,n1ql}]},
   {28679,
    [{name,<<"UPDATE statement">>},
     {description,<<"A N1QL UPDATE statement was executed">>},
     {enabled,false},
     {module,n1ql}]},
   {28680,
    [{name,<<"MERGE statement">>},
     {description,<<"A N1QL MERGE statement was executed">>},
     {enabled,false},
     {module,n1ql}]},
   {28681,
    [{name,<<"CREATE INDEX statement">>},
     {description,<<"A N1QL CREATE INDEX statement was executed">>},
     {enabled,false},
     {module,n1ql}]},
   {28682,
    [{name,<<"DROP INDEX statement">>},
     {description,<<"A N1QL DROP INDEX statement was executed">>},
     {enabled,false},
     {module,n1ql}]},
   {28683,
    [{name,<<"ALTER INDEX statement">>},
     {description,<<"A N1QL ALTER INDEX statement was executed">>},
     {enabled,false},
     {module,n1ql}]},
   {28684,
    [{name,<<"BUILD INDEX statement">>},
     {description,<<"A N1QL BUILD INDEX statement was executed">>},
     {enabled,false},
     {module,n1ql}]},
   {28685,
    [{name,<<"GRANT ROLE statement">>},
     {description,<<"A N1QL GRANT ROLE statement was executed">>},
     {enabled,false},
     {module,n1ql}]},
   {28686,
    [{name,<<"REVOKE ROLE statement">>},
     {description,<<"A N1QL REVOKE ROLE statement was executed">>},
     {enabled,false},
     {module,n1ql}]},
   {28687,
    [{name,<<"UNRECOGNIZED statement">>},
     {description,
      <<"An unrecognized statement was received by the N1QL query engine">>},
     {enabled,false},
     {module,n1ql}]},
   {28688,
    [{name,<<"CREATE PRIMARY INDEX statement">>},
     {description,<<"A N1QL CREATE PRIMARY INDEX statement was executed">>},
     {enabled,false},
     {module,n1ql}]},
   {28689,
    [{name,<<"/admin/stats API request">>},
     {description,<<"An HTTP request was made to the API at /admin/stats.">>},
     {enabled,false},
     {module,n1ql}]},
   {28690,
    [{name,<<"/admin/vitals API request">>},
     {description,<<"An HTTP request was made to the API at /admin/vitals.">>},
     {enabled,false},
     {module,n1ql}]},
   {28691,
    [{name,<<"/admin/prepareds API request">>},
     {description,
      <<"An HTTP request was made to the API at /admin/prepareds.">>},
     {enabled,false},
     {module,n1ql}]},
   {28692,
    [{name,<<"/admin/active_requests API request">>},
     {description,
      <<"An HTTP request was made to the API at /admin/active_requests.">>},
     {enabled,false},
     {module,n1ql}]},
   {28693,
    [{name,<<"/admin/indexes/prepareds API request">>},
     {description,
      <<"An HTTP request was made to the API at /admin/indexes/prepareds.">>},
     {enabled,false},
     {module,n1ql}]},
   {28694,
    [{name,<<"/admin/indexes/active_requests API request">>},
     {description,
      <<"An HTTP request was made to the API at /admin/indexes/active_requests.">>},
     {enabled,false},
     {module,n1ql}]},
   {28695,
    [{name,<<"/admin/indexes/completed_requests API request">>},
     {description,
      <<"An HTTP request was made to the API at /admin/indexes/completed_requests.">>},
     {enabled,false},
     {module,n1ql}]},
   {28697,
    [{name,<<"/admin/ping API request">>},
     {description,<<"An HTTP request was made to the API at /admin/ping.">>},
     {enabled,false},
     {module,n1ql}]},
   {28698,
    [{name,<<"/admin/config API request">>},
     {description,<<"An HTTP request was made to the API at /admin/config.">>},
     {enabled,false},
     {module,n1ql}]},
   {28699,
    [{name,<<"/admin/ssl_cert API request">>},
     {description,
      <<"An HTTP request was made to the API at /admin/ssl_cert.">>},
     {enabled,false},
     {module,n1ql}]},
   {28700,
    [{name,<<"/admin/settings API request">>},
     {description,
      <<"An HTTP request was made to the API at /admin/settings.">>},
     {enabled,false},
     {module,n1ql}]},
   {28701,
    [{name,<<"/admin/clusters API request">>},
     {description,
      <<"An HTTP request was made to the API at /admin/clusters.">>},
     {enabled,false},
     {module,n1ql}]},
   {28702,
    [{name,<<"/admin/completed_requests API request">>},
     {description,
      <<"An HTTP request was made to the API at /admin/completed_requests.">>},
     {enabled,false},
     {module,n1ql}]},
   {28704,
    [{name,<<"/admin/functions API request">>},
     {description,
      <<"An HTTP request was made to the API at /admin/functions.">>},
     {enabled,false},
     {module,n1ql}]},
   {28705,
    [{name,<<"/admin/indexes/functions API request">>},
     {description,
      <<"An HTTP request was made to the API at /admin/indexes/functions.">>},
     {enabled,false},
     {module,n1ql}]},
   {32768,
    [{name,<<"Create Function">>},
     {description,<<"Eventing function definition was created or updated">>},
     {enabled,true},
     {module,eventing}]},
   {32769,
    [{name,<<"Delete Function">>},
     {description,<<"Eventing function definition was deleted">>},
     {enabled,true},
     {module,eventing}]},
   {32770,
    [{name,<<"Fetch Functions">>},
     {description,<<"Eventing function definition was read">>},
     {enabled,false},
     {module,eventing}]},
   {32771,
    [{name,<<"List Deployed">>},
     {description,<<"Eventing deployed functions list was read">>},
     {enabled,false},
     {module,eventing}]},
   {32772,
    [{name,<<"Fetch Drafts">>},
     {description,<<"Eventing function draft definitions were read">>},
     {enabled,false},
     {module,eventing}]},
   {32773,
    [{name,<<"Delete Drafts">>},
     {description,<<"Eventing function draft definitions were deleted">>},
     {enabled,true},
     {module,eventing}]},
   {32774,
    [{name,<<"Save Draft">>},
     {description,<<"Save a draft definition to the store">>},
     {enabled,true},
     {module,eventing}]},
   {32775,
    [{name,<<"Start Debug">>},
     {description,<<"Start eventing function debugger">>},
     {enabled,true},
     {module,eventing}]},
   {32776,
    [{name,<<"Stop Debug">>},
     {description,<<"Stop eventing function debugger">>},
     {enabled,true},
     {module,eventing}]},
   {32777,
    [{name,<<"Start Tracing">>},
     {description,<<"Start tracing eventing function execution">>},
     {enabled,true},
     {module,eventing}]},
   {32778,
    [{name,<<"Stop Tracing">>},
     {description,<<"Stop tracing eventing function execution">>},
     {enabled,true},
     {module,eventing}]},
   {32779,
    [{name,<<"Set Settings">>},
     {description,<<"Save settings for a given app">>},
     {enabled,true},
     {module,eventing}]},
   {32780,
    [{name,<<"Fetch Config">>},
     {description,<<"Get config for eventing">>},
     {enabled,false},
     {module,eventing}]},
   {32781,
    [{name,<<"Save Config">>},
     {description,<<"Save config for eventing">>},
     {enabled,true},
     {module,eventing}]},
   {32782,
    [{name,<<"Cleanup Eventing">>},
     {description,<<"Clears up app definitions and settings from metakv">>},
     {enabled,true},
     {module,eventing}]},
   {32783,
    [{name,<<"Get Settings">>},
     {description,<<"Get settings for a given app">>},
     {enabled,false},
     {module,eventing}]},
   {32784,
    [{name,<<"Import Functions">>},
     {description,<<"Import a list of functions">>},
     {enabled,false},
     {module,eventing}]},
   {32785,
    [{name,<<"Export Functions">>},
     {description,<<"Export the list of functions">>},
     {enabled,false},
     {module,eventing}]},
   {32786,
    [{name,<<"List Running">>},
     {description,<<"Eventing running function list was read">>},
     {enabled,false},
     {module,eventing}]},
   {36865,
    [{name,<<"Service configuration change">>},
     {description,<<"A successful service configuration change was made.">>},
     {enabled,true},
     {module,analytics}]},
   {36866,
    [{name,<<"Node configuration change">>},
     {description,<<"A successful node configuration change was made.">>},
     {enabled,true},
     {module,analytics}]},
   {40960,
    [{name,<<"Create Design Doc">>},
     {description,<<"Design Doc is Created">>},
     {enabled,true},
     {module,view_engine}]},
   {40961,
    [{name,<<"Delete Design Doc">>},
     {description,<<"Design Doc is Deleted">>},
     {enabled,true},
     {module,view_engine}]},
   {40962,
    [{name,<<"Query DDoc Meta Data">>},
     {description,<<"Design Doc Meta Data Query Request">>},
     {enabled,true},
     {module,view_engine}]},
   {40963,
    [{name,<<"View Query">>},
     {description,<<"View Query Request">>},
     {enabled,false},
     {module,view_engine}]},
   {40964,
    [{name,<<"Update Design Doc">>},
     {description,<<"Design Doc is Updated">>},
     {enabled,true},
     {module,view_engine}]}]},
 {audit,
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557537}}]},
   {enabled,[]},
   {disabled_users,[]},
   {auditd_enabled,false},
   {rotate_interval,86400},
   {rotate_size,20971520},
   {disabled,[]},
   {sync,[]},
   {log_path,"/opt/couchbase/var/lib/couchbase/logs"}]},
 {alert_limits,
  [{max_overhead_perc,50},{max_disk_used,90},{max_indexer_ram,75}]},
 {auto_failover_cfg,
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557537}}]},
   {enabled,true},
   {timeout,120},
   {count,0},
   {failover_on_data_disk_issues,[{enabled,false},{timePeriod,120}]},
   {failover_server_group,false},
   {max_count,1},
   {failed_over_server_groups,[]},
   {can_abort_rebalance,true}]},
 {{node,'ns_1@127.0.0.1',address_family},
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
   inet]},
 {{node,'ns_1@127.0.0.1',node_encryption},
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
   false]},
 {{node,'ns_1@127.0.0.1',erl_external_listeners},
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]},
   {inet,false},
   {inet6,false}]},
 {memory_quota,292},
 {{metakv,<<"/indexing/settings/config">>},
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{8,63752558934}}]}|
   <<"{\"indexer.settings.compaction.days_of_week\":\"Sunday,Monday,Tuesday,Wednesday,Thursday,Friday,Saturday\",\"indexer.settings.compaction.interval\":\"00:00,00:00\",\"indexer.settings.compaction.compaction_mode\":\"circular\",\"indexer.settings.log_level\":\"info\",\"indexer.settings.persisted_snapshot.interval\":5000,\"indexer.settings.compaction.min_frag\":30,\"indexer.settings.inmemory_snapshot.interval\":200,\"indexer.settings.max_cpu_percent\":0,\"indexer.settings.storage_mode\":\"plasma\",\"indexer.settings.recovery.max_rollbacks\":2,\"indexer.settings.memory_quota\":536870912,\"indexer.settings.compaction.abort_exceed_interval\":false}">>]},
 {fts_memory_quota,256},
 {cbas_memory_quota,1024},
 {{metakv,<<"/eventing/settings/config">>},
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557537}}]}|
   <<"{\"ram_quota\":256}">>]},
 {cluster_name,
  [{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{3,63752558934}}]},
   70,105,114,115,116]}]
[error_logger:info,2020-03-27T20:37:39.050Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_config_sup}
             started: [{pid,<0.195.0>},
                       {id,ns_config},
                       {mfargs,
                           {ns_config,start_link,
                               ["/opt/couchbase/etc/couchbase/config",
                                ns_config_default]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:37:39.051Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_config_sup}
             started: [{pid,<0.201.0>},
                       {id,ns_config_remote},
                       {mfargs,{ns_config_replica,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:37:39.053Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_config_sup}
             started: [{pid,<0.202.0>},
                       {id,ns_config_log},
                       {mfargs,{ns_config_log,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:37:39.053Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.192.0>},
                       {id,ns_config_sup},
                       {mfargs,{ns_config_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[ns_server:debug,2020-03-27T20:37:39.055Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{local_changes_count,<<"60a6bc3db77e6c7b91c556140dcfec71">>} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{36,63752560659}}]}]
[error_logger:info,2020-03-27T20:37:39.055Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.204.0>},
                       {id,netconfig_updater},
                       {mfargs,{netconfig_updater,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-03-27T20:37:39.058Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.207.0>},
                       {id,json_rpc_connection_sup},
                       {mfargs,{json_rpc_connection_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-03-27T20:37:39.093Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.210.0>},
                       {name,remote_monitors},
                       {mfargs,{remote_monitors,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-03-27T20:37:39.109Z,ns_1@127.0.0.1:menelaus_barrier<0.211.0>:one_shot_barrier:barrier_body:58]Barrier menelaus_barrier has started
[error_logger:info,2020-03-27T20:37:39.109Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.211.0>},
                       {name,menelaus_barrier},
                       {mfargs,{menelaus_sup,barrier_start_link,[]}},
                       {restart_type,temporary},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:37:39.110Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.212.0>},
                       {name,rest_lhttpc_pool},
                       {mfargs,
                           {lhttpc_manager,start_link,
                               [[{name,rest_lhttpc_pool},
                                 {connection_timeout,120000},
                                 {pool_size,20}]]}},
                       {restart_type,{permanent,1}},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:37:39.121Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.213.0>},
                       {name,memcached_refresh},
                       {mfargs,{memcached_refresh,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:37:39.132Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_ssl_services_sup}
             started: [{pid,<0.215.0>},
                       {id,ssl_service_events},
                       {mfargs,
                           {gen_event,start_link,
                               [{local,ssl_service_events}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-03-27T20:37:39.263Z,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Restarting tls distribution protocols (if any)
[ns_server:debug,2020-03-27T20:37:39.263Z,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: ignoring closing of inet6_tls_dist because listener is not started
[ns_server:debug,2020-03-27T20:37:39.263Z,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: ignoring closing of inet_tls_dist because listener is not started
[ns_server:info,2020-03-27T20:37:39.358Z,ns_1@127.0.0.1:ns_ssl_services_setup<0.216.0>:ns_ssl_services_setup:init:462]Used ssl options:
[{keyfile,"/opt/couchbase/var/lib/couchbase/config/ssl-cert-key.pem"},
 {certfile,"/opt/couchbase/var/lib/couchbase/config/ssl-cert-key.pem"},
 {versions,['tlsv1.1','tlsv1.2']},
 {cacerts,[<<48,130,3,2,48,130,1,234,160,3,2,1,2,2,8,22,0,64,208,110,172,210,
             92,48,13,6,9,42,134,72,134,247,13,1,1,11,5,0,48,36,49,34,48,32,
             6,3,85,4,3,19,25,67,111,117,99,104,98,97,115,101,32,83,101,114,
             118,101,114,32,51,52,56,101,100,50,55,48,48,30,23,13,49,51,48,
             49,48,49,48,48,48,48,48,48,90,23,13,52,57,49,50,51,49,50,51,53,
             57,53,57,90,48,36,49,34,48,32,6,3,85,4,3,19,25,67,111,117,99,
             104,98,97,115,101,32,83,101,114,118,101,114,32,51,52,56,101,100,
             50,55,48,48,130,1,34,48,13,6,9,42,134,72,134,247,13,1,1,1,5,0,3,
             130,1,15,0,48,130,1,10,2,130,1,1,0,225,209,231,201,130,247,233,
             2,187,220,222,34,52,112,118,191,205,152,82,245,106,26,160,209,
             228,176,90,40,98,172,183,1,136,153,71,239,18,220,136,15,75,73,
             223,113,145,171,33,98,84,62,10,36,82,160,106,135,171,185,245,
             166,220,188,43,178,51,2,205,146,75,139,58,68,250,195,53,51,182,
             176,42,160,42,14,18,94,26,188,161,102,35,66,108,137,96,63,168,
             160,212,254,70,169,139,74,198,244,204,206,194,101,179,132,181,1,
             21,76,117,72,101,24,165,108,212,56,253,47,230,184,143,50,20,121,
             74,247,204,71,247,226,97,249,191,174,226,150,23,149,71,180,209,
             86,216,72,31,202,197,17,62,92,90,116,10,212,69,89,20,91,163,51,
             77,94,156,13,115,23,71,83,252,222,166,85,132,171,214,167,174,78,
             60,63,78,241,115,199,70,186,175,191,94,208,5,216,173,106,248,
             203,56,72,24,219,23,86,67,137,18,221,171,208,1,41,239,84,253,
             158,230,36,78,222,33,120,90,96,209,36,5,230,90,163,19,2,35,165,
             80,128,171,6,90,11,237,162,35,3,201,94,110,201,181,13,49,165,2,
             3,1,0,1,163,56,48,54,48,14,6,3,85,29,15,1,1,255,4,4,3,2,2,164,
             48,19,6,3,85,29,37,4,12,48,10,6,8,43,6,1,5,5,7,3,1,48,15,6,3,85,
             29,19,1,1,255,4,5,48,3,1,1,255,48,13,6,9,42,134,72,134,247,13,1,
             1,11,5,0,3,130,1,1,0,6,81,65,124,71,43,122,15,167,207,16,251,0,
             159,39,98,188,227,29,215,100,107,194,184,208,49,234,49,157,22,
             16,114,226,174,13,150,172,144,2,238,230,8,40,221,26,55,56,132,
             126,233,153,24,185,41,108,27,239,216,82,120,63,12,251,133,139,
             191,7,65,245,131,92,34,201,230,196,53,62,156,158,117,62,220,159,
             192,101,171,131,143,145,151,43,167,227,90,161,173,77,152,27,124,
             67,49,150,67,102,161,221,43,166,252,34,106,208,76,223,73,230,
             216,161,26,132,137,129,146,79,142,69,179,193,194,47,129,207,251,
             4,237,180,247,54,155,40,216,36,79,59,110,188,27,153,208,45,211,
             245,157,64,19,243,225,128,45,126,172,97,164,65,55,250,227,90,
             212,59,153,137,219,195,84,171,109,145,64,111,253,109,120,35,199,
             153,232,120,170,179,167,110,176,116,10,245,79,6,66,158,148,137,
             191,125,65,164,153,94,180,226,83,230,241,236,39,67,40,12,91,44,
             79,54,65,72,249,75,235,222,205,142,112,55,224,47,125,42,235,188,
             14,128,134,44,228,25,163,25,90,246,170,74,163,117,63,44,151>>]},
 {dh,<<48,130,1,8,2,130,1,1,0,152,202,99,248,92,201,35,238,246,5,77,93,120,10,
       118,129,36,52,111,193,167,220,49,229,106,105,152,133,121,157,73,158,
       232,153,197,197,21,171,140,30,207,52,165,45,8,221,162,21,199,183,66,
       211,247,51,224,102,214,190,130,96,253,218,193,35,43,139,145,89,200,250,
       145,92,50,80,134,135,188,205,254,148,122,136,237,220,186,147,187,104,
       159,36,147,217,117,74,35,163,145,249,175,242,18,221,124,54,140,16,246,
       169,84,252,45,47,99,136,30,60,189,203,61,86,225,117,255,4,91,46,110,
       167,173,106,51,65,10,248,94,225,223,73,40,232,140,26,11,67,170,118,190,
       67,31,127,233,39,68,88,132,171,224,62,187,207,160,189,209,101,74,8,205,
       174,146,173,80,105,144,246,25,153,86,36,24,178,163,64,202,221,95,184,
       110,244,32,226,217,34,55,188,230,55,16,216,247,173,246,139,76,187,66,
       211,159,17,46,20,18,48,80,27,250,96,189,29,214,234,241,34,69,254,147,
       103,220,133,40,164,84,8,44,241,61,164,151,9,135,41,60,75,4,202,133,173,
       72,6,69,167,89,112,174,40,229,171,2,1,2>>},
 {ciphers,[{ecdhe_ecdsa,aes_256_gcm,aead,sha384},
           {ecdhe_rsa,aes_256_gcm,aead,sha384},
           {ecdhe_ecdsa,aes_256_cbc,sha384,sha384},
           {ecdhe_rsa,aes_256_cbc,sha384,sha384},
           {ecdh_ecdsa,aes_256_gcm,aead,sha384},
           {ecdh_rsa,aes_256_gcm,aead,sha384},
           {ecdh_ecdsa,aes_256_cbc,sha384,sha384},
           {ecdh_rsa,aes_256_cbc,sha384,sha384},
           {ecdhe_ecdsa,chacha20_poly1305,aead,sha256},
           {ecdhe_rsa,chacha20_poly1305,aead,sha256},
           {dhe_rsa,chacha20_poly1305,aead,sha256},
           {dhe_rsa,aes_256_gcm,aead,sha384},
           {dhe_dss,aes_256_gcm,aead,sha384},
           {dhe_rsa,aes_256_cbc,sha256},
           {dhe_dss,aes_256_cbc,sha256},
           {rsa,aes_256_gcm,aead,sha384},
           {rsa,aes_256_cbc,sha256},
           {ecdhe_ecdsa,aes_128_gcm,aead,sha256},
           {ecdhe_rsa,aes_128_gcm,aead,sha256},
           {ecdhe_ecdsa,aes_128_cbc,sha256,sha256},
           {ecdhe_rsa,aes_128_cbc,sha256,sha256},
           {ecdh_ecdsa,aes_128_gcm,aead,sha256},
           {ecdh_rsa,aes_128_gcm,aead,sha256},
           {ecdh_ecdsa,aes_128_cbc,sha256,sha256},
           {ecdh_rsa,aes_128_cbc,sha256,sha256},
           {dhe_rsa,aes_128_gcm,aead,sha256},
           {dhe_dss,aes_128_gcm,aead,sha256},
           {dhe_rsa,aes_128_cbc,sha256},
           {dhe_dss,aes_128_cbc,sha256},
           {rsa,aes_128_gcm,aead,sha256},
           {rsa,aes_128_cbc,sha256},
           {ecdhe_ecdsa,aes_256_cbc,sha},
           {ecdhe_rsa,aes_256_cbc,sha},
           {dhe_rsa,aes_256_cbc,sha},
           {dhe_dss,aes_256_cbc,sha},
           {ecdh_ecdsa,aes_256_cbc,sha},
           {ecdh_rsa,aes_256_cbc,sha},
           {rsa,aes_256_cbc,sha},
           {ecdhe_ecdsa,aes_128_cbc,sha},
           {ecdhe_rsa,aes_128_cbc,sha},
           {dhe_rsa,aes_128_cbc,sha},
           {dhe_dss,aes_128_cbc,sha},
           {ecdh_ecdsa,aes_128_cbc,sha},
           {ecdh_rsa,aes_128_cbc,sha},
           {rsa,aes_128_cbc,sha},
           {ecdhe_ecdsa,'3des_ede_cbc',sha},
           {ecdhe_rsa,'3des_ede_cbc',sha},
           {dhe_rsa,'3des_ede_cbc',sha},
           {dhe_dss,'3des_ede_cbc',sha},
           {ecdh_ecdsa,'3des_ede_cbc',sha},
           {ecdh_rsa,'3des_ede_cbc',sha},
           {rsa,'3des_ede_cbc',sha}]},
 {honor_cipher_order,true},
 {secure_renegotiate,true},
 {client_renegotiation,false}]
[error_logger:info,2020-03-27T20:37:39.377Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_ssl_services_sup}
             started: [{pid,<0.216.0>},
                       {id,ns_ssl_services_setup},
                       {mfargs,{ns_ssl_services_setup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:info,2020-03-27T20:37:39.398Z,ns_1@127.0.0.1:<0.219.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for cbas
[ns_server:info,2020-03-27T20:37:39.398Z,ns_1@127.0.0.1:<0.219.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for eventing
[ns_server:info,2020-03-27T20:37:39.398Z,ns_1@127.0.0.1:<0.219.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for fts
[ns_server:info,2020-03-27T20:37:39.398Z,ns_1@127.0.0.1:<0.219.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for n1ql
[ns_server:info,2020-03-27T20:37:39.417Z,ns_1@127.0.0.1:<0.219.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for cbas
[ns_server:info,2020-03-27T20:37:39.417Z,ns_1@127.0.0.1:<0.219.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for eventing
[ns_server:info,2020-03-27T20:37:39.417Z,ns_1@127.0.0.1:<0.219.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for fts
[error_logger:info,2020-03-27T20:37:39.417Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {<0.219.0>,menelaus_web}
             started: [{pid,<0.220.0>},
                       {id,menelaus_web_ipv4},
                       {mfargs,
                        {menelaus_web,http_server,
                         [[{ip,"0.0.0.0"},
                           {name,menelaus_web_ssl_ipv4},
                           {ssl,true},
                           {ssl_opts,
                            [{keyfile,
                              "/opt/couchbase/var/lib/couchbase/config/ssl-cert-key.pem"},
                             {certfile,
                              "/opt/couchbase/var/lib/couchbase/config/ssl-cert-key.pem"},
                             {versions,['tlsv1.1','tlsv1.2']},
                             {cacerts,
                              [<<48,130,3,2,48,130,1,234,160,3,2,1,2,2,8,22,
                                 0,64,208,110,172,210,92,48,13,6,9,42,134,72,
                                 134,247,13,1,1,11,5,0,48,36,49,34,48,32,6,3,
                                 85,4,3,19,25,67,111,117,99,104,98,97,115,
                                 101,32,83,101,114,118,101,114,32,51,52,56,
                                 101,100,50,55,48,48,30,23,13,49,51,48,49,48,
                                 49,48,48,48,48,48,48,90,23,13,52,57,49,50,
                                 51,49,50,51,53,57,53,57,90,48,36,49,34,48,
                                 32,6,3,85,4,3,19,25,67,111,117,99,104,98,97,
                                 115,101,32,83,101,114,118,101,114,32,51,52,
                                 56,101,100,50,55,48,48,130,1,34,48,13,6,9,
                                 42,134,72,134,247,13,1,1,1,5,0,3,130,1,15,0,
                                 48,130,1,10,2,130,1,1,0,225,209,231,201,130,
                                 247,233,2,187,220,222,34,52,112,118,191,205,
                                 152,82,245,106,26,160,209,228,176,90,40,98,
                                 172,183,1,136,153,71,239,18,220,136,15,75,
                                 73,223,113,145,171,33,98,84,62,10,36,82,160,
                                 106,135,171,185,245,166,220,188,43,178,51,2,
                                 205,146,75,139,58,68,250,195,53,51,182,176,
                                 42,160,42,14,18,94,26,188,161,102,35,66,108,
                                 137,96,63,168,160,212,254,70,169,139,74,198,
                                 244,204,206,194,101,179,132,181,1,21,76,117,
                                 72,101,24,165,108,212,56,253,47,230,184,143,
                                 50,20,121,74,247,204,71,247,226,97,249,191,
                                 174,226,150,23,149,71,180,209,86,216,72,31,
                                 202,197,17,62,92,90,116,10,212,69,89,20,91,
                                 163,51,77,94,156,13,115,23,71,83,252,222,
                                 166,85,132,171,214,167,174,78,60,63,78,241,
                                 115,199,70,186,175,191,94,208,5,216,173,106,
                                 248,203,56,72,24,219,23,86,67,137,18,221,
                                 171,208,1,41,239,84,253,158,230,36,78,222,
                                 33,120,90,96,209,36,5,230,90,163,19,2,35,
                                 165,80,128,171,6,90,11,237,162,35,3,201,94,
                                 110,201,181,13,49,165,2,3,1,0,1,163,56,48,
                                 54,48,14,6,3,85,29,15,1,1,255,4,4,3,2,2,164,
                                 48,19,6,3,85,29,37,4,12,48,10,6,8,43,6,1,5,
                                 5,7,3,1,48,15,6,3,85,29,19,1,1,255,4,5,48,3,
                                 1,1,255,48,13,6,9,42,134,72,134,247,13,1,1,
                                 11,5,0,3,130,1,1,0,6,81,65,124,71,43,122,15,
                                 167,207,16,251,0,159,39,98,188,227,29,215,
                                 100,107,194,184,208,49,234,49,157,22,16,114,
                                 226,174,13,150,172,144,2,238,230,8,40,221,
                                 26,55,56,132,126,233,153,24,185,41,108,27,
                                 239,216,82,120,63,12,251,133,139,191,7,65,
                                 245,131,92,34,201,230,196,53,62,156,158,117,
                                 62,220,159,192,101,171,131,143,145,151,43,
                                 167,227,90,161,173,77,152,27,124,67,49,150,
                                 67,102,161,221,43,166,252,34,106,208,76,223,
                                 73,230,216,161,26,132,137,129,146,79,142,69,
                                 179,193,194,47,129,207,251,4,237,180,247,54,
                                 155,40,216,36,79,59,110,188,27,153,208,45,
                                 211,245,157,64,19,243,225,128,45,126,172,97,
                                 164,65,55,250,227,90,212,59,153,137,219,195,
                                 84,171,109,145,64,111,253,109,120,35,199,
                                 153,232,120,170,179,167,110,176,116,10,245,
                                 79,6,66,158,148,137,191,125,65,164,153,94,
                                 180,226,83,230,241,236,39,67,40,12,91,44,79,
                                 54,65,72,249,75,235,222,205,142,112,55,224,
                                 47,125,42,235,188,14,128,134,44,228,25,163,
                                 25,90,246,170,74,163,117,63,44,151>>]},
                             {dh,
                              <<48,130,1,8,2,130,1,1,0,152,202,99,248,92,201,
                                35,238,246,5,77,93,120,10,118,129,36,52,111,
                                193,167,220,49,229,106,105,152,133,121,157,73,
                                158,232,153,197,197,21,171,140,30,207,52,165,
                                45,8,221,162,21,199,183,66,211,247,51,224,102,
                                214,190,130,96,253,218,193,35,43,139,145,89,
                                200,250,145,92,50,80,134,135,188,205,254,148,
                                122,136,237,220,186,147,187,104,159,36,147,
                                217,117,74,35,163,145,249,175,242,18,221,124,
                                54,140,16,246,169,84,252,45,47,99,136,30,60,
                                189,203,61,86,225,117,255,4,91,46,110,167,173,
                                106,51,65,10,248,94,225,223,73,40,232,140,26,
                                11,67,170,118,190,67,31,127,233,39,68,88,132,
                                171,224,62,187,207,160,189,209,101,74,8,205,
                                174,146,173,80,105,144,246,25,153,86,36,24,
                                178,163,64,202,221,95,184,110,244,32,226,217,
                                34,55,188,230,55,16,216,247,173,246,139,76,
                                187,66,211,159,17,46,20,18,48,80,27,250,96,
                                189,29,214,234,241,34,69,254,147,103,220,133,
                                40,164,84,8,44,241,61,164,151,9,135,41,60,75,
                                4,202,133,173,72,6,69,167,89,112,174,40,229,
                                171,2,1,2>>},
                             {ciphers,
                              [{ecdhe_ecdsa,aes_256_gcm,aead,sha384},
                               {ecdhe_rsa,aes_256_gcm,aead,sha384},
                               {ecdhe_ecdsa,aes_256_cbc,sha384,sha384},
                               {ecdhe_rsa,aes_256_cbc,sha384,sha384},
                               {ecdh_ecdsa,aes_256_gcm,aead,sha384},
                               {ecdh_rsa,aes_256_gcm,aead,sha384},
                               {ecdh_ecdsa,aes_256_cbc,sha384,sha384},
                               {ecdh_rsa,aes_256_cbc,sha384,sha384},
                               {ecdhe_ecdsa,chacha20_poly1305,aead,sha256},
                               {ecdhe_rsa,chacha20_poly1305,aead,sha256},
                               {dhe_rsa,chacha20_poly1305,aead,sha256},
                               {dhe_rsa,aes_256_gcm,aead,sha384},
                               {dhe_dss,aes_256_gcm,aead,sha384},
                               {dhe_rsa,aes_256_cbc,sha256},
                               {dhe_dss,aes_256_cbc,sha256},
                               {rsa,aes_256_gcm,aead,sha384},
                               {rsa,aes_256_cbc,sha256},
                               {ecdhe_ecdsa,aes_128_gcm,aead,sha256},
                               {ecdhe_rsa,aes_128_gcm,aead,sha256},
                               {ecdhe_ecdsa,aes_128_cbc,sha256,sha256},
                               {ecdhe_rsa,aes_128_cbc,sha256,sha256},
                               {ecdh_ecdsa,aes_128_gcm,aead,sha256},
                               {ecdh_rsa,aes_128_gcm,aead,sha256},
                               {ecdh_ecdsa,aes_128_cbc,sha256,sha256},
                               {ecdh_rsa,aes_128_cbc,sha256,sha256},
                               {dhe_rsa,aes_128_gcm,aead,sha256},
                               {dhe_dss,aes_128_gcm,aead,sha256},
                               {dhe_rsa,aes_128_cbc,sha256},
                               {dhe_dss,aes_128_cbc,sha256},
                               {rsa,aes_128_gcm,aead,sha256},
                               {rsa,aes_128_cbc,sha256},
                               {ecdhe_ecdsa,aes_256_cbc,sha},
                               {ecdhe_rsa,aes_256_cbc,sha},
                               {dhe_rsa,aes_256_cbc,sha},
                               {dhe_dss,aes_256_cbc,sha},
                               {ecdh_ecdsa,aes_256_cbc,sha},
                               {ecdh_rsa,aes_256_cbc,sha},
                               {rsa,aes_256_cbc,sha},
                               {ecdhe_ecdsa,aes_128_cbc,sha},
                               {ecdhe_rsa,aes_128_cbc,sha},
                               {dhe_rsa,aes_128_cbc,sha},
                               {dhe_dss,aes_128_cbc,sha},
                               {ecdh_ecdsa,aes_128_cbc,sha},
                               {ecdh_rsa,aes_128_cbc,sha},
                               {rsa,aes_128_cbc,sha},
                               {ecdhe_ecdsa,'3des_ede_cbc',sha},
                               {ecdhe_rsa,'3des_ede_cbc',sha},
                               {dhe_rsa,'3des_ede_cbc',sha},
                               {dhe_dss,'3des_ede_cbc',sha},
                               {ecdh_ecdsa,'3des_ede_cbc',sha},
                               {ecdh_rsa,'3des_ede_cbc',sha},
                               {rsa,'3des_ede_cbc',sha}]},
                             {honor_cipher_order,true},
                             {secure_renegotiate,true},
                             {client_renegotiation,false}]},
                           {port,18091}]]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[ns_server:info,2020-03-27T20:37:39.417Z,ns_1@127.0.0.1:<0.219.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for n1ql
[ns_server:debug,2020-03-27T20:37:39.418Z,ns_1@127.0.0.1:<0.218.0>:restartable:start_child:98]Started child process <0.219.0>
  MFA: {ns_ssl_services_setup,start_link_rest_service,[]}
[error_logger:info,2020-03-27T20:37:39.418Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {<0.219.0>,menelaus_web}
             started: [{pid,<0.238.0>},
                       {id,menelaus_web_ipv6},
                       {mfargs,
                        {menelaus_web,http_server,
                         [[{ip,"::"},
                           {name,menelaus_web_ssl_ipv6},
                           {ssl,true},
                           {ssl_opts,
                            [{keyfile,
                              "/opt/couchbase/var/lib/couchbase/config/ssl-cert-key.pem"},
                             {certfile,
                              "/opt/couchbase/var/lib/couchbase/config/ssl-cert-key.pem"},
                             {versions,['tlsv1.1','tlsv1.2']},
                             {cacerts,
                              [<<48,130,3,2,48,130,1,234,160,3,2,1,2,2,8,22,
                                 0,64,208,110,172,210,92,48,13,6,9,42,134,72,
                                 134,247,13,1,1,11,5,0,48,36,49,34,48,32,6,3,
                                 85,4,3,19,25,67,111,117,99,104,98,97,115,
                                 101,32,83,101,114,118,101,114,32,51,52,56,
                                 101,100,50,55,48,48,30,23,13,49,51,48,49,48,
                                 49,48,48,48,48,48,48,90,23,13,52,57,49,50,
                                 51,49,50,51,53,57,53,57,90,48,36,49,34,48,
                                 32,6,3,85,4,3,19,25,67,111,117,99,104,98,97,
                                 115,101,32,83,101,114,118,101,114,32,51,52,
                                 56,101,100,50,55,48,48,130,1,34,48,13,6,9,
                                 42,134,72,134,247,13,1,1,1,5,0,3,130,1,15,0,
                                 48,130,1,10,2,130,1,1,0,225,209,231,201,130,
                                 247,233,2,187,220,222,34,52,112,118,191,205,
                                 152,82,245,106,26,160,209,228,176,90,40,98,
                                 172,183,1,136,153,71,239,18,220,136,15,75,
                                 73,223,113,145,171,33,98,84,62,10,36,82,160,
                                 106,135,171,185,245,166,220,188,43,178,51,2,
                                 205,146,75,139,58,68,250,195,53,51,182,176,
                                 42,160,42,14,18,94,26,188,161,102,35,66,108,
                                 137,96,63,168,160,212,254,70,169,139,74,198,
                                 244,204,206,194,101,179,132,181,1,21,76,117,
                                 72,101,24,165,108,212,56,253,47,230,184,143,
                                 50,20,121,74,247,204,71,247,226,97,249,191,
                                 174,226,150,23,149,71,180,209,86,216,72,31,
                                 202,197,17,62,92,90,116,10,212,69,89,20,91,
                                 163,51,77,94,156,13,115,23,71,83,252,222,
                                 166,85,132,171,214,167,174,78,60,63,78,241,
                                 115,199,70,186,175,191,94,208,5,216,173,106,
                                 248,203,56,72,24,219,23,86,67,137,18,221,
                                 171,208,1,41,239,84,253,158,230,36,78,222,
                                 33,120,90,96,209,36,5,230,90,163,19,2,35,
                                 165,80,128,171,6,90,11,237,162,35,3,201,94,
                                 110,201,181,13,49,165,2,3,1,0,1,163,56,48,
                                 54,48,14,6,3,85,29,15,1,1,255,4,4,3,2,2,164,
                                 48,19,6,3,85,29,37,4,12,48,10,6,8,43,6,1,5,
                                 5,7,3,1,48,15,6,3,85,29,19,1,1,255,4,5,48,3,
                                 1,1,255,48,13,6,9,42,134,72,134,247,13,1,1,
                                 11,5,0,3,130,1,1,0,6,81,65,124,71,43,122,15,
                                 167,207,16,251,0,159,39,98,188,227,29,215,
                                 100,107,194,184,208,49,234,49,157,22,16,114,
                                 226,174,13,150,172,144,2,238,230,8,40,221,
                                 26,55,56,132,126,233,153,24,185,41,108,27,
                                 239,216,82,120,63,12,251,133,139,191,7,65,
                                 245,131,92,34,201,230,196,53,62,156,158,117,
                                 62,220,159,192,101,171,131,143,145,151,43,
                                 167,227,90,161,173,77,152,27,124,67,49,150,
                                 67,102,161,221,43,166,252,34,106,208,76,223,
                                 73,230,216,161,26,132,137,129,146,79,142,69,
                                 179,193,194,47,129,207,251,4,237,180,247,54,
                                 155,40,216,36,79,59,110,188,27,153,208,45,
                                 211,245,157,64,19,243,225,128,45,126,172,97,
                                 164,65,55,250,227,90,212,59,153,137,219,195,
                                 84,171,109,145,64,111,253,109,120,35,199,
                                 153,232,120,170,179,167,110,176,116,10,245,
                                 79,6,66,158,148,137,191,125,65,164,153,94,
                                 180,226,83,230,241,236,39,67,40,12,91,44,79,
                                 54,65,72,249,75,235,222,205,142,112,55,224,
                                 47,125,42,235,188,14,128,134,44,228,25,163,
                                 25,90,246,170,74,163,117,63,44,151>>]},
                             {dh,
                              <<48,130,1,8,2,130,1,1,0,152,202,99,248,92,201,
                                35,238,246,5,77,93,120,10,118,129,36,52,111,
                                193,167,220,49,229,106,105,152,133,121,157,73,
                                158,232,153,197,197,21,171,140,30,207,52,165,
                                45,8,221,162,21,199,183,66,211,247,51,224,102,
                                214,190,130,96,253,218,193,35,43,139,145,89,
                                200,250,145,92,50,80,134,135,188,205,254,148,
                                122,136,237,220,186,147,187,104,159,36,147,
                                217,117,74,35,163,145,249,175,242,18,221,124,
                                54,140,16,246,169,84,252,45,47,99,136,30,60,
                                189,203,61,86,225,117,255,4,91,46,110,167,173,
                                106,51,65,10,248,94,225,223,73,40,232,140,26,
                                11,67,170,118,190,67,31,127,233,39,68,88,132,
                                171,224,62,187,207,160,189,209,101,74,8,205,
                                174,146,173,80,105,144,246,25,153,86,36,24,
                                178,163,64,202,221,95,184,110,244,32,226,217,
                                34,55,188,230,55,16,216,247,173,246,139,76,
                                187,66,211,159,17,46,20,18,48,80,27,250,96,
                                189,29,214,234,241,34,69,254,147,103,220,133,
                                40,164,84,8,44,241,61,164,151,9,135,41,60,75,
                                4,202,133,173,72,6,69,167,89,112,174,40,229,
                                171,2,1,2>>},
                             {ciphers,
                              [{ecdhe_ecdsa,aes_256_gcm,aead,sha384},
                               {ecdhe_rsa,aes_256_gcm,aead,sha384},
                               {ecdhe_ecdsa,aes_256_cbc,sha384,sha384},
                               {ecdhe_rsa,aes_256_cbc,sha384,sha384},
                               {ecdh_ecdsa,aes_256_gcm,aead,sha384},
                               {ecdh_rsa,aes_256_gcm,aead,sha384},
                               {ecdh_ecdsa,aes_256_cbc,sha384,sha384},
                               {ecdh_rsa,aes_256_cbc,sha384,sha384},
                               {ecdhe_ecdsa,chacha20_poly1305,aead,sha256},
                               {ecdhe_rsa,chacha20_poly1305,aead,sha256},
                               {dhe_rsa,chacha20_poly1305,aead,sha256},
                               {dhe_rsa,aes_256_gcm,aead,sha384},
                               {dhe_dss,aes_256_gcm,aead,sha384},
                               {dhe_rsa,aes_256_cbc,sha256},
                               {dhe_dss,aes_256_cbc,sha256},
                               {rsa,aes_256_gcm,aead,sha384},
                               {rsa,aes_256_cbc,sha256},
                               {ecdhe_ecdsa,aes_128_gcm,aead,sha256},
                               {ecdhe_rsa,aes_128_gcm,aead,sha256},
                               {ecdhe_ecdsa,aes_128_cbc,sha256,sha256},
                               {ecdhe_rsa,aes_128_cbc,sha256,sha256},
                               {ecdh_ecdsa,aes_128_gcm,aead,sha256},
                               {ecdh_rsa,aes_128_gcm,aead,sha256},
                               {ecdh_ecdsa,aes_128_cbc,sha256,sha256},
                               {ecdh_rsa,aes_128_cbc,sha256,sha256},
                               {dhe_rsa,aes_128_gcm,aead,sha256},
                               {dhe_dss,aes_128_gcm,aead,sha256},
                               {dhe_rsa,aes_128_cbc,sha256},
                               {dhe_dss,aes_128_cbc,sha256},
                               {rsa,aes_128_gcm,aead,sha256},
                               {rsa,aes_128_cbc,sha256},
                               {ecdhe_ecdsa,aes_256_cbc,sha},
                               {ecdhe_rsa,aes_256_cbc,sha},
                               {dhe_rsa,aes_256_cbc,sha},
                               {dhe_dss,aes_256_cbc,sha},
                               {ecdh_ecdsa,aes_256_cbc,sha},
                               {ecdh_rsa,aes_256_cbc,sha},
                               {rsa,aes_256_cbc,sha},
                               {ecdhe_ecdsa,aes_128_cbc,sha},
                               {ecdhe_rsa,aes_128_cbc,sha},
                               {dhe_rsa,aes_128_cbc,sha},
                               {dhe_dss,aes_128_cbc,sha},
                               {ecdh_ecdsa,aes_128_cbc,sha},
                               {ecdh_rsa,aes_128_cbc,sha},
                               {rsa,aes_128_cbc,sha},
                               {ecdhe_ecdsa,'3des_ede_cbc',sha},
                               {ecdhe_rsa,'3des_ede_cbc',sha},
                               {dhe_rsa,'3des_ede_cbc',sha},
                               {dhe_dss,'3des_ede_cbc',sha},
                               {ecdh_ecdsa,'3des_ede_cbc',sha},
                               {ecdh_rsa,'3des_ede_cbc',sha},
                               {rsa,'3des_ede_cbc',sha}]},
                             {honor_cipher_order,true},
                             {secure_renegotiate,true},
                             {client_renegotiation,false}]},
                           {port,18091}]]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:37:39.419Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_ssl_services_sup}
             started: [{pid,<0.218.0>},
                       {id,ns_rest_ssl_service},
                       {mfargs,
                           {restartable,start_link,
                               [{ns_ssl_services_setup,
                                    start_link_rest_service,[]},
                                1000]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:37:39.419Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.214.0>},
                       {name,ns_ssl_services_sup},
                       {mfargs,{ns_ssl_services_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-03-27T20:37:39.427Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.256.0>},
                       {name,ldap_auth_cache},
                       {mfargs,{ldap_auth_cache,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:37:39.429Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,users_sup}
             started: [{pid,<0.259.0>},
                       {id,user_storage_events},
                       {mfargs,
                           {gen_event,start_link,
                               [{local,user_storage_events}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:37:39.434Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,users_storage_sup}
             started: [{pid,<0.261.0>},
                       {id,users_replicator},
                       {mfargs,{menelaus_users,start_replicator,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-03-27T20:37:39.436Z,ns_1@127.0.0.1:users_replicator<0.261.0>:replicated_storage:wait_for_startup:54]Start waiting for startup
[ns_server:debug,2020-03-27T20:37:39.438Z,ns_1@127.0.0.1:users_storage<0.262.0>:replicated_storage:anounce_startup:68]Announce my startup to <0.261.0>
[ns_server:debug,2020-03-27T20:37:39.439Z,ns_1@127.0.0.1:users_replicator<0.261.0>:replicated_storage:wait_for_startup:57]Received replicated storage registration from <0.262.0>
[ns_server:debug,2020-03-27T20:37:39.447Z,ns_1@127.0.0.1:users_storage<0.262.0>:replicated_dets:open:177]Opening file "/opt/couchbase/var/lib/couchbase/config/users.dets"
[error_logger:info,2020-03-27T20:37:39.447Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,users_storage_sup}
             started: [{pid,<0.262.0>},
                       {id,users_storage},
                       {mfargs,{menelaus_users,start_storage,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:37:39.447Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,users_sup}
             started: [{pid,<0.260.0>},
                       {id,users_storage_sup},
                       {mfargs,{users_storage_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[ns_server:debug,2020-03-27T20:37:39.456Z,ns_1@127.0.0.1:compiled_roles_cache<0.264.0>:versioned_cache:init:47]Starting versioned cache compiled_roles_cache
[error_logger:info,2020-03-27T20:37:39.456Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,users_sup}
             started: [{pid,<0.264.0>},
                       {id,compiled_roles_cache},
                       {mfargs,{menelaus_roles,start_compiled_roles_cache,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:37:39.461Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,users_sup}
             started: [{pid,<0.267.0>},
                       {id,roles_cache},
                       {mfargs,{roles_cache,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:37:39.461Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.258.0>},
                       {name,users_sup},
                       {mfargs,{users_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-03-27T20:37:39.463Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,kernel_safe_sup}
             started: [{pid,<0.270.0>},
                       {id,dets_sup},
                       {mfargs,{dets_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,supervisor}]

[error_logger:info,2020-03-27T20:37:39.464Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,kernel_safe_sup}
             started: [{pid,<0.271.0>},
                       {id,dets},
                       {mfargs,{dets_server,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,2000},
                       {child_type,worker}]

[ns_server:debug,2020-03-27T20:37:39.494Z,ns_1@127.0.0.1:wait_link_to_couchdb_node<0.275.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:152]Waiting for ns_couchdb node to start
[error_logger:info,2020-03-27T20:37:39.494Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.274.0>},
                       {name,start_couchdb_node},
                       {mfargs,{ns_server_nodes_sup,start_couchdb_node,[]}},
                       {restart_type,{permanent,5}},
                       {shutdown,86400000},
                       {child_type,worker}]

[ns_server:debug,2020-03-27T20:37:39.494Z,ns_1@127.0.0.1:net_kernel<0.181.0>:cb_dist:info_msg:754]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[error_logger:info,2020-03-27T20:37:39.494Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{connect,normal,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2020-03-27T20:37:39.494Z,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Added connection {con,#Ref<0.2968477961.2101608449.154010>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2020-03-27T20:37:39.494Z,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Updated connection: {con,#Ref<0.2968477961.2101608449.154010>,
                                  inet_tcp_dist,<0.278.0>,
                                  #Ref<0.2968477961.2101608449.154014>}
[error_logger:info,2020-03-27T20:37:39.495Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{'EXIT',<0.278.0>,shutdown}}
[ns_server:debug,2020-03-27T20:37:39.495Z,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Connection down: {con,#Ref<0.2968477961.2101608449.154010>,
                               inet_tcp_dist,<0.278.0>,
                               #Ref<0.2968477961.2101608449.154014>}
[ns_server:debug,2020-03-27T20:37:39.495Z,ns_1@127.0.0.1:<0.276.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:166]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2020-03-27T20:37:39.495Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{net_kernel,913,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:info,2020-03-27T20:37:39.500Z,ns_1@127.0.0.1:users_storage<0.262.0>:replicated_dets:convert_docs_to_55_in_dets:209]Checking for pre 5.5 records in dets: users_storage
[ns_server:debug,2020-03-27T20:37:39.501Z,ns_1@127.0.0.1:users_storage<0.262.0>:replicated_dets:init_after_ack:170]Loading 0 items, 300 words took 53ms
[ns_server:debug,2020-03-27T20:37:39.504Z,ns_1@127.0.0.1:users_replicator<0.261.0>:doc_replicator:loop:60]doing replicate_newnodes_docs
[error_logger:info,2020-03-27T20:37:39.696Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{connect,normal,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2020-03-27T20:37:39.696Z,ns_1@127.0.0.1:net_kernel<0.181.0>:cb_dist:info_msg:754]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2020-03-27T20:37:39.696Z,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Added connection {con,#Ref<0.2968477961.2101608450.153658>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2020-03-27T20:37:39.697Z,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Updated connection: {con,#Ref<0.2968477961.2101608450.153658>,
                                  inet_tcp_dist,<0.281.0>,
                                  #Ref<0.2968477961.2101608450.153662>}
[ns_server:debug,2020-03-27T20:37:39.697Z,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Connection down: {con,#Ref<0.2968477961.2101608450.153658>,
                               inet_tcp_dist,<0.281.0>,
                               #Ref<0.2968477961.2101608450.153662>}
[error_logger:info,2020-03-27T20:37:39.697Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{'EXIT',<0.281.0>,shutdown}}
[ns_server:debug,2020-03-27T20:37:39.697Z,ns_1@127.0.0.1:<0.276.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:166]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2020-03-27T20:37:39.697Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{net_kernel,913,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2020-03-27T20:37:39.898Z,ns_1@127.0.0.1:net_kernel<0.181.0>:cb_dist:info_msg:754]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[error_logger:info,2020-03-27T20:37:39.898Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{connect,normal,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2020-03-27T20:37:39.899Z,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Added connection {con,#Ref<0.2968477961.2101608449.154048>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2020-03-27T20:37:39.899Z,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Updated connection: {con,#Ref<0.2968477961.2101608449.154048>,
                                  inet_tcp_dist,<0.284.0>,
                                  #Ref<0.2968477961.2101608449.154052>}
[ns_server:debug,2020-03-27T20:37:39.899Z,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Connection down: {con,#Ref<0.2968477961.2101608449.154048>,
                               inet_tcp_dist,<0.284.0>,
                               #Ref<0.2968477961.2101608449.154052>}
[error_logger:info,2020-03-27T20:37:39.899Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{'EXIT',<0.284.0>,shutdown}}
[ns_server:debug,2020-03-27T20:37:39.899Z,ns_1@127.0.0.1:<0.276.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:166]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2020-03-27T20:37:39.899Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{net_kernel,913,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2020-03-27T20:37:40.101Z,ns_1@127.0.0.1:net_kernel<0.181.0>:cb_dist:info_msg:754]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[error_logger:info,2020-03-27T20:37:40.101Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{connect,normal,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2020-03-27T20:37:40.101Z,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Added connection {con,#Ref<0.2968477961.2101608449.154063>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2020-03-27T20:37:40.101Z,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Updated connection: {con,#Ref<0.2968477961.2101608449.154063>,
                                  inet_tcp_dist,<0.287.0>,
                                  #Ref<0.2968477961.2101608450.153666>}
[ns_server:debug,2020-03-27T20:37:40.102Z,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Connection down: {con,#Ref<0.2968477961.2101608449.154063>,
                               inet_tcp_dist,<0.287.0>,
                               #Ref<0.2968477961.2101608450.153666>}
[ns_server:debug,2020-03-27T20:37:40.102Z,ns_1@127.0.0.1:<0.276.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:166]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2020-03-27T20:37:40.102Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{'EXIT',<0.287.0>,shutdown}}
[error_logger:info,2020-03-27T20:37:40.102Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{net_kernel,913,nodedown,'couchdb_ns_1@cb.local'}}
[error_logger:info,2020-03-27T20:37:40.302Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{connect,normal,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2020-03-27T20:37:40.302Z,ns_1@127.0.0.1:net_kernel<0.181.0>:cb_dist:info_msg:754]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2020-03-27T20:37:40.302Z,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Added connection {con,#Ref<0.2968477961.2101608450.153671>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2020-03-27T20:37:40.303Z,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Updated connection: {con,#Ref<0.2968477961.2101608450.153671>,
                                  inet_tcp_dist,<0.290.0>,
                                  #Ref<0.2968477961.2101608450.153675>}
[ns_server:debug,2020-03-27T20:37:40.430Z,ns_1@127.0.0.1:<0.276.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:166]ns_couchdb is not ready: false
[ns_server:debug,2020-03-27T20:37:40.631Z,ns_1@127.0.0.1:<0.276.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:166]ns_couchdb is not ready: false
[ns_server:debug,2020-03-27T20:37:40.833Z,ns_1@127.0.0.1:<0.276.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:166]ns_couchdb is not ready: false
[ns_server:debug,2020-03-27T20:37:41.034Z,ns_1@127.0.0.1:<0.276.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:166]ns_couchdb is not ready: false
[error_logger:info,2020-03-27T20:37:41.352Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,kernel_safe_sup}
             started: [{pid,<0.296.0>},
                       {id,timer2_server},
                       {mfargs,{timer2,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:info,2020-03-27T20:37:41.547Z,ns_1@127.0.0.1:ns_couchdb_port<0.274.0>:ns_port_server:log:224]ns_couchdb<0.274.0>: Apache CouchDB  (LogLevel=info) is starting.

[error_logger:info,2020-03-27T20:37:41.718Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.275.0>},
                       {name,wait_for_couchdb_node},
                       {mfargs,
                           {erlang,apply,
                               [#Fun<ns_server_nodes_sup.0.58023840>,[]]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-03-27T20:37:41.727Z,ns_1@127.0.0.1:ns_server_nodes_sup<0.209.0>:ns_storage_conf:setup_db_and_ix_paths:64]Initialize db_and_ix_paths variable with [{db_path,
                                           "/opt/couchbase/var/lib/couchbase/data"},
                                          {index_path,
                                           "/opt/couchbase/var/lib/couchbase/data"}]
[error_logger:info,2020-03-27T20:37:41.733Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.299.0>},
                       {name,ns_disksup},
                       {mfargs,{ns_disksup,start_link,[]}},
                       {restart_type,{permanent,4}},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:37:41.735Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.300.0>},
                       {name,diag_handler_worker},
                       {mfargs,{work_queue,start_link,[diag_handler_worker]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:info,2020-03-27T20:37:41.737Z,ns_1@127.0.0.1:ns_server_sup<0.298.0>:dir_size:start_link:39]Starting quick version of dir_size with program name: godu
[error_logger:info,2020-03-27T20:37:41.737Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.301.0>},
                       {name,dir_size},
                       {mfargs,{dir_size,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:37:41.739Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.302.0>},
                       {name,request_throttler},
                       {mfargs,{request_throttler,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:37:41.744Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.303.0>},
                       {name,ns_log},
                       {mfargs,{ns_log,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:37:41.744Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.304.0>},
                       {name,ns_crash_log_consumer},
                       {mfargs,{ns_log,start_link_crash_consumer,[]}},
                       {restart_type,{permanent,4}},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-03-27T20:37:41.766Z,ns_1@127.0.0.1:memcached_passwords<0.305.0>:memcached_cfg:init:62]Init config writer for memcached_passwords, "/opt/couchbase/var/lib/couchbase/isasl.pw"
[ns_server:debug,2020-03-27T20:37:41.767Z,ns_1@127.0.0.1:memcached_passwords<0.305.0>:memcached_cfg:write_cfg:118]Writing config file for: "/opt/couchbase/var/lib/couchbase/isasl.pw"
[ns_server:info,2020-03-27T20:37:41.769Z,ns_1@127.0.0.1:ns_couchdb_port<0.274.0>:ns_port_server:log:224]ns_couchdb<0.274.0>: Apache CouchDB has started. Time to relax.
ns_couchdb<0.274.0>: 187: Booted. Waiting for shutdown request
ns_couchdb<0.274.0>: working as port

[ns_server:debug,2020-03-27T20:37:41.838Z,ns_1@127.0.0.1:users_storage<0.262.0>:replicated_dets:handle_call:302]Suspended by process <0.305.0>
[ns_server:debug,2020-03-27T20:37:41.838Z,ns_1@127.0.0.1:memcached_passwords<0.305.0>:replicated_dets:select_from_dets_locked:350]Starting select with {users_storage,[{{docv2,{auth,{'_',local}},'_','_'},
                                      [],
                                      ['$_']}],
                                    100}
[ns_server:debug,2020-03-27T20:37:41.839Z,ns_1@127.0.0.1:users_storage<0.262.0>:replicated_dets:handle_call:309]Released by process <0.305.0>
[ns_server:debug,2020-03-27T20:37:41.848Z,ns_1@127.0.0.1:memcached_refresh<0.213.0>:memcached_refresh:handle_cast:55]Refresh of isasl requested
[error_logger:info,2020-03-27T20:37:41.849Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.305.0>},
                       {name,memcached_passwords},
                       {mfargs,{memcached_passwords,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-03-27T20:37:41.852Z,ns_1@127.0.0.1:memcached_permissions<0.308.0>:memcached_cfg:init:62]Init config writer for memcached_permissions, "/opt/couchbase/var/lib/couchbase/config/memcached.rbac"
[ns_server:warn,2020-03-27T20:37:41.863Z,ns_1@127.0.0.1:memcached_refresh<0.213.0>:ns_memcached:connect:1101]Unable to connect: {error,{badmatch,{error,econnrefused}}}.
[ns_server:debug,2020-03-27T20:37:41.864Z,ns_1@127.0.0.1:memcached_refresh<0.213.0>:memcached_refresh:handle_info:93]Refresh of [isasl] failed. Retry in 1000 ms.
[ns_server:debug,2020-03-27T20:37:41.869Z,ns_1@127.0.0.1:memcached_permissions<0.308.0>:memcached_cfg:write_cfg:118]Writing config file for: "/opt/couchbase/var/lib/couchbase/config/memcached.rbac"
[ns_server:debug,2020-03-27T20:37:41.883Z,ns_1@127.0.0.1:users_storage<0.262.0>:replicated_dets:handle_call:302]Suspended by process <0.308.0>
[ns_server:debug,2020-03-27T20:37:41.883Z,ns_1@127.0.0.1:memcached_permissions<0.308.0>:replicated_dets:select_from_dets_locked:350]Starting select with {users_storage,[{{docv2,{user,{'_',local}},'_','_'},
                                      [],
                                      ['$_']}],
                                    100}
[ns_server:debug,2020-03-27T20:37:41.883Z,ns_1@127.0.0.1:users_storage<0.262.0>:replicated_dets:handle_call:309]Released by process <0.308.0>
[error_logger:info,2020-03-27T20:37:41.899Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.308.0>},
                       {name,memcached_permissions},
                       {mfargs,{memcached_permissions,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-03-27T20:37:41.899Z,ns_1@127.0.0.1:memcached_refresh<0.213.0>:memcached_refresh:handle_cast:55]Refresh of rbac requested
[ns_server:warn,2020-03-27T20:37:41.900Z,ns_1@127.0.0.1:memcached_refresh<0.213.0>:ns_memcached:connect:1101]Unable to connect: {error,{badmatch,{error,econnrefused}}}.
[ns_server:debug,2020-03-27T20:37:41.900Z,ns_1@127.0.0.1:memcached_refresh<0.213.0>:memcached_refresh:handle_info:93]Refresh of [rbac,isasl] failed. Retry in 1000 ms.
[error_logger:info,2020-03-27T20:37:41.902Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.311.0>},
                       {name,ns_email_alert},
                       {mfargs,{ns_email_alert,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:37:41.910Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_node_disco_sup}
             started: [{pid,<0.313.0>},
                       {id,ns_node_disco_events},
                       {mfargs,
                           {gen_event,start_link,
                               [{local,ns_node_disco_events}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-03-27T20:37:41.911Z,ns_1@127.0.0.1:ns_node_disco<0.314.0>:ns_node_disco:init:128]Initting ns_node_disco with []
[ns_server:debug,2020-03-27T20:37:41.911Z,ns_1@127.0.0.1:ns_cookie_manager<0.190.0>:ns_cookie_manager:do_cookie_sync:107]ns_cookie_manager do_cookie_sync
[user:info,2020-03-27T20:37:41.911Z,ns_1@127.0.0.1:ns_cookie_manager<0.190.0>:ns_cookie_manager:do_cookie_sync:128]Node 'ns_1@127.0.0.1' synchronized otp cookie {sanitized,
                                               <<"C4PeZIe8LLStUy+d1HIN638gh0wTZW33h1wdSpFkvzc=">>} from cluster
[ns_server:debug,2020-03-27T20:37:41.912Z,ns_1@127.0.0.1:<0.315.0>:ns_node_disco:do_nodes_wanted_updated_fun:214]ns_node_disco: nodes_wanted updated: ['ns_1@127.0.0.1'], with cookie: {sanitized,
                                                                       <<"C4PeZIe8LLStUy+d1HIN638gh0wTZW33h1wdSpFkvzc=">>}
[ns_server:debug,2020-03-27T20:37:41.918Z,ns_1@127.0.0.1:<0.315.0>:ns_node_disco:do_nodes_wanted_updated_fun:220]ns_node_disco: nodes_wanted pong: ['ns_1@127.0.0.1'], with cookie: {sanitized,
                                                                    <<"C4PeZIe8LLStUy+d1HIN638gh0wTZW33h1wdSpFkvzc=">>}
[error_logger:info,2020-03-27T20:37:41.918Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_node_disco_sup}
             started: [{pid,<0.314.0>},
                       {id,ns_node_disco},
                       {mfargs,{ns_node_disco,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:37:41.921Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_node_disco_sup}
             started: [{pid,<0.316.0>},
                       {id,ns_node_disco_log},
                       {mfargs,{ns_node_disco_log,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:37:41.925Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_node_disco_sup}
             started: [{pid,<0.317.0>},
                       {id,ns_node_disco_conf_events},
                       {mfargs,{ns_node_disco_conf_events,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:37:41.930Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_node_disco_sup}
             started: [{pid,<0.318.0>},
                       {id,ns_config_rep_merger},
                       {mfargs,{ns_config_rep,start_link_merger,[]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[ns_server:debug,2020-03-27T20:37:41.930Z,ns_1@127.0.0.1:ns_config_rep<0.319.0>:ns_config_rep:init:71]init pulling
[ns_server:debug,2020-03-27T20:37:41.930Z,ns_1@127.0.0.1:ns_config_rep<0.319.0>:ns_config_rep:init:73]init pushing
[ns_server:debug,2020-03-27T20:37:41.936Z,ns_1@127.0.0.1:ns_config_rep<0.319.0>:ns_config_rep:init:77]init reannouncing
[ns_server:debug,2020-03-27T20:37:41.937Z,ns_1@127.0.0.1:compiled_roles_cache<0.264.0>:versioned_cache:handle_info:92]Flushing cache compiled_roles_cache due to version change from undefined to {[6,
                                                                              5],
                                                                             {0,
                                                                              1940836693},
                                                                             {0,
                                                                              1940836693},
                                                                             false,
                                                                             []}
[ns_server:debug,2020-03-27T20:37:41.940Z,ns_1@127.0.0.1:ns_config_events<0.193.0>:ns_node_disco_conf_events:handle_event:44]ns_node_disco_conf_events config on nodes_wanted
[ns_server:debug,2020-03-27T20:37:41.941Z,ns_1@127.0.0.1:ns_cookie_manager<0.190.0>:ns_cookie_manager:do_cookie_sync:107]ns_cookie_manager do_cookie_sync
[ns_server:debug,2020-03-27T20:37:41.941Z,ns_1@127.0.0.1:<0.325.0>:ns_node_disco:do_nodes_wanted_updated_fun:214]ns_node_disco: nodes_wanted updated: ['ns_1@127.0.0.1'], with cookie: {sanitized,
                                                                       <<"C4PeZIe8LLStUy+d1HIN638gh0wTZW33h1wdSpFkvzc=">>}
[ns_server:debug,2020-03-27T20:37:41.941Z,ns_1@127.0.0.1:<0.325.0>:ns_node_disco:do_nodes_wanted_updated_fun:220]ns_node_disco: nodes_wanted pong: ['ns_1@127.0.0.1'], with cookie: {sanitized,
                                                                    <<"C4PeZIe8LLStUy+d1HIN638gh0wTZW33h1wdSpFkvzc=">>}
[ns_server:debug,2020-03-27T20:37:41.941Z,ns_1@127.0.0.1:ns_config_events<0.193.0>:ns_node_disco_conf_events:handle_event:50]ns_node_disco_conf_events config on otp
[ns_server:debug,2020-03-27T20:37:41.941Z,ns_1@127.0.0.1:ns_cookie_manager<0.190.0>:ns_cookie_manager:do_cookie_sync:107]ns_cookie_manager do_cookie_sync
[ns_server:debug,2020-03-27T20:37:41.941Z,ns_1@127.0.0.1:<0.326.0>:ns_node_disco:do_nodes_wanted_updated_fun:214]ns_node_disco: nodes_wanted updated: ['ns_1@127.0.0.1'], with cookie: {sanitized,
                                                                       <<"C4PeZIe8LLStUy+d1HIN638gh0wTZW33h1wdSpFkvzc=">>}
[ns_server:debug,2020-03-27T20:37:41.942Z,ns_1@127.0.0.1:<0.326.0>:ns_node_disco:do_nodes_wanted_updated_fun:220]ns_node_disco: nodes_wanted pong: ['ns_1@127.0.0.1'], with cookie: {sanitized,
                                                                    <<"C4PeZIe8LLStUy+d1HIN638gh0wTZW33h1wdSpFkvzc=">>}
[ns_server:debug,2020-03-27T20:37:41.942Z,ns_1@127.0.0.1:memcached_passwords<0.305.0>:memcached_cfg:write_cfg:118]Writing config file for: "/opt/couchbase/var/lib/couchbase/isasl.pw"
[ns_server:debug,2020-03-27T20:37:41.946Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
alert_limits ->
[{max_overhead_perc,50},{max_disk_used,90},{max_indexer_ram,75}]
[ns_server:debug,2020-03-27T20:37:41.948Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
audit ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557537}}]},
 {enabled,[]},
 {disabled_users,[]},
 {auditd_enabled,false},
 {rotate_interval,86400},
 {rotate_size,20971520},
 {disabled,[]},
 {sync,[]},
 {log_path,"/opt/couchbase/var/lib/couchbase/logs"}]
[ns_server:debug,2020-03-27T20:37:41.956Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
audit_decriptors ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557537}}]},
 {8243,
  [{name,<<"mutate document">>},
   {description,<<"Document was mutated via the REST API">>},
   {enabled,true},
   {module,ns_server}]},
 {8255,
  [{name,<<"read document">>},
   {description,<<"Document was read via the REST API">>},
   {enabled,false},
   {module,ns_server}]},
 {8257,
  [{name,<<"alert email sent">>},
   {description,<<"An alert email was successfully sent">>},
   {enabled,true},
   {module,ns_server}]},
 {20480,
  [{name,<<"opened DCP connection">>},
   {description,<<"opened DCP connection">>},
   {enabled,true},
   {module,memcached}]},
 {20482,
  [{name,<<"external memcached bucket flush">>},
   {description,<<"External user flushed the content of a memcached bucket">>},
   {enabled,true},
   {module,memcached}]},
 {20483,
  [{name,<<"invalid packet">>},
   {description,<<"Rejected an invalid packet">>},
   {enabled,true},
   {module,memcached}]},
 {20485,
  [{name,<<"authentication succeeded">>},
   {description,<<"Authentication to the cluster succeeded">>},
   {enabled,false},
   {module,memcached}]},
 {20488,
  [{name,<<"document read">>},
   {description,<<"Document was read">>},
   {enabled,false},
   {module,memcached}]},
 {20489,
  [{name,<<"document locked">>},
   {description,<<"Document was locked">>},
   {enabled,false},
   {module,memcached}]},
 {20490,
  [{name,<<"document modify">>},
   {description,<<"Document was modified">>},
   {enabled,false},
   {module,memcached}]},
 {20491,
  [{name,<<"document delete">>},
   {description,<<"Document was deleted">>},
   {enabled,false},
   {module,memcached}]},
 {20492,
  [{name,<<"select bucket">>},
   {description,<<"The specified bucket was selected">>},
   {enabled,true},
   {module,memcached}]},
 {28672,
  [{name,<<"SELECT statement">>},
   {description,<<"A N1QL SELECT statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28673,
  [{name,<<"EXPLAIN statement">>},
   {description,<<"A N1QL EXPLAIN statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28674,
  [{name,<<"PREPARE statement">>},
   {description,<<"A N1QL PREPARE statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28675,
  [{name,<<"INFER statement">>},
   {description,<<"A N1QL INFER statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28676,
  [{name,<<"INSERT statement">>},
   {description,<<"A N1QL INSERT statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28677,
  [{name,<<"UPSERT statement">>},
   {description,<<"A N1QL UPSERT statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28678,
  [{name,<<"DELETE statement">>},
   {description,<<"A N1QL DELETE statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28679,
  [{name,<<"UPDATE statement">>},
   {description,<<"A N1QL UPDATE statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28680,
  [{name,<<"MERGE statement">>},
   {description,<<"A N1QL MERGE statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28681,
  [{name,<<"CREATE INDEX statement">>},
   {description,<<"A N1QL CREATE INDEX statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28682,
  [{name,<<"DROP INDEX statement">>},
   {description,<<"A N1QL DROP INDEX statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28683,
  [{name,<<"ALTER INDEX statement">>},
   {description,<<"A N1QL ALTER INDEX statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28684,
  [{name,<<"BUILD INDEX statement">>},
   {description,<<"A N1QL BUILD INDEX statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28685,
  [{name,<<"GRANT ROLE statement">>},
   {description,<<"A N1QL GRANT ROLE statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28686,
  [{name,<<"REVOKE ROLE statement">>},
   {description,<<"A N1QL REVOKE ROLE statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28687,
  [{name,<<"UNRECOGNIZED statement">>},
   {description,<<"An unrecognized statement was received by the N1QL query engine">>},
   {enabled,false},
   {module,n1ql}]},
 {28688,
  [{name,<<"CREATE PRIMARY INDEX statement">>},
   {description,<<"A N1QL CREATE PRIMARY INDEX statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28689,
  [{name,<<"/admin/stats API request">>},
   {description,<<"An HTTP request was made to the API at /admin/stats.">>},
   {enabled,false},
   {module,n1ql}]},
 {28690,
  [{name,<<"/admin/vitals API request">>},
   {description,<<"An HTTP request was made to the API at /admin/vitals.">>},
   {enabled,false},
   {module,n1ql}]},
 {28691,
  [{name,<<"/admin/prepareds API request">>},
   {description,<<"An HTTP request was made to the API at /admin/prepareds.">>},
   {enabled,false},
   {module,n1ql}]},
 {28692,
  [{name,<<"/admin/active_requests API request">>},
   {description,<<"An HTTP request was made to the API at /admin/active_requests.">>},
   {enabled,false},
   {module,n1ql}]},
 {28693,
  [{name,<<"/admin/indexes/prepareds API request">>},
   {description,<<"An HTTP request was made to the API at /admin/indexes/prepareds.">>},
   {enabled,false},
   {module,n1ql}]},
 {28694,
  [{name,<<"/admin/indexes/active_requests API request">>},
   {description,<<"An HTTP request was made to the API at /admin/indexes/active_requests.">>},
   {enabled,false},
   {module,n1ql}]},
 {28695,
  [{name,<<"/admin/indexes/completed_requests API request">>},
   {description,<<"An HTTP request was made to the API at /admin/indexes/completed_requests.">>},
   {enabled,false},
   {module,n1ql}]},
 {28697,
  [{name,<<"/admin/ping API request">>},
   {description,<<"An HTTP request was made to the API at /admin/ping.">>},
   {enabled,false},
   {module,n1ql}]},
 {28698,
  [{name,<<"/admin/config API request">>},
   {description,<<"An HTTP request was made to the API at /admin/config.">>},
   {enabled,false},
   {module,n1ql}]},
 {28699,
  [{name,<<"/admin/ssl_cert API request">>},
   {description,<<"An HTTP request was made to the API at /admin/ssl_cert.">>},
   {enabled,false},
   {module,n1ql}]},
 {28700,
  [{name,<<"/admin/settings API request">>},
   {description,<<"An HTTP request was made to the API at /admin/settings.">>},
   {enabled,false},
   {module,n1ql}]},
 {28701,
  [{name,<<"/admin/clusters API request">>},
   {description,<<"An HTTP request was made to the API at /admin/clusters.">>},
   {enabled,false},
   {module,n1ql}]},
 {28702,
  [{name,<<"/admin/completed_requests API request">>},
   {description,<<"An HTTP request was made to the API at /admin/completed_requests.">>},
   {enabled,false},
   {module,n1ql}]},
 {28704,
  [{name,<<"/admin/functions API request">>},
   {description,<<"An HTTP request was made to the API at /admin/functions.">>},
   {enabled,false},
   {module,n1ql}]},
 {28705,
  [{name,<<"/admin/indexes/functions API request">>},
   {description,<<"An HTTP request was made to the API at /admin/indexes/functions.">>},
   {enabled,false},
   {module,n1ql}]},
 {32768,
  [{name,<<"Create Function">>},
   {description,<<"Eventing function definition was created or updated">>},
   {enabled,true},
   {module,eventing}]},
 {32769,
  [{name,<<"Delete Function">>},
   {description,<<"Eventing function definition was deleted">>},
   {enabled,true},
   {module,eventing}]},
 {32770,
  [{name,<<"Fetch Functions">>},
   {description,<<"Eventing function definition was read">>},
   {enabled,false},
   {module,eventing}]},
 {32771,
  [{name,<<"List Deployed">>},
   {description,<<"Eventing deployed functions list was read">>},
   {enabled,false},
   {module,eventing}]},
 {32772,
  [{name,<<"Fetch Drafts">>},
   {description,<<"Eventing function draft definitions were read">>},
   {enabled,false},
   {module,eventing}]},
 {32773,
  [{name,<<"Delete Drafts">>},
   {description,<<"Eventing function draft definitions were deleted">>},
   {enabled,true},
   {module,eventing}]},
 {32774,
  [{name,<<"Save Draft">>},
   {description,<<"Save a draft definition to the store">>},
   {enabled,true},
   {module,eventing}]},
 {32775,
  [{name,<<"Start Debug">>},
   {description,<<"Start eventing function debugger">>},
   {enabled,true},
   {module,eventing}]},
 {32776,
  [{name,<<"Stop Debug">>},
   {description,<<"Stop eventing function debugger">>},
   {enabled,true},
   {module,eventing}]},
 {32777,
  [{name,<<"Start Tracing">>},
   {description,<<"Start tracing eventing function execution">>},
   {enabled,true},
   {module,eventing}]},
 {32778,
  [{name,<<"Stop Tracing">>},
   {description,<<"Stop tracing eventing function execution">>},
   {enabled,true},
   {module,eventing}]},
 {32779,
  [{name,<<"Set Settings">>},
   {description,<<"Save settings for a given app">>},
   {enabled,true},
   {module,eventing}]},
 {32780,
  [{name,<<"Fetch Config">>},
   {description,<<"Get config for eventing">>},
   {enabled,false},
   {module,eventing}]},
 {32781,
  [{name,<<"Save Config">>},
   {description,<<"Save config for eventing">>},
   {enabled,true},
   {module,eventing}]},
 {32782,
  [{name,<<"Cleanup Eventing">>},
   {description,<<"Clears up app definitions and settings from metakv">>},
   {enabled,true},
   {module,eventing}]},
 {32783,
  [{name,<<"Get Settings">>},
   {description,<<"Get settings for a given app">>},
   {enabled,false},
   {module,eventing}]},
 {32784,
  [{name,<<"Import Functions">>},
   {description,<<"Import a list of functions">>},
   {enabled,false},
   {module,eventing}]},
 {32785,
  [{name,<<"Export Functions">>},
   {description,<<"Export the list of functions">>},
   {enabled,false},
   {module,eventing}]},
 {32786,
  [{name,<<"List Running">>},
   {description,<<"Eventing running function list was read">>},
   {enabled,false},
   {module,eventing}]},
 {36865,
  [{name,<<"Service configuration change">>},
   {description,<<"A successful service configuration change was made.">>},
   {enabled,true},
   {module,analytics}]},
 {36866,
  [{name,<<"Node configuration change">>},
   {description,<<"A successful node configuration change was made.">>},
   {enabled,true},
   {module,analytics}]},
 {40960,
  [{name,<<"Create Design Doc">>},
   {description,<<"Design Doc is Created">>},
   {enabled,true},
   {module,view_engine}]},
 {40961,
  [{name,<<"Delete Design Doc">>},
   {description,<<"Design Doc is Deleted">>},
   {enabled,true},
   {module,view_engine}]},
 {40962,
  [{name,<<"Query DDoc Meta Data">>},
   {description,<<"Design Doc Meta Data Query Request">>},
   {enabled,true},
   {module,view_engine}]},
 {40963,
  [{name,<<"View Query">>},
   {description,<<"View Query Request">>},
   {enabled,false},
   {module,view_engine}]},
 {40964,
  [{name,<<"Update Design Doc">>},
   {description,<<"Design Doc is Updated">>},
   {enabled,true},
   {module,view_engine}]}]
[ns_server:debug,2020-03-27T20:37:41.957Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
auto_failover_cfg ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557537}}]},
 {enabled,true},
 {timeout,120},
 {count,0},
 {failover_on_data_disk_issues,[{enabled,false},{timePeriod,120}]},
 {failover_server_group,false},
 {max_count,1},
 {failed_over_server_groups,[]},
 {can_abort_rebalance,true}]
[ns_server:debug,2020-03-27T20:37:41.957Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
auto_reprovision_cfg ->
[{enabled,true},{max_nodes,1},{count,0}]
[ns_server:debug,2020-03-27T20:37:41.958Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
autocompaction ->
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}}]
[ns_server:debug,2020-03-27T20:37:41.958Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
buckets ->
[[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557537}}],{configs,[]}]
[ns_server:debug,2020-03-27T20:37:41.958Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
cbas_memory_quota ->
1024
[ns_server:debug,2020-03-27T20:37:41.958Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
cert_and_pkey ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557532}}]}|
 {<<"-----BEGIN CERTIFICATE-----\nMIIDAjCCAeqgAwIBAgIIFgBA0G6s0lwwDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciAzNDhlZDI3MDAeFw0xMzAxMDEwMDAwMDBaFw00\nOTEyMzEyMzU5NTlaMCQxIjAgBgNVBAMTGUNvdWNoYmFzZSBTZXJ2ZXIgMzQ4ZWQy\nNzAwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQDh0efJgvfpArvc3iI0\ncHa/zZhS9WoaoNHksFooYqy3AYiZR+8S3IgPS0nfcZGrIWJUPgokUqBqh6u59abc\nvCuyMwLNkkuLOkT6wzUztrAqoCoOEl4"...>>,
  <<"*****">>}]
[ns_server:debug,2020-03-27T20:37:41.958Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
client_cert_auth ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557537}}]},
 {state,"disable"},
 {prefixes,[]}]
[ns_server:debug,2020-03-27T20:37:41.958Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
cluster_compat_version ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{5,63752557537}}]},6,5]
[ns_server:debug,2020-03-27T20:37:41.958Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
cluster_name ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{3,63752558934}}]},
 70,105,114,115,116]
[ns_server:debug,2020-03-27T20:37:41.958Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
drop_request_memory_threshold_mib ->
undefined
[ns_server:debug,2020-03-27T20:37:41.959Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
email_alerts ->
[{recipients,["root@localhost"]},
 {sender,"couchbase@localhost"},
 {enabled,false},
 {email_server,[{user,[]},
                {pass,"*****"},
                {host,"localhost"},
                {port,25},
                {encrypt,false}]},
 {alerts,[auto_failover_node,auto_failover_maximum_reached,
          auto_failover_other_nodes_down,auto_failover_cluster_too_small,
          auto_failover_disabled,ip,disk,overhead,ep_oom_errors,
          ep_item_commit_failed,audit_dropped_events,indexer_ram_max_usage,
          ep_clock_cas_drift_threshold_exceeded,communication_issue]}]
[ns_server:debug,2020-03-27T20:37:41.959Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
fts_memory_quota ->
256
[ns_server:debug,2020-03-27T20:37:41.960Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
index_aware_rebalance_disabled ->
false
[ns_server:debug,2020-03-27T20:37:41.960Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
log_redaction_default_cfg ->
[{redact_level,none}]
[ns_server:debug,2020-03-27T20:37:41.960Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
max_bucket_count ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557537}}]}|30]
[ns_server:debug,2020-03-27T20:37:41.960Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
memcached ->
[]
[ns_server:debug,2020-03-27T20:37:41.960Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
memory_quota ->
292
[ns_server:debug,2020-03-27T20:37:41.961Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
nodes_wanted ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557625}}]},
 'ns_1@127.0.0.1']
[ns_server:debug,2020-03-27T20:37:41.961Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
otp ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557536}}]},
 {cookie,{sanitized,<<"C4PeZIe8LLStUy+d1HIN638gh0wTZW33h1wdSpFkvzc=">>}}]
[ns_server:debug,2020-03-27T20:37:41.961Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
password_policy ->
[{min_length,6},{must_present,[]}]
[ns_server:debug,2020-03-27T20:37:41.961Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
quorum_nodes ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]},
 'ns_1@127.0.0.1']
[ns_server:debug,2020-03-27T20:37:41.961Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
remote_clusters ->
[]
[ns_server:debug,2020-03-27T20:37:41.961Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
replication ->
[{enabled,true}]
[ns_server:debug,2020-03-27T20:37:41.961Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
rest ->
[{port,8091}]
[ns_server:debug,2020-03-27T20:37:41.961Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
rest_creds ->
null
[ns_server:debug,2020-03-27T20:37:41.961Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
retry_rebalance ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557537}}]},
 {enabled,false},
 {after_time_period,300},
 {max_attempts,1}]
[ns_server:debug,2020-03-27T20:37:41.961Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
scramsha_fallback_salt ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557537}}]}|
 <<176,211,90,3,15,137,19,29,193,47,34,182>>]
[ns_server:debug,2020-03-27T20:37:41.961Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
secure_headers ->
[]
[ns_server:debug,2020-03-27T20:37:41.961Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
server_groups ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557625}}]},
 [{uuid,<<"0">>},{name,<<"Group 1">>},{nodes,['ns_1@127.0.0.1']}]]
[ns_server:debug,2020-03-27T20:37:41.961Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
set_view_update_daemon ->
[{update_interval,5000},
 {update_min_changes,5000},
 {replica_update_min_changes,5000}]
[ns_server:debug,2020-03-27T20:37:41.961Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{couchdb,max_parallel_indexers} ->
4
[ns_server:debug,2020-03-27T20:37:41.961Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{couchdb,max_parallel_replica_indexers} ->
2
[ns_server:debug,2020-03-27T20:37:41.961Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{local_changes_count,<<"60a6bc3db77e6c7b91c556140dcfec71">>} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{36,63752560659}}]}]
[ns_server:debug,2020-03-27T20:37:41.961Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{metakv,<<"/eventing/settings/config">>} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752557537}}]}|
 <<"{\"ram_quota\":256}">>]
[ns_server:debug,2020-03-27T20:37:41.962Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{metakv,<<"/indexing/settings/config">>} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{8,63752558934}}]}|
 <<"{\"indexer.settings.compaction.days_of_week\":\"Sunday,Monday,Tuesday,Wednesday,Thursday,Friday,Saturday\",\"indexer.settings.compaction.interval\":\"00:00,00:00\",\"indexer.settings.compaction.compaction_mode\":\"circular\",\"indexer.settings.log_level\":\"info\",\"indexer.settings.persisted_snapshot.interval\":5000,\"indexer.settings.compaction.min_frag\":30,\"indexer.settings.inmemory_snapshot.interval\""...>>]
[ns_server:debug,2020-03-27T20:37:41.962Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{metakv,<<"/query/settings/config">>} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557537}}]}|
 <<"{\"timeout\":0,\"n1ql-feat-ctrl\":12,\"max-parallelism\":1,\"query.settings.curl_whitelist\":{\"all_access\":false,\"allowed_urls\":[],\"disallowed_urls\":[]},\"query.settings.tmp_space_dir\":\"/opt/couchbase/var/lib/couchbase/tmp\",\"completed-limit\":4000,\"prepared-limit\":16384,\"pipeline-batch\":16,\"pipeline-cap\":512,\"scan-cap\":512,\"loglevel\":\"info\",\"completed-threshold\":1000,\"query.settings.tmp_space_si"...>>]
[ns_server:debug,2020-03-27T20:37:41.962Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{request_limit,capi} ->
undefined
[ns_server:debug,2020-03-27T20:37:41.962Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{request_limit,rest} ->
undefined
[ns_server:debug,2020-03-27T20:37:41.962Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',address_family} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|inet]
[ns_server:debug,2020-03-27T20:37:41.962Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',audit} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}]
[ns_server:debug,2020-03-27T20:37:41.962Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',capi_port} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|8092]
[ns_server:debug,2020-03-27T20:37:41.962Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',cbas_admin_port} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|9110]
[ns_server:debug,2020-03-27T20:37:41.962Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',cbas_cc_client_port} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|9113]
[ns_server:debug,2020-03-27T20:37:41.962Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',cbas_cc_cluster_port} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|9112]
[ns_server:debug,2020-03-27T20:37:41.962Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',cbas_cc_http_port} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|9111]
[ns_server:debug,2020-03-27T20:37:41.962Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',cbas_cluster_port} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|9115]
[ns_server:debug,2020-03-27T20:37:41.963Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',cbas_console_port} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|9114]
[ns_server:debug,2020-03-27T20:37:41.963Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',cbas_data_port} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|9116]
[ns_server:debug,2020-03-27T20:37:41.963Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',cbas_debug_port} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|-1]
[ns_server:debug,2020-03-27T20:37:41.963Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',cbas_dirs} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]},
 "/opt/couchbase/var/lib/couchbase/data"]
[ns_server:debug,2020-03-27T20:37:41.963Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',cbas_http_port} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|8095]
[ns_server:debug,2020-03-27T20:37:41.964Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',cbas_messaging_port} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|9118]
[ns_server:debug,2020-03-27T20:37:41.964Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',cbas_metadata_callback_port} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|9119]
[ns_server:debug,2020-03-27T20:37:41.964Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',cbas_metadata_port} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|9121]
[ns_server:debug,2020-03-27T20:37:41.964Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',cbas_parent_port} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|9122]
[ns_server:debug,2020-03-27T20:37:41.964Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',cbas_replication_port} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|9120]
[ns_server:debug,2020-03-27T20:37:41.964Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',cbas_result_port} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|9117]
[ns_server:debug,2020-03-27T20:37:41.964Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',cbas_ssl_port} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|18095]
[ns_server:debug,2020-03-27T20:37:41.964Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',compaction_daemon} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]},
 {check_interval,30},
 {min_db_file_size,131072},
 {min_view_file_size,20971520}]
[ns_server:debug,2020-03-27T20:37:41.964Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',config_version} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|{6,5}]
[ns_server:debug,2020-03-27T20:37:41.964Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',erl_external_listeners} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]},
 {inet,false},
 {inet6,false}]
[ns_server:debug,2020-03-27T20:37:41.964Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',eventing_debug_port} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|9140]
[ns_server:debug,2020-03-27T20:37:41.965Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',eventing_dir} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]},
 47,111,112,116,47,99,111,117,99,104,98,97,115,101,47,118,97,114,47,108,105,
 98,47,99,111,117,99,104,98,97,115,101,47,100,97,116,97]
[ns_server:debug,2020-03-27T20:37:41.965Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',eventing_http_port} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|8096]
[ns_server:debug,2020-03-27T20:37:41.965Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',eventing_https_port} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|18096]
[ns_server:debug,2020-03-27T20:37:41.965Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',fts_grpc_port} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|9130]
[ns_server:debug,2020-03-27T20:37:41.965Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',fts_grpc_ssl_port} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|19130]
[ns_server:debug,2020-03-27T20:37:41.965Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',fts_http_port} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|8094]
[ns_server:debug,2020-03-27T20:37:41.965Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',fts_ssl_port} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|18094]
[ns_server:debug,2020-03-27T20:37:41.965Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',indexer_admin_port} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|9100]
[ns_server:debug,2020-03-27T20:37:41.965Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',indexer_http_port} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|9102]
[ns_server:debug,2020-03-27T20:37:41.965Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',indexer_https_port} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|19102]
[ns_server:debug,2020-03-27T20:37:41.965Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',indexer_scan_port} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|9101]
[ns_server:debug,2020-03-27T20:37:41.965Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',indexer_stcatchup_port} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|9104]
[ns_server:debug,2020-03-27T20:37:41.965Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',indexer_stinit_port} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|9103]
[ns_server:debug,2020-03-27T20:37:41.965Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',indexer_stmaint_port} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|9105]
[ns_server:debug,2020-03-27T20:37:41.965Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',is_enterprise} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|true]
[ns_server:debug,2020-03-27T20:37:41.965Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',isasl} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]},
 {path,"/opt/couchbase/var/lib/couchbase/isasl.pw"}]
[ns_server:debug,2020-03-27T20:37:41.965Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',membership} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
 active]
[ns_server:debug,2020-03-27T20:37:41.966Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',memcached} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]},
 {port,11210},
 {dedicated_port,11209},
 {dedicated_ssl_port,11206},
 {ssl_port,11207},
 {admin_user,"@ns_server"},
 {other_users,["@cbq-engine","@projector","@goxdcr","@index","@fts",
               "@eventing","@cbas"]},
 {admin_pass,"*****"},
 {engines,[{membase,[{engine,"/opt/couchbase/lib/memcached/ep.so"},
                     {static_config_string,"failpartialwarmup=false"}]},
           {memcached,[{engine,"/opt/couchbase/lib/memcached/default_engine.so"},
                       {static_config_string,"vb0=true"}]}]},
 {config_path,"/opt/couchbase/var/lib/couchbase/config/memcached.json"},
 {audit_file,"/opt/couchbase/var/lib/couchbase/config/audit.json"},
 {rbac_file,"/opt/couchbase/var/lib/couchbase/config/memcached.rbac"},
 {log_path,"/opt/couchbase/var/lib/couchbase/logs"},
 {log_prefix,"memcached.log"},
 {log_generations,20},
 {log_cyclesize,10485760},
 {log_sleeptime,19},
 {log_rotation_period,39003}]
[ns_server:debug,2020-03-27T20:37:41.966Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',memcached_config} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
 {[{interfaces,
    {memcached_config_mgr,omit_missing_mcd_ports,
     [{[{host,<<"*">>},
        {port,port},
        {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
        {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]},
      {[{host,<<"*">>},
        {port,dedicated_port},
        {system,true},
        {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
        {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]},
      {[{host,<<"*">>},
        {port,ssl_port},
        {ssl,
         {[{key,
            <<"/opt/couchbase/var/lib/couchbase/config/memcached-key.pem">>},
           {cert,
            <<"/opt/couchbase/var/lib/couchbase/config/memcached-cert.pem">>}]}},
        {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
        {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]},
      {[{host,<<"*">>},
        {port,dedicated_ssl_port},
        {system,true},
        {ssl,
         {[{key,
            <<"/opt/couchbase/var/lib/couchbase/config/memcached-key.pem">>},
           {cert,
            <<"/opt/couchbase/var/lib/couchbase/config/memcached-cert.pem">>}]}},
        {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
        {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]}]}},
   {ssl_cipher_list,{memcached_config_mgr,get_ssl_cipher_list,[]}},
   {ssl_cipher_order,{memcached_config_mgr,get_ssl_cipher_order,[]}},
   {client_cert_auth,{memcached_config_mgr,client_cert_auth,[]}},
   {ssl_minimum_protocol,{memcached_config_mgr,ssl_minimum_protocol,[]}},
   {connection_idle_time,connection_idle_time},
   {privilege_debug,privilege_debug},
   {breakpad,
    {[{enabled,breakpad_enabled},
      {minidump_dir,{memcached_config_mgr,get_minidump_dir,[]}}]}},
   {opentracing,
    {[{enabled,opentracing_enabled},
      {module,{"~s",[opentracing_module]}},
      {config,{"~s",[opentracing_config]}}]}},
   {admin,{"~s",[admin_user]}},
   {verbosity,verbosity},
   {audit_file,{"~s",[audit_file]}},
   {rbac_file,{"~s",[rbac_file]}},
   {dedupe_nmvb_maps,dedupe_nmvb_maps},
   {tracing_enabled,tracing_enabled},
   {datatype_snappy,{memcached_config_mgr,is_snappy_enabled,[]}},
   {xattr_enabled,true},
   {scramsha_fallback_salt,{memcached_config_mgr,get_fallback_salt,[]}},
   {collections_enabled,{memcached_config_mgr,collections_enabled,[]}},
   {max_connections,max_connections},
   {system_connections,system_connections},
   {num_reader_threads,num_reader_threads},
   {num_writer_threads,num_writer_threads},
   {logger,
    {[{filename,{"~s/~s",[log_path,log_prefix]}},
      {cyclesize,log_cyclesize},
      {sleeptime,log_sleeptime}]}},
   {external_auth_service,{memcached_config_mgr,get_external_auth_service,[]}},
   {active_external_users_push_interval,
    {memcached_config_mgr,get_external_users_push_interval,[]}}]}]
[ns_server:debug,2020-03-27T20:37:41.967Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',memcached_dedicated_ssl_port} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|11206]
[ns_server:debug,2020-03-27T20:37:41.967Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',memcached_defaults} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]},
 {max_connections,65000},
 {system_connections,5000},
 {connection_idle_time,0},
 {verbosity,0},
 {privilege_debug,false},
 {opentracing_enabled,false},
 {opentracing_module,[]},
 {opentracing_config,[]},
 {breakpad_enabled,true},
 {breakpad_minidump_dir_path,"/opt/couchbase/var/lib/couchbase/crash"},
 {dedupe_nmvb_maps,false},
 {tracing_enabled,true},
 {datatype_snappy,true},
 {num_reader_threads,<<"default">>},
 {num_writer_threads,<<"default">>}]
[ns_server:debug,2020-03-27T20:37:41.967Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',moxi} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]},
 {port,0}]
[ns_server:debug,2020-03-27T20:37:41.967Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',node_encryption} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|false]
[ns_server:debug,2020-03-27T20:37:41.967Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',ns_log} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]},
 {filename,"/opt/couchbase/var/lib/couchbase/ns_log"}]
[ns_server:debug,2020-03-27T20:37:41.967Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',port_servers} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}]
[ns_server:debug,2020-03-27T20:37:41.967Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',projector_port} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|9999]
[ns_server:debug,2020-03-27T20:37:41.967Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',projector_ssl_port} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|9999]
[ns_server:debug,2020-03-27T20:37:41.967Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',query_port} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|8093]
[ns_server:debug,2020-03-27T20:37:41.967Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',rest} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]},
 {port,8091},
 {port_meta,global}]
[ns_server:debug,2020-03-27T20:37:41.967Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',saslauthd_enabled} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|true]
[ns_server:debug,2020-03-27T20:37:41.967Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',ssl_capi_port} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|18092]
[ns_server:debug,2020-03-27T20:37:41.967Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',ssl_query_port} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|18093]
[ns_server:debug,2020-03-27T20:37:41.967Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',ssl_rest_port} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|18091]
[ns_server:debug,2020-03-27T20:37:41.967Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',uuid} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|
 <<"60a6bc3db77e6c7b91c556140dcfec71">>]
[ns_server:debug,2020-03-27T20:37:41.967Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',xdcr_rest_port} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|9998]
[ns_server:debug,2020-03-27T20:37:41.968Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',{project_intact,is_vulnerable}} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|false]
[error_logger:info,2020-03-27T20:37:41.979Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_node_disco_sup}
             started: [{pid,<0.319.0>},
                       {id,ns_config_rep},
                       {mfargs,{ns_config_rep,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:37:41.979Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.312.0>},
                       {name,ns_node_disco_sup},
                       {mfargs,{ns_node_disco_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[ns_server:debug,2020-03-27T20:37:41.979Z,ns_1@127.0.0.1:ns_config_rep<0.319.0>:ns_config_rep:do_push_keys:321]Replicating some config keys ([alert_limits,audit,audit_decriptors,
                               auto_failover_cfg,auto_reprovision_cfg,
                               autocompaction,buckets,cbas_memory_quota,
                               cert_and_pkey,client_cert_auth,
                               cluster_compat_version,cluster_name,
                               drop_request_memory_threshold_mib,email_alerts,
                               fts_memory_quota,
                               index_aware_rebalance_disabled,
                               log_redaction_default_cfg,max_bucket_count,
                               memcached,memory_quota,nodes_wanted,otp,
                               password_policy,quorum_nodes,remote_clusters,
                               replication,rest,rest_creds,retry_rebalance,
                               scramsha_fallback_salt,secure_headers,
                               server_groups,set_view_update_daemon,
                               {couchdb,max_parallel_indexers},
                               {couchdb,max_parallel_replica_indexers},
                               {local_changes_count,
                                   <<"60a6bc3db77e6c7b91c556140dcfec71">>},
                               {metakv,<<"/eventing/settings/config">>},
                               {metakv,<<"/indexing/settings/config">>},
                               {metakv,<<"/query/settings/config">>},
                               {request_limit,capi},
                               {request_limit,rest},
                               {node,'ns_1@127.0.0.1',address_family},
                               {node,'ns_1@127.0.0.1',audit},
                               {node,'ns_1@127.0.0.1',capi_port},
                               {node,'ns_1@127.0.0.1',cbas_admin_port},
                               {node,'ns_1@127.0.0.1',cbas_cc_client_port},
                               {node,'ns_1@127.0.0.1',cbas_cc_cluster_port},
                               {node,'ns_1@127.0.0.1',cbas_cc_http_port},
                               {node,'ns_1@127.0.0.1',cbas_cluster_port},
                               {node,'ns_1@127.0.0.1',cbas_console_port},
                               {node,'ns_1@127.0.0.1',cbas_data_port},
                               {node,'ns_1@127.0.0.1',cbas_debug_port},
                               {node,'ns_1@127.0.0.1',cbas_dirs},
                               {node,'ns_1@127.0.0.1',cbas_http_port},
                               {node,'ns_1@127.0.0.1',cbas_messaging_port},
                               {node,'ns_1@127.0.0.1',
                                   cbas_metadata_callback_port},
                               {node,'ns_1@127.0.0.1',cbas_metadata_port},
                               {node,'ns_1@127.0.0.1',cbas_parent_port},
                               {node,'ns_1@127.0.0.1',cbas_replication_port},
                               {node,'ns_1@127.0.0.1',cbas_result_port},
                               {node,'ns_1@127.0.0.1',cbas_ssl_port},
                               {node,'ns_1@127.0.0.1',compaction_daemon},
                               {node,'ns_1@127.0.0.1',config_version},
                               {node,'ns_1@127.0.0.1',erl_external_listeners}]..)
[ns_server:debug,2020-03-27T20:37:41.980Z,ns_1@127.0.0.1:ns_ssl_services_setup<0.216.0>:ns_ssl_services_setup:trigger_ssl_reload:594]Notify services [capi_ssl_service] about secure_headers_changed change
[ns_server:debug,2020-03-27T20:37:41.981Z,ns_1@127.0.0.1:ns_ssl_services_setup<0.216.0>:ns_ssl_services_setup:notify_services:740]Going to notify following services: [capi_ssl_service]
[ns_server:info,2020-03-27T20:37:42.003Z,ns_1@127.0.0.1:<0.334.0>:ns_ssl_services_setup:notify_service:772]Successfully notified service capi_ssl_service
[ns_server:info,2020-03-27T20:37:42.003Z,ns_1@127.0.0.1:ns_ssl_services_setup<0.216.0>:ns_ssl_services_setup:notify_services:756]Succesfully notified services [capi_ssl_service]
[error_logger:info,2020-03-27T20:37:42.005Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.335.0>},
                       {name,vbucket_map_mirror},
                       {mfargs,{vbucket_map_mirror,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:37:42.026Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.337.0>},
                       {name,bucket_info_cache},
                       {mfargs,{bucket_info_cache,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:37:42.026Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.340.0>},
                       {name,ns_tick_event},
                       {mfargs,{gen_event,start_link,[{local,ns_tick_event}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:37:42.026Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.341.0>},
                       {name,buckets_events},
                       {mfargs,
                           {gen_event,start_link,[{local,buckets_events}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:37:42.026Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.342.0>},
                       {name,ns_stats_event},
                       {mfargs,
                           {gen_event,start_link,[{local,ns_stats_event}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-03-27T20:37:42.029Z,ns_1@127.0.0.1:users_storage<0.262.0>:replicated_dets:handle_call:302]Suspended by process <0.305.0>
[ns_server:debug,2020-03-27T20:37:42.029Z,ns_1@127.0.0.1:memcached_passwords<0.305.0>:replicated_dets:select_from_dets_locked:350]Starting select with {users_storage,[{{docv2,{auth,{'_',local}},'_','_'},
                                      [],
                                      ['$_']}],
                                    100}
[ns_server:debug,2020-03-27T20:37:42.030Z,ns_1@127.0.0.1:users_storage<0.262.0>:replicated_dets:handle_call:309]Released by process <0.305.0>
[error_logger:info,2020-03-27T20:37:42.030Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.343.0>},
                       {name,samples_loader_tasks},
                       {mfargs,{samples_loader_tasks,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-03-27T20:37:42.058Z,ns_1@127.0.0.1:memcached_refresh<0.213.0>:memcached_refresh:handle_cast:55]Refresh of isasl requested
[ns_server:warn,2020-03-27T20:37:42.062Z,ns_1@127.0.0.1:memcached_refresh<0.213.0>:ns_memcached:connect:1101]Unable to connect: {error,{badmatch,{error,econnrefused}}}.
[ns_server:debug,2020-03-27T20:37:42.062Z,ns_1@127.0.0.1:memcached_refresh<0.213.0>:memcached_refresh:handle_info:93]Refresh of [rbac,isasl] failed. Retry in 1000 ms.
[error_logger:info,2020-03-27T20:37:42.065Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_heart_sup}
             started: [{pid,<0.345.0>},
                       {id,ns_heart},
                       {mfargs,{ns_heart,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:37:42.065Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_heart_sup}
             started: [{pid,<0.347.0>},
                       {id,ns_heart_slow_updater},
                       {mfargs,{ns_heart,start_link_slow_updater,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:37:42.065Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.344.0>},
                       {name,ns_heart_sup},
                       {mfargs,{ns_heart_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[ns_server:debug,2020-03-27T20:37:42.069Z,ns_1@127.0.0.1:ns_heart<0.345.0>:ns_heart:grab_latest_stats:263]Ignoring failure to grab "@system" stats:
{'EXIT',{badarg,[{ets,last,['stats_archiver-@system-minute'],[]},
                 {stats_archiver,latest_sample,2,
                                 [{file,"src/stats_archiver.erl"},{line,120}]},
                 {ns_heart,grab_latest_stats,1,
                           [{file,"src/ns_heart.erl"},{line,259}]},
                 {ns_heart,'-current_status_slow_inner/0-lc$^0/1-0-',1,
                           [{file,"src/ns_heart.erl"},{line,282}]},
                 {ns_heart,current_status_slow_inner,0,
                           [{file,"src/ns_heart.erl"},{line,282}]},
                 {ns_heart,current_status_slow,1,
                           [{file,"src/ns_heart.erl"},{line,250}]},
                 {ns_heart,update_current_status,1,
                           [{file,"src/ns_heart.erl"},{line,187}]},
                 {ns_heart,handle_info,2,
                           [{file,"src/ns_heart.erl"},{line,118}]}]}}

[ns_server:debug,2020-03-27T20:37:42.069Z,ns_1@127.0.0.1:ns_heart<0.345.0>:ns_heart:grab_latest_stats:263]Ignoring failure to grab "@system-processes" stats:
{'EXIT',{badarg,[{ets,last,['stats_archiver-@system-processes-minute'],[]},
                 {stats_archiver,latest_sample,2,
                                 [{file,"src/stats_archiver.erl"},{line,120}]},
                 {ns_heart,grab_latest_stats,1,
                           [{file,"src/ns_heart.erl"},{line,259}]},
                 {ns_heart,'-current_status_slow_inner/0-lc$^0/1-0-',1,
                           [{file,"src/ns_heart.erl"},{line,282}]},
                 {ns_heart,'-current_status_slow_inner/0-lc$^0/1-0-',1,
                           [{file,"src/ns_heart.erl"},{line,282}]},
                 {ns_heart,current_status_slow_inner,0,
                           [{file,"src/ns_heart.erl"},{line,282}]},
                 {ns_heart,current_status_slow,1,
                           [{file,"src/ns_heart.erl"},{line,250}]},
                 {ns_heart,update_current_status,1,
                           [{file,"src/ns_heart.erl"},{line,187}]}]}}

[error_logger:info,2020-03-27T20:37:42.072Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_doctor_sup}
             started: [{pid,<0.353.0>},
                       {id,ns_doctor_events},
                       {mfargs,
                           {gen_event,start_link,[{local,ns_doctor_events}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-03-27T20:37:42.076Z,ns_1@127.0.0.1:<0.350.0>:restartable:start_child:98]Started child process <0.352.0>
  MFA: {ns_doctor_sup,start_link,[]}
[error_logger:info,2020-03-27T20:37:42.076Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_doctor_sup}
             started: [{pid,<0.354.0>},
                       {id,ns_doctor},
                       {mfargs,{ns_doctor,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:37:42.077Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.350.0>},
                       {name,ns_doctor_sup},
                       {mfargs,
                           {restartable,start_link,
                               [{ns_doctor_sup,start_link,[]},infinity]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-03-27T20:37:42.078Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.357.0>},
                       {name,master_activity_events},
                       {mfargs,
                           {gen_event,start_link,
                               [{local,master_activity_events}]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:37:42.105Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.358.0>},
                       {name,xdcr_ckpt_store},
                       {mfargs,{simple_store,start_link,[xdcr_ckpt_data]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:37:42.105Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.359.0>},
                       {name,metakv_worker},
                       {mfargs,{work_queue,start_link,[metakv_worker]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:37:42.105Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.360.0>},
                       {name,index_events},
                       {mfargs,{gen_event,start_link,[{local,index_events}]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:37:42.106Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.361.0>},
                       {name,index_settings_manager},
                       {mfargs,{index_settings_manager,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:37:42.108Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.364.0>},
                       {name,query_settings_manager},
                       {mfargs,{query_settings_manager,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:37:42.114Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.366.0>},
                       {name,eventing_settings_manager},
                       {mfargs,{eventing_settings_manager,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:37:42.114Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.368.0>},
                       {name,audit_events},
                       {mfargs,{gen_event,start_link,[{local,audit_events}]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[ns_server:debug,2020-03-27T20:37:42.117Z,ns_1@127.0.0.1:ns_heart<0.345.0>:goxdcr_rest:get_from_goxdcr:140]Goxdcr is temporary not available. Return empty list.
[ns_server:debug,2020-03-27T20:37:42.121Z,ns_1@127.0.0.1:ns_heart<0.345.0>:cluster_logs_collection_task:maybe_build_cluster_logs_task:46]Ignoring exception trying to read cluster_logs_collection_task_status table: error:badarg
[error_logger:info,2020-03-27T20:37:42.124Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,menelaus_sup}
             started: [{pid,<0.375.0>},
                       {id,menelaus_ui_auth},
                       {mfargs,{menelaus_ui_auth,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:37:42.125Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,menelaus_sup}
             started: [{pid,<0.377.0>},
                       {id,scram_sha},
                       {mfargs,{scram_sha,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:37:42.127Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,menelaus_sup}
             started: [{pid,<0.380.0>},
                       {id,menelaus_local_auth},
                       {mfargs,{menelaus_local_auth,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[ns_server:debug,2020-03-27T20:37:42.135Z,ns_1@127.0.0.1:ns_heart_slow_status_updater<0.347.0>:ns_heart:grab_latest_stats:263]Ignoring failure to grab "@system" stats:
{'EXIT',{badarg,[{ets,last,['stats_archiver-@system-minute'],[]},
                 {stats_archiver,latest_sample,2,
                                 [{file,"src/stats_archiver.erl"},{line,120}]},
                 {ns_heart,grab_latest_stats,1,
                           [{file,"src/ns_heart.erl"},{line,259}]},
                 {ns_heart,'-current_status_slow_inner/0-lc$^0/1-0-',1,
                           [{file,"src/ns_heart.erl"},{line,282}]},
                 {ns_heart,current_status_slow_inner,0,
                           [{file,"src/ns_heart.erl"},{line,282}]},
                 {ns_heart,current_status_slow,1,
                           [{file,"src/ns_heart.erl"},{line,250}]},
                 {ns_heart,slow_updater_loop,0,
                           [{file,"src/ns_heart.erl"},{line,244}]},
                 {proc_lib,init_p_do_apply,3,
                           [{file,"proc_lib.erl"},{line,247}]}]}}

[ns_server:debug,2020-03-27T20:37:42.136Z,ns_1@127.0.0.1:ns_heart_slow_status_updater<0.347.0>:ns_heart:grab_latest_stats:263]Ignoring failure to grab "@system-processes" stats:
{'EXIT',{badarg,[{ets,last,['stats_archiver-@system-processes-minute'],[]},
                 {stats_archiver,latest_sample,2,
                                 [{file,"src/stats_archiver.erl"},{line,120}]},
                 {ns_heart,grab_latest_stats,1,
                           [{file,"src/ns_heart.erl"},{line,259}]},
                 {ns_heart,'-current_status_slow_inner/0-lc$^0/1-0-',1,
                           [{file,"src/ns_heart.erl"},{line,282}]},
                 {ns_heart,'-current_status_slow_inner/0-lc$^0/1-0-',1,
                           [{file,"src/ns_heart.erl"},{line,282}]},
                 {ns_heart,current_status_slow_inner,0,
                           [{file,"src/ns_heart.erl"},{line,282}]},
                 {ns_heart,current_status_slow,1,
                           [{file,"src/ns_heart.erl"},{line,250}]},
                 {ns_heart,slow_updater_loop,0,
                           [{file,"src/ns_heart.erl"},{line,244}]}]}}

[error_logger:info,2020-03-27T20:37:42.139Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,menelaus_sup}
             started: [{pid,<0.391.0>},
                       {id,menelaus_web_cache},
                       {mfargs,{menelaus_web_cache,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[ns_server:debug,2020-03-27T20:37:42.139Z,ns_1@127.0.0.1:ns_heart_slow_status_updater<0.347.0>:goxdcr_rest:get_from_goxdcr:140]Goxdcr is temporary not available. Return empty list.
[ns_server:debug,2020-03-27T20:37:42.139Z,ns_1@127.0.0.1:ns_heart_slow_status_updater<0.347.0>:cluster_logs_collection_task:maybe_build_cluster_logs_task:46]Ignoring exception trying to read cluster_logs_collection_task_status table: error:badarg
[error_logger:info,2020-03-27T20:37:42.145Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,menelaus_sup}
             started: [{pid,<0.392.0>},
                       {id,menelaus_stats_gatherer},
                       {mfargs,{menelaus_stats_gatherer,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:37:42.150Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,menelaus_sup}
             started: [{pid,<0.393.0>},
                       {id,json_rpc_events},
                       {mfargs,
                           {gen_event,start_link,[{local,json_rpc_events}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:info,2020-03-27T20:37:42.160Z,ns_1@127.0.0.1:<0.395.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for cbas
[ns_server:info,2020-03-27T20:37:42.161Z,ns_1@127.0.0.1:<0.395.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for eventing
[ns_server:info,2020-03-27T20:37:42.161Z,ns_1@127.0.0.1:<0.395.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for fts
[ns_server:info,2020-03-27T20:37:42.161Z,ns_1@127.0.0.1:<0.395.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for n1ql
[error_logger:info,2020-03-27T20:37:42.163Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {<0.395.0>,menelaus_web}
             started: [{pid,<0.403.0>},
                       {id,menelaus_web_ipv4},
                       {mfargs,
                           {menelaus_web,http_server,
                               [[{ip,"0.0.0.0"},{name,menelaus_web_ipv4}]]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[ns_server:info,2020-03-27T20:37:42.164Z,ns_1@127.0.0.1:<0.395.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for cbas
[ns_server:info,2020-03-27T20:37:42.165Z,ns_1@127.0.0.1:<0.395.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for eventing
[ns_server:info,2020-03-27T20:37:42.165Z,ns_1@127.0.0.1:<0.395.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for fts
[ns_server:info,2020-03-27T20:37:42.166Z,ns_1@127.0.0.1:<0.395.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for n1ql
[ns_server:debug,2020-03-27T20:37:42.167Z,ns_1@127.0.0.1:<0.394.0>:restartable:start_child:98]Started child process <0.395.0>
  MFA: {menelaus_web,start_link,[]}
[error_logger:info,2020-03-27T20:37:42.167Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {<0.395.0>,menelaus_web}
             started: [{pid,<0.422.0>},
                       {id,menelaus_web_ipv6},
                       {mfargs,
                           {menelaus_web,http_server,
                               [[{ip,"::"},{name,menelaus_web_ipv6}]]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:37:42.167Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,menelaus_sup}
             started: [{pid,<0.394.0>},
                       {id,menelaus_web},
                       {mfargs,
                           {restartable,start_link,
                               [{menelaus_web,start_link,[]},infinity]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-03-27T20:37:42.170Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,menelaus_sup}
             started: [{pid,<0.439.0>},
                       {id,menelaus_event},
                       {mfargs,{menelaus_event,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:37:42.175Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,menelaus_sup}
             started: [{pid,<0.440.0>},
                       {id,hot_keys_keeper},
                       {mfargs,{hot_keys_keeper,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:37:42.180Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,menelaus_sup}
             started: [{pid,<0.441.0>},
                       {id,menelaus_web_alerts_srv},
                       {mfargs,{menelaus_web_alerts_srv,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:37:42.190Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,menelaus_sup}
             started: [{pid,<0.442.0>},
                       {id,menelaus_cbauth},
                       {mfargs,{menelaus_cbauth,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[user:info,2020-03-27T20:37:42.190Z,ns_1@127.0.0.1:ns_server_sup<0.298.0>:menelaus_sup:start_link:48]Couchbase Server has started on web port 8091 on node 'ns_1@127.0.0.1'. Version: "6.5.0-4960-enterprise".
[error_logger:info,2020-03-27T20:37:42.190Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.369.0>},
                       {name,menelaus},
                       {mfargs,{menelaus_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-03-27T20:37:42.190Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.448.0>},
                       {name,ns_ports_setup},
                       {mfargs,{ns_ports_setup,start,[]}},
                       {restart_type,{permanent,4}},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:37:42.192Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,service_agent_sup}
             started: [{pid,<0.452.0>},
                       {id,service_agent_children_sup},
                       {mfargs,
                           {supervisor,start_link,
                               [{local,service_agent_children_sup},
                                service_agent_sup,child]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-03-27T20:37:42.192Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,service_agent_sup}
             started: [{pid,<0.453.0>},
                       {id,service_agent_worker},
                       {mfargs,
                           {erlang,apply,
                               [#Fun<service_agent_sup.0.107373856>,[]]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:37:42.192Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.451.0>},
                       {name,service_agent_sup},
                       {mfargs,{service_agent_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[ns_server:debug,2020-03-27T20:37:42.195Z,ns_1@127.0.0.1:ns_ports_setup<0.448.0>:ns_ports_manager:set_dynamic_children:54]Setting children [memcached,saslauthd_port,goxdcr]
[error_logger:info,2020-03-27T20:37:42.205Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.455.0>},
                       {name,ns_memcached_sockets_pool},
                       {mfargs,{ns_memcached_sockets_pool,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-03-27T20:37:42.219Z,ns_1@127.0.0.1:memcached_auth_server<0.456.0>:memcached_auth_server:reconnect:233]Skipping creation of 'Auth provider' connection because external users are disabled
[error_logger:info,2020-03-27T20:37:42.219Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.456.0>},
                       {name,memcached_auth_server},
                       {mfargs,{memcached_auth_server,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-03-27T20:37:42.220Z,ns_1@127.0.0.1:ns_audit_cfg<0.458.0>:ns_audit_cfg:write_audit_json:259]Writing new content to "/opt/couchbase/var/lib/couchbase/config/audit.json", Params [{descriptors_path,
                                                                                      "/opt/couchbase/etc/security"},
                                                                                     {version,
                                                                                      2},
                                                                                     {uuid,
                                                                                      "48537283"},
                                                                                     {event_states,
                                                                                      {[]}},
                                                                                     {filtering_enabled,
                                                                                      true},
                                                                                     {disabled_userids,
                                                                                      []},
                                                                                     {auditd_enabled,
                                                                                      false},
                                                                                     {log_path,
                                                                                      "/opt/couchbase/var/lib/couchbase/logs"},
                                                                                     {rotate_interval,
                                                                                      86400},
                                                                                     {rotate_size,
                                                                                      20971520},
                                                                                     {sync,
                                                                                      []}]
[ns_server:debug,2020-03-27T20:37:42.254Z,ns_1@127.0.0.1:ns_audit_cfg<0.458.0>:ns_audit_cfg:notify_memcached:170]Instruct memcached to reload audit config
[error_logger:info,2020-03-27T20:37:42.254Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.458.0>},
                       {name,ns_audit_cfg},
                       {mfargs,{ns_audit_cfg,start_link,[]}},
                       {restart_type,{permanent,4}},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:warn,2020-03-27T20:37:42.256Z,ns_1@127.0.0.1:<0.462.0>:ns_memcached:connect:1104]Unable to connect: {error,{badmatch,{error,econnrefused}}}, retrying.
[error_logger:info,2020-03-27T20:37:42.262Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.463.0>},
                       {name,ns_audit},
                       {mfargs,{ns_audit,start_link,[]}},
                       {restart_type,{permanent,4}},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-03-27T20:37:42.262Z,ns_1@127.0.0.1:memcached_config_mgr<0.464.0>:memcached_config_mgr:init:49]waiting for completion of initial ns_ports_setup round
[error_logger:info,2020-03-27T20:37:42.262Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.464.0>},
                       {name,memcached_config_mgr},
                       {mfargs,{memcached_config_mgr,start_link,[]}},
                       {restart_type,{permanent,4}},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:info,2020-03-27T20:37:42.267Z,ns_1@127.0.0.1:<0.465.0>:ns_memcached_log_rotator:init:42]Starting log rotator on "/opt/couchbase/var/lib/couchbase/logs"/"memcached.log"* with an initial period of 39003ms
[error_logger:info,2020-03-27T20:37:42.267Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.465.0>},
                       {name,ns_memcached_log_rotator},
                       {mfargs,{ns_memcached_log_rotator,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:37:42.269Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.466.0>},
                       {name,testconditions_store},
                       {mfargs,{simple_store,start_link,[testconditions]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:37:42.271Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.467.0>},
                       {name,terse_cluster_info_uploader},
                       {mfargs,{terse_cluster_info_uploader,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-03-27T20:37:42.275Z,ns_1@127.0.0.1:terse_cluster_info_uploader<0.467.0>:terse_cluster_info_uploader:handle_info:48]Refreshing terse cluster info with <<"{\"rev\":36,\"nodesExt\":[{\"services\":{\"mgmt\":8091,\"mgmtSSL\":18091,\"kv\":11210,\"kvSSL\":11207,\"capi\":8092,\"capiSSL\":18092,\"projector\":9999,\"projector\":9999},\"thisNode\":true}],\"clusterCapabilitiesVer\":[1,0],\"clusterCapabilities\":{\"n1ql\":[\"enhancedPreparedStatements\"]}}">>
[ns_server:warn,2020-03-27T20:37:42.277Z,ns_1@127.0.0.1:<0.471.0>:ns_memcached:connect:1104]Unable to connect: {error,{badmatch,{error,econnrefused}}}, retrying.
[error_logger:info,2020-03-27T20:37:42.277Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_bucket_worker_sup}
             started: [{pid,<0.472.0>},
                       {id,ns_bucket_sup},
                       {mfargs,{ns_bucket_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-03-27T20:37:42.278Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_bucket_worker_sup}
             started: [{pid,<0.473.0>},
                       {id,ns_bucket_worker},
                       {mfargs,{ns_bucket_worker,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:37:42.279Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.469.0>},
                       {name,ns_bucket_worker_sup},
                       {mfargs,{ns_bucket_worker_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-03-27T20:37:42.283Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.475.0>},
                       {name,system_stats_collector},
                       {mfargs,{system_stats_collector,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:37:42.283Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.479.0>},
                       {name,{stats_archiver,"@system"}},
                       {mfargs,{stats_archiver,start_link,["@system"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:37:42.287Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.481.0>},
                       {name,{stats_reader,"@system"}},
                       {mfargs,{stats_reader,start_link,["@system"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:37:42.288Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.482.0>},
                       {name,{stats_archiver,"@system-processes"}},
                       {mfargs,
                           {stats_archiver,start_link,["@system-processes"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:37:42.288Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.484.0>},
                       {name,{stats_reader,"@system-processes"}},
                       {mfargs,
                           {stats_reader,start_link,["@system-processes"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:37:42.288Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.485.0>},
                       {name,{stats_archiver,"@query"}},
                       {mfargs,{stats_archiver,start_link,["@query"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:37:42.289Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.487.0>},
                       {name,{stats_reader,"@query"}},
                       {mfargs,{stats_reader,start_link,["@query"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:37:42.301Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.488.0>},
                       {name,query_stats_collector},
                       {mfargs,{query_stats_collector,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-03-27T20:37:42.304Z,ns_1@127.0.0.1:ns_ports_setup<0.448.0>:ns_ports_setup:set_children:85]Monitor ns_child_ports_sup <12939.109.0>
[ns_server:debug,2020-03-27T20:37:42.304Z,ns_1@127.0.0.1:memcached_config_mgr<0.464.0>:memcached_config_mgr:init:51]ns_ports_setup seems to be ready
[error_logger:info,2020-03-27T20:37:42.305Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.490.0>},
                       {name,{stats_archiver,"@global"}},
                       {mfargs,{stats_archiver,start_link,["@global"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:37:42.305Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.493.0>},
                       {name,{stats_reader,"@global"}},
                       {mfargs,{stats_reader,start_link,["@global"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:37:42.316Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.494.0>},
                       {name,global_stats_collector},
                       {mfargs,{global_stats_collector,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-03-27T20:37:42.330Z,ns_1@127.0.0.1:memcached_config_mgr<0.464.0>:memcached_config_mgr:find_port_pid_loop:137]Found memcached port <12939.116.0>
[error_logger:info,2020-03-27T20:37:42.334Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.497.0>},
                       {name,goxdcr_status_keeper},
                       {mfargs,{goxdcr_status_keeper,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-03-27T20:37:42.338Z,ns_1@127.0.0.1:goxdcr_status_keeper<0.497.0>:goxdcr_rest:get_from_goxdcr:140]Goxdcr is temporary not available. Return empty list.
[ns_server:debug,2020-03-27T20:37:42.339Z,ns_1@127.0.0.1:goxdcr_status_keeper<0.497.0>:goxdcr_rest:get_from_goxdcr:140]Goxdcr is temporary not available. Return empty list.
[error_logger:info,2020-03-27T20:37:42.345Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,services_stats_sup}
             started: [{pid,<0.502.0>},
                       {id,service_stats_children_sup},
                       {mfargs,
                           {supervisor,start_link,
                               [{local,service_stats_children_sup},
                                services_stats_sup,child]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-03-27T20:37:42.348Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,service_status_keeper_sup}
             started: [{pid,<0.504.0>},
                       {id,service_status_keeper_worker},
                       {mfargs,
                           {work_queue,start_link,
                               [service_status_keeper_worker]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:37:42.361Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,service_status_keeper_sup}
             started: [{pid,<0.506.0>},
                       {id,service_status_keeper_index},
                       {mfargs,{service_index,start_keeper,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:37:42.366Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,service_status_keeper_sup}
             started: [{pid,<0.509.0>},
                       {id,service_status_keeper_fts},
                       {mfargs,{service_fts,start_keeper,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-03-27T20:37:42.369Z,ns_1@127.0.0.1:memcached_config_mgr<0.464.0>:memcached_config_mgr:init:82]wrote memcached config to /opt/couchbase/var/lib/couchbase/config/memcached.json. Will activate memcached port server
[error_logger:info,2020-03-27T20:37:42.370Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,service_status_keeper_sup}
             started: [{pid,<0.512.0>},
                       {id,service_status_keeper_eventing},
                       {mfargs,{service_eventing,start_keeper,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:37:42.370Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,services_stats_sup}
             started: [{pid,<0.503.0>},
                       {id,service_status_keeper_sup},
                       {mfargs,{service_status_keeper_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-03-27T20:37:42.370Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,services_stats_sup}
             started: [{pid,<0.515.0>},
                       {id,service_stats_worker},
                       {mfargs,
                           {erlang,apply,
                               [#Fun<services_stats_sup.0.108537742>,[]]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:37:42.370Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.501.0>},
                       {name,services_stats_sup},
                       {mfargs,{services_stats_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[ns_server:debug,2020-03-27T20:37:42.374Z,ns_1@127.0.0.1:memcached_config_mgr<0.464.0>:memcached_config_mgr:init:86]activated memcached port server
[ns_server:debug,2020-03-27T20:37:42.397Z,ns_1@127.0.0.1:<0.519.0>:new_concurrency_throttle:init:115]init concurrent throttle process, pid: <0.519.0>, type: kv_throttle# of available token: 1
[ns_server:debug,2020-03-27T20:37:42.402Z,ns_1@127.0.0.1:compaction_daemon<0.517.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2020-03-27T20:37:42.403Z,ns_1@127.0.0.1:compaction_daemon<0.517.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[error_logger:info,2020-03-27T20:37:42.403Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.517.0>},
                       {name,compaction_daemon},
                       {mfargs,{compaction_daemon,start_link,[]}},
                       {restart_type,{permanent,4}},
                       {shutdown,86400000},
                       {child_type,worker}]

[ns_server:debug,2020-03-27T20:37:42.403Z,ns_1@127.0.0.1:compaction_daemon<0.517.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2020-03-27T20:37:42.403Z,ns_1@127.0.0.1:compaction_daemon<0.517.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2020-03-27T20:37:42.405Z,ns_1@127.0.0.1:compaction_daemon<0.517.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_master. Rescheduling compaction.
[ns_server:debug,2020-03-27T20:37:42.405Z,ns_1@127.0.0.1:compaction_daemon<0.517.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_master too soon. Next run will be in 3600s
[error_logger:info,2020-03-27T20:37:42.406Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,cluster_logs_sup}
             started: [{pid,<0.521.0>},
                       {id,ets_holder},
                       {mfargs,
                           {cluster_logs_collection_task,
                               start_link_ets_holder,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:37:42.406Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.520.0>},
                       {name,cluster_logs_sup},
                       {mfargs,{cluster_logs_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-03-27T20:37:42.406Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.522.0>},
                       {name,leader_events},
                       {mfargs,{gen_event,start_link,[{local,leader_events}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:37:42.441Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,leader_leases_sup}
             started: [{pid,<0.526.0>},
                       {id,leader_activities},
                       {mfargs,{leader_activities,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,10000},
                       {child_type,worker}]

[ns_server:warn,2020-03-27T20:37:42.454Z,ns_1@127.0.0.1:leader_lease_agent<0.527.0>:leader_lease_agent:maybe_recover_persisted_lease:399]Found persisted lease [{node,'ns_1@127.0.0.1'},
                       {uuid,<<"d28c599b6a432dc5d7503c01390563df">>},
                       {time_left,15000},
                       {status,active}]
[error_logger:info,2020-03-27T20:37:42.454Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,leader_leases_sup}
             started: [{pid,<0.527.0>},
                       {id,leader_lease_agent},
                       {mfargs,{leader_lease_agent,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:37:42.455Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,leader_services_sup}
             started: [{pid,<0.525.0>},
                       {id,leader_leases_sup},
                       {mfargs,{leader_leases_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-03-27T20:37:42.465Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,leader_registry_sup}
             started: [{pid,<0.529.0>},
                       {id,leader_registry_server},
                       {mfargs,{leader_registry_server,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-03-27T20:37:42.468Z,ns_1@127.0.0.1:leader_registry_sup<0.528.0>:mb_master:check_master_takeover_needed:283]Sending master node question to the following nodes: []
[ns_server:debug,2020-03-27T20:37:42.468Z,ns_1@127.0.0.1:leader_registry_sup<0.528.0>:mb_master:check_master_takeover_needed:285]Got replies: []
[ns_server:debug,2020-03-27T20:37:42.468Z,ns_1@127.0.0.1:leader_registry_sup<0.528.0>:mb_master:check_master_takeover_needed:291]Was unable to discover master, not going to force mastership takeover
[user:info,2020-03-27T20:37:42.471Z,ns_1@127.0.0.1:mb_master<0.532.0>:mb_master:init:103]I'm the only node, so I'm the master.
[ns_server:debug,2020-03-27T20:37:42.471Z,ns_1@127.0.0.1:leader_registry<0.529.0>:leader_registry_server:handle_new_leader:241]New leader is 'ns_1@127.0.0.1'. Invalidating name cache.
[ns_server:debug,2020-03-27T20:37:42.481Z,ns_1@127.0.0.1:mb_master<0.532.0>:master_activity_events:submit_cast:82]Failed to send master activity event: {error,badarg}
[error_logger:info,2020-03-27T20:37:42.483Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,mb_master_sup}
             started: [{pid,<0.535.0>},
                       {id,leader_lease_acquirer},
                       {mfargs,{leader_lease_acquirer,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,10000},
                       {child_type,worker}]

[ns_server:debug,2020-03-27T20:37:42.485Z,ns_1@127.0.0.1:leader_quorum_nodes_manager<0.537.0>:leader_quorum_nodes_manager:pull_config:114]Attempting to pull config from nodes:
[]
[error_logger:info,2020-03-27T20:37:42.485Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,mb_master_sup}
             started: [{pid,<0.537.0>},
                       {id,leader_quorum_nodes_manager},
                       {mfargs,{leader_quorum_nodes_manager,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-03-27T20:37:42.486Z,ns_1@127.0.0.1:leader_quorum_nodes_manager<0.537.0>:leader_quorum_nodes_manager:pull_config:119]Pulled config successfully.
[ns_server:warn,2020-03-27T20:37:42.491Z,ns_1@127.0.0.1:<0.541.0>:leader_lease_acquire_worker:handle_lease_already_acquired:232]Failed to acquire lease from 'ns_1@127.0.0.1' because its already taken by {'ns_1@127.0.0.1',
                                                                            <<"d28c599b6a432dc5d7503c01390563df">>} (valid for 14963ms)
[ns_server:info,2020-03-27T20:37:42.493Z,ns_1@127.0.0.1:mb_master_sup<0.534.0>:misc:start_singleton:857]start_singleton(gen_server, start_link, [{via,leader_registry,ns_tick},
                                         ns_tick,[],[]]): started as <0.544.0> on 'ns_1@127.0.0.1'

[error_logger:info,2020-03-27T20:37:42.493Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,mb_master_sup}
             started: [{pid,<0.544.0>},
                       {id,ns_tick},
                       {mfargs,{ns_tick,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,10},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:37:42.495Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_orchestrator_sup}
             started: [{pid,<0.546.0>},
                       {id,compat_mode_events},
                       {mfargs,
                           {gen_event,start_link,
                               [{local,compat_mode_events}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:37:42.497Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_orchestrator_sup}
             started: [{pid,<0.547.0>},
                       {id,compat_mode_manager},
                       {mfargs,{compat_mode_manager,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:37:42.501Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_orchestrator_child_sup}
             started: [{pid,<0.549.0>},
                       {id,ns_janitor_server},
                       {mfargs,{ns_janitor_server,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:info,2020-03-27T20:37:42.503Z,ns_1@127.0.0.1:ns_orchestrator_child_sup<0.548.0>:misc:start_singleton:857]start_singleton(gen_server, start_link, [{via,leader_registry,
                                          auto_reprovision},
                                         auto_reprovision,[],[]]): started as <0.550.0> on 'ns_1@127.0.0.1'

[error_logger:info,2020-03-27T20:37:42.503Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_orchestrator_child_sup}
             started: [{pid,<0.550.0>},
                       {id,auto_reprovision},
                       {mfargs,{auto_reprovision,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:info,2020-03-27T20:37:42.505Z,ns_1@127.0.0.1:ns_orchestrator_child_sup<0.548.0>:misc:start_singleton:857]start_singleton(gen_server, start_link, [{via,leader_registry,auto_rebalance},
                                         auto_rebalance,[],[]]): started as <0.551.0> on 'ns_1@127.0.0.1'

[error_logger:info,2020-03-27T20:37:42.505Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_orchestrator_child_sup}
             started: [{pid,<0.551.0>},
                       {id,auto_rebalance},
                       {mfargs,{auto_rebalance,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:info,2020-03-27T20:37:42.505Z,ns_1@127.0.0.1:ns_orchestrator_child_sup<0.548.0>:misc:start_singleton:857]start_singleton(gen_statem, start_link, [{via,leader_registry,ns_orchestrator},
                                         ns_orchestrator,[],[]]): started as <0.552.0> on 'ns_1@127.0.0.1'

[error_logger:info,2020-03-27T20:37:42.505Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_orchestrator_child_sup}
             started: [{pid,<0.552.0>},
                       {id,ns_orchestrator},
                       {mfargs,{ns_orchestrator,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:37:42.505Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_orchestrator_sup}
             started: [{pid,<0.548.0>},
                       {id,ns_orchestrator_child_sup},
                       {mfargs,{ns_orchestrator_child_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[ns_server:debug,2020-03-27T20:37:42.508Z,ns_1@127.0.0.1:<0.554.0>:auto_failover:init:185]init auto_failover.
[user:info,2020-03-27T20:37:42.508Z,ns_1@127.0.0.1:<0.554.0>:auto_failover:handle_call:216]Enabled auto-failover with timeout 120 and max count 1
[ns_server:debug,2020-03-27T20:37:42.512Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{local_changes_count,<<"60a6bc3db77e6c7b91c556140dcfec71">>} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{37,63752560662}}]}]
[ns_server:debug,2020-03-27T20:37:42.512Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
auto_failover_cfg ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557537}}]},
 {enabled,true},
 {timeout,120},
 {count,0},
 {failover_on_data_disk_issues,[{enabled,false},{timePeriod,120}]},
 {failover_server_group,false},
 {max_count,1},
 {failed_over_server_groups,[]},
 {can_abort_rebalance,true}]
[ns_server:debug,2020-03-27T20:37:42.512Z,ns_1@127.0.0.1:ns_config_rep<0.319.0>:ns_config_rep:do_push_keys:321]Replicating some config keys ([auto_failover_cfg,
                               {local_changes_count,
                                   <<"60a6bc3db77e6c7b91c556140dcfec71">>}]..)
[ns_server:info,2020-03-27T20:37:42.512Z,ns_1@127.0.0.1:ns_orchestrator_sup<0.545.0>:misc:start_singleton:857]start_singleton(gen_server, start_link, [{via,leader_registry,auto_failover},
                                         auto_failover,[],[]]): started as <0.554.0> on 'ns_1@127.0.0.1'

[error_logger:info,2020-03-27T20:37:42.512Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_orchestrator_sup}
             started: [{pid,<0.554.0>},
                       {id,auto_failover},
                       {mfargs,{auto_failover,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:37:42.512Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,mb_master_sup}
             started: [{pid,<0.545.0>},
                       {id,ns_orchestrator_sup},
                       {mfargs,{ns_orchestrator_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[ns_server:info,2020-03-27T20:37:42.513Z,ns_1@127.0.0.1:mb_master_sup<0.534.0>:misc:start_singleton:857]start_singleton(work_queue, start_link, [{via,leader_registry,collections}]): started as <0.560.0> on 'ns_1@127.0.0.1'

[error_logger:info,2020-03-27T20:37:42.514Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,mb_master_sup}
             started: [{pid,<0.560.0>},
                       {id,collections},
                       {mfargs,{collections,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-03-27T20:37:42.518Z,ns_1@127.0.0.1:<0.562.0>:license_reporting:init:66]Starting license_reporting server
[ns_server:info,2020-03-27T20:37:42.518Z,ns_1@127.0.0.1:mb_master_sup<0.534.0>:misc:start_singleton:857]start_singleton(gen_server, start_link, [{via,leader_registry,
                                          license_reporting},
                                         license_reporting,[],[]]): started as <0.562.0> on 'ns_1@127.0.0.1'

[error_logger:info,2020-03-27T20:37:42.518Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,mb_master_sup}
             started: [{pid,<0.562.0>},
                       {id,license_reporting},
                       {mfargs,{license_reporting,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:37:42.518Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,leader_registry_sup}
             started: [{pid,<0.532.0>},
                       {id,mb_master},
                       {mfargs,{mb_master,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[ns_server:debug,2020-03-27T20:37:42.519Z,ns_1@127.0.0.1:<0.523.0>:restartable:start_child:98]Started child process <0.524.0>
  MFA: {leader_services_sup,start_link,[]}
[error_logger:info,2020-03-27T20:37:42.519Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,leader_services_sup}
             started: [{pid,<0.528.0>},
                       {id,leader_registry_sup},
                       {mfargs,{leader_registry_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-03-27T20:37:42.519Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.523.0>},
                       {name,leader_services_sup},
                       {mfargs,
                           {restartable,start_link,
                               [{leader_services_sup,start_link,[]},
                                infinity]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-03-27T20:37:42.521Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.564.0>},
                       {name,ns_tick_agent},
                       {mfargs,{ns_tick_agent,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:37:42.521Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.566.0>},
                       {name,master_activity_events_ingress},
                       {mfargs,
                           {gen_event,start_link,
                               [{local,master_activity_events_ingress}]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:37:42.522Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.567.0>},
                       {name,master_activity_events_timestamper},
                       {mfargs,
                           {master_activity_events,start_link_timestamper,[]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:37:42.525Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.568.0>},
                       {name,master_activity_events_pids_watcher},
                       {mfargs,
                           {master_activity_events_pids_watcher,start_link,
                               []}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:37:42.534Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.569.0>},
                       {name,master_activity_events_keeper},
                       {mfargs,{master_activity_events_keeper,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:37:42.554Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,health_monitor_sup}
             started: [{pid,<0.572.0>},
                       {id,ns_server_monitor},
                       {mfargs,{ns_server_monitor,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:37:42.554Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,health_monitor_sup}
             started: [{pid,<0.574.0>},
                       {id,service_monitor_children_sup},
                       {mfargs,
                           {supervisor,start_link,
                               [{local,service_monitor_children_sup},
                                health_monitor_sup,child]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-03-27T20:37:42.555Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,health_monitor_sup}
             started: [{pid,<0.575.0>},
                       {id,service_monitor_worker},
                       {mfargs,
                           {erlang,apply,
                               [#Fun<health_monitor_sup.0.112499759>,[]]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:37:42.562Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,health_monitor_sup}
             started: [{pid,<0.581.0>},
                       {id,node_monitor},
                       {mfargs,{node_monitor,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:37:42.564Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,health_monitor_sup}
             started: [{pid,<0.587.0>},
                       {id,node_status_analyzer},
                       {mfargs,{node_status_analyzer,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:37:42.564Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.571.0>},
                       {name,health_monitor_sup},
                       {mfargs,{health_monitor_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-03-27T20:37:42.566Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.589.0>},
                       {name,rebalance_agent},
                       {mfargs,{rebalance_agent,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:37:42.598Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.590.0>},
                       {name,ns_rebalance_report_manager},
                       {mfargs,{ns_rebalance_report_manager,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-03-27T20:37:42.598Z,ns_1@127.0.0.1:ns_server_nodes_sup<0.209.0>:one_shot_barrier:notify:27]Notifying on barrier menelaus_barrier
[ns_server:debug,2020-03-27T20:37:42.598Z,ns_1@127.0.0.1:menelaus_barrier<0.211.0>:one_shot_barrier:barrier_body:62]Barrier menelaus_barrier got notification from <0.209.0>
[ns_server:debug,2020-03-27T20:37:42.598Z,ns_1@127.0.0.1:ns_server_nodes_sup<0.209.0>:one_shot_barrier:notify:32]Successfuly notified on barrier menelaus_barrier
[error_logger:info,2020-03-27T20:37:42.598Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.298.0>},
                       {name,ns_server_sup},
                       {mfargs,{ns_server_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[ns_server:debug,2020-03-27T20:37:42.598Z,ns_1@127.0.0.1:<0.208.0>:restartable:start_child:98]Started child process <0.209.0>
  MFA: {ns_server_nodes_sup,start_link,[]}
[error_logger:info,2020-03-27T20:37:42.599Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.208.0>},
                       {id,ns_server_nodes_sup},
                       {mfargs,
                           {restartable,start_link,
                               [{ns_server_nodes_sup,start_link,[]},
                                infinity]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-03-27T20:37:42.602Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.592.0>},
                       {id,remote_api},
                       {mfargs,{remote_api,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:37:42.602Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,root_sup}
             started: [{pid,<0.187.0>},
                       {id,ns_server_cluster_sup},
                       {mfargs,{ns_server_cluster_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-03-27T20:37:42.603Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
         application: ns_server
          started_at: 'ns_1@127.0.0.1'

[ns_server:debug,2020-03-27T20:37:42.603Z,ns_1@127.0.0.1:<0.5.0>:child_erlang:child_loop:130]141: Entered child_loop
[ns_server:debug,2020-03-27T20:37:42.604Z,ns_1@127.0.0.1:compiled_roles_cache<0.264.0>:menelaus_roles:build_compiled_roles:753]Compile roles for user {"@",admin}
[ns_server:debug,2020-03-27T20:37:42.625Z,ns_1@127.0.0.1:json_rpc_connection-saslauthd-saslauthd-port<0.593.0>:json_rpc_connection:init:73]Observed revrpc connection: label "saslauthd-saslauthd-port", handling process <0.593.0>
[ns_server:debug,2020-03-27T20:37:42.625Z,ns_1@127.0.0.1:json_rpc_connection-goxdcr-cbauth<0.594.0>:json_rpc_connection:init:73]Observed revrpc connection: label "goxdcr-cbauth", handling process <0.594.0>
[ns_server:debug,2020-03-27T20:37:42.625Z,ns_1@127.0.0.1:menelaus_cbauth<0.442.0>:menelaus_cbauth:handle_cast:107]Observed json rpc process {"goxdcr-cbauth",<0.594.0>} started
[ns_server:debug,2020-03-27T20:37:42.637Z,ns_1@127.0.0.1:compiled_roles_cache<0.264.0>:menelaus_roles:build_compiled_roles:753]Compile roles for user {"@goxdcr-cbauth",admin}
[ns_server:debug,2020-03-27T20:37:42.888Z,ns_1@127.0.0.1:memcached_refresh<0.213.0>:memcached_refresh:handle_info:89]Refresh of [rbac,isasl] succeeded
[ns_server:debug,2020-03-27T20:37:43.510Z,ns_1@127.0.0.1:<0.554.0>:auto_failover_logic:log_master_activity:177]Transitioned node {'ns_1@127.0.0.1',<<"60a6bc3db77e6c7b91c556140dcfec71">>} state new -> up
[ns_server:debug,2020-03-27T20:37:57.455Z,ns_1@127.0.0.1:leader_lease_agent<0.527.0>:leader_lease_agent:handle_lease_expired:286]Lease held by {lease_holder,<<"d28c599b6a432dc5d7503c01390563df">>,
                            'ns_1@127.0.0.1'} expired. Starting expirer.
[ns_server:debug,2020-03-27T20:37:57.458Z,ns_1@127.0.0.1:leader_lease_agent<0.527.0>:leader_lease_agent:do_handle_acquire_lease:149]Granting lease to {lease_holder,<<"8df7bb151e3a946ee0f14918fefccbe4">>,
                                'ns_1@127.0.0.1'} for 15000ms
[ns_server:info,2020-03-27T20:37:57.481Z,ns_1@127.0.0.1:<0.541.0>:leader_lease_acquire_worker:handle_fresh_lease_acquired:302]Acquired lease from node 'ns_1@127.0.0.1' (lease uuid: <<"8df7bb151e3a946ee0f14918fefccbe4">>)
[ns_server:debug,2020-03-27T20:38:08.761Z,ns_1@127.0.0.1:compiled_roles_cache<0.264.0>:menelaus_roles:build_compiled_roles:753]Compile roles for user {[],wrong_token}
[ns_server:debug,2020-03-27T20:38:12.403Z,ns_1@127.0.0.1:compaction_daemon<0.517.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2020-03-27T20:38:12.404Z,ns_1@127.0.0.1:compaction_daemon<0.517.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2020-03-27T20:38:12.405Z,ns_1@127.0.0.1:compaction_daemon<0.517.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2020-03-27T20:38:12.406Z,ns_1@127.0.0.1:compaction_daemon<0.517.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:info,2020-03-27T20:38:37.388Z,ns_1@127.0.0.1:netconfig_updater<0.204.0>:netconfig_updater:apply_config_unprotected:158]Node is going to apply the following settings: [{externalListeners,
                                                 [{inet,false},
                                                  {inet6,false}]}]
[ns_server:debug,2020-03-27T20:38:37.403Z,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Updated cb_dist config "/opt/couchbase/var/lib/couchbase/config/dist_cfg": [{external_listeners,
                                                                                      [inet_tcp_dist,
                                                                                       inet6_tcp_dist]},
                                                                                     {preferred_external_proto,
                                                                                      inet_tcp_dist},
                                                                                     {preferred_local_proto,
                                                                                      inet_tcp_dist}]
[ns_server:debug,2020-03-27T20:38:37.406Z,ns_1@127.0.0.1:cb_dist<0.178.0>:cb_dist:info_msg:754]cb_dist: Reloading configuration: [{external_listeners,
                                       [inet_tcp_dist,inet6_tcp_dist]},
                                   {preferred_external_proto,inet_tcp_dist},
                                   {preferred_local_proto,inet_tcp_dist}]
[ns_server:debug,2020-03-27T20:38:37.407Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{local_changes_count,<<"60a6bc3db77e6c7b91c556140dcfec71">>} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{38,63752560717}}]}]
[ns_server:debug,2020-03-27T20:38:37.407Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',address_family} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|inet]
[ns_server:debug,2020-03-27T20:38:37.408Z,ns_1@127.0.0.1:ns_config_rep<0.319.0>:ns_config_rep:do_push_keys:321]Replicating some config keys ([{local_changes_count,
                                   <<"60a6bc3db77e6c7b91c556140dcfec71">>},
                               {node,'ns_1@127.0.0.1',address_family}]..)
[ns_server:debug,2020-03-27T20:38:37.408Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{local_changes_count,<<"60a6bc3db77e6c7b91c556140dcfec71">>} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{39,63752560717}}]}]
[ns_server:debug,2020-03-27T20:38:37.408Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',node_encryption} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]}|false]
[ns_server:info,2020-03-27T20:38:37.409Z,ns_1@127.0.0.1:netconfig_updater<0.204.0>:netconfig_updater:apply_config_unprotected:187]Node network settings ([{externalListeners,[{inet,false},{inet6,false}]}]) successfully applied
[ns_server:debug,2020-03-27T20:38:37.409Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{local_changes_count,<<"60a6bc3db77e6c7b91c556140dcfec71">>} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{40,63752560717}}]}]
[ns_server:debug,2020-03-27T20:38:37.409Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',erl_external_listeners} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752557625}}]},
 {inet,false},
 {inet6,false}]
[ns_server:debug,2020-03-27T20:38:37.411Z,ns_1@127.0.0.1:ns_config_rep<0.319.0>:ns_config_rep:do_push_keys:321]Replicating some config keys ([{local_changes_count,
                                   <<"60a6bc3db77e6c7b91c556140dcfec71">>},
                               {node,'ns_1@127.0.0.1',erl_external_listeners},
                               {node,'ns_1@127.0.0.1',node_encryption}]..)
[cluster:info,2020-03-27T20:38:37.474Z,ns_1@127.0.0.1:ns_cluster<0.191.0>:ns_cluster:handle_call:355]Changing address to "127.0.0.1" due to client request
[cluster:info,2020-03-27T20:38:37.474Z,ns_1@127.0.0.1:ns_cluster<0.191.0>:ns_cluster:do_change_address:596]Change of address to "127.0.0.1" is requested.
[cluster:debug,2020-03-27T20:38:37.474Z,ns_1@127.0.0.1:<0.1728.0>:ns_cluster:maybe_rename:626]Not renaming node.
[ns_server:debug,2020-03-27T20:38:37.488Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{metakv,<<"/indexing/settings/config">>} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{9,63752560717}}]}|
 <<"{\"indexer.settings.compaction.days_of_week\":\"Sunday,Monday,Tuesday,Wednesday,Thursday,Friday,Saturday\",\"indexer.settings.compaction.interval\":\"00:00,00:00\",\"indexer.settings.persisted_snapshot.interval\":5000,\"indexer.settings.log_level\":\"info\",\"indexer.settings.compaction.compaction_mode\":\"circular\",\"indexer.settings.compaction.min_frag\":30,\"indexer.settings.inmemory_snapshot.interval\""...>>]
[ns_server:debug,2020-03-27T20:38:37.488Z,ns_1@127.0.0.1:ns_config_rep<0.319.0>:ns_config_rep:do_push_keys:321]Replicating some config keys ([{local_changes_count,
                                   <<"60a6bc3db77e6c7b91c556140dcfec71">>},
                               {metakv,<<"/indexing/settings/config">>}]..)
[ns_server:debug,2020-03-27T20:38:37.488Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{local_changes_count,<<"60a6bc3db77e6c7b91c556140dcfec71">>} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{41,63752560717}}]}]
[ns_server:debug,2020-03-27T20:38:37.488Z,ns_1@127.0.0.1:ns_audit<0.463.0>:ns_audit:handle_call:125]Audit modify_index_storage_mode: [{storageMode,<<"plasma">>},
                                  {real_userid,
                                      {[{domain,wrong_token},
                                        {user,<<"<ud></ud>">>}]}},
                                  {sessionid,
                                      <<"3c9673d43ade3c5c4478657f38eb6001">>},
                                  {remote,
                                      {[{ip,<<"192.168.80.1">>},
                                        {port,50534}]}},
                                  {timestamp,<<"2020-03-27T20:38:37.488Z">>}]
[ns_server:debug,2020-03-27T20:38:37.502Z,ns_1@127.0.0.1:ns_config_rep<0.319.0>:ns_config_rep:do_push_keys:321]Replicating some config keys ([cluster_name,
                               {local_changes_count,
                                   <<"60a6bc3db77e6c7b91c556140dcfec71">>},
                               {metakv,<<"/indexing/settings/config">>}]..)
[ns_server:debug,2020-03-27T20:38:37.502Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{metakv,<<"/indexing/settings/config">>} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{10,63752560717}}]}|
 <<"{\"indexer.settings.compaction.days_of_week\":\"Sunday,Monday,Tuesday,Wednesday,Thursday,Friday,Saturday\",\"indexer.settings.compaction.interval\":\"00:00,00:00\",\"indexer.settings.compaction.compaction_mode\":\"circular\",\"indexer.settings.log_level\":\"info\",\"indexer.settings.persisted_snapshot.interval\":5000,\"indexer.settings.compaction.min_frag\":30,\"indexer.settings.inmemory_snapshot.interval\""...>>]
[ns_server:debug,2020-03-27T20:38:37.503Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{local_changes_count,<<"60a6bc3db77e6c7b91c556140dcfec71">>} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{42,63752560717}}]}]
[ns_server:debug,2020-03-27T20:38:37.504Z,ns_1@127.0.0.1:ns_audit<0.463.0>:ns_audit:handle_call:125]Audit cluster_settings: [{cluster_name,<<"BaseCluster">>},
                         {quotas,{[{kv,292},
                                   {index,512},
                                   {fts,256},
                                   {cbas,1024},
                                   {eventing,256}]}},
                         {real_userid,{[{domain,wrong_token},
                                        {user,<<"<ud></ud>">>}]}},
                         {sessionid,<<"3c9673d43ade3c5c4478657f38eb6001">>},
                         {remote,{[{ip,<<"192.168.80.1">>},{port,50534}]}},
                         {timestamp,<<"2020-03-27T20:38:37.502Z">>}]
[ns_server:debug,2020-03-27T20:38:37.504Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{local_changes_count,<<"60a6bc3db77e6c7b91c556140dcfec71">>} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{43,63752560717}}]}]
[ns_server:debug,2020-03-27T20:38:37.504Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
cluster_name ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{4,63752560717}}]},
 66,97,115,101,67,108,117,115,116,101,114]
[ns_server:debug,2020-03-27T20:38:37.520Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{local_changes_count,<<"60a6bc3db77e6c7b91c556140dcfec71">>} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{44,63752560717}}]}]
[ns_server:debug,2020-03-27T20:38:37.520Z,ns_1@127.0.0.1:ns_audit<0.463.0>:ns_audit:handle_call:125]Audit setup_node_services: [{services,[eventing,fts,index,kv,n1ql]},
                            {node,'ns_1@127.0.0.1'},
                            {real_userid,
                                {[{domain,wrong_token},
                                  {user,<<"<ud></ud>">>}]}},
                            {sessionid,<<"3c9673d43ade3c5c4478657f38eb6001">>},
                            {remote,{[{ip,<<"192.168.80.1">>},{port,50534}]}},
                            {timestamp,<<"2020-03-27T20:38:37.519Z">>}]
[ns_server:debug,2020-03-27T20:38:37.521Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',services} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752560717}}]},
 eventing,fts,index,kv,n1ql]
[ns_server:debug,2020-03-27T20:38:37.523Z,ns_1@127.0.0.1:ns_config_rep<0.319.0>:ns_config_rep:do_push_keys:321]Replicating some config keys ([{local_changes_count,
                                   <<"60a6bc3db77e6c7b91c556140dcfec71">>},
                               {node,'ns_1@127.0.0.1',services}]..)
[ns_server:debug,2020-03-27T20:38:37.524Z,ns_1@127.0.0.1:terse_cluster_info_uploader<0.467.0>:terse_cluster_info_uploader:handle_info:48]Refreshing terse cluster info with <<"{\"rev\":44,\"nodesExt\":[{\"services\":{\"mgmt\":8091,\"mgmtSSL\":18091,\"kv\":11210,\"kvSSL\":11207,\"capi\":8092,\"capiSSL\":18092,\"projector\":9999,\"projector\":9999},\"thisNode\":true}],\"clusterCapabilitiesVer\":[1,0],\"clusterCapabilities\":{\"n1ql\":[\"enhancedPreparedStatements\"]}}">>
[ns_server:debug,2020-03-27T20:38:37.530Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{local_changes_count,<<"60a6bc3db77e6c7b91c556140dcfec71">>} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{45,63752560717}}]}]
[ns_server:debug,2020-03-27T20:38:37.530Z,ns_1@127.0.0.1:ns_config_rep<0.319.0>:ns_config_rep:do_push_keys:321]Replicating some config keys ([settings,
                               {local_changes_count,
                                   <<"60a6bc3db77e6c7b91c556140dcfec71">>}]..)
[ns_server:debug,2020-03-27T20:38:37.530Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
settings ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752560717}}]},
 {stats,[{send_stats,false}]}]
[ns_server:debug,2020-03-27T20:38:37.553Z,ns_1@127.0.0.1:ns_config_rep<0.319.0>:ns_config_rep:do_push_keys:321]Replicating some config keys ([rest,
                               {local_changes_count,
                                   <<"60a6bc3db77e6c7b91c556140dcfec71">>}]..)
[ns_server:debug,2020-03-27T20:38:37.555Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{local_changes_count,<<"60a6bc3db77e6c7b91c556140dcfec71">>} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{46,63752560717}}]}]
[ns_server:debug,2020-03-27T20:38:37.556Z,ns_1@127.0.0.1:terse_cluster_info_uploader<0.467.0>:terse_cluster_info_uploader:handle_info:48]Refreshing terse cluster info with <<"{\"rev\":46,\"nodesExt\":[{\"services\":{\"mgmt\":8091,\"mgmtSSL\":18091,\"kv\":11210,\"kvSSL\":11207,\"capi\":8092,\"capiSSL\":18092,\"projector\":9999,\"projector\":9999},\"thisNode\":true}],\"clusterCapabilitiesVer\":[1,0],\"clusterCapabilities\":{\"n1ql\":[\"enhancedPreparedStatements\"]}}">>
[ns_server:debug,2020-03-27T20:38:37.555Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
rest ->
[{port,8091}]
[ns_server:debug,2020-03-27T20:38:37.598Z,ns_1@127.0.0.1:compiled_roles_cache<0.264.0>:versioned_cache:handle_info:92]Flushing cache compiled_roles_cache due to version change from {[6,5],
                                                                {0,1940836693},
                                                                {0,1940836693},
                                                                false,[]} to {[6,
                                                                               5],
                                                                              {0,
                                                                               1940836693},
                                                                              {0,
                                                                               1940836693},
                                                                              true,
                                                                              []}
[ns_server:debug,2020-03-27T20:38:37.599Z,ns_1@127.0.0.1:menelaus_ui_auth<0.375.0>:token_server:handle_cast:211]Purge tokens []
[ns_server:debug,2020-03-27T20:38:37.599Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{local_changes_count,<<"60a6bc3db77e6c7b91c556140dcfec71">>} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{47,63752560717}}]}]
[ns_server:debug,2020-03-27T20:38:37.599Z,ns_1@127.0.0.1:ns_config_rep<0.319.0>:ns_config_rep:do_push_keys:321]Replicating some config keys ([rest_creds,uuid,
                               {local_changes_count,
                                   <<"60a6bc3db77e6c7b91c556140dcfec71">>}]..)
[ns_server:debug,2020-03-27T20:38:37.601Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
rest_creds ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752560717}}]}|
 {"<ud>Administrator</ud>",
  {auth,
   [{<<"plain">>,"*****"},
    {<<"sha512">>,
     {[{<<"h">>,"*****"},
       {<<"s">>,
        <<"1Ns/HY4LeSakjvWtbK7AhYYP6zuXrbaUxAF35y0Qo/kZ5LRmqHJSrIbq6ALQGQbYLtFYd2K2zKMFgrFnNnufGg==">>},
       {<<"i">>,4000}]}},
    {<<"sha256">>,
     {[{<<"h">>,"*****"},
       {<<"s">>,<<"28McswHI1CxVI9ltWcEYpkvxHY1HC7J14s2y65Si3no=">>},
       {<<"i">>,4000}]}},
    {<<"sha1">>,
     {[{<<"h">>,"*****"},
       {<<"s">>,<<"uvlNQH9tKlygQuw7ORHc4PfOKV8=">>},
       {<<"i">>,4000}]}}]}}]
[ns_server:debug,2020-03-27T20:38:37.601Z,ns_1@127.0.0.1:memcached_permissions<0.308.0>:memcached_cfg:write_cfg:118]Writing config file for: "/opt/couchbase/var/lib/couchbase/config/memcached.rbac"
[ns_server:debug,2020-03-27T20:38:37.602Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{local_changes_count,<<"60a6bc3db77e6c7b91c556140dcfec71">>} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{48,63752560717}}]}]
[ns_server:debug,2020-03-27T20:38:37.602Z,ns_1@127.0.0.1:memcached_passwords<0.305.0>:memcached_cfg:write_cfg:118]Writing config file for: "/opt/couchbase/var/lib/couchbase/isasl.pw"
[ns_server:debug,2020-03-27T20:38:37.602Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
uuid ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752560717}}]}|
 <<"c193b2d527e53c5288042d8242332c70">>]
[ns_server:debug,2020-03-27T20:38:37.601Z,ns_1@127.0.0.1:ns_audit<0.463.0>:ns_audit:handle_call:125]Audit password_change: [{identity,{[{domain,builtin},
                                    {user,<<"<ud>Administrator</ud>">>}]}},
                        {real_userid,{[{domain,wrong_token},
                                       {user,<<"<ud></ud>">>}]}},
                        {sessionid,<<"3c9673d43ade3c5c4478657f38eb6001">>},
                        {remote,{[{ip,<<"192.168.80.1">>},{port,50534}]}},
                        {timestamp,<<"2020-03-27T20:38:37.599Z">>}]
[ns_server:debug,2020-03-27T20:38:37.619Z,ns_1@127.0.0.1:users_storage<0.262.0>:replicated_dets:handle_call:302]Suspended by process <0.308.0>
[ns_server:debug,2020-03-27T20:38:37.620Z,ns_1@127.0.0.1:compiled_roles_cache<0.264.0>:menelaus_roles:build_compiled_roles:753]Compile roles for user {"<ud>Administrator</ud>",admin}
[error_logger:info,2020-03-27T20:38:37.620Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,service_agent_children_sup}
             started: [{pid,<0.1784.0>},
                       {id,{service_agent,eventing}},
                       {mfargs,{service_agent,start_link,[eventing]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-03-27T20:38:37.620Z,ns_1@127.0.0.1:memcached_permissions<0.308.0>:replicated_dets:select_from_dets_locked:350]Starting select with {users_storage,[{{docv2,{user,{'_',local}},'_','_'},
                                      [],
                                      ['$_']}],
                                    100}
[ns_server:debug,2020-03-27T20:38:37.622Z,ns_1@127.0.0.1:users_storage<0.262.0>:replicated_dets:handle_call:309]Released by process <0.308.0>
[ns_server:debug,2020-03-27T20:38:37.623Z,ns_1@127.0.0.1:ns_audit<0.463.0>:ns_audit:handle_call:125]Audit login_success: [{roles,[<<"admin">>]},
                      {real_userid,{[{domain,builtin},
                                     {user,<<"<ud>Administrator</ud>">>}]}},
                      {sessionid,<<"a5fb028c6de4e971bdb5c6cbb45bd505">>},
                      {remote,{[{ip,<<"192.168.80.1">>},{port,50542}]}},
                      {timestamp,<<"2020-03-27T20:38:37.623Z">>}]
[ns_server:debug,2020-03-27T20:38:37.626Z,ns_1@127.0.0.1:menelaus_cbauth<0.442.0>:menelaus_cbauth:handle_cast:107]Observed json rpc process {"goxdcr-cbauth",<0.594.0>} needs_update
[error_logger:info,2020-03-27T20:38:37.627Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,service_agent_children_sup}
             started: [{pid,<0.1788.0>},
                       {id,{service_agent,fts}},
                       {mfargs,{service_agent,start_link,[fts]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:38:37.627Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,service_agent_children_sup}
             started: [{pid,<0.1797.0>},
                       {id,{service_agent,index}},
                       {mfargs,{service_agent,start_link,[index]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-03-27T20:38:37.628Z,ns_1@127.0.0.1:service_stats_collector-eventing<0.1794.0>:service_stats_collector:check_status:329]Checking if service service_eventing is started...
[error_logger:info,2020-03-27T20:38:37.628Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,service_stats_children_sup}
             started: [{pid,<0.1794.0>},
                       {id,{service_eventing,service_stats_collector}},
                       {mfargs,
                           {service_stats_collector,start_link,
                               [service_eventing]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-03-27T20:38:37.628Z,ns_1@127.0.0.1:service_stats_collector-fts<0.1801.0>:service_stats_collector:check_status:329]Checking if service service_fts is started...
[error_logger:info,2020-03-27T20:38:37.628Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,service_stats_children_sup}
             started: [{pid,<0.1801.0>},
                       {id,{service_fts,service_stats_collector}},
                       {mfargs,
                           {service_stats_collector,start_link,[service_fts]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-03-27T20:38:37.628Z,ns_1@127.0.0.1:service_stats_collector-index<0.1804.0>:service_stats_collector:check_status:329]Checking if service service_index is started...
[error_logger:info,2020-03-27T20:38:37.628Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,service_stats_children_sup}
             started: [{pid,<0.1804.0>},
                       {id,{service_index,service_stats_collector}},
                       {mfargs,
                           {service_stats_collector,start_link,
                               [service_index]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-03-27T20:38:37.629Z,ns_1@127.0.0.1:menelaus_cbauth<0.442.0>:menelaus_cbauth:handle_cast:107]Observed json rpc process {"goxdcr-cbauth",<0.594.0>} needs_update
[ns_server:debug,2020-03-27T20:38:37.632Z,ns_1@127.0.0.1:menelaus_cbauth<0.442.0>:menelaus_cbauth:handle_cast:107]Observed json rpc process {"goxdcr-cbauth",<0.594.0>} needs_update
[ns_server:debug,2020-03-27T20:38:37.639Z,ns_1@127.0.0.1:ns_ports_setup<0.448.0>:ns_ports_manager:set_dynamic_children:54]Setting children [memcached,saslauthd_port,indexer,projector,goxdcr,query,fts,
                  eventing]
[ns_server:debug,2020-03-27T20:38:37.646Z,ns_1@127.0.0.1:memcached_refresh<0.213.0>:memcached_refresh:handle_cast:55]Refresh of rbac requested
[error_logger:info,2020-03-27T20:38:37.649Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,service_stats_children_sup}
             started: [{pid,<0.1807.0>},
                       {id,{service_eventing,stats_archiver,"@eventing"}},
                       {mfargs,{stats_archiver,start_link,["@eventing"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:38:37.650Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,service_stats_children_sup}
             started: [{pid,<0.1809.0>},
                       {id,{service_eventing,stats_reader,"@eventing"}},
                       {mfargs,{stats_reader,start_link,["@eventing"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:38:37.652Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,service_stats_children_sup}
             started: [{pid,<0.1810.0>},
                       {id,{service_fts,stats_archiver,"@fts"}},
                       {mfargs,{stats_archiver,start_link,["@fts"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:38:37.652Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,service_stats_children_sup}
             started: [{pid,<0.1812.0>},
                       {id,{service_fts,stats_reader,"@fts"}},
                       {mfargs,{stats_reader,start_link,["@fts"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:38:37.661Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,service_monitor_children_sup}
             started: [{pid,<0.1814.0>},
                       {id,{kv,dcp_traffic_monitor}},
                       {mfargs,{dcp_traffic_monitor,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-03-27T20:38:37.671Z,ns_1@127.0.0.1:memcached_refresh<0.213.0>:memcached_refresh:handle_info:89]Refresh of [rbac] succeeded
[ns_server:debug,2020-03-27T20:38:37.690Z,ns_1@127.0.0.1:ns_audit<0.463.0>:ns_audit:handle_call:125]Audit modify_index_storage_mode: [{storageMode,<<"plasma">>},
                                  {real_userid,
                                      {[{domain,builtin},
                                        {user,<<"<ud>Administrator</ud>">>}]}},
                                  {sessionid,
                                      <<"a5fb028c6de4e971bdb5c6cbb45bd505">>},
                                  {remote,
                                      {[{ip,<<"192.168.80.1">>},
                                        {port,50542}]}},
                                  {timestamp,<<"2020-03-27T20:38:37.689Z">>}]
[ns_server:debug,2020-03-27T20:38:37.690Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{metakv,<<"/indexing/settings/config">>} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{11,63752560717}}]}|
 <<"{\"indexer.settings.compaction.days_of_week\":\"Sunday,Monday,Tuesday,Wednesday,Thursday,Friday,Saturday\",\"indexer.settings.compaction.interval\":\"00:00,00:00\",\"indexer.settings.persisted_snapshot.interval\":5000,\"indexer.settings.log_level\":\"info\",\"indexer.settings.compaction.compaction_mode\":\"circular\",\"indexer.settings.compaction.min_frag\":30,\"indexer.settings.inmemory_snapshot.interval\""...>>]
[ns_server:debug,2020-03-27T20:38:37.690Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{local_changes_count,<<"60a6bc3db77e6c7b91c556140dcfec71">>} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{49,63752560717}}]}]
[ns_server:debug,2020-03-27T20:38:37.691Z,ns_1@127.0.0.1:ns_config_rep<0.319.0>:ns_config_rep:do_push_keys:321]Replicating some config keys ([{local_changes_count,
                                   <<"60a6bc3db77e6c7b91c556140dcfec71">>},
                               {metakv,<<"/indexing/settings/config">>}]..)
[error_logger:info,2020-03-27T20:38:37.734Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,service_stats_children_sup}
             started: [{pid,<0.1813.0>},
                       {id,{service_index,stats_archiver,"@index"}},
                       {mfargs,{stats_archiver,start_link,["@index"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:38:37.735Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,service_stats_children_sup}
             started: [{pid,<0.1828.0>},
                       {id,{service_index,stats_reader,"@index"}},
                       {mfargs,{stats_reader,start_link,["@index"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:38:37.743Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,service_monitor_children_sup}
             started: [{pid,<0.1820.0>},
                       {id,{kv,kv_stats_monitor}},
                       {mfargs,{kv_stats_monitor,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:error,2020-03-27T20:38:37.747Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]** Generic server node_monitor terminating 
** Last message in was refresh
** When Server state == {state,
                         {dict,1,16,16,8,80,48,
                          {[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]},
                          {{[],
                            [['ns_1@127.0.0.1'|
                              {[{'ns_1@127.0.0.1',[{ns_server,active}]}],
                               {recv_ts,-576460692497746344}}]],
                            [],[],[],[],[],[],[],[],[],[],[],[],[],[]}}},
                         ['ns_1@127.0.0.1'],
                         node_monitor}
** Reason for termination == 
** {{noproc,{gen_server,call,[kv_monitor,get_nodes]}},
    [{gen_server,call,2,[{file,"gen_server.erl"},{line,206}]},
     {node_monitor,'-latest_status/1-fun-0-',1,
                   [{file,"src/node_monitor.erl"},{line,113}]},
     {lists,map,2,[{file,"lists.erl"},{line,1239}]},
     {lists,map,2,[{file,"lists.erl"},{line,1239}]},
     {node_monitor,latest_status,1,[{file,"src/node_monitor.erl"},{line,111}]},
     {node_monitor,handle_info,3,[{file,"src/node_monitor.erl"},{line,55}]},
     {health_monitor,handle_message,3,
                     [{file,"src/health_monitor.erl"},{line,106}]},
     {health_monitor,handle_info,2,
                     [{file,"src/health_monitor.erl"},{line,91}]}]}

[error_logger:info,2020-03-27T20:38:37.748Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,service_monitor_children_sup}
             started: [{pid,<0.1831.0>},
                       {id,{kv,kv_monitor}},
                       {mfargs,{kv_monitor,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-03-27T20:38:37.752Z,ns_1@127.0.0.1:users_storage<0.262.0>:replicated_dets:handle_call:302]Suspended by process <0.305.0>
[ns_server:debug,2020-03-27T20:38:37.752Z,ns_1@127.0.0.1:memcached_passwords<0.305.0>:replicated_dets:select_from_dets_locked:350]Starting select with {users_storage,[{{docv2,{auth,{'_',local}},'_','_'},
                                      [],
                                      ['$_']}],
                                    100}
[ns_server:debug,2020-03-27T20:38:37.752Z,ns_1@127.0.0.1:users_storage<0.262.0>:replicated_dets:handle_call:309]Released by process <0.305.0>
[ns_server:debug,2020-03-27T20:38:37.761Z,ns_1@127.0.0.1:memcached_refresh<0.213.0>:memcached_refresh:handle_cast:55]Refresh of isasl requested
[ns_server:debug,2020-03-27T20:38:37.771Z,ns_1@127.0.0.1:<0.582.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.581.0>} exited with reason {noproc,
                                                                                {gen_server,
                                                                                 call,
                                                                                 [kv_monitor,
                                                                                  get_nodes]}}
[ns_server:debug,2020-03-27T20:38:37.772Z,ns_1@127.0.0.1:<0.588.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.587.0>} exited with reason shutdown
[ns_server:debug,2020-03-27T20:38:37.773Z,ns_1@127.0.0.1:<0.576.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.575.0>} exited with reason shutdown
[ns_server:debug,2020-03-27T20:38:37.773Z,ns_1@127.0.0.1:<0.1832.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.1831.0>} exited with reason shutdown
[ns_server:debug,2020-03-27T20:38:37.773Z,ns_1@127.0.0.1:<0.1829.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.1820.0>} exited with reason shutdown
[ns_server:debug,2020-03-27T20:38:37.774Z,ns_1@127.0.0.1:<0.1815.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.1814.0>} exited with reason shutdown
[ns_server:debug,2020-03-27T20:38:37.774Z,ns_1@127.0.0.1:<0.573.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.572.0>} exited with reason shutdown
[error_logger:error,2020-03-27T20:38:37.777Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================CRASH REPORT=========================
  crasher:
    initial call: health_monitor:init/1
    pid: <0.581.0>
    registered_name: node_monitor
    exception exit: {noproc,{gen_server,call,[kv_monitor,get_nodes]}}
      in function  gen_server:call/2 (gen_server.erl, line 206)
      in call from node_monitor:'-latest_status/1-fun-0-'/1 (src/node_monitor.erl, line 113)
      in call from lists:map/2 (lists.erl, line 1239)
      in call from lists:map/2 (lists.erl, line 1239)
      in call from node_monitor:latest_status/1 (src/node_monitor.erl, line 111)
      in call from node_monitor:handle_info/3 (src/node_monitor.erl, line 55)
      in call from health_monitor:handle_message/3 (src/health_monitor.erl, line 106)
      in call from health_monitor:handle_info/2 (src/health_monitor.erl, line 91)
    ancestors: [health_monitor_sup,ns_server_sup,ns_server_nodes_sup,
                  <0.208.0>,ns_server_cluster_sup,root_sup,<0.118.0>]
    message_queue_len: 1
    messages: [{'$gen_call',{<0.587.0>,
                                #Ref<0.2968477961.2101608450.160187>},
                               get_nodes}]
    links: [<0.571.0>,<0.582.0>]
    dictionary: []
    trap_exit: false
    status: running
    heap_size: 2586
    stack_size: 27
    reductions: 19852
  neighbours:

[error_logger:error,2020-03-27T20:38:37.777Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================SUPERVISOR REPORT=========================
     Supervisor: {local,health_monitor_sup}
     Context:    child_terminated
     Reason:     {noproc,{gen_server,call,[kv_monitor,get_nodes]}}
     Offender:   [{pid,<0.581.0>},
                  {id,node_monitor},
                  {mfargs,{node_monitor,start_link,[]}},
                  {restart_type,permanent},
                  {shutdown,1000},
                  {child_type,worker}]


[error_logger:info,2020-03-27T20:38:37.777Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,health_monitor_sup}
             started: [{pid,<0.1833.0>},
                       {id,ns_server_monitor},
                       {mfargs,{ns_server_monitor,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:38:37.777Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,health_monitor_sup}
             started: [{pid,<0.1836.0>},
                       {id,service_monitor_children_sup},
                       {mfargs,
                           {supervisor,start_link,
                               [{local,service_monitor_children_sup},
                                health_monitor_sup,child]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-03-27T20:38:37.777Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,service_monitor_children_sup}
             started: [{pid,<0.1842.0>},
                       {id,{kv,dcp_traffic_monitor}},
                       {mfargs,{dcp_traffic_monitor,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:38:37.777Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,service_monitor_children_sup}
             started: [{pid,<0.1844.0>},
                       {id,{kv,kv_stats_monitor}},
                       {mfargs,{kv_stats_monitor,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:38:37.777Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,service_monitor_children_sup}
             started: [{pid,<0.1846.0>},
                       {id,{kv,kv_monitor}},
                       {mfargs,{kv_monitor,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:38:37.778Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,health_monitor_sup}
             started: [{pid,<0.1839.0>},
                       {id,service_monitor_worker},
                       {mfargs,
                           {erlang,apply,
                               [#Fun<health_monitor_sup.0.112499759>,[]]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:38:37.778Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,health_monitor_sup}
             started: [{pid,<0.1848.0>},
                       {id,node_monitor},
                       {mfargs,{node_monitor,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:38:37.780Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,health_monitor_sup}
             started: [{pid,<0.1850.0>},
                       {id,node_status_analyzer},
                       {mfargs,{node_status_analyzer,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-03-27T20:38:37.782Z,ns_1@127.0.0.1:users_storage<0.262.0>:replicated_dets:select_from_dets_locked:350]Starting select with {users_storage,[{{docv2,{auth,'_'},'_','_'},[],['$_']}],
                                    100}
[ns_server:debug,2020-03-27T20:38:37.795Z,ns_1@127.0.0.1:memcached_refresh<0.213.0>:memcached_refresh:handle_info:89]Refresh of [isasl] succeeded
[ns_server:debug,2020-03-27T20:38:37.875Z,ns_1@127.0.0.1:compiled_roles_cache<0.264.0>:menelaus_roles:build_compiled_roles:753]Compile roles for user {"@",admin}
[ns_server:debug,2020-03-27T20:38:37.876Z,ns_1@127.0.0.1:json_rpc_connection-index-cbauth<0.1859.0>:json_rpc_connection:init:73]Observed revrpc connection: label "index-cbauth", handling process <0.1859.0>
[ns_server:debug,2020-03-27T20:38:37.876Z,ns_1@127.0.0.1:menelaus_cbauth<0.442.0>:menelaus_cbauth:handle_cast:107]Observed json rpc process {"index-cbauth",<0.1859.0>} started
[ns_server:debug,2020-03-27T20:38:37.917Z,ns_1@127.0.0.1:compiled_roles_cache<0.264.0>:menelaus_roles:build_compiled_roles:753]Compile roles for user {"@index-cbauth",admin}
[ns_server:debug,2020-03-27T20:38:37.963Z,ns_1@127.0.0.1:compiled_roles_cache<0.264.0>:menelaus_roles:build_compiled_roles:753]Compile roles for user {"@goxdcr-cbauth",admin}
[ns_server:debug,2020-03-27T20:38:37.971Z,ns_1@127.0.0.1:<0.414.0>:menelaus_web:check_bucket_uuid:1076]Attempt to access non existent bucket "null"
[ns_server:debug,2020-03-27T20:38:38.001Z,ns_1@127.0.0.1:json_rpc_connection-projector-cbauth<0.1870.0>:json_rpc_connection:init:73]Observed revrpc connection: label "projector-cbauth", handling process <0.1870.0>
[ns_server:debug,2020-03-27T20:38:38.001Z,ns_1@127.0.0.1:menelaus_cbauth<0.442.0>:menelaus_cbauth:handle_cast:107]Observed json rpc process {"projector-cbauth",<0.1870.0>} started
[ns_server:debug,2020-03-27T20:38:38.004Z,ns_1@127.0.0.1:compiled_roles_cache<0.264.0>:menelaus_roles:build_compiled_roles:753]Compile roles for user {"@projector-cbauth",admin}
[ns_server:debug,2020-03-27T20:38:38.031Z,ns_1@127.0.0.1:users_storage<0.262.0>:replicated_storage:handle_call:115]Writing interactively saved doc {docv2,
                                 {ui_profile,{"<ud>Administrator</ud>",admin}},
                                 {[{<<"version">>,393221},
                                   {<<"scenarios">>,
                                    [{[{<<"name">>,<<"Cluster Overview">>},
                                       {<<"desc">>,
                                        <<"Stats showing the general health of your cluster.">>},
                                       {<<"groups">>,
                                        [<<"5ivtumddm">>,<<"nuu2quo0b">>]},
                                       {<<"preset">>,true},
                                       {<<"id">>,<<"6c1lmfwi4">>}]},
                                     {[{<<"name">>,<<"All Services">>},
                                       {<<"desc">>,
                                        <<"Most common stats, arranged per service. Customize and make your own dashboard with \"new dashboard... \" below.">>},
                                       {<<"groups">>,
                                        [<<"jrbzo6yeh">>,<<"0zwd4f23u">>,
                                         <<"6vpybpe6f">>,<<"d5m9vala9">>,
                                         <<"05xnmq0o0">>,<<"mm0s5n71u">>,
                                         <<"o591t78e6">>,<<"b1ylqlkkt">>,
                                         <<"4nnj1ff7p">>,<<"zwpubh4au">>]},
                                       {<<"preset">>,true},
                                       {<<"id">>,<<"pyjmqi9u9">>}]}]},
                                   {<<"groups">>,
                                    [{[{<<"name">>,<<"Cluster Overview">>},
                                       {<<"isOpen">>,true},
                                       {<<"charts">>,
                                        [<<"sc17hoi0i">>,<<"7vhstizss">>,
                                         <<"v4h49bhp4">>,<<"3ulwsp0st">>,
                                         <<"b0obqlffd">>,<<"p1mkbb0cu">>,
                                         <<"0t691ed9s">>,<<"srbaudzu4">>,
                                         <<"kq3pvcjja">>,<<"az435uv39">>,
                                         <<"s166r7d81">>]},
                                       {<<"preset">>,true},
                                       {<<"id">>,<<"5ivtumddm">>}]},
                                     {[{<<"name">>,<<"Node Resources">>},
                                       {<<"isOpen">>,false},
                                       {<<"charts">>,
                                        [<<"2v8bvxw0s">>,<<"zv2skbbb7">>,
                                         <<"j4zgdwuvg">>,<<"xn948q2i3">>]},
                                       {<<"preset">>,true},
                                       {<<"id">>,<<"nuu2quo0b">>}]},
                                     {[{<<"name">>,
                                        <<"Data (Docs/Views/XDCR)">>},
                                       {<<"isOpen">>,true},
                                       {<<"charts">>,
                                        [<<"0z8qf0d76">>,<<"gs87af1k9">>,
                                         <<"t1j3fie6p">>,<<"bqsel38s6">>,
                                         <<"n3f5k10ka">>]},
                                       {<<"preset">>,true},
                                       {<<"id">>,<<"jrbzo6yeh">>}]},
                                     {[{<<"name">>,<<"Query">>},
                                       {<<"isOpen">>,false},
                                       {<<"charts">>,
                                        [<<"oycn9kdwn">>,<<"yfnt06sy0">>,
                                         <<"jl79kpevk">>,<<"jpr8ntqj1">>,
                                         <<"pp4n8584e">>]},
                                       {<<"preset">>,true},
                                       {<<"id">>,<<"0zwd4f23u">>}]},
                                     {[{<<"name">>,<<"Index">>},
                                       {<<"isOpen">>,false},
                                       {<<"charts">>,
                                        [<<"8dpqqek8s">>,<<"alse5xx0q">>,
                                         <<"5drf3ennl">>,<<"7amproxwh">>,
                                         <<"3z9179eo0">>]},
                                       {<<"preset">>,true},
                                       {<<"id">>,<<"6vpybpe6f">>}]},
                                     {[{<<"name">>,<<"Search">>},
                                       {<<"isOpen">>,false},
                                       {<<"charts">>,
                                        [<<"rl2o3xcuk">>,<<"b6x4gprv2">>]},
                                       {<<"preset">>,true},
                                       {<<"id">>,<<"d5m9vala9">>}]},
                                     {[{<<"name">>,<<"Analytics">>},
                                       {<<"enterprise">>,true},
                                       {<<"isOpen">>,false},
                                       {<<"charts">>,
                                        [<<"nqowo8m10">>,<<"ivcz4papd">>,
                                         <<"9jhyl5bix">>,<<"iqywh0juj">>]},
                                       {<<"preset">>,true},
                                       {<<"id">>,<<"05xnmq0o0">>}]},
                                     {[{<<"name">>,<<"Eventing">>},
                                       {<<"enterprise">>,true},
                                       {<<"isOpen">>,false},
                                       {<<"charts">>,
                                        [<<"j26r814pf">>,<<"9nznthxag">>]},
                                       {<<"preset">>,true},
                                       {<<"id">>,<<"mm0s5n71u">>}]},
                                     {[{<<"name">>,<<"XDCR">>},
                                       {<<"isOpen">>,false},
                                       {<<"charts">>,
                                        [<<"eilge9691">>,<<"rnuuoatrf">>,
                                         <<"fdftc82qw">>,<<"roc1geiyw">>]},
                                       {<<"preset">>,true},
                                       {<<"id">>,<<"o591t78e6">>}]},
                                     {[{<<"name">>,<<"vBucket Resources">>},
                                       {<<"isOpen">>,false},
                                       {<<"charts">>,
                                        [<<"gfz6qcchc">>,<<"vorzcs3xz">>,
                                         <<"364r4rsbv">>,<<"0tifbs9om">>,
                                         <<"u81pdtwdn">>,<<"fmf1ukwws">>,
                                         <<"0elavidwe">>]},
                                       {<<"preset">>,true},
                                       {<<"id">>,<<"b1ylqlkkt">>}]},
                                     {[{<<"name">>,<<"DCP Queues">>},
                                       {<<"isOpen">>,false},
                                       {<<"charts">>,
                                        [<<"lgqmp9ov7">>,<<"c86lxglhu">>,
                                         <<"mgsi3igua">>,<<"xhngysaes">>,
                                         <<"hk6f2zz3d">>,<<"qldqgw2i6">>]},
                                       {<<"preset">>,true},
                                       {<<"id">>,<<"4nnj1ff7p">>}]},
                                     {[{<<"name">>,<<"Disk Queues">>},
                                       {<<"isOpen">>,false},
                                       {<<"charts">>,
                                        [<<"o08dwzx3c">>,<<"ok7sww3ee">>,
                                         <<"940fpv2du">>,<<"63syg4viz">>]},
                                       {<<"preset">>,true},
                                       {<<"id">>,<<"zwpubh4au">>}]}]},
                                   {<<"charts">>,
                                    [{[{<<"stats">>,
                                        {[{<<"@kv-.ops">>,true},
                                          {<<"@query.query_requests">>,true},
                                          {<<"@fts-.@items.total_queries">>,
                                           true},
                                          {<<"@kv-.ep_tmp_oom_errors">>,true},
                                          {<<"@kv-.ep_cache_miss_rate">>,true},
                                          {<<"@kv-.cmd_get">>,true},
                                          {<<"@kv-.cmd_set">>,true},
                                          {<<"@kv-.delete_hits">>,true},
                                          {<<"@kv-.@items.accesses">>,true}]}},
                                       {<<"size">>,<<"large">>},
                                       {<<"specificStat">>,false},
                                       {<<"preset">>,true},
                                       {<<"id">>,<<"sc17hoi0i">>}]},
                                     {[{<<"stats">>,
                                        {[{<<"@kv-.mem_used">>,true},
                                          {<<"@kv-.ep_mem_low_wat">>,true},
                                          {<<"@kv-.ep_mem_high_wat">>,true}]}},
                                       {<<"size">>,<<"medium">>},
                                       {<<"specificStat">>,false},
                                       {<<"preset">>,true},
                                       {<<"id">>,<<"7vhstizss">>}]},
                                     {[{<<"stats">>,
                                        {[{<<"@kv-.curr_items">>,true},
                                          {<<"@kv-.vb_replica_curr_items">>,
                                           true},
                                          {<<"@kv-.vb_active_resident_items_ratio">>,
                                           true},
                                          {<<"@kv-.vb_replica_resident_items_ratio">>,
                                           true}]}},
                                       {<<"size">>,<<"medium">>},
                                       {<<"specificStat">>,false},
                                       {<<"preset">>,true},
                                       {<<"id">>,<<"v4h49bhp4">>}]},
                                     {[{<<"stats">>,
                                        {[{<<"@kv-.disk_write_queue">>,
                                           true}]}},
                                       {<<"size">>,<<"small">>},
                                       {<<"specificStat">>,true},
                                       {<<"preset">>,true},
                                       {<<"id">>,<<"3ulwsp0st">>}]},
                                     {[{<<"stats">>,
                                        {[{<<"@kv-.ep_dcp_replica_items_remaining">>,
                                           true}]}},
                                       {<<"size">>,<<"small">>},
                                       {<<"specificStat">>,true},
                                       {<<"preset">>,true},
                                       {<<"id">>,<<"b0obqlffd">>}]},
                                     {[{<<"stats">>,
                                        {[{<<"@kv-.ep_data_read_failed">>,
                                           true},
                                          {<<"@kv-.ep_data_write_failed">>,
                                           true},
                                          {<<"@query.query_errors">>,true},
                                          {<<"@query.total_queries_error">>,
                                           true},
                                          {<<"@eventing.eventing/failed_count">>,
                                           true}]}},
                                       {<<"size">>,<<"small">>},
                                       {<<"specificStat">>,false},
                                       {<<"preset">>,true},
                                       {<<"id">>,<<"p1mkbb0cu">>}]},
                                     {[{<<"stats">>,
                                        {[{<<"@query.query_requests_250ms">>,
                                           true},
                                          {<<"@query.query_requests_500ms">>,
                                           true},
                                          {<<"@query.query_requests_1000ms">>,
                                           true},
                                          {<<"@query.query_requests_5000ms">>,
                                           true}]}},
                                       {<<"size">>,<<"small">>},
                                       {<<"specificStat">>,false},
                                       {<<"preset">>,true},
                                       {<<"id">>,<<"0t691ed9s">>}]},
                                     {[{<<"stats">>,
                                        {[{<<"@xdcr-.replication_changes_left">>,
                                           true}]}},
                                       {<<"size">>,<<"small">>},
                                       {<<"specificStat">>,true},
                                       {<<"preset">>,true},
                                       {<<"id">>,<<"srbaudzu4">>}]},
                                     {[{<<"stats">>,
                                        {[{<<"@index-.@items.num_docs_pending+queued">>,
                                           true}]}},
                                       {<<"size">>,<<"small">>},
                                       {<<"specificStat">>,true},
                                       {<<"preset">>,true},
                                       {<<"id">>,<<"kq3pvcjja">>}]},
                                     {[{<<"stats">>,
                                        {[{<<"@fts-.@items.num_mutations_to_index">>,
                                           true}]}},
                                       {<<"size">>,<<"small">>},
                                       {<<"specificStat">>,true},
                                       {<<"preset">>,true},
                                       {<<"id">>,<<"az435uv39">>}]},
                                     {[{<<"stats">>,
                                        {[{<<"@eventing.eventing/dcp_backlog">>,
                                           true}]}},
                                       {<<"size">>,<<"small">>},
                                       {<<"specificStat">>,true},
                                       {<<"preset">>,true},
                                       {<<"id">>,<<"s166r7d81">>}]},
                                     {[{<<"stats">>,
                                        {[{<<"@system.cpu_utilization_rate">>,
                                           true}]}},
                                       {<<"size">>,<<"medium">>},
                                       {<<"specificStat">>,true},
                                       {<<"preset">>,true},
                                       {<<"id">>,<<"2v8bvxw0s">>}]},
                                     {[{<<"stats">>,
                                        {[{<<"@system.rest_requests">>,
                                           true}]}},
                                       {<<"size">>,<<"medium">>},
                                       {<<"specificStat">>,true},
                                       {<<"preset">>,true},
                                       {<<"id">>,<<"zv2skbbb7">>}]},
                                     {[{<<"stats">>,
                                        {[{<<"@system.mem_actual_free">>,
                                           true}]}},
                                       {<<"size">>,<<"medium">>},
                                       {<<"specificStat">>,false},
                                       {<<"preset">>,true},
                                       {<<"id">>,<<"j4zgdwuvg">>}]},
                                     {[{<<"stats">>,
                                        {[{<<"@system.swap_used">>,true}]}},
                                       {<<"size">>,<<"medium">>},
                                       {<<"specificStat">>,true},
                                       {<<"preset">>,true},
                                       {<<"id">>,<<"xn948q2i3">>}]},
                                     {[{<<"stats">>,
                                        {[{<<"@kv-.mem_used">>,true},
                                          {<<"@kv-.ep_mem_low_wat">>,true},
                                          {<<"@kv-.ep_mem_high_wat">>,true},
                                          {<<"@kv-.ep_kv_size">>,true},
                                          {<<"@kv-.ep_meta_data_memory">>,
                                           true},
                                          {<<"@kv-.vb_active_resident_items_ratio">>,
                                           true}]}},
                                       {<<"size">>,<<"medium">>},
                                       {<<"specificStat">>,false},
                                       {<<"preset">>,true},
                                       {<<"id">>,<<"0z8qf0d76">>}]},
                                     {[{<<"stats">>,
                                        {[{<<"@kv-.ops">>,true},
                                          {<<"@kv-.ep_cache_miss_rate">>,true},
                                          {<<"@kv-.cmd_get">>,true},
                                          {<<"@kv-.cmd_set">>,true},
                                          {<<"@kv-.delete_hits">>,true},
                                          {<<"@kv-.@items.accesses">>,true},
                                          {<<"@kv-.ep_num_ops_set_meta">>,
                                           true}]}},
                                       {<<"size">>,<<"medium">>},
                                       {<<"specificStat">>,false},
                                       {<<"preset">>,true},
                                       {<<"id">>,<<"gs87af1k9">>}]},
                                     {[{<<"stats">>,
                                        {[{<<"@kv-.ep_dcp_views+indexes_items_remaining">>,
                                           true},
                                          {<<"@kv-.ep_dcp_cbas_items_remaining">>,
                                           true},
                                          {<<"@kv-.ep_dcp_replica_items_remaining">>,
                                           true},
                                          {<<"@kv-.ep_dcp_xdcr_items_remaining">>,
                                           true},
                                          {<<"@kv-.ep_dcp_eventing_items_remaining">>,
                                           true},
                                          {<<"@kv-.ep_dcp_other_items_remaining">>,
                                           true},
                                          {<<"@xdcr-.replication_changes_left">>,
                                           true}]}},
                                       {<<"size">>,<<"medium">>},
                                       {<<"specificStat">>,false},
                                       {<<"preset">>,true},
                                       {<<"id">>,<<"t1j3fie6p">>}]},
                                     {[{<<"stats">>,
                                        {[{<<"@kv-.ep_bg_fetched">>,true},
                                          {<<"@kv-.ep_data_read_failed">>,
                                           true},
                                          {<<"@kv-.ep_data_write_failed">>,
                                           true},
                                          {<<"@kv-.ep_ops_create">>,true},
                                          {<<"@kv-.ep_ops_update">>,true}]}},
                                       {<<"size">>,<<"medium">>},
                                       {<<"specificStat">>,false},
                                       {<<"preset">>,true},
                                       {<<"id">>,<<"bqsel38s6">>}]},
                                     {[{<<"stats">>,
                                        {[{<<"@kv-.ep_diskqueue_items">>,
                                           true}]}},
                                       {<<"size">>,<<"small">>},
                                       {<<"specificStat">>,true},
                                       {<<"preset">>,true},
                                       {<<"id">>,<<"n3f5k10ka">>}]},
                                     {[{<<"stats">>,
                                        {[{<<"@query.query_requests_1000ms">>,
                                           true},
                                          {<<"@query.query_requests_500ms">>,
                                           true},
                                          {<<"@query.query_requests_5000ms">>,
                                           true}]}},
                                       {<<"size">>,<<"medium">>},
                                       {<<"specificStat">>,false},
                                       {<<"preset">>,true},
                                       {<<"id">>,<<"oycn9kdwn">>}]},
                                     {[{<<"stats">>,
                                        {[{<<"@query.query_selects">>,true},
                                          {<<"@query.query_requests">>,true},
                                          {<<"@query.query_warnings">>,true},
                                          {<<"@query.query_invalid_requests">>,
                                           true},
                                          {<<"@query.query_errors">>,true}]}},
                                       {<<"size">>,<<"medium">>},
                                       {<<"specificStat">>,false},
                                       {<<"preset">>,true},
                                       {<<"id">>,<<"yfnt06sy0">>}]},
                                     {[{<<"stats">>,
                                        {[{<<"@query.query_avg_req_time">>,
                                           true},
                                          {<<"@query.query_avg_svc_time">>,
                                           true}]}},
                                       {<<"size">>,<<"small">>},
                                       {<<"specificStat">>,false},
                                       {<<"preset">>,true},
                                       {<<"id">>,<<"jl79kpevk">>}]},
                                     {[{<<"stats">>,
                                        {[{<<"@query.query_avg_result_count">>,
                                           true}]}},
                                       {<<"size">>,<<"small">>},
                                       {<<"specificStat">>,true},
                                       {<<"preset">>,true},
                                       {<<"id">>,<<"jpr8ntqj1">>}]},
                                     {[{<<"stats">>,
                                        {[{<<"@query.query_avg_response_size">>,
                                           true}]}},
                                       {<<"size">>,<<"small">>},
                                       {<<"specificStat">>,false},
                                       {<<"preset">>,true},
                                       {<<"id">>,<<"pp4n8584e">>}]},
                                     {[{<<"stats">>,
                                        {[{<<"@index-.index/num_rows_returned">>,
                                           true}]}},
                                       {<<"size">>,<<"small">>},
                                       {<<"specificStat">>,true},
                                       {<<"preset">>,true},
                                       {<<"id">>,<<"8dpqqek8s">>}]},
                                     {[{<<"stats">>,
                                        {[{<<"@index-.@items.num_docs_pending+queued">>,
                                           true}]}},
                                       {<<"size">>,<<"small">>},
                                       {<<"specificStat">>,true},
                                       {<<"preset">>,true},
                                       {<<"id">>,<<"alse5xx0q">>}]},
                                     {[{<<"stats">>,
                                        {[{<<"@index-.index/data_size">>,
                                           true}]}},
                                       {<<"size">>,<<"small">>},
                                       {<<"specificStat">>,true},
                                       {<<"preset">>,true},
                                       {<<"id">>,<<"5drf3ennl">>}]},
                                     {[{<<"stats">>,
                                        {[{<<"@index-.index/disk_size">>,
                                           true}]}},
                                       {<<"size">>,<<"small">>},
                                       {<<"specificStat">>,true},
                                       {<<"preset">>,true},
                                       {<<"id">>,<<"7amproxwh">>}]},
                                     {[{<<"stats">>,
                                        {[{<<"@index.index_ram_percent">>,
                                           true},
                                          {<<"@index.index_remaining_ram">>,
                                           true},
                                          {<<"@index-.index/data_size">>,true},
                                          {<<"@index-.index/disk_size">>,
                                           true}]}},
                                       {<<"size">>,<<"medium">>},
                                       {<<"specificStat">>,false},
                                       {<<"preset">>,true},
                                       {<<"id">>,<<"3z9179eo0">>}]},
                                     {[{<<"stats">>,
                                        {[{<<"@fts-.fts/num_bytes_used_disk">>,
                                           true},
                                          {<<"@fts.fts_num_bytes_used_ram">>,
                                           true}]}},
                                       {<<"size">>,<<"medium">>},
                                       {<<"specificStat">>,false},
                                       {<<"preset">>,true},
                                       {<<"id">>,<<"rl2o3xcuk">>}]},
                                     {[{<<"stats">>,
                                        {[{<<"@fts-.@items.total_queries">>,
                                           true},
                                          {<<"@fts-.@items.total_queries_error">>,
                                           true},
                                          {<<"@fts-.@items.total_queries_slow">>,
                                           true},
                                          {<<"@fts-.@items.total_queries_timeout">>,
                                           true},
                                          {<<"@fts.fts_total_queries_rejected_by_herder">>,
                                           true}]}},
                                       {<<"size">>,<<"medium">>},
                                       {<<"specificStat">>,false},
                                       {<<"preset">>,true},
                                       {<<"id">>,<<"b6x4gprv2">>}]},
                                     {[{<<"stats">>,
                                        {[{<<"@cbas-.cbas/incoming_records_count">>,
                                           true}]}},
                                       {<<"size">>,<<"small">>},
                                       {<<"specificStat">>,true},
                                       {<<"preset">>,true},
                                       {<<"id">>,<<"nqowo8m10">>}]},
                                     {[{<<"stats">>,
                                        {[{<<"@cbas.cbas_heap_used">>,true}]}},
                                       {<<"size">>,<<"small">>},
                                       {<<"specificStat">>,true},
                                       {<<"preset">>,true},
                                       {<<"id">>,<<"ivcz4papd">>}]},
                                     {[{<<"stats">>,
                                        {[{<<"@cbas.cbas_disk_used">>,true}]}},
                                       {<<"size">>,<<"small">>},
                                       {<<"specificStat">>,true},
                                       {<<"preset">>,true},
                                       {<<"id">>,<<"9jhyl5bix">>}]},
                                     {[{<<"stats">>,
                                        {[{<<"@cbas.cbas_system_load_average">>,
                                           true}]}},
                                       {<<"size">>,<<"small">>},
                                       {<<"specificStat">>,true},
                                       {<<"preset">>,true},
                                       {<<"id">>,<<"iqywh0juj">>}]},
                                     {[{<<"stats">>,
                                        {[{<<"@eventing.eventing/dcp_backlog">>,
                                           true}]}},
                                       {<<"size">>,<<"small">>},
                                       {<<"specificStat">>,true},
                                       {<<"preset">>,true},
                                       {<<"id">>,<<"j26r814pf">>}]},
                                     {[{<<"stats">>,
                                        {[{<<"@eventing.eventing/failed_count">>,
                                           true},
                                          {<<"@eventing.eventing/timeout_count">>,
                                           true}]}},
                                       {<<"size">>,<<"small">>},
                                       {<<"specificStat">>,false},
                                       {<<"preset">>,true},
                                       {<<"id">>,<<"9nznthxag">>}]},
                                     {[{<<"stats">>,
                                        {[{<<"@xdcr-.replication_changes_left">>,
                                           true}]}},
                                       {<<"size">>,<<"small">>},
                                       {<<"specificStat">>,true},
                                       {<<"preset">>,true},
                                       {<<"id">>,<<"eilge9691">>}]},
                                     {[{<<"stats">>,
                                        {[{<<"@xdcr-.@items.changes_left">>,
                                           true}]}},
                                       {<<"size">>,<<"small">>},
                                       {<<"specificStat">>,true},
                                       {<<"preset">>,true},
                                       {<<"id">>,<<"rnuuoatrf">>}]},
                                     {[{<<"stats">>,
                                        {[{<<"@xdcr-.@items.wtavg_docs_latency">>,
                                           true},
                                          {<<"@xdcr-.@items.wtavg_meta_latency">>,
                                           true}]}},
                                       {<<"size">>,<<"small">>},
                                       {<<"specificStat">>,false},
                                       {<<"preset">>,true},
                                       {<<"id">>,<<"fdftc82qw">>}]},
                                     {[{<<"stats">>,
                                        {[{<<"@xdcr-.@items.docs_failed_cr_source">>,
                                           true},
                                          {<<"@xdcr-.@items.docs_filtered">>,
                                           true}]}},
                                       {<<"size">>,<<"small">>},
                                       {<<"specificStat">>,false},
                                       {<<"preset">>,true},
                                       {<<"id">>,<<"roc1geiyw">>}]},
                                     {[{<<"stats">>,
                                        {[{<<"@kv-.vb_active_num">>,true},
                                          {<<"@kv-.vb_replica_num">>,true},
                                          {<<"@kv-.vb_pending_num">>,true},
                                          {<<"@kv-.ep_vb_total">>,true}]}},
                                       {<<"size">>,<<"medium">>},
                                       {<<"specificStat">>,false},
                                       {<<"preset">>,true},
                                       {<<"id">>,<<"gfz6qcchc">>}]},
                                     {[{<<"stats">>,
                                        {[{<<"@kv-.curr_items">>,true},
                                          {<<"@kv-.vb_replica_curr_items">>,
                                           true},
                                          {<<"@kv-.vb_pending_curr_items">>,
                                           true},
                                          {<<"@kv-.curr_items_tot">>,true}]}},
                                       {<<"size">>,<<"medium">>},
                                       {<<"specificStat">>,false},
                                       {<<"preset">>,true},
                                       {<<"id">>,<<"vorzcs3xz">>}]},
                                     {[{<<"stats">>,
                                        {[{<<"@kv-.vb_active_resident_items_ratio">>,
                                           true},
                                          {<<"@kv-.vb_replica_resident_items_ratio">>,
                                           true},
                                          {<<"@kv-.vb_pending_resident_items_ratio">>,
                                           true},
                                          {<<"@kv-.ep_resident_items_rate">>,
                                           true}]}},
                                       {<<"size">>,<<"medium">>},
                                       {<<"specificStat">>,false},
                                       {<<"preset">>,true},
                                       {<<"id">>,<<"364r4rsbv">>}]},
                                     {[{<<"stats">>,
                                        {[{<<"@kv-.vb_active_ops_create">>,
                                           true},
                                          {<<"@kv-.vb_replica_ops_create">>,
                                           true},
                                          {<<"@kv-.vb_pending_ops_create">>,
                                           true},
                                          {<<"@kv-.ep_ops_create">>,true}]}},
                                       {<<"size">>,<<"medium">>},
                                       {<<"specificStat">>,false},
                                       {<<"preset">>,true},
                                       {<<"id">>,<<"0tifbs9om">>}]},
                                     {[{<<"stats">>,
                                        {[{<<"@kv-.vb_active_eject">>,true},
                                          {<<"@kv-.vb_replica_eject">>,true},
                                          {<<"@kv-.vb_pending_eject">>,true},
                                          {<<"@kv-.ep_num_value_ejects">>,
                                           true}]}},
                                       {<<"size">>,<<"medium">>},
                                       {<<"specificStat">>,false},
                                       {<<"preset">>,true},
                                       {<<"id">>,<<"u81pdtwdn">>}]},
                                     {[{<<"stats">>,
                                        {[{<<"@kv-.vb_active_itm_memory">>,
                                           true},
                                          {<<"@kv-.vb_replica_itm_memory">>,
                                           true},
                                          {<<"@kv-.vb_pending_itm_memory">>,
                                           true},
                                          {<<"@kv-.ep_kv_size">>,true}]}},
                                       {<<"size">>,<<"medium">>},
                                       {<<"specificStat">>,false},
                                       {<<"preset">>,true},
                                       {<<"id">>,<<"fmf1ukwws">>}]},
                                     {[{<<"stats">>,
                                        {[{<<"@kv-.vb_active_meta_data_memory">>,
                                           true},
                                          {<<"@kv-.vb_replica_meta_data_memory">>,
                                           true},
                                          {<<"@kv-.vb_pending_meta_data_memory">>,
                                           true},
                                          {<<"@kv-.ep_meta_data_memory">>,
                                           true}]}},
                                       {<<"size">>,<<"medium">>},
                                       {<<"specificStat">>,false},
                                       {<<"preset">>,true},
                                       {<<"id">>,<<"0elavidwe">>}]},
                                     {[{<<"stats">>,
                                        {[{<<"@kv-.ep_dcp_views+indexes_count">>,
                                           true},
                                          {<<"@kv-.ep_dcp_cbas_count">>,true},
                                          {<<"@kv-.ep_dcp_replica_count">>,
                                           true},
                                          {<<"@kv-.ep_dcp_xdcr_count">>,true},
                                          {<<"@kv-.ep_dcp_eventing_count">>,
                                           true},
                                          {<<"@kv-.ep_dcp_other_count">>,
                                           true}]}},
                                       {<<"size">>,<<"medium">>},
                                       {<<"specificStat">>,false},
                                       {<<"preset">>,true},
                                       {<<"id">>,<<"lgqmp9ov7">>}]},
                                     {[{<<"stats">>,
                                        {[{<<"@kv-.ep_dcp_views+indexes_producer_count">>,
                                           true},
                                          {<<"@kv-.ep_dcp_cbas_producer_count">>,
                                           true},
                                          {<<"@kv-.ep_dcp_replica_producer_count">>,
                                           true},
                                          {<<"@kv-.ep_dcp_xdcr_producer_count">>,
                                           true},
                                          {<<"@kv-.ep_dcp_xdcr_eventing_count">>,
                                           true},
                                          {<<"@kv-.ep_dcp_other_producer_count">>,
                                           true}]}},
                                       {<<"size">>,<<"medium">>},
                                       {<<"specificStat">>,false},
                                       {<<"preset">>,true},
                                       {<<"id">>,<<"c86lxglhu">>}]},
                                     {[{<<"stats">>,
                                        {[{<<"@kv-.ep_dcp_views+indexes_items_remaining">>,
                                           true},
                                          {<<"@kv-.ep_dcp_cbas_items_remaining">>,
                                           true},
                                          {<<"@kv-.ep_dcp_replica_items_remaining">>,
                                           true},
                                          {<<"@kv-.ep_dcp_xdcr_items_remaining">>,
                                           true},
                                          {<<"@kv-.ep_dcp_eventing_items_remaining">>,
                                           true},
                                          {<<"@kv-.ep_dcp_other_items_remaining">>,
                                           true}]}},
                                       {<<"size">>,<<"medium">>},
                                       {<<"specificStat">>,false},
                                       {<<"preset">>,true},
                                       {<<"id">>,<<"mgsi3igua">>}]},
                                     {[{<<"stats">>,
                                        {[{<<"@kv-.ep_dcp_views+indexes_items_sent">>,
                                           true},
                                          {<<"@kv-.ep_dcp_cbas_items_sent">>,
                                           true},
                                          {<<"@kv-.ep_dcp_replica_items_sent">>,
                                           true},
                                          {<<"@kv-.ep_dcp_xdcr_items_sent">>,
                                           true},
                                          {<<"@kv-.ep_dcp_eventing_items_sent">>,
                                           true},
                                          {<<"@kv-.ep_dcp_other_items_sent">>,
                                           true}]}},
                                       {<<"size">>,<<"medium">>},
                                       {<<"specificStat">>,false},
                                       {<<"preset">>,true},
                                       {<<"id">>,<<"xhngysaes">>}]},
                                     {[{<<"stats">>,
                                        {[{<<"@kv-.ep_dcp_views+indexes_total_bytes">>,
                                           true},
                                          {<<"@kv-.ep_dcp_cbas_total_bytes">>,
                                           true},
                                          {<<"@kv-.ep_dcp_replica_total_bytes">>,
                                           true},
                                          {<<"@kv-.ep_dcp_xdcr_total_bytes">>,
                                           true},
                                          {<<"@kv-.ep_dcp_eventing_total_bytes">>,
                                           true},
                                          {<<"@kv-.ep_dcp_other_total_bytes">>,
                                           true}]}},
                                       {<<"size">>,<<"medium">>},
                                       {<<"specificStat">>,false},
                                       {<<"preset">>,true},
                                       {<<"id">>,<<"hk6f2zz3d">>}]},
                                     {[{<<"stats">>,
                                        {[{<<"@kv-.ep_dcp_views+indexes_backoff">>,
                                           true},
                                          {<<"@kv-.ep_dcp_cbas_backoff">>,
                                           true},
                                          {<<"@kv-.ep_dcp_replica_backoff">>,
                                           true},
                                          {<<"@kv-.ep_dcp_xdcr_backoff">>,
                                           true},
                                          {<<"@kv-.ep_dcp_eventing_backoff">>,
                                           true},
                                          {<<"@kv-.ep_dcp_other_backoff">>,
                                           true}]}},
                                       {<<"size">>,<<"medium">>},
                                       {<<"specificStat">>,false},
                                       {<<"preset">>,true},
                                       {<<"id">>,<<"qldqgw2i6">>}]},
                                     {[{<<"stats">>,
                                        {[{<<"@kv-.ep_diskqueue_fill">>,true},
                                          {<<"@kv-.ep_diskqueue_drain">>,true},
                                          {<<"@kv-.ep_diskqueue_items">>,
                                           true}]}},
                                       {<<"size">>,<<"medium">>},
                                       {<<"specificStat">>,false},
                                       {<<"preset">>,true},
                                       {<<"id">>,<<"o08dwzx3c">>}]},
                                     {[{<<"stats">>,
                                        {[{<<"@kv-.vb_active_queue_fill">>,
                                           true},
                                          {<<"@kv-.vb_active_queue_drain">>,
                                           true},
                                          {<<"@kv-.vb_active_queue_size">>,
                                           true}]}},
                                       {<<"size">>,<<"medium">>},
                                       {<<"specificStat">>,false},
                                       {<<"preset">>,true},
                                       {<<"id">>,<<"ok7sww3ee">>}]},
                                     {[{<<"stats">>,
                                        {[{<<"@kv-.vb_replica_queue_fill">>,
                                           true},
                                          {<<"@kv-.vb_replica_queue_drain">>,
                                           true},
                                          {<<"@kv-.vb_replica_queue_size">>,
                                           true}]}},
                                       {<<"size">>,<<"medium">>},
                                       {<<"specificStat">>,false},
                                       {<<"preset">>,true},
                                       {<<"id">>,<<"940fpv2du">>}]},
                                     {[{<<"stats">>,
                                        {[{<<"@kv-.vb_pending_queue_fill">>,
                                           true},
                                          {<<"@kv-.vb_pending_queue_drain">>,
                                           true},
                                          {<<"@kv-.vb_pending_queue_size">>,
                                           true}]}},
                                       {<<"size">>,<<"medium">>},
                                       {<<"specificStat">>,false},
                                       {<<"preset">>,true},
                                       {<<"id">>,<<"63syg4viz">>}]}]}]},
                                 [{rev,{1,<<"J">>}},
                                  {deleted,false},
                                  {last_modified,1585341518023}]}
[ns_server:debug,2020-03-27T20:38:38.066Z,ns_1@127.0.0.1:ns_audit<0.463.0>:ns_audit:handle_call:125]Audit set_user_profile: [{profile,{[{<<"version">>,393221},
                                    {<<"scenarios">>,
                                     [{[{<<"name">>,<<"Cluster Overview">>},
                                        {<<"desc">>,
                                         <<"Stats showing the general health of your cluster.">>},
                                        {<<"groups">>,
                                         [<<"5ivtumddm">>,<<"nuu2quo0b">>]},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"6c1lmfwi4">>}]},
                                      {[{<<"name">>,<<"All Services">>},
                                        {<<"desc">>,
                                         <<"Most common stats, arranged per service. Customize and make your own dashboard with \"new dashboard... \" below.">>},
                                        {<<"groups">>,
                                         [<<"jrbzo6yeh">>,<<"0zwd4f23u">>,
                                          <<"6vpybpe6f">>,<<"d5m9vala9">>,
                                          <<"05xnmq0o0">>,<<"mm0s5n71u">>,
                                          <<"o591t78e6">>,<<"b1ylqlkkt">>,
                                          <<"4nnj1ff7p">>,<<"zwpubh4au">>]},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"pyjmqi9u9">>}]}]},
                                    {<<"groups">>,
                                     [{[{<<"name">>,<<"Cluster Overview">>},
                                        {<<"isOpen">>,true},
                                        {<<"charts">>,
                                         [<<"sc17hoi0i">>,<<"7vhstizss">>,
                                          <<"v4h49bhp4">>,<<"3ulwsp0st">>,
                                          <<"b0obqlffd">>,<<"p1mkbb0cu">>,
                                          <<"0t691ed9s">>,<<"srbaudzu4">>,
                                          <<"kq3pvcjja">>,<<"az435uv39">>,
                                          <<"s166r7d81">>]},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"5ivtumddm">>}]},
                                      {[{<<"name">>,<<"Node Resources">>},
                                        {<<"isOpen">>,false},
                                        {<<"charts">>,
                                         [<<"2v8bvxw0s">>,<<"zv2skbbb7">>,
                                          <<"j4zgdwuvg">>,<<"xn948q2i3">>]},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"nuu2quo0b">>}]},
                                      {[{<<"name">>,
                                         <<"Data (Docs/Views/XDCR)">>},
                                        {<<"isOpen">>,true},
                                        {<<"charts">>,
                                         [<<"0z8qf0d76">>,<<"gs87af1k9">>,
                                          <<"t1j3fie6p">>,<<"bqsel38s6">>,
                                          <<"n3f5k10ka">>]},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"jrbzo6yeh">>}]},
                                      {[{<<"name">>,<<"Query">>},
                                        {<<"isOpen">>,false},
                                        {<<"charts">>,
                                         [<<"oycn9kdwn">>,<<"yfnt06sy0">>,
                                          <<"jl79kpevk">>,<<"jpr8ntqj1">>,
                                          <<"pp4n8584e">>]},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"0zwd4f23u">>}]},
                                      {[{<<"name">>,<<"Index">>},
                                        {<<"isOpen">>,false},
                                        {<<"charts">>,
                                         [<<"8dpqqek8s">>,<<"alse5xx0q">>,
                                          <<"5drf3ennl">>,<<"7amproxwh">>,
                                          <<"3z9179eo0">>]},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"6vpybpe6f">>}]},
                                      {[{<<"name">>,<<"Search">>},
                                        {<<"isOpen">>,false},
                                        {<<"charts">>,
                                         [<<"rl2o3xcuk">>,<<"b6x4gprv2">>]},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"d5m9vala9">>}]},
                                      {[{<<"name">>,<<"Analytics">>},
                                        {<<"enterprise">>,true},
                                        {<<"isOpen">>,false},
                                        {<<"charts">>,
                                         [<<"nqowo8m10">>,<<"ivcz4papd">>,
                                          <<"9jhyl5bix">>,<<"iqywh0juj">>]},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"05xnmq0o0">>}]},
                                      {[{<<"name">>,<<"Eventing">>},
                                        {<<"enterprise">>,true},
                                        {<<"isOpen">>,false},
                                        {<<"charts">>,
                                         [<<"j26r814pf">>,<<"9nznthxag">>]},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"mm0s5n71u">>}]},
                                      {[{<<"name">>,<<"XDCR">>},
                                        {<<"isOpen">>,false},
                                        {<<"charts">>,
                                         [<<"eilge9691">>,<<"rnuuoatrf">>,
                                          <<"fdftc82qw">>,<<"roc1geiyw">>]},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"o591t78e6">>}]},
                                      {[{<<"name">>,<<"vBucket Resources">>},
                                        {<<"isOpen">>,false},
                                        {<<"charts">>,
                                         [<<"gfz6qcchc">>,<<"vorzcs3xz">>,
                                          <<"364r4rsbv">>,<<"0tifbs9om">>,
                                          <<"u81pdtwdn">>,<<"fmf1ukwws">>,
                                          <<"0elavidwe">>]},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"b1ylqlkkt">>}]},
                                      {[{<<"name">>,<<"DCP Queues">>},
                                        {<<"isOpen">>,false},
                                        {<<"charts">>,
                                         [<<"lgqmp9ov7">>,<<"c86lxglhu">>,
                                          <<"mgsi3igua">>,<<"xhngysaes">>,
                                          <<"hk6f2zz3d">>,<<"qldqgw2i6">>]},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"4nnj1ff7p">>}]},
                                      {[{<<"name">>,<<"Disk Queues">>},
                                        {<<"isOpen">>,false},
                                        {<<"charts">>,
                                         [<<"o08dwzx3c">>,<<"ok7sww3ee">>,
                                          <<"940fpv2du">>,<<"63syg4viz">>]},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"zwpubh4au">>}]}]},
                                    {<<"charts">>,
                                     [{[{<<"stats">>,
                                         {[{<<"@kv-.ops">>,true},
                                           {<<"@query.query_requests">>,true},
                                           {<<"@fts-.@items.total_queries">>,
                                            true},
                                           {<<"@kv-.ep_tmp_oom_errors">>,true},
                                           {<<"@kv-.ep_cache_miss_rate">>,
                                            true},
                                           {<<"@kv-.cmd_get">>,true},
                                           {<<"@kv-.cmd_set">>,true},
                                           {<<"@kv-.delete_hits">>,true},
                                           {<<"@kv-.@items.accesses">>,
                                            true}]}},
                                        {<<"size">>,<<"large">>},
                                        {<<"specificStat">>,false},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"sc17hoi0i">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@kv-.mem_used">>,true},
                                           {<<"@kv-.ep_mem_low_wat">>,true},
                                           {<<"@kv-.ep_mem_high_wat">>,
                                            true}]}},
                                        {<<"size">>,<<"medium">>},
                                        {<<"specificStat">>,false},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"7vhstizss">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@kv-.curr_items">>,true},
                                           {<<"@kv-.vb_replica_curr_items">>,
                                            true},
                                           {<<"@kv-.vb_active_resident_items_ratio">>,
                                            true},
                                           {<<"@kv-.vb_replica_resident_items_ratio">>,
                                            true}]}},
                                        {<<"size">>,<<"medium">>},
                                        {<<"specificStat">>,false},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"v4h49bhp4">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@kv-.disk_write_queue">>,
                                            true}]}},
                                        {<<"size">>,<<"small">>},
                                        {<<"specificStat">>,true},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"3ulwsp0st">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@kv-.ep_dcp_replica_items_remaining">>,
                                            true}]}},
                                        {<<"size">>,<<"small">>},
                                        {<<"specificStat">>,true},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"b0obqlffd">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@kv-.ep_data_read_failed">>,
                                            true},
                                           {<<"@kv-.ep_data_write_failed">>,
                                            true},
                                           {<<"@query.query_errors">>,true},
                                           {<<"@query.total_queries_error">>,
                                            true},
                                           {<<"@eventing.eventing/failed_count">>,
                                            true}]}},
                                        {<<"size">>,<<"small">>},
                                        {<<"specificStat">>,false},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"p1mkbb0cu">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@query.query_requests_250ms">>,
                                            true},
                                           {<<"@query.query_requests_500ms">>,
                                            true},
                                           {<<"@query.query_requests_1000ms">>,
                                            true},
                                           {<<"@query.query_requests_5000ms">>,
                                            true}]}},
                                        {<<"size">>,<<"small">>},
                                        {<<"specificStat">>,false},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"0t691ed9s">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@xdcr-.replication_changes_left">>,
                                            true}]}},
                                        {<<"size">>,<<"small">>},
                                        {<<"specificStat">>,true},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"srbaudzu4">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@index-.@items.num_docs_pending+queued">>,
                                            true}]}},
                                        {<<"size">>,<<"small">>},
                                        {<<"specificStat">>,true},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"kq3pvcjja">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@fts-.@items.num_mutations_to_index">>,
                                            true}]}},
                                        {<<"size">>,<<"small">>},
                                        {<<"specificStat">>,true},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"az435uv39">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@eventing.eventing/dcp_backlog">>,
                                            true}]}},
                                        {<<"size">>,<<"small">>},
                                        {<<"specificStat">>,true},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"s166r7d81">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@system.cpu_utilization_rate">>,
                                            true}]}},
                                        {<<"size">>,<<"medium">>},
                                        {<<"specificStat">>,true},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"2v8bvxw0s">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@system.rest_requests">>,
                                            true}]}},
                                        {<<"size">>,<<"medium">>},
                                        {<<"specificStat">>,true},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"zv2skbbb7">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@system.mem_actual_free">>,
                                            true}]}},
                                        {<<"size">>,<<"medium">>},
                                        {<<"specificStat">>,false},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"j4zgdwuvg">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@system.swap_used">>,true}]}},
                                        {<<"size">>,<<"medium">>},
                                        {<<"specificStat">>,true},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"xn948q2i3">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@kv-.mem_used">>,true},
                                           {<<"@kv-.ep_mem_low_wat">>,true},
                                           {<<"@kv-.ep_mem_high_wat">>,true},
                                           {<<"@kv-.ep_kv_size">>,true},
                                           {<<"@kv-.ep_meta_data_memory">>,
                                            true},
                                           {<<"@kv-.vb_active_resident_items_ratio">>,
                                            true}]}},
                                        {<<"size">>,<<"medium">>},
                                        {<<"specificStat">>,false},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"0z8qf0d76">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@kv-.ops">>,true},
                                           {<<"@kv-.ep_cache_miss_rate">>,
                                            true},
                                           {<<"@kv-.cmd_get">>,true},
                                           {<<"@kv-.cmd_set">>,true},
                                           {<<"@kv-.delete_hits">>,true},
                                           {<<"@kv-.@items.accesses">>,true},
                                           {<<"@kv-.ep_num_ops_set_meta">>,
                                            true}]}},
                                        {<<"size">>,<<"medium">>},
                                        {<<"specificStat">>,false},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"gs87af1k9">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@kv-.ep_dcp_views+indexes_items_remaining">>,
                                            true},
                                           {<<"@kv-.ep_dcp_cbas_items_remaining">>,
                                            true},
                                           {<<"@kv-.ep_dcp_replica_items_remaining">>,
                                            true},
                                           {<<"@kv-.ep_dcp_xdcr_items_remaining">>,
                                            true},
                                           {<<"@kv-.ep_dcp_eventing_items_remaining">>,
                                            true},
                                           {<<"@kv-.ep_dcp_other_items_remaining">>,
                                            true},
                                           {<<"@xdcr-.replication_changes_left">>,
                                            true}]}},
                                        {<<"size">>,<<"medium">>},
                                        {<<"specificStat">>,false},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"t1j3fie6p">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@kv-.ep_bg_fetched">>,true},
                                           {<<"@kv-.ep_data_read_failed">>,
                                            true},
                                           {<<"@kv-.ep_data_write_failed">>,
                                            true},
                                           {<<"@kv-.ep_ops_create">>,true},
                                           {<<"@kv-.ep_ops_update">>,true}]}},
                                        {<<"size">>,<<"medium">>},
                                        {<<"specificStat">>,false},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"bqsel38s6">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@kv-.ep_diskqueue_items">>,
                                            true}]}},
                                        {<<"size">>,<<"small">>},
                                        {<<"specificStat">>,true},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"n3f5k10ka">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@query.query_requests_1000ms">>,
                                            true},
                                           {<<"@query.query_requests_500ms">>,
                                            true},
                                           {<<"@query.query_requests_5000ms">>,
                                            true}]}},
                                        {<<"size">>,<<"medium">>},
                                        {<<"specificStat">>,false},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"oycn9kdwn">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@query.query_selects">>,true},
                                           {<<"@query.query_requests">>,true},
                                           {<<"@query.query_warnings">>,true},
                                           {<<"@query.query_invalid_requests">>,
                                            true},
                                           {<<"@query.query_errors">>,true}]}},
                                        {<<"size">>,<<"medium">>},
                                        {<<"specificStat">>,false},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"yfnt06sy0">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@query.query_avg_req_time">>,
                                            true},
                                           {<<"@query.query_avg_svc_time">>,
                                            true}]}},
                                        {<<"size">>,<<"small">>},
                                        {<<"specificStat">>,false},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"jl79kpevk">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@query.query_avg_result_count">>,
                                            true}]}},
                                        {<<"size">>,<<"small">>},
                                        {<<"specificStat">>,true},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"jpr8ntqj1">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@query.query_avg_response_size">>,
                                            true}]}},
                                        {<<"size">>,<<"small">>},
                                        {<<"specificStat">>,false},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"pp4n8584e">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@index-.index/num_rows_returned">>,
                                            true}]}},
                                        {<<"size">>,<<"small">>},
                                        {<<"specificStat">>,true},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"8dpqqek8s">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@index-.@items.num_docs_pending+queued">>,
                                            true}]}},
                                        {<<"size">>,<<"small">>},
                                        {<<"specificStat">>,true},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"alse5xx0q">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@index-.index/data_size">>,
                                            true}]}},
                                        {<<"size">>,<<"small">>},
                                        {<<"specificStat">>,true},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"5drf3ennl">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@index-.index/disk_size">>,
                                            true}]}},
                                        {<<"size">>,<<"small">>},
                                        {<<"specificStat">>,true},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"7amproxwh">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@index.index_ram_percent">>,
                                            true},
                                           {<<"@index.index_remaining_ram">>,
                                            true},
                                           {<<"@index-.index/data_size">>,
                                            true},
                                           {<<"@index-.index/disk_size">>,
                                            true}]}},
                                        {<<"size">>,<<"medium">>},
                                        {<<"specificStat">>,false},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"3z9179eo0">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@fts-.fts/num_bytes_used_disk">>,
                                            true},
                                           {<<"@fts.fts_num_bytes_used_ram">>,
                                            true}]}},
                                        {<<"size">>,<<"medium">>},
                                        {<<"specificStat">>,false},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"rl2o3xcuk">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@fts-.@items.total_queries">>,
                                            true},
                                           {<<"@fts-.@items.total_queries_error">>,
                                            true},
                                           {<<"@fts-.@items.total_queries_slow">>,
                                            true},
                                           {<<"@fts-.@items.total_queries_timeout">>,
                                            true},
                                           {<<"@fts.fts_total_queries_rejected_by_herder">>,
                                            true}]}},
                                        {<<"size">>,<<"medium">>},
                                        {<<"specificStat">>,false},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"b6x4gprv2">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@cbas-.cbas/incoming_records_count">>,
                                            true}]}},
                                        {<<"size">>,<<"small">>},
                                        {<<"specificStat">>,true},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"nqowo8m10">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@cbas.cbas_heap_used">>,
                                            true}]}},
                                        {<<"size">>,<<"small">>},
                                        {<<"specificStat">>,true},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"ivcz4papd">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@cbas.cbas_disk_used">>,
                                            true}]}},
                                        {<<"size">>,<<"small">>},
                                        {<<"specificStat">>,true},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"9jhyl5bix">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@cbas.cbas_system_load_average">>,
                                            true}]}},
                                        {<<"size">>,<<"small">>},
                                        {<<"specificStat">>,true},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"iqywh0juj">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@eventing.eventing/dcp_backlog">>,
                                            true}]}},
                                        {<<"size">>,<<"small">>},
                                        {<<"specificStat">>,true},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"j26r814pf">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@eventing.eventing/failed_count">>,
                                            true},
                                           {<<"@eventing.eventing/timeout_count">>,
                                            true}]}},
                                        {<<"size">>,<<"small">>},
                                        {<<"specificStat">>,false},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"9nznthxag">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@xdcr-.replication_changes_left">>,
                                            true}]}},
                                        {<<"size">>,<<"small">>},
                                        {<<"specificStat">>,true},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"eilge9691">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@xdcr-.@items.changes_left">>,
                                            true}]}},
                                        {<<"size">>,<<"small">>},
                                        {<<"specificStat">>,true},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"rnuuoatrf">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@xdcr-.@items.wtavg_docs_latency">>,
                                            true},
                                           {<<"@xdcr-.@items.wtavg_meta_latency">>,
                                            true}]}},
                                        {<<"size">>,<<"small">>},
                                        {<<"specificStat">>,false},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"fdftc82qw">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@xdcr-.@items.docs_failed_cr_source">>,
                                            true},
                                           {<<"@xdcr-.@items.docs_filtered">>,
                                            true}]}},
                                        {<<"size">>,<<"small">>},
                                        {<<"specificStat">>,false},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"roc1geiyw">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@kv-.vb_active_num">>,true},
                                           {<<"@kv-.vb_replica_num">>,true},
                                           {<<"@kv-.vb_pending_num">>,true},
                                           {<<"@kv-.ep_vb_total">>,true}]}},
                                        {<<"size">>,<<"medium">>},
                                        {<<"specificStat">>,false},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"gfz6qcchc">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@kv-.curr_items">>,true},
                                           {<<"@kv-.vb_replica_curr_items">>,
                                            true},
                                           {<<"@kv-.vb_pending_curr_items">>,
                                            true},
                                           {<<"@kv-.curr_items_tot">>,true}]}},
                                        {<<"size">>,<<"medium">>},
                                        {<<"specificStat">>,false},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"vorzcs3xz">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@kv-.vb_active_resident_items_ratio">>,
                                            true},
                                           {<<"@kv-.vb_replica_resident_items_ratio">>,
                                            true},
                                           {<<"@kv-.vb_pending_resident_items_ratio">>,
                                            true},
                                           {<<"@kv-.ep_resident_items_rate">>,
                                            true}]}},
                                        {<<"size">>,<<"medium">>},
                                        {<<"specificStat">>,false},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"364r4rsbv">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@kv-.vb_active_ops_create">>,
                                            true},
                                           {<<"@kv-.vb_replica_ops_create">>,
                                            true},
                                           {<<"@kv-.vb_pending_ops_create">>,
                                            true},
                                           {<<"@kv-.ep_ops_create">>,true}]}},
                                        {<<"size">>,<<"medium">>},
                                        {<<"specificStat">>,false},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"0tifbs9om">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@kv-.vb_active_eject">>,true},
                                           {<<"@kv-.vb_replica_eject">>,true},
                                           {<<"@kv-.vb_pending_eject">>,true},
                                           {<<"@kv-.ep_num_value_ejects">>,
                                            true}]}},
                                        {<<"size">>,<<"medium">>},
                                        {<<"specificStat">>,false},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"u81pdtwdn">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@kv-.vb_active_itm_memory">>,
                                            true},
                                           {<<"@kv-.vb_replica_itm_memory">>,
                                            true},
                                           {<<"@kv-.vb_pending_itm_memory">>,
                                            true},
                                           {<<"@kv-.ep_kv_size">>,true}]}},
                                        {<<"size">>,<<"medium">>},
                                        {<<"specificStat">>,false},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"fmf1ukwws">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@kv-.vb_active_meta_data_memory">>,
                                            true},
                                           {<<"@kv-.vb_replica_meta_data_memory">>,
                                            true},
                                           {<<"@kv-.vb_pending_meta_data_memory">>,
                                            true},
                                           {<<"@kv-.ep_meta_data_memory">>,
                                            true}]}},
                                        {<<"size">>,<<"medium">>},
                                        {<<"specificStat">>,false},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"0elavidwe">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@kv-.ep_dcp_views+indexes_count">>,
                                            true},
                                           {<<"@kv-.ep_dcp_cbas_count">>,true},
                                           {<<"@kv-.ep_dcp_replica_count">>,
                                            true},
                                           {<<"@kv-.ep_dcp_xdcr_count">>,true},
                                           {<<"@kv-.ep_dcp_eventing_count">>,
                                            true},
                                           {<<"@kv-.ep_dcp_other_count">>,
                                            true}]}},
                                        {<<"size">>,<<"medium">>},
                                        {<<"specificStat">>,false},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"lgqmp9ov7">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@kv-.ep_dcp_views+indexes_producer_count">>,
                                            true},
                                           {<<"@kv-.ep_dcp_cbas_producer_count">>,
                                            true},
                                           {<<"@kv-.ep_dcp_replica_producer_count">>,
                                            true},
                                           {<<"@kv-.ep_dcp_xdcr_producer_count">>,
                                            true},
                                           {<<"@kv-.ep_dcp_xdcr_eventing_count">>,
                                            true},
                                           {<<"@kv-.ep_dcp_other_producer_count">>,
                                            true}]}},
                                        {<<"size">>,<<"medium">>},
                                        {<<"specificStat">>,false},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"c86lxglhu">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@kv-.ep_dcp_views+indexes_items_remaining">>,
                                            true},
                                           {<<"@kv-.ep_dcp_cbas_items_remaining">>,
                                            true},
                                           {<<"@kv-.ep_dcp_replica_items_remaining">>,
                                            true},
                                           {<<"@kv-.ep_dcp_xdcr_items_remaining">>,
                                            true},
                                           {<<"@kv-.ep_dcp_eventing_items_remaining">>,
                                            true},
                                           {<<"@kv-.ep_dcp_other_items_remaining">>,
                                            true}]}},
                                        {<<"size">>,<<"medium">>},
                                        {<<"specificStat">>,false},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"mgsi3igua">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@kv-.ep_dcp_views+indexes_items_sent">>,
                                            true},
                                           {<<"@kv-.ep_dcp_cbas_items_sent">>,
                                            true},
                                           {<<"@kv-.ep_dcp_replica_items_sent">>,
                                            true},
                                           {<<"@kv-.ep_dcp_xdcr_items_sent">>,
                                            true},
                                           {<<"@kv-.ep_dcp_eventing_items_sent">>,
                                            true},
                                           {<<"@kv-.ep_dcp_other_items_sent">>,
                                            true}]}},
                                        {<<"size">>,<<"medium">>},
                                        {<<"specificStat">>,false},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"xhngysaes">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@kv-.ep_dcp_views+indexes_total_bytes">>,
                                            true},
                                           {<<"@kv-.ep_dcp_cbas_total_bytes">>,
                                            true},
                                           {<<"@kv-.ep_dcp_replica_total_bytes">>,
                                            true},
                                           {<<"@kv-.ep_dcp_xdcr_total_bytes">>,
                                            true},
                                           {<<"@kv-.ep_dcp_eventing_total_bytes">>,
                                            true},
                                           {<<"@kv-.ep_dcp_other_total_bytes">>,
                                            true}]}},
                                        {<<"size">>,<<"medium">>},
                                        {<<"specificStat">>,false},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"hk6f2zz3d">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@kv-.ep_dcp_views+indexes_backoff">>,
                                            true},
                                           {<<"@kv-.ep_dcp_cbas_backoff">>,
                                            true},
                                           {<<"@kv-.ep_dcp_replica_backoff">>,
                                            true},
                                           {<<"@kv-.ep_dcp_xdcr_backoff">>,
                                            true},
                                           {<<"@kv-.ep_dcp_eventing_backoff">>,
                                            true},
                                           {<<"@kv-.ep_dcp_other_backoff">>,
                                            true}]}},
                                        {<<"size">>,<<"medium">>},
                                        {<<"specificStat">>,false},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"qldqgw2i6">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@kv-.ep_diskqueue_fill">>,true},
                                           {<<"@kv-.ep_diskqueue_drain">>,
                                            true},
                                           {<<"@kv-.ep_diskqueue_items">>,
                                            true}]}},
                                        {<<"size">>,<<"medium">>},
                                        {<<"specificStat">>,false},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"o08dwzx3c">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@kv-.vb_active_queue_fill">>,
                                            true},
                                           {<<"@kv-.vb_active_queue_drain">>,
                                            true},
                                           {<<"@kv-.vb_active_queue_size">>,
                                            true}]}},
                                        {<<"size">>,<<"medium">>},
                                        {<<"specificStat">>,false},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"ok7sww3ee">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@kv-.vb_replica_queue_fill">>,
                                            true},
                                           {<<"@kv-.vb_replica_queue_drain">>,
                                            true},
                                           {<<"@kv-.vb_replica_queue_size">>,
                                            true}]}},
                                        {<<"size">>,<<"medium">>},
                                        {<<"specificStat">>,false},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"940fpv2du">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@kv-.vb_pending_queue_fill">>,
                                            true},
                                           {<<"@kv-.vb_pending_queue_drain">>,
                                            true},
                                           {<<"@kv-.vb_pending_queue_size">>,
                                            true}]}},
                                        {<<"size">>,<<"medium">>},
                                        {<<"specificStat">>,false},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"63syg4viz">>}]}]}]}},
                         {identity,{[{domain,builtin},
                                     {user,<<"<ud>Administrator</ud>">>}]}},
                         {real_userid,{[{domain,builtin},
                                        {user,<<"<ud>Administrator</ud>">>}]}},
                         {sessionid,<<"a5fb028c6de4e971bdb5c6cbb45bd505">>},
                         {remote,{[{ip,<<"192.168.80.1">>},{port,50526}]}},
                         {timestamp,<<"2020-03-27T20:38:38.052Z">>}]
[ns_server:debug,2020-03-27T20:38:38.207Z,ns_1@127.0.0.1:json_rpc_connection-index-service_api<0.1885.0>:json_rpc_connection:init:73]Observed revrpc connection: label "index-service_api", handling process <0.1885.0>
[ns_server:debug,2020-03-27T20:38:38.207Z,ns_1@127.0.0.1:service_agent-index<0.1797.0>:service_agent:do_handle_connection:376]Observed new json rpc connection for index: <0.1885.0>
[ns_server:debug,2020-03-27T20:38:38.207Z,ns_1@127.0.0.1:<0.1800.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {json_rpc_events,<0.1798.0>} exited with reason normal
[ns_server:error,2020-03-27T20:38:38.494Z,ns_1@127.0.0.1:query_stats_collector<0.488.0>:rest_utils:get_json:62]Request to (n1ql) /admin/stats failed: {error,
                                        {econnrefused,
                                         [{lhttpc_client,send_request,1,
                                           [{file,
                                             "/home/couchbase/jenkins/workspace/couchbase-server-unix/couchdb/src/lhttpc/lhttpc_client.erl"},
                                            {line,220}]},
                                          {lhttpc_client,execute,9,
                                           [{file,
                                             "/home/couchbase/jenkins/workspace/couchbase-server-unix/couchdb/src/lhttpc/lhttpc_client.erl"},
                                            {line,169}]},
                                          {lhttpc_client,request,9,
                                           [{file,
                                             "/home/couchbase/jenkins/workspace/couchbase-server-unix/couchdb/src/lhttpc/lhttpc_client.erl"},
                                            {line,92}]}]}}
[ns_server:debug,2020-03-27T20:38:38.637Z,ns_1@127.0.0.1:service_stats_collector-index<0.1804.0>:service_stats_collector:check_status:329]Checking if service service_index is started...
[ns_server:debug,2020-03-27T20:38:38.637Z,ns_1@127.0.0.1:service_stats_collector-index<0.1804.0>:service_stats_collector:check_status:333]Service service_index is started
[ns_server:debug,2020-03-27T20:38:38.637Z,ns_1@127.0.0.1:service_stats_collector-fts<0.1801.0>:service_stats_collector:check_status:329]Checking if service service_fts is started...
[ns_server:debug,2020-03-27T20:38:38.637Z,ns_1@127.0.0.1:service_stats_collector-eventing<0.1794.0>:service_stats_collector:check_status:329]Checking if service service_eventing is started...
[ns_server:debug,2020-03-27T20:38:38.906Z,ns_1@127.0.0.1:json_rpc_connection-fts-cbauth<0.1915.0>:json_rpc_connection:init:73]Observed revrpc connection: label "fts-cbauth", handling process <0.1915.0>
[ns_server:debug,2020-03-27T20:38:38.906Z,ns_1@127.0.0.1:menelaus_cbauth<0.442.0>:menelaus_cbauth:handle_cast:107]Observed json rpc process {"fts-cbauth",<0.1915.0>} started
[ns_server:debug,2020-03-27T20:38:39.229Z,ns_1@127.0.0.1:compiled_roles_cache<0.264.0>:menelaus_roles:build_compiled_roles:753]Compile roles for user {"@fts-cbauth",admin}
[ns_server:debug,2020-03-27T20:38:39.235Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{local_changes_count,<<"60a6bc3db77e6c7b91c556140dcfec71">>} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{50,63752560719}}]}]
[ns_server:debug,2020-03-27T20:38:39.235Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{metakv,<<"/fts/cbgt/cfg/nodeDefs-known/60a6bc3db77e6c7b91c556140dcfec71">>} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752560719}}]}|
 <<"{\"uuid\":\"333b694edb5fd6a6\",\"nodeDefs\":{\"60a6bc3db77e6c7b91c556140dcfec71\":{\"hostPort\":\"127.0.0.1:8094\",\"uuid\":\"60a6bc3db77e6c7b91c556140dcfec71\",\"implVersion\":\"5.5.0\",\"tags\":[\"feed\",\"janitor\",\"pindex\",\"queryer\",\"cbauth_service\"],\"container\":\"\",\"weight\":1,\"extras\":\"{\\\"bindGRPC\\\":\\\"127.0.0.1:9130\\\",\\\"bindGRPCSSL\\\":\\\"127.0.0.1:19130\\\",\\\"bindHTTPS\\\":\\\":18094\\\",\\\"features\\\":\\\"leanPlan,index"...>>]
[ns_server:debug,2020-03-27T20:38:39.235Z,ns_1@127.0.0.1:ns_config_rep<0.319.0>:ns_config_rep:do_push_keys:321]Replicating some config keys ([{local_changes_count,
                                   <<"60a6bc3db77e6c7b91c556140dcfec71">>},
                               {metakv,
                                   <<"/fts/cbgt/cfg/nodeDefs-known/60a6bc3db77e6c7b91c556140dcfec71">>}]..)
[ns_server:debug,2020-03-27T20:38:39.247Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{local_changes_count,<<"60a6bc3db77e6c7b91c556140dcfec71">>} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{51,63752560719}}]}]
[ns_server:debug,2020-03-27T20:38:39.247Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{metakv,<<"/fts/cbgt/cfg/nodeDefs-wanted/60a6bc3db77e6c7b91c556140dcfec71">>} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752560719}}]}|
 <<"{\"uuid\":\"31bc9c2adb6acd47\",\"nodeDefs\":{\"60a6bc3db77e6c7b91c556140dcfec71\":{\"hostPort\":\"127.0.0.1:8094\",\"uuid\":\"60a6bc3db77e6c7b91c556140dcfec71\",\"implVersion\":\"5.5.0\",\"tags\":[\"feed\",\"janitor\",\"pindex\",\"queryer\",\"cbauth_service\"],\"container\":\"\",\"weight\":1,\"extras\":\"{\\\"bindGRPC\\\":\\\"127.0.0.1:9130\\\",\\\"bindGRPCSSL\\\":\\\"127.0.0.1:19130\\\",\\\"bindHTTPS\\\":\\\":18094\\\",\\\"features\\\":\\\"leanPlan,index"...>>]
[ns_server:debug,2020-03-27T20:38:39.248Z,ns_1@127.0.0.1:ns_config_rep<0.319.0>:ns_config_rep:do_push_keys:321]Replicating some config keys ([{local_changes_count,
                                   <<"60a6bc3db77e6c7b91c556140dcfec71">>},
                               {metakv,
                                   <<"/fts/cbgt/cfg/nodeDefs-wanted/60a6bc3db77e6c7b91c556140dcfec71">>}]..)
[ns_server:debug,2020-03-27T20:38:39.284Z,ns_1@127.0.0.1:json_rpc_connection-eventing-cbauth<0.1938.0>:json_rpc_connection:init:73]Observed revrpc connection: label "eventing-cbauth", handling process <0.1938.0>
[ns_server:debug,2020-03-27T20:38:39.284Z,ns_1@127.0.0.1:menelaus_cbauth<0.442.0>:menelaus_cbauth:handle_cast:107]Observed json rpc process {"eventing-cbauth",<0.1938.0>} started
[ns_server:debug,2020-03-27T20:38:39.319Z,ns_1@127.0.0.1:ns_config_rep<0.319.0>:ns_config_rep:do_push_keys:321]Replicating some config keys ([{local_changes_count,
                                   <<"60a6bc3db77e6c7b91c556140dcfec71">>},
                               {metakv,<<"/fts/cbgt/cfg/version">>}]..)
[ns_server:debug,2020-03-27T20:38:39.319Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{local_changes_count,<<"60a6bc3db77e6c7b91c556140dcfec71">>} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{52,63752560719}}]}]
[ns_server:debug,2020-03-27T20:38:39.319Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{metakv,<<"/fts/cbgt/cfg/version">>} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752560719}}]}|
 <<"5.5.0">>]
[ns_server:debug,2020-03-27T20:38:39.327Z,ns_1@127.0.0.1:json_rpc_connection-fts-service_api<0.1955.0>:json_rpc_connection:init:73]Observed revrpc connection: label "fts-service_api", handling process <0.1955.0>
[ns_server:debug,2020-03-27T20:38:39.327Z,ns_1@127.0.0.1:service_agent-fts<0.1788.0>:service_agent:do_handle_connection:376]Observed new json rpc connection for fts: <0.1955.0>
[ns_server:debug,2020-03-27T20:38:39.327Z,ns_1@127.0.0.1:<0.1791.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {json_rpc_events,<0.1789.0>} exited with reason normal
[ns_server:debug,2020-03-27T20:38:39.338Z,ns_1@127.0.0.1:json_rpc_connection-cbq-engine-cbauth<0.1960.0>:json_rpc_connection:init:73]Observed revrpc connection: label "cbq-engine-cbauth", handling process <0.1960.0>
[ns_server:debug,2020-03-27T20:38:39.338Z,ns_1@127.0.0.1:menelaus_cbauth<0.442.0>:menelaus_cbauth:handle_cast:107]Observed json rpc process {"cbq-engine-cbauth",<0.1960.0>} started
[ns_server:debug,2020-03-27T20:38:39.475Z,ns_1@127.0.0.1:compiled_roles_cache<0.264.0>:menelaus_roles:build_compiled_roles:753]Compile roles for user {"@eventing-cbauth",admin}
[ns_server:debug,2020-03-27T20:38:39.493Z,ns_1@127.0.0.1:json_rpc_connection-eventing-service_api<0.1974.0>:json_rpc_connection:init:73]Observed revrpc connection: label "eventing-service_api", handling process <0.1974.0>
[ns_server:debug,2020-03-27T20:38:39.493Z,ns_1@127.0.0.1:service_agent-eventing<0.1784.0>:service_agent:do_handle_connection:376]Observed new json rpc connection for eventing: <0.1974.0>
[ns_server:debug,2020-03-27T20:38:39.493Z,ns_1@127.0.0.1:<0.1787.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {json_rpc_events,<0.1785.0>} exited with reason normal
[ns_server:error,2020-03-27T20:38:39.496Z,ns_1@127.0.0.1:query_stats_collector<0.488.0>:rest_utils:get_json:62]Request to (n1ql) /admin/stats failed: {error,
                                        {econnrefused,
                                         [{lhttpc_client,send_request,1,
                                           [{file,
                                             "/home/couchbase/jenkins/workspace/couchbase-server-unix/couchdb/src/lhttpc/lhttpc_client.erl"},
                                            {line,220}]},
                                          {lhttpc_client,execute,9,
                                           [{file,
                                             "/home/couchbase/jenkins/workspace/couchbase-server-unix/couchdb/src/lhttpc/lhttpc_client.erl"},
                                            {line,169}]},
                                          {lhttpc_client,request,9,
                                           [{file,
                                             "/home/couchbase/jenkins/workspace/couchbase-server-unix/couchdb/src/lhttpc/lhttpc_client.erl"},
                                            {line,92}]}]}}
[ns_server:debug,2020-03-27T20:38:39.497Z,ns_1@127.0.0.1:ns_config_rep<0.319.0>:ns_config_rep:do_push_keys:321]Replicating some config keys ([{local_changes_count,
                                   <<"60a6bc3db77e6c7b91c556140dcfec71">>},
                               {metakv,<<"/eventing/settings/config">>}]..)
[ns_server:debug,2020-03-27T20:38:39.498Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{local_changes_count,<<"60a6bc3db77e6c7b91c556140dcfec71">>} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{53,63752560719}}]}]
[ns_server:debug,2020-03-27T20:38:39.498Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{metakv,<<"/eventing/settings/config">>} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752560719}}]}|
 <<"{\n \"enable_debugger\": false,\n \"ram_quota\": 256\n}">>]
[ns_server:debug,2020-03-27T20:38:39.638Z,ns_1@127.0.0.1:service_stats_collector-fts<0.1801.0>:service_stats_collector:check_status:329]Checking if service service_fts is started...
[ns_server:debug,2020-03-27T20:38:39.639Z,ns_1@127.0.0.1:service_stats_collector-eventing<0.1794.0>:service_stats_collector:check_status:329]Checking if service service_eventing is started...
[ns_server:debug,2020-03-27T20:38:39.641Z,ns_1@127.0.0.1:service_stats_collector-fts<0.1801.0>:service_stats_collector:check_status:333]Service service_fts is started
[ns_server:debug,2020-03-27T20:38:39.641Z,ns_1@127.0.0.1:service_stats_collector-eventing<0.1794.0>:service_stats_collector:check_status:333]Service service_eventing is started
[ns_server:debug,2020-03-27T20:38:39.697Z,ns_1@127.0.0.1:compiled_roles_cache<0.264.0>:menelaus_roles:build_compiled_roles:753]Compile roles for user {"@cbq-engine-cbauth",admin}
[ns_server:debug,2020-03-27T20:38:39.716Z,ns_1@127.0.0.1:ns_config_rep<0.319.0>:ns_config_rep:do_push_keys:321]Replicating some config keys ([{local_changes_count,
                                   <<"60a6bc3db77e6c7b91c556140dcfec71">>},
                               {metakv,<<"/query/dictionary_cache/counter">>}]..)
[ns_server:debug,2020-03-27T20:38:39.716Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{metakv,<<"/query/dictionary_cache/counter">>} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752560719}}]}|
 <<":0">>]
[ns_server:debug,2020-03-27T20:38:39.716Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{local_changes_count,<<"60a6bc3db77e6c7b91c556140dcfec71">>} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{54,63752560719}}]}]
[ns_server:debug,2020-03-27T20:38:39.772Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{metakv,<<"/query/functions_cache/counter">>} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752560719}}]}|
 <<"0">>]
[ns_server:debug,2020-03-27T20:38:39.772Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{local_changes_count,<<"60a6bc3db77e6c7b91c556140dcfec71">>} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{55,63752560719}}]}]
[ns_server:debug,2020-03-27T20:38:39.772Z,ns_1@127.0.0.1:ns_config_rep<0.319.0>:ns_config_rep:do_push_keys:321]Replicating some config keys ([{local_changes_count,
                                   <<"60a6bc3db77e6c7b91c556140dcfec71">>},
                               {metakv,<<"/query/functions_cache/counter">>}]..)
[ns_server:debug,2020-03-27T20:38:42.406Z,ns_1@127.0.0.1:compaction_daemon<0.517.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2020-03-27T20:38:42.406Z,ns_1@127.0.0.1:compaction_daemon<0.517.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2020-03-27T20:38:42.407Z,ns_1@127.0.0.1:compaction_daemon<0.517.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2020-03-27T20:38:42.407Z,ns_1@127.0.0.1:compaction_daemon<0.517.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2020-03-27T20:38:42.509Z,ns_1@127.0.0.1:cleanup_process<0.2118.0>:service_janitor:maybe_init_topology_aware_service:80]Doing initial topology change for service `eventing'
[ns_server:debug,2020-03-27T20:38:42.512Z,ns_1@127.0.0.1:service_rebalancer-eventing<0.2121.0>:service_agent:wait_for_agents:74]Waiting for the service agents for service eventing to come up on nodes:
['ns_1@127.0.0.1']
[ns_server:debug,2020-03-27T20:38:42.513Z,ns_1@127.0.0.1:service_rebalancer-eventing<0.2121.0>:service_agent:wait_for_agents_loop:92]All service agents are ready for eventing
[rebalance:info,2020-03-27T20:38:42.513Z,ns_1@127.0.0.1:service_rebalancer-eventing-worker<0.2131.0>:service_rebalancer:rebalance_worker:153]Rebalancing service eventing with id <<"bff810e6ee2f82b0d6857f0eb9a43d83">>.
KeepNodes: ['ns_1@127.0.0.1']
EjectNodes: []
DeltaNodes: []
[ns_server:debug,2020-03-27T20:38:42.514Z,ns_1@127.0.0.1:service_rebalancer-eventing-worker<0.2131.0>:service_rebalancer:rebalance_worker:159]Got node infos:
[{'ns_1@127.0.0.1',[{node_id,<<"60a6bc3db77e6c7b91c556140dcfec71">>},
                    {priority,0},
                    {opaque,null}]}]
[ns_server:debug,2020-03-27T20:38:42.516Z,ns_1@127.0.0.1:service_rebalancer-eventing-worker<0.2131.0>:service_rebalancer:rebalance_worker:168]Using node 'ns_1@127.0.0.1' as a leader
[ns_server:debug,2020-03-27T20:38:42.529Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{local_changes_count,<<"60a6bc3db77e6c7b91c556140dcfec71">>} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{56,63752560722}}]}]
[ns_server:debug,2020-03-27T20:38:42.529Z,ns_1@127.0.0.1:ns_config_rep<0.319.0>:ns_config_rep:do_push_keys:321]Replicating some config keys ([{local_changes_count,
                                   <<"60a6bc3db77e6c7b91c556140dcfec71">>},
                               {metakv,<<"/eventing/config/keepNodes">>}]..)
[ns_server:debug,2020-03-27T20:38:42.530Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{metakv,<<"/eventing/config/keepNodes">>} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752560722}}]}|
 <<"[\"60a6bc3db77e6c7b91c556140dcfec71\"]">>]
[ns_server:debug,2020-03-27T20:38:42.532Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{local_changes_count,<<"60a6bc3db77e6c7b91c556140dcfec71">>} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{57,63752560722}}]}]
[ns_server:debug,2020-03-27T20:38:42.533Z,ns_1@127.0.0.1:ns_config_rep<0.319.0>:ns_config_rep:do_push_keys:321]Replicating some config keys ([{local_changes_count,
                                   <<"60a6bc3db77e6c7b91c556140dcfec71">>}]..)
[ns_server:debug,2020-03-27T20:38:42.533Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{local_changes_count,<<"60a6bc3db77e6c7b91c556140dcfec71">>} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{58,63752560722}}]}]
[ns_server:debug,2020-03-27T20:38:42.533Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{metakv,<<"/eventing/rebalanceToken/bff810e6ee2f82b0d6857f0eb9a43d83">>} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752560722}}]}|
 <<"bff810e6ee2f82b0d6857f0eb9a43d83">>]
[ns_server:debug,2020-03-27T20:38:42.534Z,ns_1@127.0.0.1:ns_config_rep<0.319.0>:ns_config_rep:do_push_keys:321]Replicating some config keys ([{local_changes_count,
                                   <<"60a6bc3db77e6c7b91c556140dcfec71">>},
                               {metakv,
                                   <<"/eventing/rebalanceToken/bff810e6ee2f82b0d6857f0eb9a43d83">>}]..)
[ns_server:debug,2020-03-27T20:38:42.539Z,ns_1@127.0.0.1:service_rebalancer-eventing<0.2121.0>:service_rebalancer:run_rebalance_worker:122]Worker terminated normally
[ns_server:debug,2020-03-27T20:38:42.543Z,ns_1@127.0.0.1:cleanup_process<0.2118.0>:service_janitor:maybe_init_topology_aware_service:83]Initial rebalance for `eventing` finished successfully
[ns_server:debug,2020-03-27T20:38:42.543Z,ns_1@127.0.0.1:cleanup_process<0.2118.0>:service_janitor:maybe_init_topology_aware_service:80]Doing initial topology change for service `fts'
[ns_server:debug,2020-03-27T20:38:42.543Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{local_changes_count,<<"60a6bc3db77e6c7b91c556140dcfec71">>} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{59,63752560722}}]}]
[ns_server:debug,2020-03-27T20:38:42.543Z,ns_1@127.0.0.1:service_rebalancer-fts<0.2164.0>:service_agent:wait_for_agents:74]Waiting for the service agents for service fts to come up on nodes:
['ns_1@127.0.0.1']
[ns_server:debug,2020-03-27T20:38:42.543Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{service_map,eventing} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752560722}}]},
 'ns_1@127.0.0.1']
[ns_server:debug,2020-03-27T20:38:42.543Z,ns_1@127.0.0.1:ns_config_rep<0.319.0>:ns_config_rep:do_push_keys:321]Replicating some config keys ([{local_changes_count,
                                   <<"60a6bc3db77e6c7b91c556140dcfec71">>},
                               {service_map,eventing}]..)
[ns_server:debug,2020-03-27T20:38:42.543Z,ns_1@127.0.0.1:service_rebalancer-fts<0.2164.0>:service_agent:wait_for_agents_loop:92]All service agents are ready for fts
[rebalance:info,2020-03-27T20:38:42.544Z,ns_1@127.0.0.1:service_rebalancer-fts-worker<0.2178.0>:service_rebalancer:rebalance_worker:153]Rebalancing service fts with id <<"4b198d258b028d7d7f888873d4226a0b">>.
KeepNodes: ['ns_1@127.0.0.1']
EjectNodes: []
DeltaNodes: []
[ns_server:debug,2020-03-27T20:38:42.544Z,ns_1@127.0.0.1:terse_cluster_info_uploader<0.467.0>:terse_cluster_info_uploader:handle_info:48]Refreshing terse cluster info with <<"{\"rev\":59,\"nodesExt\":[{\"services\":{\"mgmt\":8091,\"mgmtSSL\":18091,\"eventingAdminPort\":8096,\"eventingDebug\":9140,\"eventingSSL\":18096,\"kv\":11210,\"kvSSL\":11207,\"capi\":8092,\"capiSSL\":18092,\"projector\":9999,\"projector\":9999},\"thisNode\":true}],\"clusterCapabilitiesVer\":[1,0],\"clusterCapabilities\":{\"n1ql\":[\"enhancedPreparedStatements\"]}}">>
[ns_server:debug,2020-03-27T20:38:42.545Z,ns_1@127.0.0.1:service_rebalancer-fts-worker<0.2178.0>:service_rebalancer:rebalance_worker:159]Got node infos:
[{'ns_1@127.0.0.1',[{node_id,<<"60a6bc3db77e6c7b91c556140dcfec71">>},
                    {priority,0},
                    {opaque,null}]}]
[ns_server:debug,2020-03-27T20:38:42.548Z,ns_1@127.0.0.1:service_rebalancer-fts-worker<0.2178.0>:service_rebalancer:rebalance_worker:168]Using node 'ns_1@127.0.0.1' as a leader
[ns_server:debug,2020-03-27T20:38:42.554Z,ns_1@127.0.0.1:service_rebalancer-fts-worker<0.2178.0>:service_janitor:do_orchestrate_initial_rebalance:101]Initial rebalance progress for `fts': [{'ns_1@127.0.0.1',0}]
[ns_server:debug,2020-03-27T20:38:42.587Z,ns_1@127.0.0.1:service_rebalancer-fts<0.2164.0>:service_rebalancer:run_rebalance_worker:122]Worker terminated normally
[ns_server:debug,2020-03-27T20:38:42.591Z,ns_1@127.0.0.1:cleanup_process<0.2118.0>:service_janitor:maybe_init_topology_aware_service:83]Initial rebalance for `fts` finished successfully
[ns_server:debug,2020-03-27T20:38:42.591Z,ns_1@127.0.0.1:cleanup_process<0.2118.0>:service_janitor:maybe_init_topology_aware_service:80]Doing initial topology change for service `index'
[ns_server:debug,2020-03-27T20:38:42.592Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{local_changes_count,<<"60a6bc3db77e6c7b91c556140dcfec71">>} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{60,63752560722}}]}]
[ns_server:debug,2020-03-27T20:38:42.592Z,ns_1@127.0.0.1:service_rebalancer-index<0.2207.0>:service_agent:wait_for_agents:74]Waiting for the service agents for service index to come up on nodes:
['ns_1@127.0.0.1']
[ns_server:debug,2020-03-27T20:38:42.592Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{service_map,fts} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752560722}}]},
 'ns_1@127.0.0.1']
[ns_server:debug,2020-03-27T20:38:42.593Z,ns_1@127.0.0.1:ns_config_rep<0.319.0>:ns_config_rep:do_push_keys:321]Replicating some config keys ([{local_changes_count,
                                   <<"60a6bc3db77e6c7b91c556140dcfec71">>},
                               {service_map,fts}]..)
[ns_server:debug,2020-03-27T20:38:42.593Z,ns_1@127.0.0.1:service_rebalancer-index<0.2207.0>:service_agent:wait_for_agents_loop:92]All service agents are ready for index
[rebalance:info,2020-03-27T20:38:42.594Z,ns_1@127.0.0.1:service_rebalancer-index-worker<0.2218.0>:service_rebalancer:rebalance_worker:153]Rebalancing service index with id <<"a05e32e789d09c3f74007dd7abcb21e7">>.
KeepNodes: ['ns_1@127.0.0.1']
EjectNodes: []
DeltaNodes: []
[ns_server:debug,2020-03-27T20:38:42.594Z,ns_1@127.0.0.1:terse_cluster_info_uploader<0.467.0>:terse_cluster_info_uploader:handle_info:48]Refreshing terse cluster info with <<"{\"rev\":60,\"nodesExt\":[{\"services\":{\"mgmt\":8091,\"mgmtSSL\":18091,\"eventingAdminPort\":8096,\"eventingDebug\":9140,\"eventingSSL\":18096,\"fts\":8094,\"ftsSSL\":18094,\"ftsGRPC\":9130,\"ftsGRPCSSL\":19130,\"kv\":11210,\"kvSSL\":11207,\"capi\":8092,\"capiSSL\":18092,\"projector\":9999,\"projector\":9999},\"thisNode\":true}],\"clusterCapabilitiesVer\":[1,0],\"clusterCapabilities\":{\"n1ql\":[\"enhancedPreparedStatements\"]}}">>
[ns_server:debug,2020-03-27T20:38:42.599Z,ns_1@127.0.0.1:service_rebalancer-index-worker<0.2218.0>:service_rebalancer:rebalance_worker:159]Got node infos:
[{'ns_1@127.0.0.1',[{node_id,<<"60a6bc3db77e6c7b91c556140dcfec71">>},
                    {priority,4},
                    {opaque,null}]}]
[ns_server:debug,2020-03-27T20:38:42.611Z,ns_1@127.0.0.1:service_rebalancer-index-worker<0.2218.0>:service_rebalancer:rebalance_worker:168]Using node 'ns_1@127.0.0.1' as a leader
[ns_server:debug,2020-03-27T20:38:42.626Z,ns_1@127.0.0.1:ns_config_rep<0.319.0>:ns_config_rep:do_push_keys:321]Replicating some config keys ([{local_changes_count,
                                   <<"60a6bc3db77e6c7b91c556140dcfec71">>},
                               {metakv,
                                   <<"/indexing/rebalance/RebalanceToken">>}]..)
[ns_server:debug,2020-03-27T20:38:42.628Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{local_changes_count,<<"60a6bc3db77e6c7b91c556140dcfec71">>} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{61,63752560722}}]}]
[ns_server:debug,2020-03-27T20:38:42.628Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{metakv,<<"/indexing/rebalance/RebalanceToken">>} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752560722}}]}|
 <<"{\"MasterId\":\"60a6bc3db77e6c7b91c556140dcfec71\",\"RebalId\":\"a05e32e789d09c3f74007dd7abcb21e7\",\"Source\":0,\"Error\":\"\",\"MasterIP\":\"127.0.0.1\"}">>]
[ns_server:debug,2020-03-27T20:38:42.645Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{local_changes_count,<<"60a6bc3db77e6c7b91c556140dcfec71">>} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{62,63752560722}}]}]
[ns_server:debug,2020-03-27T20:38:42.646Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{metakv,<<"/indexing/rebalance/RebalanceToken">>} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{2,63752560722}}]}|
 '_deleted']
[ns_server:debug,2020-03-27T20:38:42.646Z,ns_1@127.0.0.1:ns_config_rep<0.319.0>:ns_config_rep:do_push_keys:321]Replicating some config keys ([{local_changes_count,
                                   <<"60a6bc3db77e6c7b91c556140dcfec71">>},
                               {metakv,
                                   <<"/indexing/rebalance/RebalanceToken">>}]..)
[ns_server:debug,2020-03-27T20:38:42.654Z,ns_1@127.0.0.1:service_rebalancer-index<0.2207.0>:service_rebalancer:run_rebalance_worker:122]Worker terminated normally
[ns_server:debug,2020-03-27T20:38:42.659Z,ns_1@127.0.0.1:cleanup_process<0.2118.0>:service_janitor:maybe_init_topology_aware_service:83]Initial rebalance for `index` finished successfully
[ns_server:debug,2020-03-27T20:38:42.659Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{local_changes_count,<<"60a6bc3db77e6c7b91c556140dcfec71">>} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{63,63752560722}}]}]
[ns_server:debug,2020-03-27T20:38:42.659Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{service_map,index} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752560722}}]},
 'ns_1@127.0.0.1']
[ns_server:debug,2020-03-27T20:38:42.660Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{local_changes_count,<<"60a6bc3db77e6c7b91c556140dcfec71">>} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{64,63752560722}}]}]
[ns_server:debug,2020-03-27T20:38:42.660Z,ns_1@127.0.0.1:cleanup_process<0.2118.0>:service_janitor:maybe_init_simple_service:71]Created initial service map for service `n1ql'
[ns_server:debug,2020-03-27T20:38:42.661Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{service_map,n1ql} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752560722}}]},
 'ns_1@127.0.0.1']
[ns_server:debug,2020-03-27T20:38:42.661Z,ns_1@127.0.0.1:ns_config_rep<0.319.0>:ns_config_rep:do_push_keys:321]Replicating some config keys ([{local_changes_count,
                                   <<"60a6bc3db77e6c7b91c556140dcfec71">>},
                               {service_map,index},
                               {service_map,n1ql}]..)
[ns_server:debug,2020-03-27T20:38:42.661Z,ns_1@127.0.0.1:terse_cluster_info_uploader<0.467.0>:terse_cluster_info_uploader:handle_info:48]Refreshing terse cluster info with <<"{\"rev\":63,\"nodesExt\":[{\"services\":{\"mgmt\":8091,\"mgmtSSL\":18091,\"eventingAdminPort\":8096,\"eventingDebug\":9140,\"eventingSSL\":18096,\"fts\":8094,\"ftsSSL\":18094,\"ftsGRPC\":9130,\"ftsGRPCSSL\":19130,\"indexAdmin\":9100,\"indexScan\":9101,\"indexHttp\":9102,\"indexStreamInit\":9103,\"indexStreamCatchup\":9104,\"indexStreamMaint\":9105,\"indexHttps\":19102,\"kv\":11210,\"kvSSL\":11207,\"capi\":8092,\"capiSSL\":18092,\"projector\":9999,\"projector\":9999},\"thisNode\":true}],\"clusterCapabilitiesVer\":[1,0],\"clusterCapabilities\":{\"n1ql\":[\"enhancedPreparedStatements\"]}}">>
[ns_server:debug,2020-03-27T20:38:42.668Z,ns_1@127.0.0.1:terse_cluster_info_uploader<0.467.0>:terse_cluster_info_uploader:handle_info:48]Refreshing terse cluster info with <<"{\"rev\":64,\"nodesExt\":[{\"services\":{\"mgmt\":8091,\"mgmtSSL\":18091,\"eventingAdminPort\":8096,\"eventingDebug\":9140,\"eventingSSL\":18096,\"fts\":8094,\"ftsSSL\":18094,\"ftsGRPC\":9130,\"ftsGRPCSSL\":19130,\"indexAdmin\":9100,\"indexScan\":9101,\"indexHttp\":9102,\"indexStreamInit\":9103,\"indexStreamCatchup\":9104,\"indexStreamMaint\":9105,\"indexHttps\":19102,\"kv\":11210,\"kvSSL\":11207,\"capi\":8092,\"capiSSL\":18092,\"projector\":9999,\"projector\":9999,\"n1ql\":8093,\"n1qlSSL\":18093},\"thisNode\":true}],\"clusterCapabilitiesVer\":[1,0],\"clusterCapabilities\":{\"n1ql\":[\"enhancedPreparedStatements\"]}}">>
[ns_server:debug,2020-03-27T20:38:42.711Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{local_changes_count,<<"60a6bc3db77e6c7b91c556140dcfec71">>} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{65,63752560722}}]}]
[ns_server:debug,2020-03-27T20:38:42.712Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{metakv,<<"/indexing/info/versionToken">>} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752560722}}]}|
 <<"{\"Version\":4}">>]
[ns_server:debug,2020-03-27T20:38:42.712Z,ns_1@127.0.0.1:ns_config_rep<0.319.0>:ns_config_rep:do_push_keys:321]Replicating some config keys ([{local_changes_count,
                                   <<"60a6bc3db77e6c7b91c556140dcfec71">>},
                               {metakv,<<"/indexing/info/versionToken">>}]..)
[ns_server:debug,2020-03-27T20:38:46.510Z,ns_1@127.0.0.1:<0.2141.0>:menelaus_web:check_bucket_uuid:1076]Attempt to access non existent bucket "null"
[ns_server:debug,2020-03-27T20:38:54.430Z,ns_1@127.0.0.1:ldap_auth_cache<0.256.0>:active_cache:cleanup:231]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2020-03-27T20:39:12.407Z,ns_1@127.0.0.1:compaction_daemon<0.517.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2020-03-27T20:39:12.409Z,ns_1@127.0.0.1:compaction_daemon<0.517.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2020-03-27T20:39:12.410Z,ns_1@127.0.0.1:compaction_daemon<0.517.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2020-03-27T20:39:12.411Z,ns_1@127.0.0.1:compaction_daemon<0.517.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2020-03-27T20:39:27.974Z,ns_1@127.0.0.1:<0.2142.0>:menelaus_web:check_bucket_uuid:1076]Attempt to access non existent bucket "null"
[ns_server:debug,2020-03-27T20:39:42.411Z,ns_1@127.0.0.1:compaction_daemon<0.517.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2020-03-27T20:39:42.411Z,ns_1@127.0.0.1:compaction_daemon<0.517.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2020-03-27T20:39:42.412Z,ns_1@127.0.0.1:compaction_daemon<0.517.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2020-03-27T20:39:42.412Z,ns_1@127.0.0.1:compaction_daemon<0.517.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2020-03-27T20:40:09.430Z,ns_1@127.0.0.1:ldap_auth_cache<0.256.0>:active_cache:cleanup:231]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2020-03-27T20:40:12.415Z,ns_1@127.0.0.1:compaction_daemon<0.517.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2020-03-27T20:40:12.415Z,ns_1@127.0.0.1:compaction_daemon<0.517.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2020-03-27T20:40:12.415Z,ns_1@127.0.0.1:compaction_daemon<0.517.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2020-03-27T20:40:12.415Z,ns_1@127.0.0.1:compaction_daemon<0.517.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2020-03-27T20:40:42.420Z,ns_1@127.0.0.1:compaction_daemon<0.517.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2020-03-27T20:40:42.420Z,ns_1@127.0.0.1:compaction_daemon<0.517.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2020-03-27T20:40:42.420Z,ns_1@127.0.0.1:compaction_daemon<0.517.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2020-03-27T20:40:42.420Z,ns_1@127.0.0.1:compaction_daemon<0.517.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2020-03-27T20:40:52.804Z,ns_1@127.0.0.1:<0.4966.0>:menelaus_web:check_bucket_uuid:1076]Attempt to access non existent bucket "null"
[ns_server:debug,2020-03-27T20:41:12.421Z,ns_1@127.0.0.1:compaction_daemon<0.517.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2020-03-27T20:41:12.421Z,ns_1@127.0.0.1:compaction_daemon<0.517.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2020-03-27T20:41:12.421Z,ns_1@127.0.0.1:compaction_daemon<0.517.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2020-03-27T20:41:12.421Z,ns_1@127.0.0.1:compaction_daemon<0.517.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2020-03-27T20:41:24.432Z,ns_1@127.0.0.1:ldap_auth_cache<0.256.0>:active_cache:cleanup:231]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2020-03-27T20:41:42.421Z,ns_1@127.0.0.1:compaction_daemon<0.517.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2020-03-27T20:41:42.421Z,ns_1@127.0.0.1:compaction_daemon<0.517.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2020-03-27T20:41:42.421Z,ns_1@127.0.0.1:compaction_daemon<0.517.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2020-03-27T20:41:42.421Z,ns_1@127.0.0.1:compaction_daemon<0.517.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2020-03-27T20:42:12.422Z,ns_1@127.0.0.1:compaction_daemon<0.517.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2020-03-27T20:42:12.422Z,ns_1@127.0.0.1:compaction_daemon<0.517.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2020-03-27T20:42:12.422Z,ns_1@127.0.0.1:compaction_daemon<0.517.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2020-03-27T20:42:12.422Z,ns_1@127.0.0.1:compaction_daemon<0.517.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2020-03-27T20:42:39.434Z,ns_1@127.0.0.1:ldap_auth_cache<0.256.0>:active_cache:cleanup:231]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2020-03-27T20:42:42.424Z,ns_1@127.0.0.1:compaction_daemon<0.517.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2020-03-27T20:42:42.424Z,ns_1@127.0.0.1:compaction_daemon<0.517.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2020-03-27T20:42:42.424Z,ns_1@127.0.0.1:compaction_daemon<0.517.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2020-03-27T20:42:42.424Z,ns_1@127.0.0.1:compaction_daemon<0.517.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2020-03-27T20:43:12.427Z,ns_1@127.0.0.1:compaction_daemon<0.517.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2020-03-27T20:43:12.427Z,ns_1@127.0.0.1:compaction_daemon<0.517.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2020-03-27T20:43:12.427Z,ns_1@127.0.0.1:compaction_daemon<0.517.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2020-03-27T20:43:12.427Z,ns_1@127.0.0.1:compaction_daemon<0.517.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2020-03-27T20:43:42.428Z,ns_1@127.0.0.1:compaction_daemon<0.517.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2020-03-27T20:43:42.428Z,ns_1@127.0.0.1:compaction_daemon<0.517.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2020-03-27T20:43:42.428Z,ns_1@127.0.0.1:compaction_daemon<0.517.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2020-03-27T20:43:42.428Z,ns_1@127.0.0.1:compaction_daemon<0.517.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2020-03-27T20:43:54.436Z,ns_1@127.0.0.1:ldap_auth_cache<0.256.0>:active_cache:cleanup:231]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2020-03-27T20:44:12.429Z,ns_1@127.0.0.1:compaction_daemon<0.517.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2020-03-27T20:44:12.429Z,ns_1@127.0.0.1:compaction_daemon<0.517.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2020-03-27T20:44:12.429Z,ns_1@127.0.0.1:compaction_daemon<0.517.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2020-03-27T20:44:12.430Z,ns_1@127.0.0.1:compaction_daemon<0.517.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2020-03-27T20:44:42.431Z,ns_1@127.0.0.1:compaction_daemon<0.517.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2020-03-27T20:44:42.431Z,ns_1@127.0.0.1:compaction_daemon<0.517.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2020-03-27T20:44:42.431Z,ns_1@127.0.0.1:compaction_daemon<0.517.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2020-03-27T20:44:42.431Z,ns_1@127.0.0.1:compaction_daemon<0.517.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2020-03-27T20:45:09.438Z,ns_1@127.0.0.1:ldap_auth_cache<0.256.0>:active_cache:cleanup:231]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2020-03-27T20:45:09.464Z,ns_1@127.0.0.1:roles_cache<0.267.0>:active_cache:renew:211]Starting roles_cache cache renewal
[ns_server:debug,2020-03-27T20:45:09.464Z,ns_1@127.0.0.1:roles_cache<0.267.0>:active_cache:renew:217]roles_cache cache renewal process originated 0 queries
[ns_server:debug,2020-03-27T20:45:12.433Z,ns_1@127.0.0.1:compaction_daemon<0.517.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2020-03-27T20:45:12.433Z,ns_1@127.0.0.1:compaction_daemon<0.517.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2020-03-27T20:45:12.434Z,ns_1@127.0.0.1:compaction_daemon<0.517.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2020-03-27T20:45:12.434Z,ns_1@127.0.0.1:compaction_daemon<0.517.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2020-03-27T20:45:42.435Z,ns_1@127.0.0.1:compaction_daemon<0.517.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2020-03-27T20:45:42.435Z,ns_1@127.0.0.1:compaction_daemon<0.517.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2020-03-27T20:45:42.436Z,ns_1@127.0.0.1:compaction_daemon<0.517.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2020-03-27T20:45:42.436Z,ns_1@127.0.0.1:compaction_daemon<0.517.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2020-03-27T20:46:12.439Z,ns_1@127.0.0.1:compaction_daemon<0.517.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2020-03-27T20:46:12.439Z,ns_1@127.0.0.1:compaction_daemon<0.517.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2020-03-27T20:46:12.440Z,ns_1@127.0.0.1:compaction_daemon<0.517.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2020-03-27T20:46:12.440Z,ns_1@127.0.0.1:compaction_daemon<0.517.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2020-03-27T20:46:24.439Z,ns_1@127.0.0.1:ldap_auth_cache<0.256.0>:active_cache:cleanup:231]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2020-03-27T20:46:27.561Z,ns_1@127.0.0.1:<0.15674.0>:menelaus_web:check_bucket_uuid:1076]Attempt to access non existent bucket "null"
[ns_server:debug,2020-03-27T20:46:30.762Z,ns_1@127.0.0.1:users_storage<0.262.0>:replicated_dets:handle_call:302]Suspended by process <0.15846.0>
[ns_server:debug,2020-03-27T20:46:30.762Z,ns_1@127.0.0.1:<0.15846.0>:replicated_dets:select_from_dets_locked:350]Starting select with {users_storage,[{{docv2,{user,{'_','_'}},'_','_'},
                                      [],
                                      ['$_']}],
                                    100}
[ns_server:debug,2020-03-27T20:46:30.763Z,ns_1@127.0.0.1:users_storage<0.262.0>:replicated_dets:handle_call:309]Released by process <0.15846.0>
[ns_server:debug,2020-03-27T20:46:30.763Z,ns_1@127.0.0.1:users_storage<0.262.0>:replicated_dets:handle_call:302]Suspended by process <0.15283.0>
[ns_server:debug,2020-03-27T20:46:30.763Z,ns_1@127.0.0.1:<0.15283.0>:replicated_dets:select_from_dets_locked:350]Starting select with {users_storage,[{{docv2,{user,{'_','_'}},'_','_'},
                                      [],
                                      ['$_']}],
                                    100}
[ns_server:debug,2020-03-27T20:46:30.764Z,ns_1@127.0.0.1:users_storage<0.262.0>:replicated_dets:handle_call:309]Released by process <0.15283.0>
[ns_server:debug,2020-03-27T20:46:30.764Z,ns_1@127.0.0.1:users_storage<0.262.0>:replicated_dets:handle_call:302]Suspended by process <0.15221.0>
[ns_server:debug,2020-03-27T20:46:30.764Z,ns_1@127.0.0.1:<0.15221.0>:replicated_dets:select_from_dets_locked:350]Starting select with {users_storage,[{{docv2,{user,{'_','_'}},'_','_'},
                                      [],
                                      ['$_']}],
                                    100}
[ns_server:debug,2020-03-27T20:46:30.765Z,ns_1@127.0.0.1:users_storage<0.262.0>:replicated_dets:handle_call:309]Released by process <0.15221.0>
[ns_server:debug,2020-03-27T20:46:30.765Z,ns_1@127.0.0.1:users_storage<0.262.0>:replicated_dets:handle_call:302]Suspended by process <0.14826.0>
[ns_server:debug,2020-03-27T20:46:30.765Z,ns_1@127.0.0.1:<0.14826.0>:replicated_dets:select_from_dets_locked:350]Starting select with {users_storage,[{{docv2,{user,{'_','_'}},'_','_'},
                                      [],
                                      ['$_']}],
                                    100}
[ns_server:debug,2020-03-27T20:46:30.765Z,ns_1@127.0.0.1:users_storage<0.262.0>:replicated_dets:handle_call:309]Released by process <0.14826.0>
[ns_server:debug,2020-03-27T20:46:30.765Z,ns_1@127.0.0.1:users_storage<0.262.0>:replicated_dets:handle_call:302]Suspended by process <0.15674.0>
[ns_server:debug,2020-03-27T20:46:30.766Z,ns_1@127.0.0.1:<0.15674.0>:replicated_dets:select_from_dets_locked:350]Starting select with {users_storage,[{{docv2,{user,{'_','_'}},'_','_'},
                                      [],
                                      ['$_']}],
                                    100}
[ns_server:debug,2020-03-27T20:46:30.766Z,ns_1@127.0.0.1:users_storage<0.262.0>:replicated_dets:handle_call:309]Released by process <0.15674.0>
[ns_server:debug,2020-03-27T20:46:30.767Z,ns_1@127.0.0.1:users_storage<0.262.0>:replicated_dets:handle_call:302]Suspended by process <0.15846.0>
[ns_server:debug,2020-03-27T20:46:30.768Z,ns_1@127.0.0.1:<0.15846.0>:replicated_dets:select_from_dets_locked:350]Starting select with {users_storage,[{{docv2,{user,{'_','_'}},'_','_'},
                                      [],
                                      ['$_']}],
                                    100}
[ns_server:debug,2020-03-27T20:46:30.771Z,ns_1@127.0.0.1:users_storage<0.262.0>:replicated_dets:handle_call:309]Released by process <0.15846.0>
[ns_server:debug,2020-03-27T20:46:30.774Z,ns_1@127.0.0.1:users_storage<0.262.0>:replicated_dets:handle_call:302]Suspended by process <0.15283.0>
[ns_server:debug,2020-03-27T20:46:30.774Z,ns_1@127.0.0.1:<0.15283.0>:replicated_dets:select_from_dets_locked:350]Starting select with {users_storage,[{{docv2,{user,{'_','_'}},'_','_'},
                                      [],
                                      ['$_']}],
                                    100}
[ns_server:debug,2020-03-27T20:46:30.774Z,ns_1@127.0.0.1:users_storage<0.262.0>:replicated_dets:handle_call:309]Released by process <0.15283.0>
[ns_server:debug,2020-03-27T20:46:30.774Z,ns_1@127.0.0.1:users_storage<0.262.0>:replicated_dets:handle_call:302]Suspended by process <0.15674.0>
[ns_server:debug,2020-03-27T20:46:30.775Z,ns_1@127.0.0.1:<0.15674.0>:replicated_dets:select_from_dets_locked:350]Starting select with {users_storage,[{{docv2,{user,{'_','_'}},'_','_'},
                                      [],
                                      ['$_']}],
                                    100}
[ns_server:debug,2020-03-27T20:46:30.775Z,ns_1@127.0.0.1:users_storage<0.262.0>:replicated_dets:handle_call:309]Released by process <0.15674.0>
[ns_server:debug,2020-03-27T20:46:30.775Z,ns_1@127.0.0.1:users_storage<0.262.0>:replicated_dets:handle_call:302]Suspended by process <0.15221.0>
[ns_server:debug,2020-03-27T20:46:30.775Z,ns_1@127.0.0.1:<0.15221.0>:replicated_dets:select_from_dets_locked:350]Starting select with {users_storage,[{{docv2,{user,{'_','_'}},'_','_'},
                                      [],
                                      ['$_']}],
                                    100}
[ns_server:debug,2020-03-27T20:46:30.776Z,ns_1@127.0.0.1:users_storage<0.262.0>:replicated_dets:handle_call:309]Released by process <0.15221.0>
[ns_server:debug,2020-03-27T20:46:30.776Z,ns_1@127.0.0.1:users_storage<0.262.0>:replicated_dets:handle_call:302]Suspended by process <0.14826.0>
[ns_server:debug,2020-03-27T20:46:30.776Z,ns_1@127.0.0.1:<0.14826.0>:replicated_dets:select_from_dets_locked:350]Starting select with {users_storage,[{{docv2,{user,{'_','_'}},'_','_'},
                                      [],
                                      ['$_']}],
                                    100}
[ns_server:debug,2020-03-27T20:46:30.779Z,ns_1@127.0.0.1:users_storage<0.262.0>:replicated_dets:handle_call:309]Released by process <0.14826.0>
[ns_server:debug,2020-03-27T20:46:30.779Z,ns_1@127.0.0.1:users_storage<0.262.0>:replicated_dets:handle_call:302]Suspended by process <0.15846.0>
[ns_server:debug,2020-03-27T20:46:30.779Z,ns_1@127.0.0.1:<0.15846.0>:replicated_dets:select_from_dets_locked:350]Starting select with {users_storage,[{{docv2,{user,{'_','_'}},'_','_'},
                                      [],
                                      ['$_']}],
                                    100}
[ns_server:debug,2020-03-27T20:46:30.779Z,ns_1@127.0.0.1:users_storage<0.262.0>:replicated_dets:handle_call:309]Released by process <0.15846.0>
[ns_server:debug,2020-03-27T20:46:30.784Z,ns_1@127.0.0.1:users_storage<0.262.0>:replicated_dets:handle_call:302]Suspended by process <0.15283.0>
[ns_server:debug,2020-03-27T20:46:30.785Z,ns_1@127.0.0.1:<0.15283.0>:replicated_dets:select_from_dets_locked:350]Starting select with {users_storage,[{{docv2,{user,{'_','_'}},'_','_'},
                                      [],
                                      ['$_']}],
                                    100}
[ns_server:debug,2020-03-27T20:46:30.785Z,ns_1@127.0.0.1:users_storage<0.262.0>:replicated_dets:handle_call:309]Released by process <0.15283.0>
[ns_server:debug,2020-03-27T20:46:30.786Z,ns_1@127.0.0.1:users_storage<0.262.0>:replicated_dets:handle_call:302]Suspended by process <0.15674.0>
[ns_server:debug,2020-03-27T20:46:30.786Z,ns_1@127.0.0.1:<0.15674.0>:replicated_dets:select_from_dets_locked:350]Starting select with {users_storage,[{{docv2,{user,{'_','_'}},'_','_'},
                                      [],
                                      ['$_']}],
                                    100}
[ns_server:debug,2020-03-27T20:46:30.786Z,ns_1@127.0.0.1:users_storage<0.262.0>:replicated_dets:handle_call:309]Released by process <0.15674.0>
[ns_server:debug,2020-03-27T20:46:30.786Z,ns_1@127.0.0.1:users_storage<0.262.0>:replicated_dets:handle_call:302]Suspended by process <0.15221.0>
[ns_server:debug,2020-03-27T20:46:30.786Z,ns_1@127.0.0.1:<0.15221.0>:replicated_dets:select_from_dets_locked:350]Starting select with {users_storage,[{{docv2,{user,{'_','_'}},'_','_'},
                                      [],
                                      ['$_']}],
                                    100}
[ns_server:debug,2020-03-27T20:46:30.786Z,ns_1@127.0.0.1:users_storage<0.262.0>:replicated_dets:handle_call:309]Released by process <0.15221.0>
[ns_server:debug,2020-03-27T20:46:30.788Z,ns_1@127.0.0.1:users_storage<0.262.0>:replicated_dets:handle_call:302]Suspended by process <0.14826.0>
[ns_server:debug,2020-03-27T20:46:30.788Z,ns_1@127.0.0.1:<0.14826.0>:replicated_dets:select_from_dets_locked:350]Starting select with {users_storage,[{{docv2,{user,{'_','_'}},'_','_'},
                                      [],
                                      ['$_']}],
                                    100}
[ns_server:debug,2020-03-27T20:46:30.788Z,ns_1@127.0.0.1:users_storage<0.262.0>:replicated_dets:handle_call:309]Released by process <0.14826.0>
[ns_server:debug,2020-03-27T20:46:30.788Z,ns_1@127.0.0.1:users_storage<0.262.0>:replicated_dets:handle_call:302]Suspended by process <0.15846.0>
[ns_server:debug,2020-03-27T20:46:30.788Z,ns_1@127.0.0.1:<0.15846.0>:replicated_dets:select_from_dets_locked:350]Starting select with {users_storage,[{{docv2,{user,{'_','_'}},'_','_'},
                                      [],
                                      ['$_']}],
                                    100}
[ns_server:debug,2020-03-27T20:46:30.788Z,ns_1@127.0.0.1:users_storage<0.262.0>:replicated_dets:handle_call:309]Released by process <0.15846.0>
[ns_server:debug,2020-03-27T20:46:30.789Z,ns_1@127.0.0.1:users_storage<0.262.0>:replicated_dets:handle_call:302]Suspended by process <0.15283.0>
[ns_server:debug,2020-03-27T20:46:30.789Z,ns_1@127.0.0.1:<0.15283.0>:replicated_dets:select_from_dets_locked:350]Starting select with {users_storage,[{{docv2,{user,{'_','_'}},'_','_'},
                                      [],
                                      ['$_']}],
                                    100}
[ns_server:debug,2020-03-27T20:46:30.789Z,ns_1@127.0.0.1:users_storage<0.262.0>:replicated_dets:handle_call:309]Released by process <0.15283.0>
[ns_server:debug,2020-03-27T20:46:30.791Z,ns_1@127.0.0.1:users_storage<0.262.0>:replicated_dets:handle_call:302]Suspended by process <0.15674.0>
[ns_server:debug,2020-03-27T20:46:30.791Z,ns_1@127.0.0.1:<0.15674.0>:replicated_dets:select_from_dets_locked:350]Starting select with {users_storage,[{{docv2,{user,{'_','_'}},'_','_'},
                                      [],
                                      ['$_']}],
                                    100}
[ns_server:debug,2020-03-27T20:46:30.791Z,ns_1@127.0.0.1:users_storage<0.262.0>:replicated_dets:handle_call:309]Released by process <0.15674.0>
[ns_server:debug,2020-03-27T20:46:30.791Z,ns_1@127.0.0.1:users_storage<0.262.0>:replicated_dets:handle_call:302]Suspended by process <0.15221.0>
[ns_server:debug,2020-03-27T20:46:30.791Z,ns_1@127.0.0.1:<0.15221.0>:replicated_dets:select_from_dets_locked:350]Starting select with {users_storage,[{{docv2,{user,{'_','_'}},'_','_'},
                                      [],
                                      ['$_']}],
                                    100}
[ns_server:debug,2020-03-27T20:46:30.791Z,ns_1@127.0.0.1:users_storage<0.262.0>:replicated_dets:handle_call:309]Released by process <0.15221.0>
[ns_server:debug,2020-03-27T20:46:30.795Z,ns_1@127.0.0.1:users_storage<0.262.0>:replicated_dets:handle_call:302]Suspended by process <0.15846.0>
[ns_server:debug,2020-03-27T20:46:30.795Z,ns_1@127.0.0.1:<0.15846.0>:replicated_dets:select_from_dets_locked:350]Starting select with {users_storage,[{{docv2,{user,{'_','_'}},'_','_'},
                                      [],
                                      ['$_']}],
                                    100}
[ns_server:debug,2020-03-27T20:46:30.796Z,ns_1@127.0.0.1:users_storage<0.262.0>:replicated_dets:handle_call:309]Released by process <0.15846.0>
[ns_server:debug,2020-03-27T20:46:30.796Z,ns_1@127.0.0.1:users_storage<0.262.0>:replicated_dets:handle_call:302]Suspended by process <0.15283.0>
[ns_server:debug,2020-03-27T20:46:30.796Z,ns_1@127.0.0.1:<0.15283.0>:replicated_dets:select_from_dets_locked:350]Starting select with {users_storage,[{{docv2,{user,{'_','_'}},'_','_'},
                                      [],
                                      ['$_']}],
                                    100}
[ns_server:debug,2020-03-27T20:46:30.796Z,ns_1@127.0.0.1:users_storage<0.262.0>:replicated_dets:handle_call:309]Released by process <0.15283.0>
[ns_server:debug,2020-03-27T20:46:30.796Z,ns_1@127.0.0.1:users_storage<0.262.0>:replicated_dets:handle_call:302]Suspended by process <0.14826.0>
[ns_server:debug,2020-03-27T20:46:30.796Z,ns_1@127.0.0.1:<0.14826.0>:replicated_dets:select_from_dets_locked:350]Starting select with {users_storage,[{{docv2,{user,{'_','_'}},'_','_'},
                                      [],
                                      ['$_']}],
                                    100}
[ns_server:debug,2020-03-27T20:46:30.797Z,ns_1@127.0.0.1:users_storage<0.262.0>:replicated_dets:handle_call:309]Released by process <0.14826.0>
[ns_server:debug,2020-03-27T20:46:30.802Z,ns_1@127.0.0.1:users_storage<0.262.0>:replicated_dets:handle_call:302]Suspended by process <0.15221.0>
[ns_server:debug,2020-03-27T20:46:30.802Z,ns_1@127.0.0.1:<0.15221.0>:replicated_dets:select_from_dets_locked:350]Starting select with {users_storage,[{{docv2,{user,{'_','_'}},'_','_'},
                                      [],
                                      ['$_']}],
                                    100}
[ns_server:debug,2020-03-27T20:46:30.802Z,ns_1@127.0.0.1:users_storage<0.262.0>:replicated_dets:handle_call:309]Released by process <0.15221.0>
[ns_server:debug,2020-03-27T20:46:30.802Z,ns_1@127.0.0.1:users_storage<0.262.0>:replicated_dets:handle_call:302]Suspended by process <0.15283.0>
[ns_server:debug,2020-03-27T20:46:30.802Z,ns_1@127.0.0.1:<0.15283.0>:replicated_dets:select_from_dets_locked:350]Starting select with {users_storage,[{{docv2,{user,{'_','_'}},'_','_'},
                                      [],
                                      ['$_']}],
                                    100}
[ns_server:debug,2020-03-27T20:46:30.802Z,ns_1@127.0.0.1:users_storage<0.262.0>:replicated_dets:handle_call:309]Released by process <0.15283.0>
[ns_server:debug,2020-03-27T20:46:30.802Z,ns_1@127.0.0.1:users_storage<0.262.0>:replicated_dets:handle_call:302]Suspended by process <0.14826.0>
[ns_server:debug,2020-03-27T20:46:30.803Z,ns_1@127.0.0.1:<0.14826.0>:replicated_dets:select_from_dets_locked:350]Starting select with {users_storage,[{{docv2,{user,{'_','_'}},'_','_'},
                                      [],
                                      ['$_']}],
                                    100}
[ns_server:debug,2020-03-27T20:46:30.803Z,ns_1@127.0.0.1:users_storage<0.262.0>:replicated_dets:handle_call:309]Released by process <0.14826.0>
[ns_server:debug,2020-03-27T20:46:30.803Z,ns_1@127.0.0.1:users_storage<0.262.0>:replicated_dets:handle_call:302]Suspended by process <0.15846.0>
[ns_server:debug,2020-03-27T20:46:30.803Z,ns_1@127.0.0.1:<0.15846.0>:replicated_dets:select_from_dets_locked:350]Starting select with {users_storage,[{{docv2,{user,{'_','_'}},'_','_'},
                                      [],
                                      ['$_']}],
                                    100}
[ns_server:debug,2020-03-27T20:46:30.803Z,ns_1@127.0.0.1:users_storage<0.262.0>:replicated_dets:handle_call:309]Released by process <0.15846.0>
[ns_server:debug,2020-03-27T20:46:30.803Z,ns_1@127.0.0.1:users_storage<0.262.0>:replicated_dets:handle_call:302]Suspended by process <0.15674.0>
[ns_server:debug,2020-03-27T20:46:30.803Z,ns_1@127.0.0.1:<0.15674.0>:replicated_dets:select_from_dets_locked:350]Starting select with {users_storage,[{{docv2,{user,{'_','_'}},'_','_'},
                                      [],
                                      ['$_']}],
                                    100}
[ns_server:debug,2020-03-27T20:46:30.804Z,ns_1@127.0.0.1:users_storage<0.262.0>:replicated_dets:handle_call:309]Released by process <0.15674.0>
[ns_server:debug,2020-03-27T20:46:34.220Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{local_changes_count,<<"60a6bc3db77e6c7b91c556140dcfec71">>} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{66,63752561194}}]}]
[ns_server:debug,2020-03-27T20:46:34.220Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
buckets ->
[[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{3,63752561194}}],
 {configs,[[{map,[]},
            {fastForwardMap,[]},
            {uuid,<<"e080f4429e7bd7d930bb34ead0abb5dc">>},
            {auth_type,sasl},
            {num_replicas,1},
            {replica_index,false},
            {ram_quota,306184192},
            {autocompaction,false},
            {purge_interval,undefined},
            {flush_enabled,false},
            {num_threads,3},
            {eviction_policy,value_only},
            {conflict_resolution_type,seqno},
            {storage_mode,couchstore},
            {max_ttl,0},
            {compression_mode,passive},
            {type,membase},
            {num_vbuckets,1024},
            {replication_topology,star},
            {repl_type,dcp},
            {servers,[]},
            {sasl_password,"*****"}]]}]
[ns_server:debug,2020-03-27T20:46:34.221Z,ns_1@127.0.0.1:ns_config_rep<0.319.0>:ns_config_rep:do_push_keys:321]Replicating some config keys ([buckets,
                               {local_changes_count,
                                   <<"60a6bc3db77e6c7b91c556140dcfec71">>}]..)
[ns_server:debug,2020-03-27T20:46:34.222Z,ns_1@127.0.0.1:compiled_roles_cache<0.264.0>:versioned_cache:handle_info:92]Flushing cache compiled_roles_cache due to version change from {[6,5],
                                                                {0,1940836693},
                                                                {0,1940836693},
                                                                true,[]} to {[6,
                                                                              5],
                                                                             {0,
                                                                              1940836693},
                                                                             {0,
                                                                              1940836693},
                                                                             true,
                                                                             [{"HelloServiceBucket",
                                                                               <<"e080f4429e7bd7d930bb34ead0abb5dc">>}]}
[ns_server:debug,2020-03-27T20:46:34.223Z,ns_1@127.0.0.1:ns_audit<0.463.0>:ns_audit:handle_call:125]Audit create_bucket: [{props,{[{compression_mode,passive},
                               {max_ttl,0},
                               {storage_mode,couchstore},
                               {conflict_resolution_type,seqno},
                               {eviction_policy,value_only},
                               {num_threads,3},
                               {flush_enabled,false},
                               {purge_interval,undefined},
                               {ram_quota,306184192},
                               {replica_index,false},
                               {num_replicas,1}]}},
                      {type,membase},
                      {bucket_name,<<"HelloServiceBucket">>},
                      {real_userid,{[{domain,builtin},
                                     {user,<<"<ud>Administrator</ud>">>}]}},
                      {sessionid,<<"c0ff51a51891316eeaab059a29e5a2cd">>},
                      {remote,{[{ip,<<"192.168.80.1">>},{port,51420}]}},
                      {timestamp,<<"2020-03-27T20:46:34.222Z">>}]
[menelaus:info,2020-03-27T20:46:34.224Z,ns_1@127.0.0.1:<0.15846.0>:menelaus_web_buckets:do_bucket_create:645]Created bucket "HelloServiceBucket" of type: couchbase
[{num_replicas,1},
 {replica_index,false},
 {ram_quota,306184192},
 {autocompaction,false},
 {purge_interval,undefined},
 {flush_enabled,false},
 {num_threads,3},
 {eviction_policy,value_only},
 {conflict_resolution_type,seqno},
 {storage_mode,couchstore},
 {max_ttl,0},
 {compression_mode,passive}]
[ns_server:debug,2020-03-27T20:46:34.226Z,ns_1@127.0.0.1:memcached_passwords<0.305.0>:memcached_cfg:write_cfg:118]Writing config file for: "/opt/couchbase/var/lib/couchbase/isasl.pw"
[ns_server:debug,2020-03-27T20:46:34.226Z,ns_1@127.0.0.1:memcached_permissions<0.308.0>:memcached_cfg:write_cfg:118]Writing config file for: "/opt/couchbase/var/lib/couchbase/config/memcached.rbac"
[ns_server:debug,2020-03-27T20:46:34.244Z,ns_1@127.0.0.1:<0.16611.0>:ns_janitor:update_servers:86]janitor decided to update servers list for bucket "HelloServiceBucket" to ['ns_1@127.0.0.1']
[ns_server:debug,2020-03-27T20:46:34.244Z,ns_1@127.0.0.1:<0.16611.0>:ns_janitor:config_sync:259]Going to push config to/from nodes:
['ns_1@127.0.0.1']
[ns_server:debug,2020-03-27T20:46:34.245Z,ns_1@127.0.0.1:ns_bucket_worker<0.473.0>:ns_bucket_worker:start_one_bucket:127]Starting new bucket: "HelloServiceBucket"
[ns_server:debug,2020-03-27T20:46:34.245Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{local_changes_count,<<"60a6bc3db77e6c7b91c556140dcfec71">>} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{67,63752561194}}]}]
[ns_server:debug,2020-03-27T20:46:34.246Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
buckets ->
[[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{4,63752561194}}],
 {configs,[{"HelloServiceBucket",
            [{map,[]},
             {fastForwardMap,[]},
             {uuid,<<"e080f4429e7bd7d930bb34ead0abb5dc">>},
             {auth_type,sasl},
             {num_replicas,1},
             {replica_index,false},
             {ram_quota,306184192},
             {autocompaction,false},
             {purge_interval,undefined},
             {flush_enabled,false},
             {num_threads,3},
             {eviction_policy,value_only},
             {conflict_resolution_type,seqno},
             {storage_mode,couchstore},
             {max_ttl,0},
             {compression_mode,passive},
             {type,membase},
             {num_vbuckets,1024},
             {replication_topology,star},
             {repl_type,dcp},
             {servers,['ns_1@127.0.0.1']},
             {sasl_password,"*****"}]}]}]
[ns_server:debug,2020-03-27T20:46:34.249Z,ns_1@127.0.0.1:ns_config_rep<0.319.0>:ns_config_rep:do_push_keys:321]Replicating some config keys ([buckets,
                               {local_changes_count,
                                   <<"60a6bc3db77e6c7b91c556140dcfec71">>}]..)
[ns_server:debug,2020-03-27T20:46:34.250Z,ns_1@127.0.0.1:ns_config_rep<0.319.0>:ns_config_rep:handle_call:122]Got full synchronization request from 'ns_1@127.0.0.1'
[ns_server:debug,2020-03-27T20:46:34.250Z,ns_1@127.0.0.1:ns_config_rep<0.319.0>:ns_config_rep:handle_call:128]Fully synchronized config in 13 us
[ns_server:info,2020-03-27T20:46:34.251Z,ns_1@127.0.0.1:<0.16622.0>:ns_janitor:cleanup_with_membase_bucket_check_map:98]janitor decided to generate initial vbucket map
[ns_server:debug,2020-03-27T20:46:34.255Z,ns_1@127.0.0.1:users_storage<0.262.0>:replicated_dets:handle_call:302]Suspended by process <0.308.0>
[ns_server:debug,2020-03-27T20:46:34.256Z,ns_1@127.0.0.1:memcached_permissions<0.308.0>:replicated_dets:select_from_dets_locked:350]Starting select with {users_storage,[{{docv2,{user,{'_',local}},'_','_'},
                                      [],
                                      ['$_']}],
                                    100}
[ns_server:debug,2020-03-27T20:46:34.262Z,ns_1@127.0.0.1:users_storage<0.262.0>:replicated_dets:handle_call:309]Released by process <0.308.0>
[error_logger:info,2020-03-27T20:46:34.264Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,service_stats_children_sup}
             started: [{pid,<0.16599.0>},
                       {id,{service_eventing,stats_archiver,
                               "HelloServiceBucket"}},
                       {mfargs,
                           {stats_archiver,start_link,
                               ["@eventing-HelloServiceBucket"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-03-27T20:46:34.266Z,ns_1@127.0.0.1:compiled_roles_cache<0.264.0>:menelaus_roles:build_compiled_roles:753]Compile roles for user {"<ud>Administrator</ud>",admin}
[error_logger:info,2020-03-27T20:46:34.274Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,service_stats_children_sup}
             started: [{pid,<0.16625.0>},
                       {id,{service_eventing,stats_reader,
                               "HelloServiceBucket"}},
                       {mfargs,
                           {stats_reader,start_link,
                               ["@eventing-HelloServiceBucket"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-03-27T20:46:34.277Z,ns_1@127.0.0.1:compiled_roles_cache<0.264.0>:menelaus_roles:build_compiled_roles:753]Compile roles for user {"@cbq-engine-cbauth",admin}
[ns_server:debug,2020-03-27T20:46:34.278Z,ns_1@127.0.0.1:compiled_roles_cache<0.264.0>:menelaus_roles:build_compiled_roles:753]Compile roles for user {"@index-cbauth",admin}
[ns_server:debug,2020-03-27T20:46:34.291Z,ns_1@127.0.0.1:memcached_refresh<0.213.0>:memcached_refresh:handle_cast:55]Refresh of rbac requested
[stats:error,2020-03-27T20:46:34.304Z,ns_1@127.0.0.1:<0.15846.0>:stats_reader:log_bad_responses:238]Some nodes didn't respond: ['ns_1@127.0.0.1']
[error_logger:info,2020-03-27T20:46:34.308Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,service_stats_children_sup}
             started: [{pid,<0.16627.0>},
                       {id,{service_fts,stats_archiver,"HelloServiceBucket"}},
                       {mfargs,
                           {stats_archiver,start_link,
                               ["@fts-HelloServiceBucket"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:46:34.309Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,service_stats_children_sup}
             started: [{pid,<0.16630.0>},
                       {id,{service_fts,stats_reader,"HelloServiceBucket"}},
                       {mfargs,
                           {stats_reader,start_link,
                               ["@fts-HelloServiceBucket"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[stats:error,2020-03-27T20:46:34.336Z,ns_1@127.0.0.1:<0.16484.0>:stats_reader:log_bad_responses:238]Some nodes didn't respond: ['ns_1@127.0.0.1']
[ns_server:debug,2020-03-27T20:46:34.336Z,ns_1@127.0.0.1:memcached_refresh<0.213.0>:memcached_refresh:handle_info:89]Refresh of [rbac] succeeded
[stats:error,2020-03-27T20:46:34.339Z,ns_1@127.0.0.1:<0.2046.0>:stats_reader:log_bad_responses:238]Some nodes didn't respond: ['ns_1@127.0.0.1']
[error_logger:info,2020-03-27T20:46:34.346Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,service_stats_children_sup}
             started: [{pid,<0.16631.0>},
                       {id,{service_index,stats_archiver,
                               "HelloServiceBucket"}},
                       {mfargs,
                           {stats_archiver,start_link,
                               ["@index-HelloServiceBucket"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:46:34.346Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,service_stats_children_sup}
             started: [{pid,<0.16636.0>},
                       {id,{service_index,stats_reader,"HelloServiceBucket"}},
                       {mfargs,
                           {stats_reader,start_link,
                               ["@index-HelloServiceBucket"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-03-27T20:46:34.359Z,ns_1@127.0.0.1:single_bucket_kv_sup-HelloServiceBucket<0.16638.0>:single_bucket_kv_sup:sync_config_to_couchdb_node:85]Syncing config to couchdb node
[ns_server:debug,2020-03-27T20:46:34.365Z,ns_1@127.0.0.1:single_bucket_kv_sup-HelloServiceBucket<0.16638.0>:single_bucket_kv_sup:sync_config_to_couchdb_node:91]Synced config to couchdb node successfully
[ns_server:debug,2020-03-27T20:46:34.367Z,ns_1@127.0.0.1:users_storage<0.262.0>:replicated_dets:handle_call:302]Suspended by process <0.305.0>
[ns_server:debug,2020-03-27T20:46:34.367Z,ns_1@127.0.0.1:memcached_passwords<0.305.0>:replicated_dets:select_from_dets_locked:350]Starting select with {users_storage,[{{docv2,{auth,{'_',local}},'_','_'},
                                      [],
                                      ['$_']}],
                                    100}
[ns_server:debug,2020-03-27T20:46:34.368Z,ns_1@127.0.0.1:users_storage<0.262.0>:replicated_dets:handle_call:309]Released by process <0.305.0>
[ns_server:debug,2020-03-27T20:46:34.385Z,ns_1@127.0.0.1:memcached_refresh<0.213.0>:memcached_refresh:handle_cast:55]Refresh of isasl requested
[ns_server:debug,2020-03-27T20:46:34.388Z,ns_1@127.0.0.1:<0.16622.0>:mb_map:generate_map_old:362]Natural map score: {1024,0}
[ns_server:debug,2020-03-27T20:46:34.400Z,ns_1@127.0.0.1:<0.16622.0>:mb_map:generate_map_old:369]Rnd maps scores: {1024,0}, {1024,0}
[ns_server:debug,2020-03-27T20:46:34.401Z,ns_1@127.0.0.1:<0.16622.0>:mb_map:generate_map_old:376]Considering 1 maps:
[{1024,0}]
[ns_server:debug,2020-03-27T20:46:34.401Z,ns_1@127.0.0.1:<0.16622.0>:mb_map:generate_map_old:381]Best map score: {1024,0} (true,true,true)
[ns_server:debug,2020-03-27T20:46:34.404Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{local_changes_count,<<"60a6bc3db77e6c7b91c556140dcfec71">>} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{68,63752561194}}]}]
[ns_server:debug,2020-03-27T20:46:34.405Z,ns_1@127.0.0.1:ns_config_rep<0.319.0>:ns_config_rep:do_push_keys:321]Replicating some config keys ([vbucket_map_history,
                               {local_changes_count,
                                   <<"60a6bc3db77e6c7b91c556140dcfec71">>}]..)
[ns_server:debug,2020-03-27T20:46:34.406Z,ns_1@127.0.0.1:memcached_refresh<0.213.0>:memcached_refresh:handle_info:89]Refresh of [isasl] succeeded
[ns_server:debug,2020-03-27T20:46:34.407Z,ns_1@127.0.0.1:<0.16622.0>:ns_janitor:config_sync:259]Going to push config to/from nodes:
['ns_1@127.0.0.1']
[ns_server:debug,2020-03-27T20:46:34.408Z,ns_1@127.0.0.1:ns_config_rep<0.319.0>:ns_config_rep:do_push_keys:321]Replicating some config keys ([buckets,
                               {local_changes_count,
                                   <<"60a6bc3db77e6c7b91c556140dcfec71">>}]..)
[ns_server:debug,2020-03-27T20:46:34.409Z,ns_1@127.0.0.1:capi_doc_replicator-HelloServiceBucket<0.16645.0>:replicated_storage:wait_for_startup:54]Start waiting for startup
[error_logger:info,2020-03-27T20:46:34.415Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {<0.16644.0>,docs_kv_sup}
             started: [{pid,<0.16645.0>},
                       {id,doc_replicator},
                       {mfargs,
                           {capi_ddoc_manager,start_replicator,
                               ["HelloServiceBucket"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-03-27T20:46:34.415Z,ns_1@127.0.0.1:capi_ddoc_replication_srv-HelloServiceBucket<0.16655.0>:replicated_storage:wait_for_startup:54]Start waiting for startup
[error_logger:info,2020-03-27T20:46:34.419Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {<0.16644.0>,docs_kv_sup}
             started: [{pid,<0.16655.0>},
                       {id,doc_replication_srv},
                       {mfargs,
                           {doc_replication_srv,start_link,
                               ["HelloServiceBucket"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-03-27T20:46:34.421Z,ns_1@127.0.0.1:ns_config_rep<0.319.0>:ns_config_rep:handle_call:122]Got full synchronization request from 'ns_1@127.0.0.1'
[ns_server:debug,2020-03-27T20:46:34.421Z,ns_1@127.0.0.1:ns_config_rep<0.319.0>:ns_config_rep:handle_call:128]Fully synchronized config in 9 us
[error_logger:info,2020-03-27T20:46:34.447Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,'capi_ddoc_manager_sup-HelloServiceBucket'}
             started: [{pid,<12940.583.0>},
                       {id,capi_ddoc_manager_events},
                       {mfargs,
                           {capi_ddoc_manager,start_link_event_manager,
                               ["HelloServiceBucket"]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[ns_server:debug,2020-03-27T20:46:34.453Z,ns_1@127.0.0.1:capi_doc_replicator-HelloServiceBucket<0.16645.0>:replicated_storage:wait_for_startup:57]Received replicated storage registration from <12940.584.0>
[ns_server:debug,2020-03-27T20:46:34.453Z,ns_1@127.0.0.1:<0.16667.0>:janitor_agent:query_vbuckets_loop:106]Exception from {query_vbuckets,all,[],[]} of "HelloServiceBucket":'ns_1@127.0.0.1'
{'EXIT',
    {noproc,
        {gen_server,call,
            [{'janitor_agent-HelloServiceBucket','ns_1@127.0.0.1'},
             {query_vbuckets,all,[],[]},
             infinity]}}}
[ns_server:debug,2020-03-27T20:46:34.453Z,ns_1@127.0.0.1:<0.16667.0>:janitor_agent:query_vbuckets_loop_next_step:117]Waiting for "HelloServiceBucket" on 'ns_1@127.0.0.1'
[ns_server:debug,2020-03-27T20:46:34.454Z,ns_1@127.0.0.1:capi_ddoc_replication_srv-HelloServiceBucket<0.16655.0>:replicated_storage:wait_for_startup:57]Received replicated storage registration from <12940.584.0>
[error_logger:info,2020-03-27T20:46:34.454Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,'capi_ddoc_manager_sup-HelloServiceBucket'}
             started: [{pid,<12940.584.0>},
                       {id,capi_ddoc_manager},
                       {mfargs,
                           {capi_ddoc_manager,start_link,
                               ["HelloServiceBucket",<0.16645.0>,
                                <0.16655.0>]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:46:34.454Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {<0.16644.0>,docs_kv_sup}
             started: [{pid,<12940.582.0>},
                       {id,capi_ddoc_manager_sup},
                       {mfargs,
                           {capi_ddoc_manager_sup,start_link_remote,
                               ['couchdb_ns_1@cb.local',
                                "HelloServiceBucket"]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[ns_server:debug,2020-03-27T20:46:34.459Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
vbucket_map_history ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752561194}}]},
 {[['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1'|...],
   [...]|...],
  [{replication_topology,star},{tags,undefined},{max_slaves,10}]}]
[ns_server:debug,2020-03-27T20:46:34.459Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{local_changes_count,<<"60a6bc3db77e6c7b91c556140dcfec71">>} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{69,63752561194}}]}]
[ns_server:debug,2020-03-27T20:46:34.486Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
buckets ->
[[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{5,63752561194}}],
 {configs,[{"HelloServiceBucket",
            [{map,[{0,[],['ns_1@127.0.0.1',undefined]},
                   {1,[],['ns_1@127.0.0.1',undefined]},
                   {2,[],['ns_1@127.0.0.1',undefined]},
                   {3,[],['ns_1@127.0.0.1',undefined]},
                   {4,[],['ns_1@127.0.0.1',undefined]},
                   {5,[],['ns_1@127.0.0.1',undefined]},
                   {6,[],['ns_1@127.0.0.1',undefined]},
                   {7,[],['ns_1@127.0.0.1',undefined]},
                   {8,[],['ns_1@127.0.0.1',undefined]},
                   {9,[],['ns_1@127.0.0.1',undefined]},
                   {10,[],['ns_1@127.0.0.1',undefined]},
                   {11,[],['ns_1@127.0.0.1',undefined]},
                   {12,[],['ns_1@127.0.0.1',undefined]},
                   {13,[],['ns_1@127.0.0.1',undefined]},
                   {14,[],['ns_1@127.0.0.1',undefined]},
                   {15,[],['ns_1@127.0.0.1',undefined]},
                   {16,[],['ns_1@127.0.0.1',undefined]},
                   {17,[],['ns_1@127.0.0.1',undefined]},
                   {18,[],['ns_1@127.0.0.1',undefined]},
                   {19,[],['ns_1@127.0.0.1',undefined]},
                   {20,[],['ns_1@127.0.0.1',undefined]},
                   {21,[],['ns_1@127.0.0.1',undefined]},
                   {22,[],['ns_1@127.0.0.1',undefined]},
                   {23,[],['ns_1@127.0.0.1',undefined]},
                   {24,[],['ns_1@127.0.0.1',undefined]},
                   {25,[],['ns_1@127.0.0.1',undefined]},
                   {26,[],['ns_1@127.0.0.1',undefined]},
                   {27,[],['ns_1@127.0.0.1',undefined]},
                   {28,[],['ns_1@127.0.0.1',undefined]},
                   {29,[],['ns_1@127.0.0.1',undefined]},
                   {30,[],['ns_1@127.0.0.1',undefined]},
                   {31,[],['ns_1@127.0.0.1',undefined]},
                   {32,[],['ns_1@127.0.0.1',undefined]},
                   {33,[],['ns_1@127.0.0.1',undefined]},
                   {34,[],['ns_1@127.0.0.1',undefined]},
                   {35,[],['ns_1@127.0.0.1',undefined]},
                   {36,[],['ns_1@127.0.0.1',undefined]},
                   {37,[],['ns_1@127.0.0.1',undefined]},
                   {38,[],['ns_1@127.0.0.1',undefined]},
                   {39,[],['ns_1@127.0.0.1',undefined]},
                   {40,[],['ns_1@127.0.0.1',undefined]},
                   {41,[],['ns_1@127.0.0.1',undefined]},
                   {42,[],['ns_1@127.0.0.1',undefined]},
                   {43,[],['ns_1@127.0.0.1',undefined]},
                   {44,[],['ns_1@127.0.0.1',undefined]},
                   {45,[],['ns_1@127.0.0.1',undefined]},
                   {46,[],['ns_1@127.0.0.1',undefined]},
                   {47,[],['ns_1@127.0.0.1',undefined]},
                   {48,[],['ns_1@127.0.0.1',undefined]},
                   {49,[],['ns_1@127.0.0.1',undefined]},
                   {50,[],['ns_1@127.0.0.1',undefined]},
                   {51,[],['ns_1@127.0.0.1',undefined]},
                   {52,[],['ns_1@127.0.0.1',undefined]},
                   {53,[],['ns_1@127.0.0.1',undefined]},
                   {54,[],['ns_1@127.0.0.1',undefined]},
                   {55,[],['ns_1@127.0.0.1',undefined]},
                   {56,[],['ns_1@127.0.0.1',undefined]},
                   {57,[],['ns_1@127.0.0.1',undefined]},
                   {58,[],['ns_1@127.0.0.1',undefined]},
                   {59,[],['ns_1@127.0.0.1',undefined]},
                   {60,[],['ns_1@127.0.0.1',undefined]},
                   {61,[],['ns_1@127.0.0.1',undefined]},
                   {62,[],['ns_1@127.0.0.1',undefined]},
                   {63,[],['ns_1@127.0.0.1',undefined]},
                   {64,[],['ns_1@127.0.0.1',undefined]},
                   {65,[],['ns_1@127.0.0.1',undefined]},
                   {66,[],['ns_1@127.0.0.1',undefined]},
                   {67,[],['ns_1@127.0.0.1',undefined]},
                   {68,[],['ns_1@127.0.0.1',undefined]},
                   {69,[],['ns_1@127.0.0.1',undefined]},
                   {70,[],['ns_1@127.0.0.1',undefined]},
                   {71,[],['ns_1@127.0.0.1',undefined]},
                   {72,[],['ns_1@127.0.0.1',undefined]},
                   {73,[],['ns_1@127.0.0.1',undefined]},
                   {74,[],['ns_1@127.0.0.1',undefined]},
                   {75,[],['ns_1@127.0.0.1',undefined]},
                   {76,[],['ns_1@127.0.0.1',undefined]},
                   {77,[],['ns_1@127.0.0.1',undefined]},
                   {78,[],['ns_1@127.0.0.1',undefined]},
                   {79,[],['ns_1@127.0.0.1',undefined]},
                   {80,[],['ns_1@127.0.0.1',undefined]},
                   {81,[],['ns_1@127.0.0.1',undefined]},
                   {82,[],['ns_1@127.0.0.1',undefined]},
                   {83,[],['ns_1@127.0.0.1',undefined]},
                   {84,[],['ns_1@127.0.0.1'|...]},
                   {85,[],[...]},
                   {86,[],...},
                   {87,...},
                   {...}|...]},
             {fastForwardMap,[]},
             {uuid,<<"e080f4429e7bd7d930bb34ead0abb5dc">>},
             {auth_type,sasl},
             {num_replicas,1},
             {replica_index,false},
             {ram_quota,306184192},
             {autocompaction,false},
             {purge_interval,undefined},
             {flush_enabled,false},
             {num_threads,3},
             {eviction_policy,value_only},
             {conflict_resolution_type,seqno},
             {storage_mode,couchstore},
             {max_ttl,0},
             {compression_mode,passive},
             {type,membase},
             {num_vbuckets,1024},
             {replication_topology,star},
             {repl_type,dcp},
             {servers,['ns_1@127.0.0.1']},
             {sasl_password,"*****"}]}]}]
[ns_server:debug,2020-03-27T20:46:34.488Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{local_changes_count,<<"60a6bc3db77e6c7b91c556140dcfec71">>} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{70,63752561194}}]}]
[ns_server:debug,2020-03-27T20:46:34.489Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
buckets ->
[[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{6,63752561194}}],
 {configs,[{"HelloServiceBucket",
            [{map,[]},
             {fastForwardMap,[]},
             {uuid,<<"e080f4429e7bd7d930bb34ead0abb5dc">>},
             {auth_type,sasl},
             {num_replicas,1},
             {replica_index,false},
             {ram_quota,306184192},
             {autocompaction,false},
             {purge_interval,undefined},
             {flush_enabled,false},
             {num_threads,3},
             {eviction_policy,value_only},
             {conflict_resolution_type,seqno},
             {storage_mode,couchstore},
             {max_ttl,0},
             {compression_mode,passive},
             {type,membase},
             {num_vbuckets,1024},
             {replication_topology,star},
             {repl_type,dcp},
             {servers,['ns_1@127.0.0.1']},
             {sasl_password,"*****"},
             {map_opts_hash,133465355}]}]}]
[ns_server:debug,2020-03-27T20:46:34.489Z,ns_1@127.0.0.1:compiled_roles_cache<0.264.0>:menelaus_roles:build_compiled_roles:753]Compile roles for user {"@goxdcr-cbauth",admin}
[ns_server:debug,2020-03-27T20:46:34.489Z,ns_1@127.0.0.1:compiled_roles_cache<0.264.0>:menelaus_roles:build_compiled_roles:753]Compile roles for user {"@",admin}
[stats:error,2020-03-27T20:46:34.509Z,ns_1@127.0.0.1:<0.15674.0>:stats_reader:log_bad_responses:238]Some nodes didn't respond: ['ns_1@127.0.0.1']
[error_logger:info,2020-03-27T20:46:34.512Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {<0.16644.0>,docs_kv_sup}
             started: [{pid,<12940.587.0>},
                       {id,capi_set_view_manager},
                       {mfargs,
                           {capi_set_view_manager,start_link_remote,
                               ['couchdb_ns_1@cb.local',
                                "HelloServiceBucket"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-03-27T20:46:34.523Z,ns_1@127.0.0.1:compiled_roles_cache<0.264.0>:menelaus_roles:build_compiled_roles:753]Compile roles for user {"@fts-cbauth",admin}
[ns_server:debug,2020-03-27T20:46:34.523Z,ns_1@127.0.0.1:compiled_roles_cache<0.264.0>:menelaus_roles:build_compiled_roles:753]Compile roles for user {"@eventing-cbauth",admin}
[error_logger:info,2020-03-27T20:46:34.538Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {<0.16644.0>,docs_kv_sup}
             started: [{pid,<12940.593.0>},
                       {id,couch_stats_reader},
                       {mfargs,
                           {couch_stats_reader,start_link_remote,
                               ['couchdb_ns_1@cb.local',
                                "HelloServiceBucket"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:46:34.540Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,'single_bucket_kv_sup-HelloServiceBucket'}
             started: [{pid,<0.16644.0>},
                       {id,{docs_kv_sup,"HelloServiceBucket"}},
                       {mfargs,
                           {docs_kv_sup,start_link,["HelloServiceBucket"]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[ns_server:debug,2020-03-27T20:46:34.547Z,ns_1@127.0.0.1:ns_memcached-HelloServiceBucket<0.16685.0>:ns_memcached:init:146]Starting ns_memcached
[ns_server:debug,2020-03-27T20:46:34.547Z,ns_1@127.0.0.1:<0.16686.0>:ns_memcached:run_connect_phase:201]Started 'connecting' phase of ns_memcached-HelloServiceBucket. Parent is <0.16685.0>
[stats:error,2020-03-27T20:46:34.548Z,ns_1@127.0.0.1:<0.16484.0>:stats_reader:log_bad_responses:238]Some nodes didn't respond: ['ns_1@127.0.0.1']
[error_logger:info,2020-03-27T20:46:34.552Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {<0.16684.0>,ns_memcached_sup}
             started: [{pid,<0.16685.0>},
                       {id,{ns_memcached,"HelloServiceBucket"}},
                       {mfargs,
                           {ns_memcached,start_link,["HelloServiceBucket"]}},
                       {restart_type,permanent},
                       {shutdown,86400000},
                       {child_type,worker}]

[stats:error,2020-03-27T20:46:34.554Z,ns_1@127.0.0.1:<0.2046.0>:stats_reader:log_bad_responses:238]Some nodes didn't respond: ['ns_1@127.0.0.1']
[error_logger:info,2020-03-27T20:46:34.570Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {<0.16684.0>,ns_memcached_sup}
             started: [{pid,<0.16691.0>},
                       {id,{terse_bucket_info_uploader,"HelloServiceBucket"}},
                       {mfargs,
                           {terse_bucket_info_uploader,start_link,
                               ["HelloServiceBucket"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:46:34.570Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,'single_bucket_kv_sup-HelloServiceBucket'}
             started: [{pid,<0.16684.0>},
                       {id,{ns_memcached_sup,"HelloServiceBucket"}},
                       {mfargs,
                           {ns_memcached_sup,start_link,
                               ["HelloServiceBucket"]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-03-27T20:46:34.576Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,'single_bucket_kv_sup-HelloServiceBucket'}
             started: [{pid,<0.16693.0>},
                       {id,{dcp_sup,"HelloServiceBucket"}},
                       {mfargs,{dcp_sup,start_link,["HelloServiceBucket"]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-03-27T20:46:34.579Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,'single_bucket_kv_sup-HelloServiceBucket'}
             started: [{pid,<0.16694.0>},
                       {id,{dcp_replication_manager,"HelloServiceBucket"}},
                       {mfargs,
                           {dcp_replication_manager,start_link,
                               ["HelloServiceBucket"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:46:34.582Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,'single_bucket_kv_sup-HelloServiceBucket'}
             started: [{pid,<0.16695.0>},
                       {id,{replication_manager,"HelloServiceBucket"}},
                       {mfargs,
                           {replication_manager,start_link,
                               ["HelloServiceBucket"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-03-27T20:46:34.588Z,ns_1@127.0.0.1:capi_doc_replicator-HelloServiceBucket<0.16645.0>:doc_replicator:loop:60]doing replicate_newnodes_docs
[error_logger:info,2020-03-27T20:46:34.590Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,'janitor_agent_sup-HelloServiceBucket'}
             started: [{pid,<0.16697.0>},
                       {id,rebalance_subprocesses_registry},
                       {mfargs,
                           {ns_process_registry,start_link,
                               ['rebalance_subprocesses_registry-HelloServiceBucket',
                                [{terminate_command,kill}]]}},
                       {restart_type,permanent},
                       {shutdown,86400000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:46:34.591Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,'janitor_agent_sup-HelloServiceBucket'}
             started: [{pid,<0.16698.0>},
                       {id,janitor_agent},
                       {mfargs,
                           {janitor_agent,start_link,["HelloServiceBucket"]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:46:34.591Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,'single_bucket_kv_sup-HelloServiceBucket'}
             started: [{pid,<0.16696.0>},
                       {id,{janitor_agent_sup,"HelloServiceBucket"}},
                       {mfargs,
                           {janitor_agent_sup,start_link,
                               ["HelloServiceBucket"]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-03-27T20:46:34.591Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,'single_bucket_kv_sup-HelloServiceBucket'}
             started: [{pid,<0.16699.0>},
                       {id,{stats_collector,"HelloServiceBucket"}},
                       {mfargs,
                           {stats_collector,start_link,
                               ["HelloServiceBucket"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-03-27T20:46:34.592Z,ns_1@127.0.0.1:janitor_agent-HelloServiceBucket<0.16698.0>:dcp_sup:nuke:113]Nuking DCP replicators for bucket "HelloServiceBucket":
[]
[error_logger:info,2020-03-27T20:46:34.592Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,'single_bucket_kv_sup-HelloServiceBucket'}
             started: [{pid,<0.16702.0>},
                       {id,{stats_archiver,"HelloServiceBucket"}},
                       {mfargs,
                           {stats_archiver,start_link,["HelloServiceBucket"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:46:34.593Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,'single_bucket_kv_sup-HelloServiceBucket'}
             started: [{pid,<0.16704.0>},
                       {id,{stats_reader,"HelloServiceBucket"}},
                       {mfargs,
                           {stats_reader,start_link,["HelloServiceBucket"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:info,2020-03-27T20:46:34.607Z,ns_1@127.0.0.1:ns_memcached-HelloServiceBucket<0.16685.0>:ns_memcached:do_ensure_bucket:1177]Created bucket "HelloServiceBucket" with config string "max_size=306184192;dbname=/opt/couchbase/var/lib/couchbase/data/HelloServiceBucket;backend=couchdb;couch_bucket=HelloServiceBucket;max_vbuckets=1024;alog_path=/opt/couchbase/var/lib/couchbase/data/HelloServiceBucket/access.log;data_traffic_enabled=false;max_num_workers=3;uuid=e080f4429e7bd7d930bb34ead0abb5dc;conflict_resolution_type=seqno;bucket_type=persistent;item_eviction_policy=value_only;max_ttl=0;ht_locks=47;compression_mode=passive;failpartialwarmup=false"
[error_logger:info,2020-03-27T20:46:34.608Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,'single_bucket_kv_sup-HelloServiceBucket'}
             started: [{pid,<0.16705.0>},
                       {id,{goxdcr_stats_collector,"HelloServiceBucket"}},
                       {mfargs,
                           {goxdcr_stats_collector,start_link,
                               ["HelloServiceBucket"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:info,2020-03-27T20:46:34.608Z,ns_1@127.0.0.1:ns_memcached-HelloServiceBucket<0.16685.0>:ns_memcached:handle_cast:629]Main ns_memcached connection established: {ok,#Port<0.10263>}
[error_logger:info,2020-03-27T20:46:34.611Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,'single_bucket_kv_sup-HelloServiceBucket'}
             started: [{pid,<0.16707.0>},
                       {id,{goxdcr_stats_archiver,"HelloServiceBucket"}},
                       {mfargs,
                           {stats_archiver,start_link,
                               ["@xdcr-HelloServiceBucket"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:46:34.611Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,'single_bucket_kv_sup-HelloServiceBucket'}
             started: [{pid,<0.16713.0>},
                       {id,{goxdcr_stats_reader,"HelloServiceBucket"}},
                       {mfargs,
                           {stats_reader,start_link,
                               ["@xdcr-HelloServiceBucket"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:46:34.611Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,'single_bucket_kv_sup-HelloServiceBucket'}
             started: [{pid,<0.16714.0>},
                       {id,{failover_safeness_level,"HelloServiceBucket"}},
                       {mfargs,
                           {failover_safeness_level,start_link,
                               ["HelloServiceBucket"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-03-27T20:46:34.612Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_bucket_sup}
             started: [{pid,<0.16638.0>},
                       {id,{single_bucket_kv_sup,"HelloServiceBucket"}},
                       {mfargs,
                           {single_bucket_kv_sup,start_link,
                               ["HelloServiceBucket"]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[ns_server:info,2020-03-27T20:46:34.617Z,ns_1@127.0.0.1:janitor_agent-HelloServiceBucket<0.16698.0>:janitor_agent:read_flush_counter:960]Loading flushseq failed: {error,enoent}. Assuming it's equal to global config.
[ns_server:info,2020-03-27T20:46:34.618Z,ns_1@127.0.0.1:janitor_agent-HelloServiceBucket<0.16698.0>:janitor_agent:read_flush_counter_from_config:968]Initialized flushseq 0 from bucket config
[ns_server:debug,2020-03-27T20:46:34.756Z,ns_1@127.0.0.1:ns_heart_slow_status_updater<0.347.0>:ns_heart:grab_latest_stats:263]Ignoring failure to grab "HelloServiceBucket" stats:
{error,no_samples}

[user:info,2020-03-27T20:46:35.110Z,ns_1@127.0.0.1:ns_memcached-HelloServiceBucket<0.16685.0>:ns_memcached:handle_cast:661]Bucket "HelloServiceBucket" loaded on node 'ns_1@127.0.0.1' in 0 seconds.
[ns_server:debug,2020-03-27T20:46:35.312Z,ns_1@127.0.0.1:ns_heart_slow_status_updater<0.347.0>:ns_heart:grab_latest_stats:263]Ignoring failure to grab "HelloServiceBucket" stats:
{error,no_samples}

[ns_server:debug,2020-03-27T20:46:35.455Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:maybe_config_sync:242]Found states mismatch in bucket "HelloServiceBucket":
[{0,['ns_1@127.0.0.1',undefined],[]},
 {1,['ns_1@127.0.0.1',undefined],[]},
 {2,['ns_1@127.0.0.1',undefined],[]},
 {3,['ns_1@127.0.0.1',undefined],[]},
 {4,['ns_1@127.0.0.1',undefined],[]},
 {5,['ns_1@127.0.0.1',undefined],[]},
 {6,['ns_1@127.0.0.1',undefined],[]},
 {7,['ns_1@127.0.0.1',undefined],[]},
 {8,['ns_1@127.0.0.1',undefined],[]},
 {9,['ns_1@127.0.0.1',undefined],[]},
 {10,['ns_1@127.0.0.1',undefined],[]},
 {11,['ns_1@127.0.0.1',undefined],[]},
 {12,['ns_1@127.0.0.1',undefined],[]},
 {13,['ns_1@127.0.0.1',undefined],[]},
 {14,['ns_1@127.0.0.1',undefined],[]},
 {15,['ns_1@127.0.0.1',undefined],[]},
 {16,['ns_1@127.0.0.1',undefined],[]},
 {17,['ns_1@127.0.0.1',undefined],[]},
 {18,['ns_1@127.0.0.1',undefined],[]},
 {19,['ns_1@127.0.0.1',undefined],[]},
 {20,['ns_1@127.0.0.1',undefined],[]},
 {21,['ns_1@127.0.0.1',undefined],[]},
 {22,['ns_1@127.0.0.1',undefined],[]},
 {23,['ns_1@127.0.0.1',undefined],[]},
 {24,['ns_1@127.0.0.1',undefined],[]},
 {25,['ns_1@127.0.0.1',undefined],[]},
 {26,['ns_1@127.0.0.1',undefined],[]},
 {27,['ns_1@127.0.0.1',undefined],[]},
 {28,['ns_1@127.0.0.1',undefined],[]},
 {29,['ns_1@127.0.0.1',undefined],[]},
 {30,['ns_1@127.0.0.1',undefined],[]},
 {31,['ns_1@127.0.0.1',undefined],[]},
 {32,['ns_1@127.0.0.1',undefined],[]},
 {33,['ns_1@127.0.0.1',undefined],[]},
 {34,['ns_1@127.0.0.1',undefined],[]},
 {35,['ns_1@127.0.0.1',undefined],[]},
 {36,['ns_1@127.0.0.1',undefined],[]},
 {37,['ns_1@127.0.0.1',undefined],[]},
 {38,['ns_1@127.0.0.1',undefined],[]},
 {39,['ns_1@127.0.0.1',undefined],[]},
 {40,['ns_1@127.0.0.1',undefined],[]},
 {41,['ns_1@127.0.0.1',undefined],[]},
 {42,['ns_1@127.0.0.1',undefined],[]},
 {43,['ns_1@127.0.0.1',undefined],[]},
 {44,['ns_1@127.0.0.1',undefined],[]},
 {45,['ns_1@127.0.0.1',undefined],[]},
 {46,['ns_1@127.0.0.1',undefined],[]},
 {47,['ns_1@127.0.0.1',undefined],[]},
 {48,['ns_1@127.0.0.1',undefined],[]},
 {49,['ns_1@127.0.0.1',undefined],[]},
 {50,['ns_1@127.0.0.1',undefined],[]},
 {51,['ns_1@127.0.0.1',undefined],[]},
 {52,['ns_1@127.0.0.1',undefined],[]},
 {53,['ns_1@127.0.0.1',undefined],[]},
 {54,['ns_1@127.0.0.1',undefined],[]},
 {55,['ns_1@127.0.0.1',undefined],[]},
 {56,['ns_1@127.0.0.1',undefined],[]},
 {57,['ns_1@127.0.0.1',undefined],[]},
 {58,['ns_1@127.0.0.1',undefined],[]},
 {59,['ns_1@127.0.0.1',undefined],[]},
 {60,['ns_1@127.0.0.1',undefined],[]},
 {61,['ns_1@127.0.0.1',undefined],[]},
 {62,['ns_1@127.0.0.1',undefined],[]},
 {63,['ns_1@127.0.0.1',undefined],[]},
 {64,['ns_1@127.0.0.1',undefined],[]},
 {65,['ns_1@127.0.0.1',undefined],[]},
 {66,['ns_1@127.0.0.1',undefined],[]},
 {67,['ns_1@127.0.0.1',undefined],[]},
 {68,['ns_1@127.0.0.1',undefined],[]},
 {69,['ns_1@127.0.0.1',undefined],[]},
 {70,['ns_1@127.0.0.1',undefined],[]},
 {71,['ns_1@127.0.0.1',undefined],[]},
 {72,['ns_1@127.0.0.1',undefined],[]},
 {73,['ns_1@127.0.0.1',undefined],[]},
 {74,['ns_1@127.0.0.1',undefined],[]},
 {75,['ns_1@127.0.0.1',undefined],[]},
 {76,['ns_1@127.0.0.1',undefined],[]},
 {77,['ns_1@127.0.0.1',undefined],[]},
 {78,['ns_1@127.0.0.1',undefined],[]},
 {79,['ns_1@127.0.0.1',undefined],[]},
 {80,['ns_1@127.0.0.1',undefined],[]},
 {81,['ns_1@127.0.0.1',undefined],[]},
 {82,['ns_1@127.0.0.1',undefined],[]},
 {83,['ns_1@127.0.0.1',undefined],[]},
 {84,['ns_1@127.0.0.1',undefined],[]},
 {85,['ns_1@127.0.0.1',undefined],[]},
 {86,['ns_1@127.0.0.1',undefined],[]},
 {87,['ns_1@127.0.0.1',undefined],[]},
 {88,['ns_1@127.0.0.1',undefined],[]},
 {89,['ns_1@127.0.0.1',undefined],[]},
 {90,['ns_1@127.0.0.1',undefined],[]},
 {91,['ns_1@127.0.0.1',undefined],[]},
 {92,['ns_1@127.0.0.1',undefined],[]},
 {93,['ns_1@127.0.0.1',undefined],[]},
 {94,['ns_1@127.0.0.1',undefined],[]},
 {95,['ns_1@127.0.0.1',undefined],[]},
 {96,['ns_1@127.0.0.1',undefined],[]},
 {97,['ns_1@127.0.0.1',undefined],[]},
 {98,['ns_1@127.0.0.1',undefined],[]},
 {99,['ns_1@127.0.0.1',undefined],[]},
 {100,['ns_1@127.0.0.1',undefined],[]},
 {101,['ns_1@127.0.0.1',undefined],[]},
 {102,['ns_1@127.0.0.1',undefined],[]},
 {103,['ns_1@127.0.0.1',undefined],[]},
 {104,['ns_1@127.0.0.1',undefined],[]},
 {105,['ns_1@127.0.0.1',undefined],[]},
 {106,['ns_1@127.0.0.1',undefined],[]},
 {107,['ns_1@127.0.0.1',undefined],[]},
 {108,['ns_1@127.0.0.1',undefined],[]},
 {109,['ns_1@127.0.0.1',undefined],[]},
 {110,['ns_1@127.0.0.1',undefined],[]},
 {111,['ns_1@127.0.0.1',undefined],[]},
 {112,['ns_1@127.0.0.1',undefined],[]},
 {113,['ns_1@127.0.0.1',undefined],[]},
 {114,['ns_1@127.0.0.1',undefined],[]},
 {115,['ns_1@127.0.0.1',undefined],[]},
 {116,['ns_1@127.0.0.1',undefined],[]},
 {117,['ns_1@127.0.0.1',undefined],[]},
 {118,['ns_1@127.0.0.1',undefined],[]},
 {119,['ns_1@127.0.0.1',undefined],[]},
 {120,['ns_1@127.0.0.1',undefined],[]},
 {121,['ns_1@127.0.0.1',undefined],[]},
 {122,['ns_1@127.0.0.1',undefined],[]},
 {123,['ns_1@127.0.0.1',undefined],[]},
 {124,['ns_1@127.0.0.1',undefined],[]},
 {125,['ns_1@127.0.0.1',undefined],[]},
 {126,['ns_1@127.0.0.1',undefined],[]},
 {127,['ns_1@127.0.0.1',undefined],[]},
 {128,['ns_1@127.0.0.1',undefined],[]},
 {129,['ns_1@127.0.0.1',undefined],[]},
 {130,['ns_1@127.0.0.1',undefined],[]},
 {131,['ns_1@127.0.0.1',undefined],[]},
 {132,['ns_1@127.0.0.1',undefined],[]},
 {133,['ns_1@127.0.0.1',undefined],[]},
 {134,['ns_1@127.0.0.1',undefined],[]},
 {135,['ns_1@127.0.0.1',undefined],[]},
 {136,['ns_1@127.0.0.1',undefined],[]},
 {137,['ns_1@127.0.0.1',undefined],[]},
 {138,['ns_1@127.0.0.1',undefined],[]},
 {139,['ns_1@127.0.0.1',undefined],[]},
 {140,['ns_1@127.0.0.1',undefined],[]},
 {141,['ns_1@127.0.0.1',undefined],[]},
 {142,['ns_1@127.0.0.1',undefined],[]},
 {143,['ns_1@127.0.0.1',undefined],[]},
 {144,['ns_1@127.0.0.1',undefined],[]},
 {145,['ns_1@127.0.0.1',undefined],[]},
 {146,['ns_1@127.0.0.1',undefined],[]},
 {147,['ns_1@127.0.0.1',undefined],[]},
 {148,['ns_1@127.0.0.1',undefined],[]},
 {149,['ns_1@127.0.0.1',undefined],[]},
 {150,['ns_1@127.0.0.1',undefined],[]},
 {151,['ns_1@127.0.0.1',undefined],[]},
 {152,['ns_1@127.0.0.1',undefined],[]},
 {153,['ns_1@127.0.0.1',undefined],[]},
 {154,['ns_1@127.0.0.1',undefined],[]},
 {155,['ns_1@127.0.0.1',undefined],[]},
 {156,['ns_1@127.0.0.1',undefined],[]},
 {157,['ns_1@127.0.0.1',undefined],[]},
 {158,['ns_1@127.0.0.1',undefined],[]},
 {159,['ns_1@127.0.0.1',undefined],[]},
 {160,['ns_1@127.0.0.1',undefined],[]},
 {161,['ns_1@127.0.0.1',undefined],[]},
 {162,['ns_1@127.0.0.1',undefined],[]},
 {163,['ns_1@127.0.0.1',undefined],[]},
 {164,['ns_1@127.0.0.1',undefined],[]},
 {165,['ns_1@127.0.0.1',undefined],[]},
 {166,['ns_1@127.0.0.1',undefined],[]},
 {167,['ns_1@127.0.0.1',undefined],[]},
 {168,['ns_1@127.0.0.1',undefined],[]},
 {169,['ns_1@127.0.0.1',undefined],[]},
 {170,['ns_1@127.0.0.1',undefined],[]},
 {171,['ns_1@127.0.0.1',undefined],[]},
 {172,['ns_1@127.0.0.1',undefined],[]},
 {173,['ns_1@127.0.0.1',undefined],[]},
 {174,['ns_1@127.0.0.1',undefined],[]},
 {175,['ns_1@127.0.0.1',undefined],[]},
 {176,['ns_1@127.0.0.1',undefined],[]},
 {177,['ns_1@127.0.0.1',undefined],[]},
 {178,['ns_1@127.0.0.1',undefined],[]},
 {179,['ns_1@127.0.0.1',undefined],[]},
 {180,['ns_1@127.0.0.1',undefined],[]},
 {181,['ns_1@127.0.0.1',undefined],[]},
 {182,['ns_1@127.0.0.1',undefined],[]},
 {183,['ns_1@127.0.0.1',undefined],[]},
 {184,['ns_1@127.0.0.1',undefined],[]},
 {185,['ns_1@127.0.0.1',undefined],[]},
 {186,['ns_1@127.0.0.1',undefined],[]},
 {187,['ns_1@127.0.0.1',undefined],[]},
 {188,['ns_1@127.0.0.1',undefined],[]},
 {189,['ns_1@127.0.0.1',undefined],[]},
 {190,['ns_1@127.0.0.1',undefined],[]},
 {191,['ns_1@127.0.0.1',undefined],[]},
 {192,['ns_1@127.0.0.1',undefined],[]},
 {193,['ns_1@127.0.0.1',undefined],[]},
 {194,['ns_1@127.0.0.1',undefined],[]},
 {195,['ns_1@127.0.0.1',undefined],[]},
 {196,['ns_1@127.0.0.1',undefined],[]},
 {197,['ns_1@127.0.0.1',undefined],[]},
 {198,['ns_1@127.0.0.1',undefined],[]},
 {199,['ns_1@127.0.0.1',undefined],[]},
 {200,['ns_1@127.0.0.1',undefined],[]},
 {201,['ns_1@127.0.0.1',undefined],[]},
 {202,['ns_1@127.0.0.1',undefined],[]},
 {203,['ns_1@127.0.0.1',undefined],[]},
 {204,['ns_1@127.0.0.1',undefined],[]},
 {205,['ns_1@127.0.0.1',undefined],[]},
 {206,['ns_1@127.0.0.1',undefined],[]},
 {207,['ns_1@127.0.0.1',undefined],[]},
 {208,['ns_1@127.0.0.1',undefined],[]},
 {209,['ns_1@127.0.0.1',undefined],[]},
 {210,['ns_1@127.0.0.1',undefined],[]},
 {211,['ns_1@127.0.0.1',undefined],[]},
 {212,['ns_1@127.0.0.1',undefined],[]},
 {213,['ns_1@127.0.0.1',undefined],[]},
 {214,['ns_1@127.0.0.1',undefined],[]},
 {215,['ns_1@127.0.0.1',undefined],[]},
 {216,['ns_1@127.0.0.1',undefined],[]},
 {217,['ns_1@127.0.0.1',undefined],[]},
 {218,['ns_1@127.0.0.1',undefined],[]},
 {219,['ns_1@127.0.0.1',undefined],[]},
 {220,['ns_1@127.0.0.1',undefined],[]},
 {221,['ns_1@127.0.0.1',undefined],[]},
 {222,['ns_1@127.0.0.1',undefined],[]},
 {223,['ns_1@127.0.0.1',undefined],[]},
 {224,['ns_1@127.0.0.1',undefined],[]},
 {225,['ns_1@127.0.0.1',undefined],[]},
 {226,['ns_1@127.0.0.1',undefined],[]},
 {227,['ns_1@127.0.0.1',undefined],[]},
 {228,['ns_1@127.0.0.1',undefined],[]},
 {229,['ns_1@127.0.0.1',undefined],[]},
 {230,['ns_1@127.0.0.1',undefined],[]},
 {231,['ns_1@127.0.0.1',undefined],[]},
 {232,['ns_1@127.0.0.1',undefined],[]},
 {233,['ns_1@127.0.0.1',undefined],[]},
 {234,['ns_1@127.0.0.1',undefined],[]},
 {235,['ns_1@127.0.0.1',undefined],[]},
 {236,['ns_1@127.0.0.1',undefined],[]},
 {237,['ns_1@127.0.0.1',undefined],[]},
 {238,['ns_1@127.0.0.1',undefined],[]},
 {239,['ns_1@127.0.0.1',undefined],[]},
 {240,['ns_1@127.0.0.1',undefined],[]},
 {241,['ns_1@127.0.0.1',undefined],[]},
 {242,['ns_1@127.0.0.1',undefined],[]},
 {243,['ns_1@127.0.0.1',undefined],[]},
 {244,['ns_1@127.0.0.1',undefined],[]},
 {245,['ns_1@127.0.0.1',undefined],[]},
 {246,['ns_1@127.0.0.1',undefined],[]},
 {247,['ns_1@127.0.0.1',undefined],[]},
 {248,['ns_1@127.0.0.1',undefined],[]},
 {249,['ns_1@127.0.0.1',undefined],[]},
 {250,['ns_1@127.0.0.1',undefined],[]},
 {251,['ns_1@127.0.0.1',undefined],[]},
 {252,['ns_1@127.0.0.1',undefined],[]},
 {253,['ns_1@127.0.0.1',undefined],[]},
 {254,['ns_1@127.0.0.1',undefined],[]},
 {255,['ns_1@127.0.0.1',undefined],[]},
 {256,['ns_1@127.0.0.1',undefined],[]},
 {257,['ns_1@127.0.0.1',undefined],[]},
 {258,['ns_1@127.0.0.1',undefined],[]},
 {259,['ns_1@127.0.0.1',undefined],[]},
 {260,['ns_1@127.0.0.1',undefined],[]},
 {261,['ns_1@127.0.0.1',undefined],[]},
 {262,['ns_1@127.0.0.1',undefined],[]},
 {263,['ns_1@127.0.0.1',undefined],[]},
 {264,['ns_1@127.0.0.1',undefined],[]},
 {265,['ns_1@127.0.0.1',undefined],[]},
 {266,['ns_1@127.0.0.1',undefined],[]},
 {267,['ns_1@127.0.0.1',undefined],[]},
 {268,['ns_1@127.0.0.1',undefined],[]},
 {269,['ns_1@127.0.0.1',undefined],[]},
 {270,['ns_1@127.0.0.1',undefined],[]},
 {271,['ns_1@127.0.0.1',undefined],[]},
 {272,['ns_1@127.0.0.1',undefined],[]},
 {273,['ns_1@127.0.0.1',undefined],[]},
 {274,['ns_1@127.0.0.1',undefined],[]},
 {275,['ns_1@127.0.0.1',undefined],[]},
 {276,['ns_1@127.0.0.1',undefined],[]},
 {277,['ns_1@127.0.0.1',undefined],[]},
 {278,['ns_1@127.0.0.1',undefined],[]},
 {279,['ns_1@127.0.0.1',undefined],[]},
 {280,['ns_1@127.0.0.1',undefined],[]},
 {281,['ns_1@127.0.0.1',undefined],[]},
 {282,['ns_1@127.0.0.1',undefined],[]},
 {283,['ns_1@127.0.0.1',undefined],[]},
 {284,['ns_1@127.0.0.1',undefined],[]},
 {285,['ns_1@127.0.0.1',undefined],[]},
 {286,['ns_1@127.0.0.1',undefined],[]},
 {287,['ns_1@127.0.0.1',undefined],[]},
 {288,['ns_1@127.0.0.1',undefined],[]},
 {289,['ns_1@127.0.0.1',undefined],[]},
 {290,['ns_1@127.0.0.1',undefined],[]},
 {291,['ns_1@127.0.0.1',undefined],[]},
 {292,['ns_1@127.0.0.1',undefined],[]},
 {293,['ns_1@127.0.0.1',undefined],[]},
 {294,['ns_1@127.0.0.1',undefined],[]},
 {295,['ns_1@127.0.0.1',undefined],[]},
 {296,['ns_1@127.0.0.1',undefined],[]},
 {297,['ns_1@127.0.0.1',undefined],[]},
 {298,['ns_1@127.0.0.1',undefined],[]},
 {299,['ns_1@127.0.0.1',undefined],[]},
 {300,['ns_1@127.0.0.1',undefined],[]},
 {301,['ns_1@127.0.0.1',undefined],[]},
 {302,['ns_1@127.0.0.1',undefined],[]},
 {303,['ns_1@127.0.0.1',undefined],[]},
 {304,['ns_1@127.0.0.1',undefined],[]},
 {305,['ns_1@127.0.0.1',undefined],[]},
 {306,['ns_1@127.0.0.1',undefined],[]},
 {307,['ns_1@127.0.0.1',undefined],[]},
 {308,['ns_1@127.0.0.1',undefined],[]},
 {309,['ns_1@127.0.0.1',undefined],[]},
 {310,['ns_1@127.0.0.1',undefined],[]},
 {311,['ns_1@127.0.0.1',undefined],[]},
 {312,['ns_1@127.0.0.1',undefined],[]},
 {313,['ns_1@127.0.0.1',undefined],[]},
 {314,['ns_1@127.0.0.1',undefined],[]},
 {315,['ns_1@127.0.0.1',undefined],[]},
 {316,['ns_1@127.0.0.1',undefined],[]},
 {317,['ns_1@127.0.0.1',undefined],[]},
 {318,['ns_1@127.0.0.1',undefined],[]},
 {319,['ns_1@127.0.0.1',undefined],[]},
 {320,['ns_1@127.0.0.1',undefined],[]},
 {321,['ns_1@127.0.0.1',undefined],[]},
 {322,['ns_1@127.0.0.1',undefined],[]},
 {323,['ns_1@127.0.0.1',undefined],[]},
 {324,['ns_1@127.0.0.1',undefined],[]},
 {325,['ns_1@127.0.0.1',undefined],[]},
 {326,['ns_1@127.0.0.1',undefined],[]},
 {327,['ns_1@127.0.0.1',undefined],[]},
 {328,['ns_1@127.0.0.1',undefined],[]},
 {329,['ns_1@127.0.0.1',undefined],[]},
 {330,['ns_1@127.0.0.1',undefined],[]},
 {331,['ns_1@127.0.0.1',undefined],[]},
 {332,['ns_1@127.0.0.1',undefined],[]},
 {333,['ns_1@127.0.0.1',undefined],[]},
 {334,['ns_1@127.0.0.1',undefined],[]},
 {335,['ns_1@127.0.0.1',undefined],[]},
 {336,['ns_1@127.0.0.1',undefined],[]},
 {337,['ns_1@127.0.0.1',undefined],[]},
 {338,['ns_1@127.0.0.1',undefined],[]},
 {339,['ns_1@127.0.0.1',undefined],[]},
 {340,['ns_1@127.0.0.1',undefined],[]},
 {341,['ns_1@127.0.0.1',undefined],[]},
 {342,['ns_1@127.0.0.1',undefined],[]},
 {343,['ns_1@127.0.0.1',undefined],[]},
 {344,['ns_1@127.0.0.1',undefined],[]},
 {345,['ns_1@127.0.0.1',undefined],[]},
 {346,['ns_1@127.0.0.1',undefined],[]},
 {347,['ns_1@127.0.0.1',undefined],[]},
 {348,['ns_1@127.0.0.1',undefined],[]},
 {349,['ns_1@127.0.0.1',undefined],[]},
 {350,['ns_1@127.0.0.1',undefined],[]},
 {351,['ns_1@127.0.0.1',undefined],[]},
 {352,['ns_1@127.0.0.1',undefined],[]},
 {353,['ns_1@127.0.0.1',undefined],[]},
 {354,['ns_1@127.0.0.1',undefined],[]},
 {355,['ns_1@127.0.0.1',undefined],[]},
 {356,['ns_1@127.0.0.1',undefined],[]},
 {357,['ns_1@127.0.0.1',undefined],[]},
 {358,['ns_1@127.0.0.1',undefined],[]},
 {359,['ns_1@127.0.0.1',undefined],[]},
 {360,['ns_1@127.0.0.1',undefined],[]},
 {361,['ns_1@127.0.0.1',undefined],[]},
 {362,['ns_1@127.0.0.1',undefined],[]},
 {363,['ns_1@127.0.0.1',undefined],[]},
 {364,['ns_1@127.0.0.1',undefined],[]},
 {365,['ns_1@127.0.0.1',undefined],[]},
 {366,['ns_1@127.0.0.1',undefined],[]},
 {367,['ns_1@127.0.0.1',undefined],[]},
 {368,['ns_1@127.0.0.1',undefined],[]},
 {369,['ns_1@127.0.0.1',undefined],[]},
 {370,['ns_1@127.0.0.1',undefined],[]},
 {371,['ns_1@127.0.0.1',undefined],[]},
 {372,['ns_1@127.0.0.1',undefined],[]},
 {373,['ns_1@127.0.0.1',undefined],[]},
 {374,['ns_1@127.0.0.1',undefined],[]},
 {375,['ns_1@127.0.0.1',undefined],[]},
 {376,['ns_1@127.0.0.1',undefined],[]},
 {377,['ns_1@127.0.0.1',undefined],[]},
 {378,['ns_1@127.0.0.1',undefined],[]},
 {379,['ns_1@127.0.0.1',undefined],[]},
 {380,['ns_1@127.0.0.1',undefined],[]},
 {381,['ns_1@127.0.0.1',undefined],[]},
 {382,['ns_1@127.0.0.1',undefined],[]},
 {383,['ns_1@127.0.0.1',undefined],[]},
 {384,['ns_1@127.0.0.1',undefined],[]},
 {385,['ns_1@127.0.0.1',undefined],[]},
 {386,['ns_1@127.0.0.1',undefined],[]},
 {387,['ns_1@127.0.0.1',undefined],[]},
 {388,['ns_1@127.0.0.1',undefined],[]},
 {389,['ns_1@127.0.0.1',undefined],[]},
 {390,['ns_1@127.0.0.1',undefined],[]},
 {391,['ns_1@127.0.0.1',undefined],[]},
 {392,['ns_1@127.0.0.1',undefined],[]},
 {393,['ns_1@127.0.0.1',undefined],[]},
 {394,['ns_1@127.0.0.1',undefined],[]},
 {395,['ns_1@127.0.0.1',undefined],[]},
 {396,['ns_1@127.0.0.1',undefined],[]},
 {397,['ns_1@127.0.0.1',undefined],[]},
 {398,['ns_1@127.0.0.1',undefined],[]},
 {399,['ns_1@127.0.0.1',undefined],[]},
 {400,['ns_1@127.0.0.1',undefined],[]},
 {401,['ns_1@127.0.0.1',undefined],[]},
 {402,['ns_1@127.0.0.1',undefined],[]},
 {403,['ns_1@127.0.0.1',undefined],[]},
 {404,['ns_1@127.0.0.1',undefined],[]},
 {405,['ns_1@127.0.0.1',undefined],[]},
 {406,['ns_1@127.0.0.1',undefined],[]},
 {407,['ns_1@127.0.0.1',undefined],[]},
 {408,['ns_1@127.0.0.1',undefined],[]},
 {409,['ns_1@127.0.0.1',undefined],[]},
 {410,['ns_1@127.0.0.1',undefined],[]},
 {411,['ns_1@127.0.0.1',undefined],[]},
 {412,['ns_1@127.0.0.1',undefined],[]},
 {413,['ns_1@127.0.0.1',undefined],[]},
 {414,['ns_1@127.0.0.1',undefined],[]},
 {415,['ns_1@127.0.0.1',undefined],[]},
 {416,['ns_1@127.0.0.1',undefined],[]},
 {417,['ns_1@127.0.0.1',undefined],[]},
 {418,['ns_1@127.0.0.1',undefined],[]},
 {419,['ns_1@127.0.0.1',undefined],[]},
 {420,['ns_1@127.0.0.1',undefined],[]},
 {421,['ns_1@127.0.0.1',undefined],[]},
 {422,['ns_1@127.0.0.1',undefined],[]},
 {423,['ns_1@127.0.0.1',undefined],[]},
 {424,['ns_1@127.0.0.1',undefined],[]},
 {425,['ns_1@127.0.0.1',undefined],[]},
 {426,['ns_1@127.0.0.1',undefined],[]},
 {427,['ns_1@127.0.0.1',undefined],[]},
 {428,['ns_1@127.0.0.1',undefined],[]},
 {429,['ns_1@127.0.0.1',undefined],[]},
 {430,['ns_1@127.0.0.1',undefined],[]},
 {431,['ns_1@127.0.0.1',undefined],[]},
 {432,['ns_1@127.0.0.1',undefined],[]},
 {433,['ns_1@127.0.0.1',undefined],[]},
 {434,['ns_1@127.0.0.1',undefined],[]},
 {435,['ns_1@127.0.0.1',undefined],[]},
 {436,['ns_1@127.0.0.1',undefined],[]},
 {437,['ns_1@127.0.0.1',undefined],[]},
 {438,['ns_1@127.0.0.1',undefined],[]},
 {439,['ns_1@127.0.0.1',undefined],[]},
 {440,['ns_1@127.0.0.1',undefined],[]},
 {441,['ns_1@127.0.0.1',undefined],[]},
 {442,['ns_1@127.0.0.1',undefined],[]},
 {443,['ns_1@127.0.0.1',undefined],[]},
 {444,['ns_1@127.0.0.1',undefined],[]},
 {445,['ns_1@127.0.0.1',undefined],[]},
 {446,['ns_1@127.0.0.1',undefined],[]},
 {447,['ns_1@127.0.0.1',undefined],[]},
 {448,['ns_1@127.0.0.1',undefined],[]},
 {449,['ns_1@127.0.0.1',undefined],[]},
 {450,['ns_1@127.0.0.1',undefined],[]},
 {451,['ns_1@127.0.0.1',undefined],[]},
 {452,['ns_1@127.0.0.1',undefined],[]},
 {453,['ns_1@127.0.0.1',undefined],[]},
 {454,['ns_1@127.0.0.1',undefined],[]},
 {455,['ns_1@127.0.0.1',undefined],[]},
 {456,['ns_1@127.0.0.1',undefined],[]},
 {457,['ns_1@127.0.0.1',undefined],[]},
 {458,['ns_1@127.0.0.1',undefined],[]},
 {459,['ns_1@127.0.0.1',undefined],[]},
 {460,['ns_1@127.0.0.1',undefined],[]},
 {461,['ns_1@127.0.0.1',undefined],[]},
 {462,['ns_1@127.0.0.1',undefined],[]},
 {463,['ns_1@127.0.0.1',undefined],[]},
 {464,['ns_1@127.0.0.1',undefined],[]},
 {465,['ns_1@127.0.0.1',undefined],[]},
 {466,['ns_1@127.0.0.1',undefined],[]},
 {467,['ns_1@127.0.0.1',undefined],[]},
 {468,['ns_1@127.0.0.1',undefined],[]},
 {469,['ns_1@127.0.0.1',undefined],[]},
 {470,['ns_1@127.0.0.1',undefined],[]},
 {471,['ns_1@127.0.0.1',undefined],[]},
 {472,['ns_1@127.0.0.1',undefined],[]},
 {473,['ns_1@127.0.0.1',undefined],[]},
 {474,['ns_1@127.0.0.1',undefined],[]},
 {475,['ns_1@127.0.0.1',undefined],[]},
 {476,['ns_1@127.0.0.1',undefined],[]},
 {477,['ns_1@127.0.0.1',undefined],[]},
 {478,['ns_1@127.0.0.1',undefined],[]},
 {479,['ns_1@127.0.0.1',undefined],[]},
 {480,['ns_1@127.0.0.1',undefined],[]},
 {481,['ns_1@127.0.0.1',undefined],[]},
 {482,['ns_1@127.0.0.1',undefined],[]},
 {483,['ns_1@127.0.0.1',undefined],[]},
 {484,['ns_1@127.0.0.1',undefined],[]},
 {485,['ns_1@127.0.0.1',undefined],[]},
 {486,['ns_1@127.0.0.1',undefined],[]},
 {487,['ns_1@127.0.0.1',undefined],[]},
 {488,['ns_1@127.0.0.1',undefined],[]},
 {489,['ns_1@127.0.0.1',undefined],[]},
 {490,['ns_1@127.0.0.1',undefined],[]},
 {491,['ns_1@127.0.0.1',undefined],[]},
 {492,['ns_1@127.0.0.1',undefined],[]},
 {493,['ns_1@127.0.0.1',undefined],[]},
 {494,['ns_1@127.0.0.1',undefined],[]},
 {495,['ns_1@127.0.0.1',undefined],[]},
 {496,['ns_1@127.0.0.1',undefined],[]},
 {497,['ns_1@127.0.0.1',undefined],[]},
 {498,['ns_1@127.0.0.1',undefined],[]},
 {499,['ns_1@127.0.0.1',undefined],[]},
 {500,['ns_1@127.0.0.1',undefined],[]},
 {501,['ns_1@127.0.0.1',undefined],[]},
 {502,['ns_1@127.0.0.1',undefined],[]},
 {503,['ns_1@127.0.0.1',undefined],[]},
 {504,['ns_1@127.0.0.1',undefined],[]},
 {505,['ns_1@127.0.0.1',undefined],[]},
 {506,['ns_1@127.0.0.1',undefined],[]},
 {507,['ns_1@127.0.0.1',undefined],[]},
 {508,['ns_1@127.0.0.1',undefined],[]},
 {509,['ns_1@127.0.0.1',undefined],[]},
 {510,['ns_1@127.0.0.1',undefined],[]},
 {511,['ns_1@127.0.0.1',undefined],[]},
 {512,['ns_1@127.0.0.1',undefined],[]},
 {513,['ns_1@127.0.0.1',undefined],[]},
 {514,['ns_1@127.0.0.1',undefined],[]},
 {515,['ns_1@127.0.0.1',undefined],[]},
 {516,['ns_1@127.0.0.1',undefined],[]},
 {517,['ns_1@127.0.0.1',undefined],[]},
 {518,['ns_1@127.0.0.1',undefined],[]},
 {519,['ns_1@127.0.0.1',undefined],[]},
 {520,['ns_1@127.0.0.1',undefined],[]},
 {521,['ns_1@127.0.0.1',undefined],[]},
 {522,['ns_1@127.0.0.1',undefined],[]},
 {523,['ns_1@127.0.0.1',undefined],[]},
 {524,['ns_1@127.0.0.1',undefined],[]},
 {525,['ns_1@127.0.0.1',undefined],[]},
 {526,['ns_1@127.0.0.1',undefined],[]},
 {527,['ns_1@127.0.0.1',undefined],[]},
 {528,['ns_1@127.0.0.1',undefined],[]},
 {529,['ns_1@127.0.0.1',undefined],[]},
 {530,['ns_1@127.0.0.1',undefined],[]},
 {531,['ns_1@127.0.0.1',undefined],[]},
 {532,['ns_1@127.0.0.1',undefined],[]},
 {533,['ns_1@127.0.0.1',undefined],[]},
 {534,['ns_1@127.0.0.1',undefined],[]},
 {535,['ns_1@127.0.0.1',undefined],[]},
 {536,['ns_1@127.0.0.1',undefined],[]},
 {537,['ns_1@127.0.0.1',undefined],[]},
 {538,['ns_1@127.0.0.1',undefined],[]},
 {539,['ns_1@127.0.0.1',undefined],[]},
 {540,['ns_1@127.0.0.1',undefined],[]},
 {541,['ns_1@127.0.0.1',undefined],[]},
 {542,['ns_1@127.0.0.1',undefined],[]},
 {543,['ns_1@127.0.0.1',undefined],[]},
 {544,['ns_1@127.0.0.1',undefined],[]},
 {545,['ns_1@127.0.0.1',undefined],[]},
 {546,['ns_1@127.0.0.1',undefined],[]},
 {547,['ns_1@127.0.0.1',undefined],[]},
 {548,['ns_1@127.0.0.1',undefined],[]},
 {549,['ns_1@127.0.0.1',undefined],[]},
 {550,['ns_1@127.0.0.1',undefined],[]},
 {551,['ns_1@127.0.0.1',undefined],[]},
 {552,['ns_1@127.0.0.1',undefined],[]},
 {553,['ns_1@127.0.0.1',undefined],[]},
 {554,['ns_1@127.0.0.1',undefined],[]},
 {555,['ns_1@127.0.0.1',undefined],[]},
 {556,['ns_1@127.0.0.1',undefined],[]},
 {557,['ns_1@127.0.0.1',undefined],[]},
 {558,['ns_1@127.0.0.1',undefined],[]},
 {559,['ns_1@127.0.0.1',undefined],[]},
 {560,['ns_1@127.0.0.1',undefined],[]},
 {561,['ns_1@127.0.0.1',undefined],[]},
 {562,['ns_1@127.0.0.1',undefined],[]},
 {563,['ns_1@127.0.0.1',undefined],[]},
 {564,['ns_1@127.0.0.1',undefined],[]},
 {565,['ns_1@127.0.0.1',undefined],[]},
 {566,['ns_1@127.0.0.1',undefined],[]},
 {567,['ns_1@127.0.0.1',undefined],[]},
 {568,['ns_1@127.0.0.1',undefined],[]},
 {569,['ns_1@127.0.0.1',undefined],[]},
 {570,['ns_1@127.0.0.1',undefined],[]},
 {571,['ns_1@127.0.0.1',undefined],[]},
 {572,['ns_1@127.0.0.1',undefined],[]},
 {573,['ns_1@127.0.0.1',undefined],[]},
 {574,['ns_1@127.0.0.1',undefined],[]},
 {575,['ns_1@127.0.0.1',undefined],[]},
 {576,['ns_1@127.0.0.1',undefined],[]},
 {577,['ns_1@127.0.0.1',undefined],[]},
 {578,['ns_1@127.0.0.1',undefined],[]},
 {579,['ns_1@127.0.0.1',undefined],[]},
 {580,['ns_1@127.0.0.1',undefined],[]},
 {581,['ns_1@127.0.0.1',undefined],[]},
 {582,['ns_1@127.0.0.1',undefined],[]},
 {583,['ns_1@127.0.0.1',undefined],[]},
 {584,['ns_1@127.0.0.1',undefined],[]},
 {585,['ns_1@127.0.0.1',undefined],[]},
 {586,['ns_1@127.0.0.1',undefined],[]},
 {587,['ns_1@127.0.0.1',undefined],[]},
 {588,['ns_1@127.0.0.1',undefined],[]},
 {589,['ns_1@127.0.0.1',undefined],[]},
 {590,['ns_1@127.0.0.1',undefined],[]},
 {591,['ns_1@127.0.0.1',undefined],[]},
 {592,['ns_1@127.0.0.1',undefined],[]},
 {593,['ns_1@127.0.0.1',undefined],[]},
 {594,['ns_1@127.0.0.1',undefined],[]},
 {595,['ns_1@127.0.0.1',undefined],[]},
 {596,['ns_1@127.0.0.1',undefined],[]},
 {597,['ns_1@127.0.0.1',undefined],[]},
 {598,['ns_1@127.0.0.1',undefined],[]},
 {599,['ns_1@127.0.0.1',undefined],[]},
 {600,['ns_1@127.0.0.1',undefined],[]},
 {601,['ns_1@127.0.0.1',undefined],[]},
 {602,['ns_1@127.0.0.1',undefined],[]},
 {603,['ns_1@127.0.0.1',undefined],[]},
 {604,['ns_1@127.0.0.1',undefined],[]},
 {605,['ns_1@127.0.0.1',undefined],[]},
 {606,['ns_1@127.0.0.1',undefined],[]},
 {607,['ns_1@127.0.0.1',undefined],[]},
 {608,['ns_1@127.0.0.1',undefined],[]},
 {609,['ns_1@127.0.0.1',undefined],[]},
 {610,['ns_1@127.0.0.1',undefined],[]},
 {611,['ns_1@127.0.0.1',undefined],[]},
 {612,['ns_1@127.0.0.1',undefined],[]},
 {613,['ns_1@127.0.0.1',undefined],[]},
 {614,['ns_1@127.0.0.1',undefined],[]},
 {615,['ns_1@127.0.0.1',undefined],[]},
 {616,['ns_1@127.0.0.1',undefined],[]},
 {617,['ns_1@127.0.0.1',undefined],[]},
 {618,['ns_1@127.0.0.1',undefined],[]},
 {619,['ns_1@127.0.0.1',undefined],[]},
 {620,['ns_1@127.0.0.1',undefined],[]},
 {621,['ns_1@127.0.0.1',undefined],[]},
 {622,['ns_1@127.0.0.1',undefined],[]},
 {623,['ns_1@127.0.0.1',undefined],[]},
 {624,['ns_1@127.0.0.1',undefined],[]},
 {625,['ns_1@127.0.0.1',undefined],[]},
 {626,['ns_1@127.0.0.1',undefined],[]},
 {627,['ns_1@127.0.0.1',undefined],[]},
 {628,['ns_1@127.0.0.1',undefined],[]},
 {629,['ns_1@127.0.0.1',undefined],[]},
 {630,['ns_1@127.0.0.1',undefined],[]},
 {631,['ns_1@127.0.0.1',undefined],[]},
 {632,['ns_1@127.0.0.1',undefined],[]},
 {633,['ns_1@127.0.0.1',undefined],[]},
 {634,['ns_1@127.0.0.1',undefined],[]},
 {635,['ns_1@127.0.0.1',undefined],[]},
 {636,['ns_1@127.0.0.1',undefined],[]},
 {637,['ns_1@127.0.0.1',undefined],[]},
 {638,['ns_1@127.0.0.1',undefined],[]},
 {639,['ns_1@127.0.0.1',undefined],[]},
 {640,['ns_1@127.0.0.1',undefined],[]},
 {641,['ns_1@127.0.0.1',undefined],[]},
 {642,['ns_1@127.0.0.1',undefined],[]},
 {643,['ns_1@127.0.0.1',undefined],[]},
 {644,['ns_1@127.0.0.1',undefined],[]},
 {645,['ns_1@127.0.0.1',undefined],[]},
 {646,['ns_1@127.0.0.1',undefined],[]},
 {647,['ns_1@127.0.0.1',undefined],[]},
 {648,['ns_1@127.0.0.1',undefined],[]},
 {649,['ns_1@127.0.0.1',undefined],[]},
 {650,['ns_1@127.0.0.1',undefined],[]},
 {651,['ns_1@127.0.0.1',undefined],[]},
 {652,['ns_1@127.0.0.1',undefined],[]},
 {653,['ns_1@127.0.0.1',undefined],[]},
 {654,['ns_1@127.0.0.1',undefined],[]},
 {655,['ns_1@127.0.0.1',undefined],[]},
 {656,['ns_1@127.0.0.1',undefined],[]},
 {657,['ns_1@127.0.0.1',undefined],[]},
 {658,['ns_1@127.0.0.1',undefined],[]},
 {659,['ns_1@127.0.0.1',undefined],[]},
 {660,['ns_1@127.0.0.1',undefined],[]},
 {661,['ns_1@127.0.0.1',undefined],[]},
 {662,['ns_1@127.0.0.1',undefined],[]},
 {663,['ns_1@127.0.0.1',undefined],[]},
 {664,['ns_1@127.0.0.1',undefined],[]},
 {665,['ns_1@127.0.0.1',undefined],[]},
 {666,['ns_1@127.0.0.1',undefined],[]},
 {667,['ns_1@127.0.0.1',undefined],[]},
 {668,['ns_1@127.0.0.1',undefined],[]},
 {669,['ns_1@127.0.0.1',undefined],[]},
 {670,['ns_1@127.0.0.1',undefined],[]},
 {671,['ns_1@127.0.0.1',undefined],[]},
 {672,['ns_1@127.0.0.1',undefined],[]},
 {673,['ns_1@127.0.0.1',undefined],[]},
 {674,['ns_1@127.0.0.1',undefined],[]},
 {675,['ns_1@127.0.0.1',undefined],[]},
 {676,['ns_1@127.0.0.1',undefined],[]},
 {677,['ns_1@127.0.0.1',undefined],[]},
 {678,['ns_1@127.0.0.1',undefined],[]},
 {679,['ns_1@127.0.0.1',undefined],[]},
 {680,['ns_1@127.0.0.1',undefined],[]},
 {681,['ns_1@127.0.0.1',undefined],[]},
 {682,['ns_1@127.0.0.1',undefined],[]},
 {683,['ns_1@127.0.0.1',undefined],[]},
 {684,['ns_1@127.0.0.1',undefined],[]},
 {685,['ns_1@127.0.0.1',undefined],[]},
 {686,['ns_1@127.0.0.1',undefined],[]},
 {687,['ns_1@127.0.0.1',undefined],[]},
 {688,['ns_1@127.0.0.1',undefined],[]},
 {689,['ns_1@127.0.0.1',undefined],[]},
 {690,['ns_1@127.0.0.1',undefined],[]},
 {691,['ns_1@127.0.0.1',undefined],[]},
 {692,['ns_1@127.0.0.1',undefined],[]},
 {693,['ns_1@127.0.0.1',undefined],[]},
 {694,['ns_1@127.0.0.1',undefined],[]},
 {695,['ns_1@127.0.0.1',undefined],[]},
 {696,['ns_1@127.0.0.1',undefined],[]},
 {697,['ns_1@127.0.0.1',undefined],[]},
 {698,['ns_1@127.0.0.1',undefined],[]},
 {699,['ns_1@127.0.0.1',undefined],[]},
 {700,['ns_1@127.0.0.1',undefined],[]},
 {701,['ns_1@127.0.0.1',undefined],[]},
 {702,['ns_1@127.0.0.1',undefined],[]},
 {703,['ns_1@127.0.0.1',undefined],[]},
 {704,['ns_1@127.0.0.1',undefined],[]},
 {705,['ns_1@127.0.0.1',undefined],[]},
 {706,['ns_1@127.0.0.1',undefined],[]},
 {707,['ns_1@127.0.0.1',undefined],[]},
 {708,['ns_1@127.0.0.1',undefined],[]},
 {709,['ns_1@127.0.0.1',undefined],[]},
 {710,['ns_1@127.0.0.1',undefined],[]},
 {711,['ns_1@127.0.0.1',undefined],[]},
 {712,['ns_1@127.0.0.1',undefined],[]},
 {713,['ns_1@127.0.0.1',undefined],[]},
 {714,['ns_1@127.0.0.1',undefined],[]},
 {715,['ns_1@127.0.0.1',undefined],[]},
 {716,['ns_1@127.0.0.1',undefined],[]},
 {717,['ns_1@127.0.0.1',undefined],[]},
 {718,['ns_1@127.0.0.1',undefined],[]},
 {719,['ns_1@127.0.0.1',undefined],[]},
 {720,['ns_1@127.0.0.1',undefined],[]},
 {721,['ns_1@127.0.0.1',undefined],[]},
 {722,['ns_1@127.0.0.1',undefined],[]},
 {723,['ns_1@127.0.0.1',undefined],[]},
 {724,['ns_1@127.0.0.1',undefined],[]},
 {725,['ns_1@127.0.0.1',undefined],[]},
 {726,['ns_1@127.0.0.1',undefined],[]},
 {727,['ns_1@127.0.0.1',undefined],[]},
 {728,['ns_1@127.0.0.1',undefined],[]},
 {729,['ns_1@127.0.0.1',undefined],[]},
 {730,['ns_1@127.0.0.1',undefined],[]},
 {731,['ns_1@127.0.0.1',undefined],[]},
 {732,['ns_1@127.0.0.1',undefined],[]},
 {733,['ns_1@127.0.0.1',undefined],[]},
 {734,['ns_1@127.0.0.1',undefined],[]},
 {735,['ns_1@127.0.0.1',undefined],[]},
 {736,['ns_1@127.0.0.1',undefined],[]},
 {737,['ns_1@127.0.0.1',undefined],[]},
 {738,['ns_1@127.0.0.1',undefined],[]},
 {739,['ns_1@127.0.0.1',undefined],[]},
 {740,['ns_1@127.0.0.1',undefined],[]},
 {741,['ns_1@127.0.0.1',undefined],[]},
 {742,['ns_1@127.0.0.1',undefined],[]},
 {743,['ns_1@127.0.0.1',undefined],[]},
 {744,['ns_1@127.0.0.1',undefined],[]},
 {745,['ns_1@127.0.0.1',undefined],[]},
 {746,['ns_1@127.0.0.1',undefined],[]},
 {747,['ns_1@127.0.0.1',undefined],[]},
 {748,['ns_1@127.0.0.1',undefined],[]},
 {749,['ns_1@127.0.0.1',undefined],[]},
 {750,['ns_1@127.0.0.1',undefined],[]},
 {751,['ns_1@127.0.0.1',undefined],[]},
 {752,['ns_1@127.0.0.1',undefined],[]},
 {753,['ns_1@127.0.0.1',undefined],[]},
 {754,['ns_1@127.0.0.1',undefined],[]},
 {755,['ns_1@127.0.0.1',undefined],[]},
 {756,['ns_1@127.0.0.1',undefined],[]},
 {757,['ns_1@127.0.0.1',undefined],[]},
 {758,['ns_1@127.0.0.1',undefined],[]},
 {759,['ns_1@127.0.0.1',undefined],[]},
 {760,['ns_1@127.0.0.1',undefined],[]},
 {761,['ns_1@127.0.0.1',undefined],[]},
 {762,['ns_1@127.0.0.1',undefined],[]},
 {763,['ns_1@127.0.0.1',undefined],[]},
 {764,['ns_1@127.0.0.1',undefined],[]},
 {765,['ns_1@127.0.0.1',undefined],[]},
 {766,['ns_1@127.0.0.1',undefined],[]},
 {767,['ns_1@127.0.0.1',undefined],[]},
 {768,['ns_1@127.0.0.1',undefined],[]},
 {769,['ns_1@127.0.0.1',undefined],[]},
 {770,['ns_1@127.0.0.1',undefined],[]},
 {771,['ns_1@127.0.0.1',undefined],[]},
 {772,['ns_1@127.0.0.1',undefined],[]},
 {773,['ns_1@127.0.0.1',undefined],[]},
 {774,['ns_1@127.0.0.1',undefined],[]},
 {775,['ns_1@127.0.0.1',undefined],[]},
 {776,['ns_1@127.0.0.1',undefined],[]},
 {777,['ns_1@127.0.0.1',undefined],[]},
 {778,['ns_1@127.0.0.1',undefined],[]},
 {779,['ns_1@127.0.0.1',undefined],[]},
 {780,['ns_1@127.0.0.1',undefined],[]},
 {781,['ns_1@127.0.0.1',undefined],[]},
 {782,['ns_1@127.0.0.1',undefined],[]},
 {783,['ns_1@127.0.0.1',undefined],[]},
 {784,['ns_1@127.0.0.1',undefined],[]},
 {785,['ns_1@127.0.0.1',undefined],[]},
 {786,['ns_1@127.0.0.1',undefined],[]},
 {787,['ns_1@127.0.0.1',undefined],[]},
 {788,['ns_1@127.0.0.1',undefined],[]},
 {789,['ns_1@127.0.0.1',undefined],[]},
 {790,['ns_1@127.0.0.1',undefined],[]},
 {791,['ns_1@127.0.0.1',undefined],[]},
 {792,['ns_1@127.0.0.1',undefined],[]},
 {793,['ns_1@127.0.0.1',undefined],[]},
 {794,['ns_1@127.0.0.1',undefined],[]},
 {795,['ns_1@127.0.0.1',undefined],[]},
 {796,['ns_1@127.0.0.1',undefined],[]},
 {797,['ns_1@127.0.0.1',undefined],[]},
 {798,['ns_1@127.0.0.1',undefined],[]},
 {799,['ns_1@127.0.0.1',undefined],[]},
 {800,['ns_1@127.0.0.1',undefined],[]},
 {801,['ns_1@127.0.0.1',undefined],[]},
 {802,['ns_1@127.0.0.1',undefined],[]},
 {803,['ns_1@127.0.0.1',undefined],[]},
 {804,['ns_1@127.0.0.1',undefined],[]},
 {805,['ns_1@127.0.0.1',undefined],[]},
 {806,['ns_1@127.0.0.1',undefined],[]},
 {807,['ns_1@127.0.0.1',undefined],[]},
 {808,['ns_1@127.0.0.1',undefined],[]},
 {809,['ns_1@127.0.0.1',undefined],[]},
 {810,['ns_1@127.0.0.1',undefined],[]},
 {811,['ns_1@127.0.0.1',undefined],[]},
 {812,['ns_1@127.0.0.1',undefined],[]},
 {813,['ns_1@127.0.0.1',undefined],[]},
 {814,['ns_1@127.0.0.1',undefined],[]},
 {815,['ns_1@127.0.0.1',undefined],[]},
 {816,['ns_1@127.0.0.1',undefined],[]},
 {817,['ns_1@127.0.0.1',undefined],[]},
 {818,['ns_1@127.0.0.1',undefined],[]},
 {819,['ns_1@127.0.0.1',undefined],[]},
 {820,['ns_1@127.0.0.1',undefined],[]},
 {821,['ns_1@127.0.0.1',undefined],[]},
 {822,['ns_1@127.0.0.1',undefined],[]},
 {823,['ns_1@127.0.0.1',undefined],[]},
 {824,['ns_1@127.0.0.1',undefined],[]},
 {825,['ns_1@127.0.0.1',undefined],[]},
 {826,['ns_1@127.0.0.1',undefined],[]},
 {827,['ns_1@127.0.0.1',undefined],[]},
 {828,['ns_1@127.0.0.1',undefined],[]},
 {829,['ns_1@127.0.0.1',undefined],[]},
 {830,['ns_1@127.0.0.1',undefined],[]},
 {831,['ns_1@127.0.0.1',undefined],[]},
 {832,['ns_1@127.0.0.1',undefined],[]},
 {833,['ns_1@127.0.0.1',undefined],[]},
 {834,['ns_1@127.0.0.1',undefined],[]},
 {835,['ns_1@127.0.0.1',undefined],[]},
 {836,['ns_1@127.0.0.1',undefined],[]},
 {837,['ns_1@127.0.0.1',undefined],[]},
 {838,['ns_1@127.0.0.1',undefined],[]},
 {839,['ns_1@127.0.0.1',undefined],[]},
 {840,['ns_1@127.0.0.1',undefined],[]},
 {841,['ns_1@127.0.0.1',undefined],[]},
 {842,['ns_1@127.0.0.1',undefined],[]},
 {843,['ns_1@127.0.0.1',undefined],[]},
 {844,['ns_1@127.0.0.1',undefined],[]},
 {845,['ns_1@127.0.0.1',undefined],[]},
 {846,['ns_1@127.0.0.1',undefined],[]},
 {847,['ns_1@127.0.0.1',undefined],[]},
 {848,['ns_1@127.0.0.1',undefined],[]},
 {849,['ns_1@127.0.0.1',undefined],[]},
 {850,['ns_1@127.0.0.1',undefined],[]},
 {851,['ns_1@127.0.0.1',undefined],[]},
 {852,['ns_1@127.0.0.1',undefined],[]},
 {853,['ns_1@127.0.0.1',undefined],[]},
 {854,['ns_1@127.0.0.1',undefined],[]},
 {855,['ns_1@127.0.0.1',undefined],[]},
 {856,['ns_1@127.0.0.1',undefined],[]},
 {857,['ns_1@127.0.0.1',undefined],[]},
 {858,['ns_1@127.0.0.1',undefined],[]},
 {859,['ns_1@127.0.0.1',undefined],[]},
 {860,['ns_1@127.0.0.1',undefined],[]},
 {861,['ns_1@127.0.0.1',undefined],[]},
 {862,['ns_1@127.0.0.1',undefined],[]},
 {863,['ns_1@127.0.0.1',undefined],[]},
 {864,['ns_1@127.0.0.1',undefined],[]},
 {865,['ns_1@127.0.0.1',undefined],[]},
 {866,['ns_1@127.0.0.1',undefined],[]},
 {867,['ns_1@127.0.0.1',undefined],[]},
 {868,['ns_1@127.0.0.1',undefined],[]},
 {869,['ns_1@127.0.0.1',undefined],[]},
 {870,['ns_1@127.0.0.1',undefined],[]},
 {871,['ns_1@127.0.0.1',undefined],[]},
 {872,['ns_1@127.0.0.1',undefined],[]},
 {873,['ns_1@127.0.0.1',undefined],[]},
 {874,['ns_1@127.0.0.1',undefined],[]},
 {875,['ns_1@127.0.0.1',undefined],[]},
 {876,['ns_1@127.0.0.1',undefined],[]},
 {877,['ns_1@127.0.0.1',undefined],[]},
 {878,['ns_1@127.0.0.1',undefined],[]},
 {879,['ns_1@127.0.0.1',undefined],[]},
 {880,['ns_1@127.0.0.1',undefined],[]},
 {881,['ns_1@127.0.0.1',undefined],[]},
 {882,['ns_1@127.0.0.1',undefined],[]},
 {883,['ns_1@127.0.0.1',undefined],[]},
 {884,['ns_1@127.0.0.1',undefined],[]},
 {885,['ns_1@127.0.0.1',undefined],[]},
 {886,['ns_1@127.0.0.1',undefined],[]},
 {887,['ns_1@127.0.0.1',undefined],[]},
 {888,['ns_1@127.0.0.1',undefined],[]},
 {889,['ns_1@127.0.0.1',undefined],[]},
 {890,['ns_1@127.0.0.1',undefined],[]},
 {891,['ns_1@127.0.0.1',undefined],[]},
 {892,['ns_1@127.0.0.1',undefined],[]},
 {893,['ns_1@127.0.0.1',undefined],[]},
 {894,['ns_1@127.0.0.1',undefined],[]},
 {895,['ns_1@127.0.0.1',undefined],[]},
 {896,['ns_1@127.0.0.1',undefined],[]},
 {897,['ns_1@127.0.0.1',undefined],[]},
 {898,['ns_1@127.0.0.1',undefined],[]},
 {899,['ns_1@127.0.0.1',undefined],[]},
 {900,['ns_1@127.0.0.1',undefined],[]},
 {901,['ns_1@127.0.0.1',undefined],[]},
 {902,['ns_1@127.0.0.1',undefined],[]},
 {903,['ns_1@127.0.0.1',undefined],[]},
 {904,['ns_1@127.0.0.1',undefined],[]},
 {905,['ns_1@127.0.0.1',undefined],[]},
 {906,['ns_1@127.0.0.1',undefined],[]},
 {907,['ns_1@127.0.0.1',undefined],[]},
 {908,['ns_1@127.0.0.1',undefined],[]},
 {909,['ns_1@127.0.0.1',undefined],[]},
 {910,['ns_1@127.0.0.1',undefined],[]},
 {911,['ns_1@127.0.0.1',undefined],[]},
 {912,['ns_1@127.0.0.1',undefined],[]},
 {913,['ns_1@127.0.0.1',undefined],[]},
 {914,['ns_1@127.0.0.1',undefined],[]},
 {915,['ns_1@127.0.0.1',undefined],[]},
 {916,['ns_1@127.0.0.1',undefined],[]},
 {917,['ns_1@127.0.0.1',undefined],[]},
 {918,['ns_1@127.0.0.1',undefined],[]},
 {919,['ns_1@127.0.0.1',undefined],[]},
 {920,['ns_1@127.0.0.1',undefined],[]},
 {921,['ns_1@127.0.0.1',undefined],[]},
 {922,['ns_1@127.0.0.1',undefined],[]},
 {923,['ns_1@127.0.0.1',undefined],[]},
 {924,['ns_1@127.0.0.1',undefined],[]},
 {925,['ns_1@127.0.0.1',undefined],[]},
 {926,['ns_1@127.0.0.1',undefined],[]},
 {927,['ns_1@127.0.0.1',undefined],[]},
 {928,['ns_1@127.0.0.1',undefined],[]},
 {929,['ns_1@127.0.0.1',undefined],[]},
 {930,['ns_1@127.0.0.1',undefined],[]},
 {931,['ns_1@127.0.0.1',undefined],[]},
 {932,['ns_1@127.0.0.1',undefined],[]},
 {933,['ns_1@127.0.0.1',undefined],[]},
 {934,['ns_1@127.0.0.1',undefined],[]},
 {935,['ns_1@127.0.0.1',undefined],[]},
 {936,['ns_1@127.0.0.1',undefined],[]},
 {937,['ns_1@127.0.0.1',undefined],[]},
 {938,['ns_1@127.0.0.1',undefined],[]},
 {939,['ns_1@127.0.0.1',undefined],[]},
 {940,['ns_1@127.0.0.1',undefined],[]},
 {941,['ns_1@127.0.0.1',undefined],[]},
 {942,['ns_1@127.0.0.1',undefined],[]},
 {943,['ns_1@127.0.0.1',undefined],[]},
 {944,['ns_1@127.0.0.1',undefined],[]},
 {945,['ns_1@127.0.0.1',undefined],[]},
 {946,['ns_1@127.0.0.1',undefined],[]},
 {947,['ns_1@127.0.0.1',undefined],[]},
 {948,['ns_1@127.0.0.1',undefined],[]},
 {949,['ns_1@127.0.0.1',undefined],[]},
 {950,['ns_1@127.0.0.1',undefined],[]},
 {951,['ns_1@127.0.0.1',undefined],[]},
 {952,['ns_1@127.0.0.1',undefined],[]},
 {953,['ns_1@127.0.0.1',undefined],[]},
 {954,['ns_1@127.0.0.1',undefined],[]},
 {955,['ns_1@127.0.0.1',undefined],[]},
 {956,['ns_1@127.0.0.1',undefined],[]},
 {957,['ns_1@127.0.0.1',undefined],[]},
 {958,['ns_1@127.0.0.1',undefined],[]},
 {959,['ns_1@127.0.0.1',undefined],[]},
 {960,['ns_1@127.0.0.1',undefined],[]},
 {961,['ns_1@127.0.0.1',undefined],[]},
 {962,['ns_1@127.0.0.1',undefined],[]},
 {963,['ns_1@127.0.0.1',undefined],[]},
 {964,['ns_1@127.0.0.1',undefined],[]},
 {965,['ns_1@127.0.0.1',undefined],[]},
 {966,['ns_1@127.0.0.1',undefined],[]},
 {967,['ns_1@127.0.0.1',undefined],[]},
 {968,['ns_1@127.0.0.1',undefined],[]},
 {969,['ns_1@127.0.0.1',undefined],[]},
 {970,['ns_1@127.0.0.1',undefined],[]},
 {971,['ns_1@127.0.0.1',undefined],[]},
 {972,['ns_1@127.0.0.1',undefined],[]},
 {973,['ns_1@127.0.0.1',undefined],[]},
 {974,['ns_1@127.0.0.1',undefined],[]},
 {975,['ns_1@127.0.0.1',undefined],[]},
 {976,['ns_1@127.0.0.1',undefined],[]},
 {977,['ns_1@127.0.0.1',undefined],[]},
 {978,['ns_1@127.0.0.1',undefined],[]},
 {979,['ns_1@127.0.0.1',undefined],[]},
 {980,['ns_1@127.0.0.1',undefined],[]},
 {981,['ns_1@127.0.0.1',undefined],[]},
 {982,['ns_1@127.0.0.1',undefined],[]},
 {983,['ns_1@127.0.0.1',undefined],[]},
 {984,['ns_1@127.0.0.1',undefined],[]},
 {985,['ns_1@127.0.0.1',undefined],[]},
 {986,['ns_1@127.0.0.1',undefined],[]},
 {987,['ns_1@127.0.0.1',undefined],[]},
 {988,['ns_1@127.0.0.1',undefined],[]},
 {989,['ns_1@127.0.0.1',undefined],[]},
 {990,['ns_1@127.0.0.1',undefined],[]},
 {991,['ns_1@127.0.0.1',undefined],[]},
 {992,['ns_1@127.0.0.1',undefined],[]},
 {993,['ns_1@127.0.0.1',undefined],[]},
 {994,['ns_1@127.0.0.1',undefined],[]},
 {995,['ns_1@127.0.0.1',undefined],[]},
 {996,['ns_1@127.0.0.1',undefined],[]},
 {997,['ns_1@127.0.0.1',undefined],[]},
 {998,['ns_1@127.0.0.1',undefined],[]},
 {999,['ns_1@127.0.0.1',undefined],[]},
 {1000,['ns_1@127.0.0.1',undefined],[]},
 {1001,['ns_1@127.0.0.1',undefined],[]},
 {1002,['ns_1@127.0.0.1',undefined],[]},
 {1003,['ns_1@127.0.0.1',undefined],[]},
 {1004,['ns_1@127.0.0.1',undefined],[]},
 {1005,['ns_1@127.0.0.1',undefined],[]},
 {1006,['ns_1@127.0.0.1',undefined],[]},
 {1007,['ns_1@127.0.0.1',undefined],[]},
 {1008,['ns_1@127.0.0.1',undefined],[]},
 {1009,['ns_1@127.0.0.1',undefined],[]},
 {1010,['ns_1@127.0.0.1',undefined],[]},
 {1011,['ns_1@127.0.0.1',undefined],[]},
 {1012,['ns_1@127.0.0.1',undefined],[]},
 {1013,['ns_1@127.0.0.1',undefined],[]},
 {1014,['ns_1@127.0.0.1',undefined],[]},
 {1015,['ns_1@127.0.0.1',undefined],[]},
 {1016,['ns_1@127.0.0.1',undefined],[]},
 {1017,['ns_1@127.0.0.1',undefined],[]},
 {1018,['ns_1@127.0.0.1',undefined],[]},
 {1019,['ns_1@127.0.0.1',undefined],[]},
 {1020,['ns_1@127.0.0.1',undefined],[]},
 {1021,['ns_1@127.0.0.1',undefined],[]},
 {1022,['ns_1@127.0.0.1',undefined],[]},
 {1023,['ns_1@127.0.0.1',undefined],[]}]
[ns_server:debug,2020-03-27T20:46:35.459Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:config_sync:259]Going to pull config to/from nodes:
['ns_1@127.0.0.1']
[ns_server:info,2020-03-27T20:46:35.462Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 0 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.462Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 1 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.462Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 2 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.462Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 3 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.463Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 4 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.463Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 5 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.463Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 6 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.463Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 7 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.463Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 8 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.463Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 9 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.463Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 10 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.463Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 11 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.463Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 12 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.463Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 13 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.463Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 14 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.463Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 15 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.463Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 16 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.463Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 17 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.463Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 18 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.463Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 19 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.463Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 20 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.463Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 21 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.463Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 22 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.463Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 23 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.463Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 24 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.463Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 25 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.463Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 26 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.463Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 27 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.464Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 28 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.464Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 29 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.464Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 30 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.464Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 31 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.464Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 32 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.464Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 33 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.464Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 34 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.464Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 35 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.464Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 36 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.464Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 37 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.464Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 38 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.464Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 39 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.464Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 40 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.464Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 41 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.464Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 42 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.464Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 43 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.464Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 44 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.464Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 45 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.464Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 46 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.464Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 47 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.464Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 48 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.464Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 49 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.464Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 50 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.464Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 51 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.464Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 52 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.464Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 53 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.465Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 54 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.465Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 55 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.465Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 56 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.465Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 57 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.465Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 58 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.465Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 59 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.465Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 60 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.465Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 61 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.465Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 62 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.465Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 63 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.465Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 64 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.465Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 65 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.465Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 66 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.465Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 67 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.465Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 68 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.465Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 69 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.465Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 70 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.465Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 71 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.465Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 72 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.465Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 73 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.465Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 74 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.465Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 75 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.465Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 76 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.466Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 77 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.466Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 78 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.466Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 79 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.466Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 80 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.466Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 81 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.466Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 82 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.466Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 83 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.466Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 84 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.466Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 85 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.466Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 86 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.466Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 87 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.466Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 88 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.466Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 89 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.466Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 90 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.466Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 91 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.466Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 92 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.466Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 93 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.466Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 94 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.466Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 95 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.466Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 96 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.466Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 97 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.466Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 98 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.466Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 99 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.466Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 100 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.466Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 101 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.466Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 102 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.466Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 103 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.466Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 104 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.466Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 105 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.466Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 106 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.467Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 107 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.467Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 108 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.467Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 109 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.467Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 110 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.467Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 111 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.467Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 112 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.467Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 113 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.467Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 114 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.467Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 115 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.467Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 116 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.467Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 117 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.467Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 118 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.467Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 119 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.467Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 120 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.467Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 121 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.467Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 122 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.467Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 123 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.467Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 124 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.467Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 125 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.467Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 126 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.467Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 127 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.467Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 128 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.467Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 129 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.467Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 130 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.467Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 131 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.467Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 132 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.467Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 133 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.468Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 134 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.468Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 135 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.468Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 136 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.468Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 137 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.468Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 138 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.468Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 139 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.468Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 140 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.468Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 141 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.468Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 142 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.468Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 143 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.468Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 144 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.468Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 145 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.468Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 146 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.468Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 147 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.468Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 148 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.468Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 149 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.468Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 150 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.468Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 151 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.468Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 152 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.468Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 153 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.468Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 154 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.468Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 155 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.468Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 156 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.468Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 157 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.468Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 158 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.468Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 159 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.469Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 160 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.469Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 161 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.469Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 162 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.469Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 163 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.469Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 164 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.469Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 165 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.469Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 166 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.469Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 167 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.469Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 168 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.469Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 169 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.469Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 170 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.469Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 171 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.469Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 172 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.469Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 173 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.469Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 174 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.469Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 175 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.469Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 176 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.469Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 177 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.469Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 178 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.469Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 179 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.469Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 180 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.469Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 181 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.469Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 182 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.469Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 183 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.469Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 184 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.469Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 185 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.469Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 186 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.469Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 187 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.469Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 188 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.469Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 189 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.469Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 190 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.469Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 191 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.470Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 192 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.470Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 193 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.470Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 194 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.470Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 195 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.470Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 196 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.470Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 197 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.470Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 198 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.470Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 199 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.470Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 200 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.470Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 201 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.470Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 202 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.470Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 203 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.470Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 204 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.470Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 205 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.470Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 206 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.470Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 207 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.470Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 208 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.470Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 209 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.470Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 210 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.470Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 211 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.470Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 212 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.470Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 213 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.470Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 214 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.470Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 215 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.470Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 216 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.470Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 217 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.470Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 218 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.470Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 219 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.471Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 220 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.471Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 221 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.471Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 222 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.471Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 223 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.471Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 224 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.471Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 225 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.471Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 226 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.471Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 227 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.471Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 228 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.471Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 229 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.471Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 230 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.471Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 231 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.471Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 232 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.471Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 233 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.471Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 234 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.471Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 235 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.471Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 236 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.471Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 237 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.471Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 238 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.471Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 239 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.471Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 240 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.471Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 241 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.471Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 242 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.471Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 243 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.471Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 244 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.471Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 245 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.471Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 246 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.471Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 247 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.471Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 248 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.471Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 249 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.471Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 250 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.471Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 251 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.472Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 252 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.472Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 253 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.472Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 254 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.472Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 255 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.472Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 256 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.472Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 257 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.472Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 258 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.472Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 259 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.472Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 260 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.472Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 261 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.472Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 262 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.472Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 263 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.472Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 264 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.472Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 265 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.472Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 266 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.472Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 267 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.472Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 268 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.472Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 269 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.472Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 270 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.472Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 271 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.472Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 272 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.472Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 273 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.472Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 274 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.472Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 275 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.472Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 276 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.472Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 277 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.472Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 278 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.472Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 279 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.472Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 280 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.472Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 281 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.472Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 282 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.472Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 283 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.472Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 284 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.472Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 285 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.473Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 286 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.473Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 287 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.473Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 288 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.473Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 289 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.473Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 290 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.473Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 291 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.473Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 292 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.473Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 293 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.473Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 294 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.473Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 295 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.473Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 296 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.473Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 297 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.473Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 298 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.473Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 299 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.473Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 300 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.473Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 301 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.473Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 302 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.473Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 303 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.473Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 304 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.473Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 305 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.473Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 306 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.473Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 307 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.473Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 308 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.473Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 309 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.473Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 310 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.473Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 311 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.473Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 312 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.473Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 313 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.473Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 314 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.473Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 315 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.473Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 316 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.474Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 317 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.474Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 318 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.474Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 319 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.474Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 320 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.474Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 321 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.474Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 322 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.474Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 323 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.474Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 324 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.474Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 325 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.474Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 326 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.474Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 327 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.474Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 328 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.474Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 329 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.474Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 330 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.474Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 331 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.474Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 332 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.474Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 333 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.474Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 334 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.474Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 335 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.474Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 336 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.474Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 337 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.475Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 338 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.475Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 339 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.475Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 340 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.475Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 341 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.475Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 342 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.475Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 343 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.475Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 344 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.475Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 345 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.475Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 346 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.475Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 347 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.475Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 348 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.475Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 349 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.475Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 350 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.475Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 351 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.475Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 352 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.475Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 353 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.475Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 354 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.475Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 355 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.475Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 356 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.476Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 357 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.476Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 358 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.476Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 359 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.476Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 360 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.476Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 361 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.476Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 362 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.476Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 363 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.476Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 364 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.476Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 365 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.476Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 366 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.476Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 367 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.476Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 368 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.476Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 369 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.476Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 370 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.476Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 371 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.476Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 372 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.476Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 373 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.476Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 374 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.476Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 375 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.476Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 376 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.476Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 377 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.476Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 378 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.476Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 379 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.477Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 380 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.477Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 381 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.477Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 382 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.477Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 383 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.477Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 384 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.477Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 385 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.477Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 386 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.477Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 387 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.477Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 388 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.477Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 389 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.477Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 390 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.477Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 391 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.477Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 392 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.477Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 393 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.477Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 394 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.477Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 395 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.477Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 396 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.477Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 397 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.477Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 398 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.477Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 399 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.477Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 400 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.477Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 401 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.477Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 402 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.477Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 403 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.477Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 404 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.477Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 405 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.477Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 406 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.477Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 407 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.477Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 408 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.477Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 409 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.477Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 410 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.477Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 411 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.478Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 412 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.478Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 413 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.478Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 414 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.478Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 415 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.478Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 416 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.478Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 417 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.478Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 418 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.478Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 419 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.478Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 420 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.478Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 421 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.478Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 422 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.478Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 423 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.478Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 424 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.478Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 425 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.478Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 426 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.478Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 427 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.478Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 428 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.478Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 429 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.478Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 430 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.478Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 431 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.478Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 432 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.478Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 433 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.478Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 434 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.478Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 435 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.478Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 436 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.478Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 437 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.478Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 438 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.478Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 439 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.478Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 440 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.479Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 441 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.479Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 442 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.479Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 443 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.479Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 444 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.479Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 445 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.479Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 446 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.479Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 447 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.479Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 448 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.479Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 449 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.479Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 450 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.479Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 451 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.479Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 452 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.479Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 453 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.479Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 454 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.479Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 455 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.479Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 456 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.479Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 457 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.479Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 458 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.479Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 459 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.479Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 460 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.479Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 461 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.479Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 462 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.479Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 463 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.479Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 464 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.479Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 465 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.479Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 466 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.479Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 467 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.480Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 468 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.480Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 469 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.480Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 470 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.480Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 471 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.480Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 472 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.480Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 473 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.480Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 474 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.480Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 475 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.480Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 476 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.480Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 477 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.480Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 478 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.480Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 479 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.480Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 480 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.480Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 481 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.480Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 482 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.480Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 483 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.480Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 484 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.480Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 485 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.480Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 486 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.480Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 487 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.480Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 488 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.480Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 489 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.480Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 490 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.480Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 491 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.480Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 492 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.480Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 493 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.480Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 494 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.480Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 495 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.480Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 496 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.481Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 497 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.481Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 498 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.481Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 499 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.481Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 500 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.481Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 501 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.481Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 502 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.481Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 503 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.481Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 504 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.481Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 505 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.481Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 506 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.481Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 507 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.481Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 508 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.481Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 509 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.481Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 510 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.481Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 511 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.481Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 512 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.481Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 513 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.481Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 514 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.481Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 515 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.481Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 516 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.481Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 517 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.481Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 518 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.481Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 519 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.481Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 520 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.481Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 521 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.481Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 522 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.481Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 523 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.481Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 524 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.482Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 525 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.482Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 526 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.482Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 527 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.482Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 528 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.482Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 529 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.482Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 530 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.482Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 531 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.482Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 532 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.482Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 533 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.482Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 534 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.482Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 535 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.482Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 536 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.482Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 537 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.482Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 538 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.482Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 539 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.482Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 540 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.482Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 541 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.482Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 542 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.482Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 543 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.482Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 544 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.482Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 545 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.482Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 546 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.482Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 547 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.482Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 548 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.482Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 549 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.482Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 550 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.482Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 551 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.482Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 552 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.482Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 553 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.482Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 554 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.483Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 555 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.483Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 556 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.483Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 557 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.483Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 558 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.483Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 559 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.483Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 560 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.483Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 561 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.483Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 562 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.483Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 563 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.483Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 564 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.483Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 565 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.483Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 566 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.483Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 567 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.483Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 568 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.483Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 569 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.483Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 570 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.483Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 571 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.483Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 572 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.483Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 573 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.483Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 574 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.483Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 575 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.483Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 576 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.483Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 577 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.483Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 578 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.483Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 579 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.483Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 580 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.483Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 581 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.483Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 582 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.484Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 583 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.484Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 584 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.484Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 585 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.484Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 586 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.484Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 587 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.484Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 588 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.484Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 589 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.484Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 590 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.484Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 591 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.484Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 592 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.484Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 593 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.484Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 594 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.484Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 595 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.484Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 596 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.484Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 597 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.484Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 598 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.484Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 599 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.484Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 600 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.484Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 601 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.484Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 602 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.484Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 603 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.484Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 604 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.484Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 605 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.484Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 606 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.484Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 607 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.484Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 608 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.484Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 609 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.484Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 610 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.484Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 611 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.484Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 612 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.485Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 613 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.485Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 614 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.485Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 615 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.485Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 616 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.485Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 617 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.485Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 618 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.485Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 619 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.485Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 620 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.485Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 621 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.485Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 622 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.485Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 623 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.485Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 624 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.485Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 625 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.485Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 626 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.485Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 627 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.485Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 628 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.485Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 629 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.485Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 630 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.485Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 631 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.485Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 632 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.485Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 633 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.485Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 634 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.485Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 635 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.485Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 636 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.485Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 637 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.485Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 638 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.485Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 639 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.485Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 640 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.486Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 641 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.486Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 642 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.486Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 643 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.486Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 644 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.486Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 645 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.486Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 646 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.486Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 647 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.486Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 648 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.486Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 649 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.486Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 650 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.486Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 651 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.486Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 652 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.486Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 653 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.486Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 654 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.486Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 655 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.486Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 656 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.486Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 657 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.486Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 658 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.486Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 659 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.486Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 660 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.486Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 661 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.486Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 662 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.486Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 663 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.486Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 664 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.486Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 665 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.486Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 666 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.486Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 667 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.486Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 668 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.486Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 669 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.486Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 670 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.486Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 671 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.487Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 672 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.487Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 673 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.487Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 674 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.487Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 675 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.487Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 676 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.487Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 677 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.487Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 678 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.487Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 679 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.487Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 680 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.487Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 681 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.487Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 682 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.487Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 683 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.487Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 684 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.487Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 685 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.487Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 686 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.487Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 687 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.487Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 688 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.487Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 689 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.487Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 690 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.487Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 691 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.487Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 692 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.487Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 693 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.487Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 694 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.487Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 695 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.487Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 696 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.488Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 697 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.488Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 698 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.488Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 699 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.488Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 700 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.488Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 701 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.488Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 702 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.488Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 703 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.488Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 704 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.488Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 705 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.488Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 706 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.488Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 707 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.488Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 708 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.488Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 709 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.488Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 710 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.488Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 711 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.488Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 712 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.488Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 713 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.488Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 714 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.488Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 715 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.489Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 716 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.489Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 717 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.489Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 718 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.489Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 719 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.489Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 720 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.489Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 721 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.489Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 722 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.489Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 723 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.489Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 724 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.489Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 725 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.489Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 726 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.489Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 727 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.489Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 728 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.489Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 729 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.489Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 730 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.489Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 731 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.489Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 732 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.489Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 733 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.489Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 734 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.489Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 735 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.489Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 736 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.489Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 737 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.489Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 738 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.489Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 739 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.489Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 740 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.489Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 741 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.489Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 742 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.489Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 743 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.489Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 744 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.490Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 745 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.490Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 746 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.490Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 747 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.490Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 748 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.490Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 749 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.490Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 750 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.490Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 751 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.490Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 752 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.490Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 753 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.490Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 754 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.490Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 755 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.490Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 756 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.490Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 757 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.490Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 758 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.490Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 759 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.490Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 760 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.490Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 761 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.490Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 762 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.490Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 763 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.490Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 764 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.490Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 765 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.490Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 766 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.490Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 767 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.490Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 768 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.490Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 769 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.490Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 770 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.490Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 771 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.490Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 772 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.490Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 773 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.491Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 774 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.491Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 775 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.491Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 776 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.491Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 777 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.491Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 778 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.491Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 779 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.491Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 780 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.491Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 781 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.491Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 782 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.491Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 783 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.491Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 784 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.491Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 785 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.491Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 786 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.491Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 787 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.491Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 788 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.491Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 789 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.491Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 790 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.491Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 791 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.491Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 792 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.491Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 793 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.491Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 794 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.491Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 795 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.491Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 796 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.491Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 797 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.491Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 798 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.491Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 799 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.491Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 800 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.491Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 801 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.492Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 802 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.492Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 803 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.492Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 804 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.492Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 805 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.492Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 806 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.492Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 807 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.492Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 808 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.492Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 809 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.492Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 810 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.492Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 811 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.492Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 812 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.492Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 813 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.492Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 814 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.492Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 815 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.492Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 816 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.492Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 817 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.492Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 818 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.492Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 819 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.492Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 820 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.492Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 821 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.492Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 822 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.492Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 823 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.492Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 824 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.492Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 825 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.492Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 826 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.492Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 827 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.492Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 828 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.492Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 829 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.493Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 830 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.493Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 831 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.493Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 832 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.493Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 833 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.493Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 834 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.493Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 835 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.493Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 836 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.493Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 837 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.493Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 838 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.493Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 839 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.493Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 840 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.493Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 841 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.493Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 842 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.493Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 843 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.493Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 844 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.493Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 845 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.493Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 846 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.493Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 847 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.493Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 848 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.493Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 849 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.494Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 850 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.494Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 851 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.495Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 852 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.495Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 853 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.495Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 854 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.495Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 855 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.495Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 856 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.495Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 857 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.495Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 858 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.495Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 859 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.495Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 860 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.495Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 861 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.496Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 862 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.496Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 863 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.496Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 864 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.496Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 865 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.496Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 866 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.496Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 867 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.496Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 868 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.496Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 869 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.496Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 870 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.497Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 871 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.497Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 872 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.497Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 873 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.497Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 874 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.497Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 875 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.497Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 876 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.497Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 877 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.497Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 878 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.497Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 879 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.497Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 880 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.497Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 881 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.497Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 882 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.497Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 883 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.497Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 884 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.497Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 885 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.498Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 886 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.498Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 887 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.498Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 888 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.498Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 889 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.498Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 890 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.498Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 891 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.498Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 892 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.498Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 893 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.498Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 894 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.498Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 895 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.498Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 896 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.498Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 897 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.498Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 898 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.498Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 899 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.498Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 900 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.498Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 901 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.498Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 902 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.498Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 903 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.498Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 904 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.498Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 905 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.498Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 906 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.498Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 907 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.498Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 908 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.498Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 909 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.498Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 910 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.498Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 911 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.498Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 912 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.499Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 913 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.499Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 914 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.499Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 915 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.499Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 916 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.499Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 917 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.499Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 918 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.500Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 919 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.500Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 920 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.500Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 921 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.500Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 922 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.500Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 923 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.500Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 924 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.500Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 925 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.500Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 926 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.500Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 927 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.500Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 928 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.500Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 929 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.500Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 930 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.500Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 931 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.500Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 932 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.500Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 933 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.500Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 934 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.500Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 935 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.500Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 936 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.500Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 937 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.500Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 938 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.500Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 939 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.500Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 940 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.500Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 941 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.500Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 942 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.500Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 943 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.500Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 944 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.500Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 945 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.501Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 946 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.501Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 947 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.501Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 948 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.501Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 949 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.501Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 950 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.501Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 951 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.501Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 952 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.501Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 953 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.501Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 954 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.501Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 955 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.501Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 956 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.501Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 957 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.501Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 958 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.501Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 959 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.501Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 960 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.501Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 961 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.501Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 962 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.501Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 963 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.501Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 964 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.501Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 965 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.501Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 966 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.501Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 967 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.501Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 968 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.501Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 969 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.501Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 970 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.501Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 971 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.501Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 972 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.501Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 973 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.501Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 974 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.501Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 975 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.502Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 976 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.502Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 977 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.502Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 978 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.502Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 979 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.502Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 980 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.502Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 981 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.502Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 982 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.502Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 983 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.502Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 984 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.502Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 985 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.502Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 986 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.502Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 987 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.502Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 988 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.502Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 989 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.502Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 990 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.502Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 991 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.502Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 992 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.502Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 993 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.502Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 994 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.502Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 995 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.502Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 996 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.502Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 997 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.502Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 998 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.502Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 999 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.502Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 1000 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.502Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 1001 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.502Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 1002 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.502Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 1003 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.502Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 1004 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.503Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 1005 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.503Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 1006 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.503Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 1007 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.503Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 1008 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.503Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 1009 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.503Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 1010 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.503Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 1011 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.503Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 1012 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.503Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 1013 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.503Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 1014 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.503Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 1015 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.503Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 1016 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.503Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 1017 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.503Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 1018 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.503Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 1019 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.503Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 1020 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.503Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 1021 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.503Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 1022 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-03-27T20:46:35.503Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:sanify_chain:499]Setting vbucket 1023 in "HelloServiceBucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:debug,2020-03-27T20:46:35.504Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:maybe_config_sync:242]Found states mismatch in bucket "HelloServiceBucket":
[{0,['ns_1@127.0.0.1',undefined],[]},
 {1,['ns_1@127.0.0.1',undefined],[]},
 {2,['ns_1@127.0.0.1',undefined],[]},
 {3,['ns_1@127.0.0.1',undefined],[]},
 {4,['ns_1@127.0.0.1',undefined],[]},
 {5,['ns_1@127.0.0.1',undefined],[]},
 {6,['ns_1@127.0.0.1',undefined],[]},
 {7,['ns_1@127.0.0.1',undefined],[]},
 {8,['ns_1@127.0.0.1',undefined],[]},
 {9,['ns_1@127.0.0.1',undefined],[]},
 {10,['ns_1@127.0.0.1',undefined],[]},
 {11,['ns_1@127.0.0.1',undefined],[]},
 {12,['ns_1@127.0.0.1',undefined],[]},
 {13,['ns_1@127.0.0.1',undefined],[]},
 {14,['ns_1@127.0.0.1',undefined],[]},
 {15,['ns_1@127.0.0.1',undefined],[]},
 {16,['ns_1@127.0.0.1',undefined],[]},
 {17,['ns_1@127.0.0.1',undefined],[]},
 {18,['ns_1@127.0.0.1',undefined],[]},
 {19,['ns_1@127.0.0.1',undefined],[]},
 {20,['ns_1@127.0.0.1',undefined],[]},
 {21,['ns_1@127.0.0.1',undefined],[]},
 {22,['ns_1@127.0.0.1',undefined],[]},
 {23,['ns_1@127.0.0.1',undefined],[]},
 {24,['ns_1@127.0.0.1',undefined],[]},
 {25,['ns_1@127.0.0.1',undefined],[]},
 {26,['ns_1@127.0.0.1',undefined],[]},
 {27,['ns_1@127.0.0.1',undefined],[]},
 {28,['ns_1@127.0.0.1',undefined],[]},
 {29,['ns_1@127.0.0.1',undefined],[]},
 {30,['ns_1@127.0.0.1',undefined],[]},
 {31,['ns_1@127.0.0.1',undefined],[]},
 {32,['ns_1@127.0.0.1',undefined],[]},
 {33,['ns_1@127.0.0.1',undefined],[]},
 {34,['ns_1@127.0.0.1',undefined],[]},
 {35,['ns_1@127.0.0.1',undefined],[]},
 {36,['ns_1@127.0.0.1',undefined],[]},
 {37,['ns_1@127.0.0.1',undefined],[]},
 {38,['ns_1@127.0.0.1',undefined],[]},
 {39,['ns_1@127.0.0.1',undefined],[]},
 {40,['ns_1@127.0.0.1',undefined],[]},
 {41,['ns_1@127.0.0.1',undefined],[]},
 {42,['ns_1@127.0.0.1',undefined],[]},
 {43,['ns_1@127.0.0.1',undefined],[]},
 {44,['ns_1@127.0.0.1',undefined],[]},
 {45,['ns_1@127.0.0.1',undefined],[]},
 {46,['ns_1@127.0.0.1',undefined],[]},
 {47,['ns_1@127.0.0.1',undefined],[]},
 {48,['ns_1@127.0.0.1',undefined],[]},
 {49,['ns_1@127.0.0.1',undefined],[]},
 {50,['ns_1@127.0.0.1',undefined],[]},
 {51,['ns_1@127.0.0.1',undefined],[]},
 {52,['ns_1@127.0.0.1',undefined],[]},
 {53,['ns_1@127.0.0.1',undefined],[]},
 {54,['ns_1@127.0.0.1',undefined],[]},
 {55,['ns_1@127.0.0.1',undefined],[]},
 {56,['ns_1@127.0.0.1',undefined],[]},
 {57,['ns_1@127.0.0.1',undefined],[]},
 {58,['ns_1@127.0.0.1',undefined],[]},
 {59,['ns_1@127.0.0.1',undefined],[]},
 {60,['ns_1@127.0.0.1',undefined],[]},
 {61,['ns_1@127.0.0.1',undefined],[]},
 {62,['ns_1@127.0.0.1',undefined],[]},
 {63,['ns_1@127.0.0.1',undefined],[]},
 {64,['ns_1@127.0.0.1',undefined],[]},
 {65,['ns_1@127.0.0.1',undefined],[]},
 {66,['ns_1@127.0.0.1',undefined],[]},
 {67,['ns_1@127.0.0.1',undefined],[]},
 {68,['ns_1@127.0.0.1',undefined],[]},
 {69,['ns_1@127.0.0.1',undefined],[]},
 {70,['ns_1@127.0.0.1',undefined],[]},
 {71,['ns_1@127.0.0.1',undefined],[]},
 {72,['ns_1@127.0.0.1',undefined],[]},
 {73,['ns_1@127.0.0.1',undefined],[]},
 {74,['ns_1@127.0.0.1',undefined],[]},
 {75,['ns_1@127.0.0.1',undefined],[]},
 {76,['ns_1@127.0.0.1',undefined],[]},
 {77,['ns_1@127.0.0.1',undefined],[]},
 {78,['ns_1@127.0.0.1',undefined],[]},
 {79,['ns_1@127.0.0.1',undefined],[]},
 {80,['ns_1@127.0.0.1',undefined],[]},
 {81,['ns_1@127.0.0.1',undefined],[]},
 {82,['ns_1@127.0.0.1',undefined],[]},
 {83,['ns_1@127.0.0.1',undefined],[]},
 {84,['ns_1@127.0.0.1',undefined],[]},
 {85,['ns_1@127.0.0.1',undefined],[]},
 {86,['ns_1@127.0.0.1',undefined],[]},
 {87,['ns_1@127.0.0.1',undefined],[]},
 {88,['ns_1@127.0.0.1',undefined],[]},
 {89,['ns_1@127.0.0.1',undefined],[]},
 {90,['ns_1@127.0.0.1',undefined],[]},
 {91,['ns_1@127.0.0.1',undefined],[]},
 {92,['ns_1@127.0.0.1',undefined],[]},
 {93,['ns_1@127.0.0.1',undefined],[]},
 {94,['ns_1@127.0.0.1',undefined],[]},
 {95,['ns_1@127.0.0.1',undefined],[]},
 {96,['ns_1@127.0.0.1',undefined],[]},
 {97,['ns_1@127.0.0.1',undefined],[]},
 {98,['ns_1@127.0.0.1',undefined],[]},
 {99,['ns_1@127.0.0.1',undefined],[]},
 {100,['ns_1@127.0.0.1',undefined],[]},
 {101,['ns_1@127.0.0.1',undefined],[]},
 {102,['ns_1@127.0.0.1',undefined],[]},
 {103,['ns_1@127.0.0.1',undefined],[]},
 {104,['ns_1@127.0.0.1',undefined],[]},
 {105,['ns_1@127.0.0.1',undefined],[]},
 {106,['ns_1@127.0.0.1',undefined],[]},
 {107,['ns_1@127.0.0.1',undefined],[]},
 {108,['ns_1@127.0.0.1',undefined],[]},
 {109,['ns_1@127.0.0.1',undefined],[]},
 {110,['ns_1@127.0.0.1',undefined],[]},
 {111,['ns_1@127.0.0.1',undefined],[]},
 {112,['ns_1@127.0.0.1',undefined],[]},
 {113,['ns_1@127.0.0.1',undefined],[]},
 {114,['ns_1@127.0.0.1',undefined],[]},
 {115,['ns_1@127.0.0.1',undefined],[]},
 {116,['ns_1@127.0.0.1',undefined],[]},
 {117,['ns_1@127.0.0.1',undefined],[]},
 {118,['ns_1@127.0.0.1',undefined],[]},
 {119,['ns_1@127.0.0.1',undefined],[]},
 {120,['ns_1@127.0.0.1',undefined],[]},
 {121,['ns_1@127.0.0.1',undefined],[]},
 {122,['ns_1@127.0.0.1',undefined],[]},
 {123,['ns_1@127.0.0.1',undefined],[]},
 {124,['ns_1@127.0.0.1',undefined],[]},
 {125,['ns_1@127.0.0.1',undefined],[]},
 {126,['ns_1@127.0.0.1',undefined],[]},
 {127,['ns_1@127.0.0.1',undefined],[]},
 {128,['ns_1@127.0.0.1',undefined],[]},
 {129,['ns_1@127.0.0.1',undefined],[]},
 {130,['ns_1@127.0.0.1',undefined],[]},
 {131,['ns_1@127.0.0.1',undefined],[]},
 {132,['ns_1@127.0.0.1',undefined],[]},
 {133,['ns_1@127.0.0.1',undefined],[]},
 {134,['ns_1@127.0.0.1',undefined],[]},
 {135,['ns_1@127.0.0.1',undefined],[]},
 {136,['ns_1@127.0.0.1',undefined],[]},
 {137,['ns_1@127.0.0.1',undefined],[]},
 {138,['ns_1@127.0.0.1',undefined],[]},
 {139,['ns_1@127.0.0.1',undefined],[]},
 {140,['ns_1@127.0.0.1',undefined],[]},
 {141,['ns_1@127.0.0.1',undefined],[]},
 {142,['ns_1@127.0.0.1',undefined],[]},
 {143,['ns_1@127.0.0.1',undefined],[]},
 {144,['ns_1@127.0.0.1',undefined],[]},
 {145,['ns_1@127.0.0.1',undefined],[]},
 {146,['ns_1@127.0.0.1',undefined],[]},
 {147,['ns_1@127.0.0.1',undefined],[]},
 {148,['ns_1@127.0.0.1',undefined],[]},
 {149,['ns_1@127.0.0.1',undefined],[]},
 {150,['ns_1@127.0.0.1',undefined],[]},
 {151,['ns_1@127.0.0.1',undefined],[]},
 {152,['ns_1@127.0.0.1',undefined],[]},
 {153,['ns_1@127.0.0.1',undefined],[]},
 {154,['ns_1@127.0.0.1',undefined],[]},
 {155,['ns_1@127.0.0.1',undefined],[]},
 {156,['ns_1@127.0.0.1',undefined],[]},
 {157,['ns_1@127.0.0.1',undefined],[]},
 {158,['ns_1@127.0.0.1',undefined],[]},
 {159,['ns_1@127.0.0.1',undefined],[]},
 {160,['ns_1@127.0.0.1',undefined],[]},
 {161,['ns_1@127.0.0.1',undefined],[]},
 {162,['ns_1@127.0.0.1',undefined],[]},
 {163,['ns_1@127.0.0.1',undefined],[]},
 {164,['ns_1@127.0.0.1',undefined],[]},
 {165,['ns_1@127.0.0.1',undefined],[]},
 {166,['ns_1@127.0.0.1',undefined],[]},
 {167,['ns_1@127.0.0.1',undefined],[]},
 {168,['ns_1@127.0.0.1',undefined],[]},
 {169,['ns_1@127.0.0.1',undefined],[]},
 {170,['ns_1@127.0.0.1',undefined],[]},
 {171,['ns_1@127.0.0.1',undefined],[]},
 {172,['ns_1@127.0.0.1',undefined],[]},
 {173,['ns_1@127.0.0.1',undefined],[]},
 {174,['ns_1@127.0.0.1',undefined],[]},
 {175,['ns_1@127.0.0.1',undefined],[]},
 {176,['ns_1@127.0.0.1',undefined],[]},
 {177,['ns_1@127.0.0.1',undefined],[]},
 {178,['ns_1@127.0.0.1',undefined],[]},
 {179,['ns_1@127.0.0.1',undefined],[]},
 {180,['ns_1@127.0.0.1',undefined],[]},
 {181,['ns_1@127.0.0.1',undefined],[]},
 {182,['ns_1@127.0.0.1',undefined],[]},
 {183,['ns_1@127.0.0.1',undefined],[]},
 {184,['ns_1@127.0.0.1',undefined],[]},
 {185,['ns_1@127.0.0.1',undefined],[]},
 {186,['ns_1@127.0.0.1',undefined],[]},
 {187,['ns_1@127.0.0.1',undefined],[]},
 {188,['ns_1@127.0.0.1',undefined],[]},
 {189,['ns_1@127.0.0.1',undefined],[]},
 {190,['ns_1@127.0.0.1',undefined],[]},
 {191,['ns_1@127.0.0.1',undefined],[]},
 {192,['ns_1@127.0.0.1',undefined],[]},
 {193,['ns_1@127.0.0.1',undefined],[]},
 {194,['ns_1@127.0.0.1',undefined],[]},
 {195,['ns_1@127.0.0.1',undefined],[]},
 {196,['ns_1@127.0.0.1',undefined],[]},
 {197,['ns_1@127.0.0.1',undefined],[]},
 {198,['ns_1@127.0.0.1',undefined],[]},
 {199,['ns_1@127.0.0.1',undefined],[]},
 {200,['ns_1@127.0.0.1',undefined],[]},
 {201,['ns_1@127.0.0.1',undefined],[]},
 {202,['ns_1@127.0.0.1',undefined],[]},
 {203,['ns_1@127.0.0.1',undefined],[]},
 {204,['ns_1@127.0.0.1',undefined],[]},
 {205,['ns_1@127.0.0.1',undefined],[]},
 {206,['ns_1@127.0.0.1',undefined],[]},
 {207,['ns_1@127.0.0.1',undefined],[]},
 {208,['ns_1@127.0.0.1',undefined],[]},
 {209,['ns_1@127.0.0.1',undefined],[]},
 {210,['ns_1@127.0.0.1',undefined],[]},
 {211,['ns_1@127.0.0.1',undefined],[]},
 {212,['ns_1@127.0.0.1',undefined],[]},
 {213,['ns_1@127.0.0.1',undefined],[]},
 {214,['ns_1@127.0.0.1',undefined],[]},
 {215,['ns_1@127.0.0.1',undefined],[]},
 {216,['ns_1@127.0.0.1',undefined],[]},
 {217,['ns_1@127.0.0.1',undefined],[]},
 {218,['ns_1@127.0.0.1',undefined],[]},
 {219,['ns_1@127.0.0.1',undefined],[]},
 {220,['ns_1@127.0.0.1',undefined],[]},
 {221,['ns_1@127.0.0.1',undefined],[]},
 {222,['ns_1@127.0.0.1',undefined],[]},
 {223,['ns_1@127.0.0.1',undefined],[]},
 {224,['ns_1@127.0.0.1',undefined],[]},
 {225,['ns_1@127.0.0.1',undefined],[]},
 {226,['ns_1@127.0.0.1',undefined],[]},
 {227,['ns_1@127.0.0.1',undefined],[]},
 {228,['ns_1@127.0.0.1',undefined],[]},
 {229,['ns_1@127.0.0.1',undefined],[]},
 {230,['ns_1@127.0.0.1',undefined],[]},
 {231,['ns_1@127.0.0.1',undefined],[]},
 {232,['ns_1@127.0.0.1',undefined],[]},
 {233,['ns_1@127.0.0.1',undefined],[]},
 {234,['ns_1@127.0.0.1',undefined],[]},
 {235,['ns_1@127.0.0.1',undefined],[]},
 {236,['ns_1@127.0.0.1',undefined],[]},
 {237,['ns_1@127.0.0.1',undefined],[]},
 {238,['ns_1@127.0.0.1',undefined],[]},
 {239,['ns_1@127.0.0.1',undefined],[]},
 {240,['ns_1@127.0.0.1',undefined],[]},
 {241,['ns_1@127.0.0.1',undefined],[]},
 {242,['ns_1@127.0.0.1',undefined],[]},
 {243,['ns_1@127.0.0.1',undefined],[]},
 {244,['ns_1@127.0.0.1',undefined],[]},
 {245,['ns_1@127.0.0.1',undefined],[]},
 {246,['ns_1@127.0.0.1',undefined],[]},
 {247,['ns_1@127.0.0.1',undefined],[]},
 {248,['ns_1@127.0.0.1',undefined],[]},
 {249,['ns_1@127.0.0.1',undefined],[]},
 {250,['ns_1@127.0.0.1',undefined],[]},
 {251,['ns_1@127.0.0.1',undefined],[]},
 {252,['ns_1@127.0.0.1',undefined],[]},
 {253,['ns_1@127.0.0.1',undefined],[]},
 {254,['ns_1@127.0.0.1',undefined],[]},
 {255,['ns_1@127.0.0.1',undefined],[]},
 {256,['ns_1@127.0.0.1',undefined],[]},
 {257,['ns_1@127.0.0.1',undefined],[]},
 {258,['ns_1@127.0.0.1',undefined],[]},
 {259,['ns_1@127.0.0.1',undefined],[]},
 {260,['ns_1@127.0.0.1',undefined],[]},
 {261,['ns_1@127.0.0.1',undefined],[]},
 {262,['ns_1@127.0.0.1',undefined],[]},
 {263,['ns_1@127.0.0.1',undefined],[]},
 {264,['ns_1@127.0.0.1',undefined],[]},
 {265,['ns_1@127.0.0.1',undefined],[]},
 {266,['ns_1@127.0.0.1',undefined],[]},
 {267,['ns_1@127.0.0.1',undefined],[]},
 {268,['ns_1@127.0.0.1',undefined],[]},
 {269,['ns_1@127.0.0.1',undefined],[]},
 {270,['ns_1@127.0.0.1',undefined],[]},
 {271,['ns_1@127.0.0.1',undefined],[]},
 {272,['ns_1@127.0.0.1',undefined],[]},
 {273,['ns_1@127.0.0.1',undefined],[]},
 {274,['ns_1@127.0.0.1',undefined],[]},
 {275,['ns_1@127.0.0.1',undefined],[]},
 {276,['ns_1@127.0.0.1',undefined],[]},
 {277,['ns_1@127.0.0.1',undefined],[]},
 {278,['ns_1@127.0.0.1',undefined],[]},
 {279,['ns_1@127.0.0.1',undefined],[]},
 {280,['ns_1@127.0.0.1',undefined],[]},
 {281,['ns_1@127.0.0.1',undefined],[]},
 {282,['ns_1@127.0.0.1',undefined],[]},
 {283,['ns_1@127.0.0.1',undefined],[]},
 {284,['ns_1@127.0.0.1',undefined],[]},
 {285,['ns_1@127.0.0.1',undefined],[]},
 {286,['ns_1@127.0.0.1',undefined],[]},
 {287,['ns_1@127.0.0.1',undefined],[]},
 {288,['ns_1@127.0.0.1',undefined],[]},
 {289,['ns_1@127.0.0.1',undefined],[]},
 {290,['ns_1@127.0.0.1',undefined],[]},
 {291,['ns_1@127.0.0.1',undefined],[]},
 {292,['ns_1@127.0.0.1',undefined],[]},
 {293,['ns_1@127.0.0.1',undefined],[]},
 {294,['ns_1@127.0.0.1',undefined],[]},
 {295,['ns_1@127.0.0.1',undefined],[]},
 {296,['ns_1@127.0.0.1',undefined],[]},
 {297,['ns_1@127.0.0.1',undefined],[]},
 {298,['ns_1@127.0.0.1',undefined],[]},
 {299,['ns_1@127.0.0.1',undefined],[]},
 {300,['ns_1@127.0.0.1',undefined],[]},
 {301,['ns_1@127.0.0.1',undefined],[]},
 {302,['ns_1@127.0.0.1',undefined],[]},
 {303,['ns_1@127.0.0.1',undefined],[]},
 {304,['ns_1@127.0.0.1',undefined],[]},
 {305,['ns_1@127.0.0.1',undefined],[]},
 {306,['ns_1@127.0.0.1',undefined],[]},
 {307,['ns_1@127.0.0.1',undefined],[]},
 {308,['ns_1@127.0.0.1',undefined],[]},
 {309,['ns_1@127.0.0.1',undefined],[]},
 {310,['ns_1@127.0.0.1',undefined],[]},
 {311,['ns_1@127.0.0.1',undefined],[]},
 {312,['ns_1@127.0.0.1',undefined],[]},
 {313,['ns_1@127.0.0.1',undefined],[]},
 {314,['ns_1@127.0.0.1',undefined],[]},
 {315,['ns_1@127.0.0.1',undefined],[]},
 {316,['ns_1@127.0.0.1',undefined],[]},
 {317,['ns_1@127.0.0.1',undefined],[]},
 {318,['ns_1@127.0.0.1',undefined],[]},
 {319,['ns_1@127.0.0.1',undefined],[]},
 {320,['ns_1@127.0.0.1',undefined],[]},
 {321,['ns_1@127.0.0.1',undefined],[]},
 {322,['ns_1@127.0.0.1',undefined],[]},
 {323,['ns_1@127.0.0.1',undefined],[]},
 {324,['ns_1@127.0.0.1',undefined],[]},
 {325,['ns_1@127.0.0.1',undefined],[]},
 {326,['ns_1@127.0.0.1',undefined],[]},
 {327,['ns_1@127.0.0.1',undefined],[]},
 {328,['ns_1@127.0.0.1',undefined],[]},
 {329,['ns_1@127.0.0.1',undefined],[]},
 {330,['ns_1@127.0.0.1',undefined],[]},
 {331,['ns_1@127.0.0.1',undefined],[]},
 {332,['ns_1@127.0.0.1',undefined],[]},
 {333,['ns_1@127.0.0.1',undefined],[]},
 {334,['ns_1@127.0.0.1',undefined],[]},
 {335,['ns_1@127.0.0.1',undefined],[]},
 {336,['ns_1@127.0.0.1',undefined],[]},
 {337,['ns_1@127.0.0.1',undefined],[]},
 {338,['ns_1@127.0.0.1',undefined],[]},
 {339,['ns_1@127.0.0.1',undefined],[]},
 {340,['ns_1@127.0.0.1',undefined],[]},
 {341,['ns_1@127.0.0.1',undefined],[]},
 {342,['ns_1@127.0.0.1',undefined],[]},
 {343,['ns_1@127.0.0.1',undefined],[]},
 {344,['ns_1@127.0.0.1',undefined],[]},
 {345,['ns_1@127.0.0.1',undefined],[]},
 {346,['ns_1@127.0.0.1',undefined],[]},
 {347,['ns_1@127.0.0.1',undefined],[]},
 {348,['ns_1@127.0.0.1',undefined],[]},
 {349,['ns_1@127.0.0.1',undefined],[]},
 {350,['ns_1@127.0.0.1',undefined],[]},
 {351,['ns_1@127.0.0.1',undefined],[]},
 {352,['ns_1@127.0.0.1',undefined],[]},
 {353,['ns_1@127.0.0.1',undefined],[]},
 {354,['ns_1@127.0.0.1',undefined],[]},
 {355,['ns_1@127.0.0.1',undefined],[]},
 {356,['ns_1@127.0.0.1',undefined],[]},
 {357,['ns_1@127.0.0.1',undefined],[]},
 {358,['ns_1@127.0.0.1',undefined],[]},
 {359,['ns_1@127.0.0.1',undefined],[]},
 {360,['ns_1@127.0.0.1',undefined],[]},
 {361,['ns_1@127.0.0.1',undefined],[]},
 {362,['ns_1@127.0.0.1',undefined],[]},
 {363,['ns_1@127.0.0.1',undefined],[]},
 {364,['ns_1@127.0.0.1',undefined],[]},
 {365,['ns_1@127.0.0.1',undefined],[]},
 {366,['ns_1@127.0.0.1',undefined],[]},
 {367,['ns_1@127.0.0.1',undefined],[]},
 {368,['ns_1@127.0.0.1',undefined],[]},
 {369,['ns_1@127.0.0.1',undefined],[]},
 {370,['ns_1@127.0.0.1',undefined],[]},
 {371,['ns_1@127.0.0.1',undefined],[]},
 {372,['ns_1@127.0.0.1',undefined],[]},
 {373,['ns_1@127.0.0.1',undefined],[]},
 {374,['ns_1@127.0.0.1',undefined],[]},
 {375,['ns_1@127.0.0.1',undefined],[]},
 {376,['ns_1@127.0.0.1',undefined],[]},
 {377,['ns_1@127.0.0.1',undefined],[]},
 {378,['ns_1@127.0.0.1',undefined],[]},
 {379,['ns_1@127.0.0.1',undefined],[]},
 {380,['ns_1@127.0.0.1',undefined],[]},
 {381,['ns_1@127.0.0.1',undefined],[]},
 {382,['ns_1@127.0.0.1',undefined],[]},
 {383,['ns_1@127.0.0.1',undefined],[]},
 {384,['ns_1@127.0.0.1',undefined],[]},
 {385,['ns_1@127.0.0.1',undefined],[]},
 {386,['ns_1@127.0.0.1',undefined],[]},
 {387,['ns_1@127.0.0.1',undefined],[]},
 {388,['ns_1@127.0.0.1',undefined],[]},
 {389,['ns_1@127.0.0.1',undefined],[]},
 {390,['ns_1@127.0.0.1',undefined],[]},
 {391,['ns_1@127.0.0.1',undefined],[]},
 {392,['ns_1@127.0.0.1',undefined],[]},
 {393,['ns_1@127.0.0.1',undefined],[]},
 {394,['ns_1@127.0.0.1',undefined],[]},
 {395,['ns_1@127.0.0.1',undefined],[]},
 {396,['ns_1@127.0.0.1',undefined],[]},
 {397,['ns_1@127.0.0.1',undefined],[]},
 {398,['ns_1@127.0.0.1',undefined],[]},
 {399,['ns_1@127.0.0.1',undefined],[]},
 {400,['ns_1@127.0.0.1',undefined],[]},
 {401,['ns_1@127.0.0.1',undefined],[]},
 {402,['ns_1@127.0.0.1',undefined],[]},
 {403,['ns_1@127.0.0.1',undefined],[]},
 {404,['ns_1@127.0.0.1',undefined],[]},
 {405,['ns_1@127.0.0.1',undefined],[]},
 {406,['ns_1@127.0.0.1',undefined],[]},
 {407,['ns_1@127.0.0.1',undefined],[]},
 {408,['ns_1@127.0.0.1',undefined],[]},
 {409,['ns_1@127.0.0.1',undefined],[]},
 {410,['ns_1@127.0.0.1',undefined],[]},
 {411,['ns_1@127.0.0.1',undefined],[]},
 {412,['ns_1@127.0.0.1',undefined],[]},
 {413,['ns_1@127.0.0.1',undefined],[]},
 {414,['ns_1@127.0.0.1',undefined],[]},
 {415,['ns_1@127.0.0.1',undefined],[]},
 {416,['ns_1@127.0.0.1',undefined],[]},
 {417,['ns_1@127.0.0.1',undefined],[]},
 {418,['ns_1@127.0.0.1',undefined],[]},
 {419,['ns_1@127.0.0.1',undefined],[]},
 {420,['ns_1@127.0.0.1',undefined],[]},
 {421,['ns_1@127.0.0.1',undefined],[]},
 {422,['ns_1@127.0.0.1',undefined],[]},
 {423,['ns_1@127.0.0.1',undefined],[]},
 {424,['ns_1@127.0.0.1',undefined],[]},
 {425,['ns_1@127.0.0.1',undefined],[]},
 {426,['ns_1@127.0.0.1',undefined],[]},
 {427,['ns_1@127.0.0.1',undefined],[]},
 {428,['ns_1@127.0.0.1',undefined],[]},
 {429,['ns_1@127.0.0.1',undefined],[]},
 {430,['ns_1@127.0.0.1',undefined],[]},
 {431,['ns_1@127.0.0.1',undefined],[]},
 {432,['ns_1@127.0.0.1',undefined],[]},
 {433,['ns_1@127.0.0.1',undefined],[]},
 {434,['ns_1@127.0.0.1',undefined],[]},
 {435,['ns_1@127.0.0.1',undefined],[]},
 {436,['ns_1@127.0.0.1',undefined],[]},
 {437,['ns_1@127.0.0.1',undefined],[]},
 {438,['ns_1@127.0.0.1',undefined],[]},
 {439,['ns_1@127.0.0.1',undefined],[]},
 {440,['ns_1@127.0.0.1',undefined],[]},
 {441,['ns_1@127.0.0.1',undefined],[]},
 {442,['ns_1@127.0.0.1',undefined],[]},
 {443,['ns_1@127.0.0.1',undefined],[]},
 {444,['ns_1@127.0.0.1',undefined],[]},
 {445,['ns_1@127.0.0.1',undefined],[]},
 {446,['ns_1@127.0.0.1',undefined],[]},
 {447,['ns_1@127.0.0.1',undefined],[]},
 {448,['ns_1@127.0.0.1',undefined],[]},
 {449,['ns_1@127.0.0.1',undefined],[]},
 {450,['ns_1@127.0.0.1',undefined],[]},
 {451,['ns_1@127.0.0.1',undefined],[]},
 {452,['ns_1@127.0.0.1',undefined],[]},
 {453,['ns_1@127.0.0.1',undefined],[]},
 {454,['ns_1@127.0.0.1',undefined],[]},
 {455,['ns_1@127.0.0.1',undefined],[]},
 {456,['ns_1@127.0.0.1',undefined],[]},
 {457,['ns_1@127.0.0.1',undefined],[]},
 {458,['ns_1@127.0.0.1',undefined],[]},
 {459,['ns_1@127.0.0.1',undefined],[]},
 {460,['ns_1@127.0.0.1',undefined],[]},
 {461,['ns_1@127.0.0.1',undefined],[]},
 {462,['ns_1@127.0.0.1',undefined],[]},
 {463,['ns_1@127.0.0.1',undefined],[]},
 {464,['ns_1@127.0.0.1',undefined],[]},
 {465,['ns_1@127.0.0.1',undefined],[]},
 {466,['ns_1@127.0.0.1',undefined],[]},
 {467,['ns_1@127.0.0.1',undefined],[]},
 {468,['ns_1@127.0.0.1',undefined],[]},
 {469,['ns_1@127.0.0.1',undefined],[]},
 {470,['ns_1@127.0.0.1',undefined],[]},
 {471,['ns_1@127.0.0.1',undefined],[]},
 {472,['ns_1@127.0.0.1',undefined],[]},
 {473,['ns_1@127.0.0.1',undefined],[]},
 {474,['ns_1@127.0.0.1',undefined],[]},
 {475,['ns_1@127.0.0.1',undefined],[]},
 {476,['ns_1@127.0.0.1',undefined],[]},
 {477,['ns_1@127.0.0.1',undefined],[]},
 {478,['ns_1@127.0.0.1',undefined],[]},
 {479,['ns_1@127.0.0.1',undefined],[]},
 {480,['ns_1@127.0.0.1',undefined],[]},
 {481,['ns_1@127.0.0.1',undefined],[]},
 {482,['ns_1@127.0.0.1',undefined],[]},
 {483,['ns_1@127.0.0.1',undefined],[]},
 {484,['ns_1@127.0.0.1',undefined],[]},
 {485,['ns_1@127.0.0.1',undefined],[]},
 {486,['ns_1@127.0.0.1',undefined],[]},
 {487,['ns_1@127.0.0.1',undefined],[]},
 {488,['ns_1@127.0.0.1',undefined],[]},
 {489,['ns_1@127.0.0.1',undefined],[]},
 {490,['ns_1@127.0.0.1',undefined],[]},
 {491,['ns_1@127.0.0.1',undefined],[]},
 {492,['ns_1@127.0.0.1',undefined],[]},
 {493,['ns_1@127.0.0.1',undefined],[]},
 {494,['ns_1@127.0.0.1',undefined],[]},
 {495,['ns_1@127.0.0.1',undefined],[]},
 {496,['ns_1@127.0.0.1',undefined],[]},
 {497,['ns_1@127.0.0.1',undefined],[]},
 {498,['ns_1@127.0.0.1',undefined],[]},
 {499,['ns_1@127.0.0.1',undefined],[]},
 {500,['ns_1@127.0.0.1',undefined],[]},
 {501,['ns_1@127.0.0.1',undefined],[]},
 {502,['ns_1@127.0.0.1',undefined],[]},
 {503,['ns_1@127.0.0.1',undefined],[]},
 {504,['ns_1@127.0.0.1',undefined],[]},
 {505,['ns_1@127.0.0.1',undefined],[]},
 {506,['ns_1@127.0.0.1',undefined],[]},
 {507,['ns_1@127.0.0.1',undefined],[]},
 {508,['ns_1@127.0.0.1',undefined],[]},
 {509,['ns_1@127.0.0.1',undefined],[]},
 {510,['ns_1@127.0.0.1',undefined],[]},
 {511,['ns_1@127.0.0.1',undefined],[]},
 {512,['ns_1@127.0.0.1',undefined],[]},
 {513,['ns_1@127.0.0.1',undefined],[]},
 {514,['ns_1@127.0.0.1',undefined],[]},
 {515,['ns_1@127.0.0.1',undefined],[]},
 {516,['ns_1@127.0.0.1',undefined],[]},
 {517,['ns_1@127.0.0.1',undefined],[]},
 {518,['ns_1@127.0.0.1',undefined],[]},
 {519,['ns_1@127.0.0.1',undefined],[]},
 {520,['ns_1@127.0.0.1',undefined],[]},
 {521,['ns_1@127.0.0.1',undefined],[]},
 {522,['ns_1@127.0.0.1',undefined],[]},
 {523,['ns_1@127.0.0.1',undefined],[]},
 {524,['ns_1@127.0.0.1',undefined],[]},
 {525,['ns_1@127.0.0.1',undefined],[]},
 {526,['ns_1@127.0.0.1',undefined],[]},
 {527,['ns_1@127.0.0.1',undefined],[]},
 {528,['ns_1@127.0.0.1',undefined],[]},
 {529,['ns_1@127.0.0.1',undefined],[]},
 {530,['ns_1@127.0.0.1',undefined],[]},
 {531,['ns_1@127.0.0.1',undefined],[]},
 {532,['ns_1@127.0.0.1',undefined],[]},
 {533,['ns_1@127.0.0.1',undefined],[]},
 {534,['ns_1@127.0.0.1',undefined],[]},
 {535,['ns_1@127.0.0.1',undefined],[]},
 {536,['ns_1@127.0.0.1',undefined],[]},
 {537,['ns_1@127.0.0.1',undefined],[]},
 {538,['ns_1@127.0.0.1',undefined],[]},
 {539,['ns_1@127.0.0.1',undefined],[]},
 {540,['ns_1@127.0.0.1',undefined],[]},
 {541,['ns_1@127.0.0.1',undefined],[]},
 {542,['ns_1@127.0.0.1',undefined],[]},
 {543,['ns_1@127.0.0.1',undefined],[]},
 {544,['ns_1@127.0.0.1',undefined],[]},
 {545,['ns_1@127.0.0.1',undefined],[]},
 {546,['ns_1@127.0.0.1',undefined],[]},
 {547,['ns_1@127.0.0.1',undefined],[]},
 {548,['ns_1@127.0.0.1',undefined],[]},
 {549,['ns_1@127.0.0.1',undefined],[]},
 {550,['ns_1@127.0.0.1',undefined],[]},
 {551,['ns_1@127.0.0.1',undefined],[]},
 {552,['ns_1@127.0.0.1',undefined],[]},
 {553,['ns_1@127.0.0.1',undefined],[]},
 {554,['ns_1@127.0.0.1',undefined],[]},
 {555,['ns_1@127.0.0.1',undefined],[]},
 {556,['ns_1@127.0.0.1',undefined],[]},
 {557,['ns_1@127.0.0.1',undefined],[]},
 {558,['ns_1@127.0.0.1',undefined],[]},
 {559,['ns_1@127.0.0.1',undefined],[]},
 {560,['ns_1@127.0.0.1',undefined],[]},
 {561,['ns_1@127.0.0.1',undefined],[]},
 {562,['ns_1@127.0.0.1',undefined],[]},
 {563,['ns_1@127.0.0.1',undefined],[]},
 {564,['ns_1@127.0.0.1',undefined],[]},
 {565,['ns_1@127.0.0.1',undefined],[]},
 {566,['ns_1@127.0.0.1',undefined],[]},
 {567,['ns_1@127.0.0.1',undefined],[]},
 {568,['ns_1@127.0.0.1',undefined],[]},
 {569,['ns_1@127.0.0.1',undefined],[]},
 {570,['ns_1@127.0.0.1',undefined],[]},
 {571,['ns_1@127.0.0.1',undefined],[]},
 {572,['ns_1@127.0.0.1',undefined],[]},
 {573,['ns_1@127.0.0.1',undefined],[]},
 {574,['ns_1@127.0.0.1',undefined],[]},
 {575,['ns_1@127.0.0.1',undefined],[]},
 {576,['ns_1@127.0.0.1',undefined],[]},
 {577,['ns_1@127.0.0.1',undefined],[]},
 {578,['ns_1@127.0.0.1',undefined],[]},
 {579,['ns_1@127.0.0.1',undefined],[]},
 {580,['ns_1@127.0.0.1',undefined],[]},
 {581,['ns_1@127.0.0.1',undefined],[]},
 {582,['ns_1@127.0.0.1',undefined],[]},
 {583,['ns_1@127.0.0.1',undefined],[]},
 {584,['ns_1@127.0.0.1',undefined],[]},
 {585,['ns_1@127.0.0.1',undefined],[]},
 {586,['ns_1@127.0.0.1',undefined],[]},
 {587,['ns_1@127.0.0.1',undefined],[]},
 {588,['ns_1@127.0.0.1',undefined],[]},
 {589,['ns_1@127.0.0.1',undefined],[]},
 {590,['ns_1@127.0.0.1',undefined],[]},
 {591,['ns_1@127.0.0.1',undefined],[]},
 {592,['ns_1@127.0.0.1',undefined],[]},
 {593,['ns_1@127.0.0.1',undefined],[]},
 {594,['ns_1@127.0.0.1',undefined],[]},
 {595,['ns_1@127.0.0.1',undefined],[]},
 {596,['ns_1@127.0.0.1',undefined],[]},
 {597,['ns_1@127.0.0.1',undefined],[]},
 {598,['ns_1@127.0.0.1',undefined],[]},
 {599,['ns_1@127.0.0.1',undefined],[]},
 {600,['ns_1@127.0.0.1',undefined],[]},
 {601,['ns_1@127.0.0.1',undefined],[]},
 {602,['ns_1@127.0.0.1',undefined],[]},
 {603,['ns_1@127.0.0.1',undefined],[]},
 {604,['ns_1@127.0.0.1',undefined],[]},
 {605,['ns_1@127.0.0.1',undefined],[]},
 {606,['ns_1@127.0.0.1',undefined],[]},
 {607,['ns_1@127.0.0.1',undefined],[]},
 {608,['ns_1@127.0.0.1',undefined],[]},
 {609,['ns_1@127.0.0.1',undefined],[]},
 {610,['ns_1@127.0.0.1',undefined],[]},
 {611,['ns_1@127.0.0.1',undefined],[]},
 {612,['ns_1@127.0.0.1',undefined],[]},
 {613,['ns_1@127.0.0.1',undefined],[]},
 {614,['ns_1@127.0.0.1',undefined],[]},
 {615,['ns_1@127.0.0.1',undefined],[]},
 {616,['ns_1@127.0.0.1',undefined],[]},
 {617,['ns_1@127.0.0.1',undefined],[]},
 {618,['ns_1@127.0.0.1',undefined],[]},
 {619,['ns_1@127.0.0.1',undefined],[]},
 {620,['ns_1@127.0.0.1',undefined],[]},
 {621,['ns_1@127.0.0.1',undefined],[]},
 {622,['ns_1@127.0.0.1',undefined],[]},
 {623,['ns_1@127.0.0.1',undefined],[]},
 {624,['ns_1@127.0.0.1',undefined],[]},
 {625,['ns_1@127.0.0.1',undefined],[]},
 {626,['ns_1@127.0.0.1',undefined],[]},
 {627,['ns_1@127.0.0.1',undefined],[]},
 {628,['ns_1@127.0.0.1',undefined],[]},
 {629,['ns_1@127.0.0.1',undefined],[]},
 {630,['ns_1@127.0.0.1',undefined],[]},
 {631,['ns_1@127.0.0.1',undefined],[]},
 {632,['ns_1@127.0.0.1',undefined],[]},
 {633,['ns_1@127.0.0.1',undefined],[]},
 {634,['ns_1@127.0.0.1',undefined],[]},
 {635,['ns_1@127.0.0.1',undefined],[]},
 {636,['ns_1@127.0.0.1',undefined],[]},
 {637,['ns_1@127.0.0.1',undefined],[]},
 {638,['ns_1@127.0.0.1',undefined],[]},
 {639,['ns_1@127.0.0.1',undefined],[]},
 {640,['ns_1@127.0.0.1',undefined],[]},
 {641,['ns_1@127.0.0.1',undefined],[]},
 {642,['ns_1@127.0.0.1',undefined],[]},
 {643,['ns_1@127.0.0.1',undefined],[]},
 {644,['ns_1@127.0.0.1',undefined],[]},
 {645,['ns_1@127.0.0.1',undefined],[]},
 {646,['ns_1@127.0.0.1',undefined],[]},
 {647,['ns_1@127.0.0.1',undefined],[]},
 {648,['ns_1@127.0.0.1',undefined],[]},
 {649,['ns_1@127.0.0.1',undefined],[]},
 {650,['ns_1@127.0.0.1',undefined],[]},
 {651,['ns_1@127.0.0.1',undefined],[]},
 {652,['ns_1@127.0.0.1',undefined],[]},
 {653,['ns_1@127.0.0.1',undefined],[]},
 {654,['ns_1@127.0.0.1',undefined],[]},
 {655,['ns_1@127.0.0.1',undefined],[]},
 {656,['ns_1@127.0.0.1',undefined],[]},
 {657,['ns_1@127.0.0.1',undefined],[]},
 {658,['ns_1@127.0.0.1',undefined],[]},
 {659,['ns_1@127.0.0.1',undefined],[]},
 {660,['ns_1@127.0.0.1',undefined],[]},
 {661,['ns_1@127.0.0.1',undefined],[]},
 {662,['ns_1@127.0.0.1',undefined],[]},
 {663,['ns_1@127.0.0.1',undefined],[]},
 {664,['ns_1@127.0.0.1',undefined],[]},
 {665,['ns_1@127.0.0.1',undefined],[]},
 {666,['ns_1@127.0.0.1',undefined],[]},
 {667,['ns_1@127.0.0.1',undefined],[]},
 {668,['ns_1@127.0.0.1',undefined],[]},
 {669,['ns_1@127.0.0.1',undefined],[]},
 {670,['ns_1@127.0.0.1',undefined],[]},
 {671,['ns_1@127.0.0.1',undefined],[]},
 {672,['ns_1@127.0.0.1',undefined],[]},
 {673,['ns_1@127.0.0.1',undefined],[]},
 {674,['ns_1@127.0.0.1',undefined],[]},
 {675,['ns_1@127.0.0.1',undefined],[]},
 {676,['ns_1@127.0.0.1',undefined],[]},
 {677,['ns_1@127.0.0.1',undefined],[]},
 {678,['ns_1@127.0.0.1',undefined],[]},
 {679,['ns_1@127.0.0.1',undefined],[]},
 {680,['ns_1@127.0.0.1',undefined],[]},
 {681,['ns_1@127.0.0.1',undefined],[]},
 {682,['ns_1@127.0.0.1',undefined],[]},
 {683,['ns_1@127.0.0.1',undefined],[]},
 {684,['ns_1@127.0.0.1',undefined],[]},
 {685,['ns_1@127.0.0.1',undefined],[]},
 {686,['ns_1@127.0.0.1',undefined],[]},
 {687,['ns_1@127.0.0.1',undefined],[]},
 {688,['ns_1@127.0.0.1',undefined],[]},
 {689,['ns_1@127.0.0.1',undefined],[]},
 {690,['ns_1@127.0.0.1',undefined],[]},
 {691,['ns_1@127.0.0.1',undefined],[]},
 {692,['ns_1@127.0.0.1',undefined],[]},
 {693,['ns_1@127.0.0.1',undefined],[]},
 {694,['ns_1@127.0.0.1',undefined],[]},
 {695,['ns_1@127.0.0.1',undefined],[]},
 {696,['ns_1@127.0.0.1',undefined],[]},
 {697,['ns_1@127.0.0.1',undefined],[]},
 {698,['ns_1@127.0.0.1',undefined],[]},
 {699,['ns_1@127.0.0.1',undefined],[]},
 {700,['ns_1@127.0.0.1',undefined],[]},
 {701,['ns_1@127.0.0.1',undefined],[]},
 {702,['ns_1@127.0.0.1',undefined],[]},
 {703,['ns_1@127.0.0.1',undefined],[]},
 {704,['ns_1@127.0.0.1',undefined],[]},
 {705,['ns_1@127.0.0.1',undefined],[]},
 {706,['ns_1@127.0.0.1',undefined],[]},
 {707,['ns_1@127.0.0.1',undefined],[]},
 {708,['ns_1@127.0.0.1',undefined],[]},
 {709,['ns_1@127.0.0.1',undefined],[]},
 {710,['ns_1@127.0.0.1',undefined],[]},
 {711,['ns_1@127.0.0.1',undefined],[]},
 {712,['ns_1@127.0.0.1',undefined],[]},
 {713,['ns_1@127.0.0.1',undefined],[]},
 {714,['ns_1@127.0.0.1',undefined],[]},
 {715,['ns_1@127.0.0.1',undefined],[]},
 {716,['ns_1@127.0.0.1',undefined],[]},
 {717,['ns_1@127.0.0.1',undefined],[]},
 {718,['ns_1@127.0.0.1',undefined],[]},
 {719,['ns_1@127.0.0.1',undefined],[]},
 {720,['ns_1@127.0.0.1',undefined],[]},
 {721,['ns_1@127.0.0.1',undefined],[]},
 {722,['ns_1@127.0.0.1',undefined],[]},
 {723,['ns_1@127.0.0.1',undefined],[]},
 {724,['ns_1@127.0.0.1',undefined],[]},
 {725,['ns_1@127.0.0.1',undefined],[]},
 {726,['ns_1@127.0.0.1',undefined],[]},
 {727,['ns_1@127.0.0.1',undefined],[]},
 {728,['ns_1@127.0.0.1',undefined],[]},
 {729,['ns_1@127.0.0.1',undefined],[]},
 {730,['ns_1@127.0.0.1',undefined],[]},
 {731,['ns_1@127.0.0.1',undefined],[]},
 {732,['ns_1@127.0.0.1',undefined],[]},
 {733,['ns_1@127.0.0.1',undefined],[]},
 {734,['ns_1@127.0.0.1',undefined],[]},
 {735,['ns_1@127.0.0.1',undefined],[]},
 {736,['ns_1@127.0.0.1',undefined],[]},
 {737,['ns_1@127.0.0.1',undefined],[]},
 {738,['ns_1@127.0.0.1',undefined],[]},
 {739,['ns_1@127.0.0.1',undefined],[]},
 {740,['ns_1@127.0.0.1',undefined],[]},
 {741,['ns_1@127.0.0.1',undefined],[]},
 {742,['ns_1@127.0.0.1',undefined],[]},
 {743,['ns_1@127.0.0.1',undefined],[]},
 {744,['ns_1@127.0.0.1',undefined],[]},
 {745,['ns_1@127.0.0.1',undefined],[]},
 {746,['ns_1@127.0.0.1',undefined],[]},
 {747,['ns_1@127.0.0.1',undefined],[]},
 {748,['ns_1@127.0.0.1',undefined],[]},
 {749,['ns_1@127.0.0.1',undefined],[]},
 {750,['ns_1@127.0.0.1',undefined],[]},
 {751,['ns_1@127.0.0.1',undefined],[]},
 {752,['ns_1@127.0.0.1',undefined],[]},
 {753,['ns_1@127.0.0.1',undefined],[]},
 {754,['ns_1@127.0.0.1',undefined],[]},
 {755,['ns_1@127.0.0.1',undefined],[]},
 {756,['ns_1@127.0.0.1',undefined],[]},
 {757,['ns_1@127.0.0.1',undefined],[]},
 {758,['ns_1@127.0.0.1',undefined],[]},
 {759,['ns_1@127.0.0.1',undefined],[]},
 {760,['ns_1@127.0.0.1',undefined],[]},
 {761,['ns_1@127.0.0.1',undefined],[]},
 {762,['ns_1@127.0.0.1',undefined],[]},
 {763,['ns_1@127.0.0.1',undefined],[]},
 {764,['ns_1@127.0.0.1',undefined],[]},
 {765,['ns_1@127.0.0.1',undefined],[]},
 {766,['ns_1@127.0.0.1',undefined],[]},
 {767,['ns_1@127.0.0.1',undefined],[]},
 {768,['ns_1@127.0.0.1',undefined],[]},
 {769,['ns_1@127.0.0.1',undefined],[]},
 {770,['ns_1@127.0.0.1',undefined],[]},
 {771,['ns_1@127.0.0.1',undefined],[]},
 {772,['ns_1@127.0.0.1',undefined],[]},
 {773,['ns_1@127.0.0.1',undefined],[]},
 {774,['ns_1@127.0.0.1',undefined],[]},
 {775,['ns_1@127.0.0.1',undefined],[]},
 {776,['ns_1@127.0.0.1',undefined],[]},
 {777,['ns_1@127.0.0.1',undefined],[]},
 {778,['ns_1@127.0.0.1',undefined],[]},
 {779,['ns_1@127.0.0.1',undefined],[]},
 {780,['ns_1@127.0.0.1',undefined],[]},
 {781,['ns_1@127.0.0.1',undefined],[]},
 {782,['ns_1@127.0.0.1',undefined],[]},
 {783,['ns_1@127.0.0.1',undefined],[]},
 {784,['ns_1@127.0.0.1',undefined],[]},
 {785,['ns_1@127.0.0.1',undefined],[]},
 {786,['ns_1@127.0.0.1',undefined],[]},
 {787,['ns_1@127.0.0.1',undefined],[]},
 {788,['ns_1@127.0.0.1',undefined],[]},
 {789,['ns_1@127.0.0.1',undefined],[]},
 {790,['ns_1@127.0.0.1',undefined],[]},
 {791,['ns_1@127.0.0.1',undefined],[]},
 {792,['ns_1@127.0.0.1',undefined],[]},
 {793,['ns_1@127.0.0.1',undefined],[]},
 {794,['ns_1@127.0.0.1',undefined],[]},
 {795,['ns_1@127.0.0.1',undefined],[]},
 {796,['ns_1@127.0.0.1',undefined],[]},
 {797,['ns_1@127.0.0.1',undefined],[]},
 {798,['ns_1@127.0.0.1',undefined],[]},
 {799,['ns_1@127.0.0.1',undefined],[]},
 {800,['ns_1@127.0.0.1',undefined],[]},
 {801,['ns_1@127.0.0.1',undefined],[]},
 {802,['ns_1@127.0.0.1',undefined],[]},
 {803,['ns_1@127.0.0.1',undefined],[]},
 {804,['ns_1@127.0.0.1',undefined],[]},
 {805,['ns_1@127.0.0.1',undefined],[]},
 {806,['ns_1@127.0.0.1',undefined],[]},
 {807,['ns_1@127.0.0.1',undefined],[]},
 {808,['ns_1@127.0.0.1',undefined],[]},
 {809,['ns_1@127.0.0.1',undefined],[]},
 {810,['ns_1@127.0.0.1',undefined],[]},
 {811,['ns_1@127.0.0.1',undefined],[]},
 {812,['ns_1@127.0.0.1',undefined],[]},
 {813,['ns_1@127.0.0.1',undefined],[]},
 {814,['ns_1@127.0.0.1',undefined],[]},
 {815,['ns_1@127.0.0.1',undefined],[]},
 {816,['ns_1@127.0.0.1',undefined],[]},
 {817,['ns_1@127.0.0.1',undefined],[]},
 {818,['ns_1@127.0.0.1',undefined],[]},
 {819,['ns_1@127.0.0.1',undefined],[]},
 {820,['ns_1@127.0.0.1',undefined],[]},
 {821,['ns_1@127.0.0.1',undefined],[]},
 {822,['ns_1@127.0.0.1',undefined],[]},
 {823,['ns_1@127.0.0.1',undefined],[]},
 {824,['ns_1@127.0.0.1',undefined],[]},
 {825,['ns_1@127.0.0.1',undefined],[]},
 {826,['ns_1@127.0.0.1',undefined],[]},
 {827,['ns_1@127.0.0.1',undefined],[]},
 {828,['ns_1@127.0.0.1',undefined],[]},
 {829,['ns_1@127.0.0.1',undefined],[]},
 {830,['ns_1@127.0.0.1',undefined],[]},
 {831,['ns_1@127.0.0.1',undefined],[]},
 {832,['ns_1@127.0.0.1',undefined],[]},
 {833,['ns_1@127.0.0.1',undefined],[]},
 {834,['ns_1@127.0.0.1',undefined],[]},
 {835,['ns_1@127.0.0.1',undefined],[]},
 {836,['ns_1@127.0.0.1',undefined],[]},
 {837,['ns_1@127.0.0.1',undefined],[]},
 {838,['ns_1@127.0.0.1',undefined],[]},
 {839,['ns_1@127.0.0.1',undefined],[]},
 {840,['ns_1@127.0.0.1',undefined],[]},
 {841,['ns_1@127.0.0.1',undefined],[]},
 {842,['ns_1@127.0.0.1',undefined],[]},
 {843,['ns_1@127.0.0.1',undefined],[]},
 {844,['ns_1@127.0.0.1',undefined],[]},
 {845,['ns_1@127.0.0.1',undefined],[]},
 {846,['ns_1@127.0.0.1',undefined],[]},
 {847,['ns_1@127.0.0.1',undefined],[]},
 {848,['ns_1@127.0.0.1',undefined],[]},
 {849,['ns_1@127.0.0.1',undefined],[]},
 {850,['ns_1@127.0.0.1',undefined],[]},
 {851,['ns_1@127.0.0.1',undefined],[]},
 {852,['ns_1@127.0.0.1',undefined],[]},
 {853,['ns_1@127.0.0.1',undefined],[]},
 {854,['ns_1@127.0.0.1',undefined],[]},
 {855,['ns_1@127.0.0.1',undefined],[]},
 {856,['ns_1@127.0.0.1',undefined],[]},
 {857,['ns_1@127.0.0.1',undefined],[]},
 {858,['ns_1@127.0.0.1',undefined],[]},
 {859,['ns_1@127.0.0.1',undefined],[]},
 {860,['ns_1@127.0.0.1',undefined],[]},
 {861,['ns_1@127.0.0.1',undefined],[]},
 {862,['ns_1@127.0.0.1',undefined],[]},
 {863,['ns_1@127.0.0.1',undefined],[]},
 {864,['ns_1@127.0.0.1',undefined],[]},
 {865,['ns_1@127.0.0.1',undefined],[]},
 {866,['ns_1@127.0.0.1',undefined],[]},
 {867,['ns_1@127.0.0.1',undefined],[]},
 {868,['ns_1@127.0.0.1',undefined],[]},
 {869,['ns_1@127.0.0.1',undefined],[]},
 {870,['ns_1@127.0.0.1',undefined],[]},
 {871,['ns_1@127.0.0.1',undefined],[]},
 {872,['ns_1@127.0.0.1',undefined],[]},
 {873,['ns_1@127.0.0.1',undefined],[]},
 {874,['ns_1@127.0.0.1',undefined],[]},
 {875,['ns_1@127.0.0.1',undefined],[]},
 {876,['ns_1@127.0.0.1',undefined],[]},
 {877,['ns_1@127.0.0.1',undefined],[]},
 {878,['ns_1@127.0.0.1',undefined],[]},
 {879,['ns_1@127.0.0.1',undefined],[]},
 {880,['ns_1@127.0.0.1',undefined],[]},
 {881,['ns_1@127.0.0.1',undefined],[]},
 {882,['ns_1@127.0.0.1',undefined],[]},
 {883,['ns_1@127.0.0.1',undefined],[]},
 {884,['ns_1@127.0.0.1',undefined],[]},
 {885,['ns_1@127.0.0.1',undefined],[]},
 {886,['ns_1@127.0.0.1',undefined],[]},
 {887,['ns_1@127.0.0.1',undefined],[]},
 {888,['ns_1@127.0.0.1',undefined],[]},
 {889,['ns_1@127.0.0.1',undefined],[]},
 {890,['ns_1@127.0.0.1',undefined],[]},
 {891,['ns_1@127.0.0.1',undefined],[]},
 {892,['ns_1@127.0.0.1',undefined],[]},
 {893,['ns_1@127.0.0.1',undefined],[]},
 {894,['ns_1@127.0.0.1',undefined],[]},
 {895,['ns_1@127.0.0.1',undefined],[]},
 {896,['ns_1@127.0.0.1',undefined],[]},
 {897,['ns_1@127.0.0.1',undefined],[]},
 {898,['ns_1@127.0.0.1',undefined],[]},
 {899,['ns_1@127.0.0.1',undefined],[]},
 {900,['ns_1@127.0.0.1',undefined],[]},
 {901,['ns_1@127.0.0.1',undefined],[]},
 {902,['ns_1@127.0.0.1',undefined],[]},
 {903,['ns_1@127.0.0.1',undefined],[]},
 {904,['ns_1@127.0.0.1',undefined],[]},
 {905,['ns_1@127.0.0.1',undefined],[]},
 {906,['ns_1@127.0.0.1',undefined],[]},
 {907,['ns_1@127.0.0.1',undefined],[]},
 {908,['ns_1@127.0.0.1',undefined],[]},
 {909,['ns_1@127.0.0.1',undefined],[]},
 {910,['ns_1@127.0.0.1',undefined],[]},
 {911,['ns_1@127.0.0.1',undefined],[]},
 {912,['ns_1@127.0.0.1',undefined],[]},
 {913,['ns_1@127.0.0.1',undefined],[]},
 {914,['ns_1@127.0.0.1',undefined],[]},
 {915,['ns_1@127.0.0.1',undefined],[]},
 {916,['ns_1@127.0.0.1',undefined],[]},
 {917,['ns_1@127.0.0.1',undefined],[]},
 {918,['ns_1@127.0.0.1',undefined],[]},
 {919,['ns_1@127.0.0.1',undefined],[]},
 {920,['ns_1@127.0.0.1',undefined],[]},
 {921,['ns_1@127.0.0.1',undefined],[]},
 {922,['ns_1@127.0.0.1',undefined],[]},
 {923,['ns_1@127.0.0.1',undefined],[]},
 {924,['ns_1@127.0.0.1',undefined],[]},
 {925,['ns_1@127.0.0.1',undefined],[]},
 {926,['ns_1@127.0.0.1',undefined],[]},
 {927,['ns_1@127.0.0.1',undefined],[]},
 {928,['ns_1@127.0.0.1',undefined],[]},
 {929,['ns_1@127.0.0.1',undefined],[]},
 {930,['ns_1@127.0.0.1',undefined],[]},
 {931,['ns_1@127.0.0.1',undefined],[]},
 {932,['ns_1@127.0.0.1',undefined],[]},
 {933,['ns_1@127.0.0.1',undefined],[]},
 {934,['ns_1@127.0.0.1',undefined],[]},
 {935,['ns_1@127.0.0.1',undefined],[]},
 {936,['ns_1@127.0.0.1',undefined],[]},
 {937,['ns_1@127.0.0.1',undefined],[]},
 {938,['ns_1@127.0.0.1',undefined],[]},
 {939,['ns_1@127.0.0.1',undefined],[]},
 {940,['ns_1@127.0.0.1',undefined],[]},
 {941,['ns_1@127.0.0.1',undefined],[]},
 {942,['ns_1@127.0.0.1',undefined],[]},
 {943,['ns_1@127.0.0.1',undefined],[]},
 {944,['ns_1@127.0.0.1',undefined],[]},
 {945,['ns_1@127.0.0.1',undefined],[]},
 {946,['ns_1@127.0.0.1',undefined],[]},
 {947,['ns_1@127.0.0.1',undefined],[]},
 {948,['ns_1@127.0.0.1',undefined],[]},
 {949,['ns_1@127.0.0.1',undefined],[]},
 {950,['ns_1@127.0.0.1',undefined],[]},
 {951,['ns_1@127.0.0.1',undefined],[]},
 {952,['ns_1@127.0.0.1',undefined],[]},
 {953,['ns_1@127.0.0.1',undefined],[]},
 {954,['ns_1@127.0.0.1',undefined],[]},
 {955,['ns_1@127.0.0.1',undefined],[]},
 {956,['ns_1@127.0.0.1',undefined],[]},
 {957,['ns_1@127.0.0.1',undefined],[]},
 {958,['ns_1@127.0.0.1',undefined],[]},
 {959,['ns_1@127.0.0.1',undefined],[]},
 {960,['ns_1@127.0.0.1',undefined],[]},
 {961,['ns_1@127.0.0.1',undefined],[]},
 {962,['ns_1@127.0.0.1',undefined],[]},
 {963,['ns_1@127.0.0.1',undefined],[]},
 {964,['ns_1@127.0.0.1',undefined],[]},
 {965,['ns_1@127.0.0.1',undefined],[]},
 {966,['ns_1@127.0.0.1',undefined],[]},
 {967,['ns_1@127.0.0.1',undefined],[]},
 {968,['ns_1@127.0.0.1',undefined],[]},
 {969,['ns_1@127.0.0.1',undefined],[]},
 {970,['ns_1@127.0.0.1',undefined],[]},
 {971,['ns_1@127.0.0.1',undefined],[]},
 {972,['ns_1@127.0.0.1',undefined],[]},
 {973,['ns_1@127.0.0.1',undefined],[]},
 {974,['ns_1@127.0.0.1',undefined],[]},
 {975,['ns_1@127.0.0.1',undefined],[]},
 {976,['ns_1@127.0.0.1',undefined],[]},
 {977,['ns_1@127.0.0.1',undefined],[]},
 {978,['ns_1@127.0.0.1',undefined],[]},
 {979,['ns_1@127.0.0.1',undefined],[]},
 {980,['ns_1@127.0.0.1',undefined],[]},
 {981,['ns_1@127.0.0.1',undefined],[]},
 {982,['ns_1@127.0.0.1',undefined],[]},
 {983,['ns_1@127.0.0.1',undefined],[]},
 {984,['ns_1@127.0.0.1',undefined],[]},
 {985,['ns_1@127.0.0.1',undefined],[]},
 {986,['ns_1@127.0.0.1',undefined],[]},
 {987,['ns_1@127.0.0.1',undefined],[]},
 {988,['ns_1@127.0.0.1',undefined],[]},
 {989,['ns_1@127.0.0.1',undefined],[]},
 {990,['ns_1@127.0.0.1',undefined],[]},
 {991,['ns_1@127.0.0.1',undefined],[]},
 {992,['ns_1@127.0.0.1',undefined],[]},
 {993,['ns_1@127.0.0.1',undefined],[]},
 {994,['ns_1@127.0.0.1',undefined],[]},
 {995,['ns_1@127.0.0.1',undefined],[]},
 {996,['ns_1@127.0.0.1',undefined],[]},
 {997,['ns_1@127.0.0.1',undefined],[]},
 {998,['ns_1@127.0.0.1',undefined],[]},
 {999,['ns_1@127.0.0.1',undefined],[]},
 {1000,['ns_1@127.0.0.1',undefined],[]},
 {1001,['ns_1@127.0.0.1',undefined],[]},
 {1002,['ns_1@127.0.0.1',undefined],[]},
 {1003,['ns_1@127.0.0.1',undefined],[]},
 {1004,['ns_1@127.0.0.1',undefined],[]},
 {1005,['ns_1@127.0.0.1',undefined],[]},
 {1006,['ns_1@127.0.0.1',undefined],[]},
 {1007,['ns_1@127.0.0.1',undefined],[]},
 {1008,['ns_1@127.0.0.1',undefined],[]},
 {1009,['ns_1@127.0.0.1',undefined],[]},
 {1010,['ns_1@127.0.0.1',undefined],[]},
 {1011,['ns_1@127.0.0.1',undefined],[]},
 {1012,['ns_1@127.0.0.1',undefined],[]},
 {1013,['ns_1@127.0.0.1',undefined],[]},
 {1014,['ns_1@127.0.0.1',undefined],[]},
 {1015,['ns_1@127.0.0.1',undefined],[]},
 {1016,['ns_1@127.0.0.1',undefined],[]},
 {1017,['ns_1@127.0.0.1',undefined],[]},
 {1018,['ns_1@127.0.0.1',undefined],[]},
 {1019,['ns_1@127.0.0.1',undefined],[]},
 {1020,['ns_1@127.0.0.1',undefined],[]},
 {1021,['ns_1@127.0.0.1',undefined],[]},
 {1022,['ns_1@127.0.0.1',undefined],[]},
 {1023,['ns_1@127.0.0.1',undefined],[]}]
[ns_server:debug,2020-03-27T20:46:35.507Z,ns_1@127.0.0.1:<0.16664.0>:ns_janitor:config_sync:259]Going to push config to/from nodes:
['ns_1@127.0.0.1']
[ns_server:debug,2020-03-27T20:46:35.507Z,ns_1@127.0.0.1:ns_config_rep<0.319.0>:ns_config_rep:do_push_keys:321]Replicating some config keys ([buckets]..)
[ns_server:debug,2020-03-27T20:46:35.508Z,ns_1@127.0.0.1:ns_config_rep<0.319.0>:ns_config_rep:handle_call:122]Got full synchronization request from 'ns_1@127.0.0.1'
[ns_server:debug,2020-03-27T20:46:35.508Z,ns_1@127.0.0.1:ns_config_rep<0.319.0>:ns_config_rep:handle_call:128]Fully synchronized config in 4 us
[ns_server:info,2020-03-27T20:46:35.518Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 1023 state to active
[ns_server:info,2020-03-27T20:46:35.519Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 1022 state to active
[ns_server:info,2020-03-27T20:46:35.520Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 1021 state to active
[ns_server:info,2020-03-27T20:46:35.520Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 1020 state to active
[ns_server:info,2020-03-27T20:46:35.521Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 1019 state to active
[ns_server:info,2020-03-27T20:46:35.522Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 1018 state to active
[ns_server:info,2020-03-27T20:46:35.523Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 1017 state to active
[ns_server:info,2020-03-27T20:46:35.523Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 1016 state to active
[ns_server:info,2020-03-27T20:46:35.524Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 1015 state to active
[ns_server:info,2020-03-27T20:46:35.526Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 1014 state to active
[ns_server:info,2020-03-27T20:46:35.528Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 1013 state to active
[ns_server:info,2020-03-27T20:46:35.537Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 1012 state to active
[ns_server:info,2020-03-27T20:46:35.537Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 1011 state to active
[ns_server:info,2020-03-27T20:46:35.538Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 1010 state to active
[ns_server:info,2020-03-27T20:46:35.538Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 1009 state to active
[ns_server:info,2020-03-27T20:46:35.539Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 1008 state to active
[ns_server:info,2020-03-27T20:46:35.539Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 1007 state to active
[ns_server:info,2020-03-27T20:46:35.539Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 1006 state to active
[ns_server:info,2020-03-27T20:46:35.539Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 1005 state to active
[ns_server:info,2020-03-27T20:46:35.539Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 1004 state to active
[ns_server:info,2020-03-27T20:46:35.540Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 1003 state to active
[ns_server:info,2020-03-27T20:46:35.540Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 1002 state to active
[ns_server:info,2020-03-27T20:46:35.542Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 1001 state to active
[ns_server:info,2020-03-27T20:46:35.542Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 1000 state to active
[ns_server:info,2020-03-27T20:46:35.543Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 999 state to active
[ns_server:info,2020-03-27T20:46:35.543Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 998 state to active
[ns_server:info,2020-03-27T20:46:35.543Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 997 state to active
[ns_server:info,2020-03-27T20:46:35.544Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 996 state to active
[ns_server:info,2020-03-27T20:46:35.545Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 995 state to active
[ns_server:info,2020-03-27T20:46:35.546Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 994 state to active
[ns_server:info,2020-03-27T20:46:35.546Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 993 state to active
[ns_server:info,2020-03-27T20:46:35.547Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 992 state to active
[ns_server:info,2020-03-27T20:46:35.549Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 991 state to active
[ns_server:info,2020-03-27T20:46:35.553Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 990 state to active
[ns_server:info,2020-03-27T20:46:35.555Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 989 state to active
[ns_server:info,2020-03-27T20:46:35.556Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 988 state to active
[ns_server:info,2020-03-27T20:46:35.557Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 987 state to active
[ns_server:info,2020-03-27T20:46:35.557Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 986 state to active
[ns_server:info,2020-03-27T20:46:35.558Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 985 state to active
[ns_server:info,2020-03-27T20:46:35.558Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 984 state to active
[ns_server:info,2020-03-27T20:46:35.560Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 983 state to active
[ns_server:info,2020-03-27T20:46:35.561Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 982 state to active
[ns_server:info,2020-03-27T20:46:35.562Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 981 state to active
[ns_server:info,2020-03-27T20:46:35.562Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 980 state to active
[ns_server:info,2020-03-27T20:46:35.563Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 979 state to active
[ns_server:info,2020-03-27T20:46:35.563Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 978 state to active
[ns_server:info,2020-03-27T20:46:35.563Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 977 state to active
[ns_server:info,2020-03-27T20:46:35.563Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 976 state to active
[ns_server:info,2020-03-27T20:46:35.564Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 975 state to active
[ns_server:info,2020-03-27T20:46:35.564Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 974 state to active
[ns_server:info,2020-03-27T20:46:35.564Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 973 state to active
[ns_server:info,2020-03-27T20:46:35.564Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 972 state to active
[ns_server:info,2020-03-27T20:46:35.564Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 971 state to active
[ns_server:info,2020-03-27T20:46:35.565Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 970 state to active
[ns_server:info,2020-03-27T20:46:35.565Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 969 state to active
[ns_server:info,2020-03-27T20:46:35.565Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 968 state to active
[ns_server:info,2020-03-27T20:46:35.565Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 967 state to active
[ns_server:info,2020-03-27T20:46:35.565Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 966 state to active
[ns_server:info,2020-03-27T20:46:35.566Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 965 state to active
[ns_server:info,2020-03-27T20:46:35.566Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 964 state to active
[ns_server:info,2020-03-27T20:46:35.566Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 963 state to active
[ns_server:info,2020-03-27T20:46:35.566Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 962 state to active
[ns_server:info,2020-03-27T20:46:35.567Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 961 state to active
[ns_server:info,2020-03-27T20:46:35.567Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 960 state to active
[ns_server:info,2020-03-27T20:46:35.567Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 959 state to active
[ns_server:info,2020-03-27T20:46:35.567Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 958 state to active
[ns_server:info,2020-03-27T20:46:35.568Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 957 state to active
[ns_server:info,2020-03-27T20:46:35.568Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 956 state to active
[ns_server:info,2020-03-27T20:46:35.568Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 955 state to active
[ns_server:info,2020-03-27T20:46:35.568Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 954 state to active
[ns_server:info,2020-03-27T20:46:35.569Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 953 state to active
[ns_server:info,2020-03-27T20:46:35.569Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 952 state to active
[ns_server:info,2020-03-27T20:46:35.569Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 951 state to active
[ns_server:info,2020-03-27T20:46:35.569Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 950 state to active
[ns_server:info,2020-03-27T20:46:35.569Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 949 state to active
[ns_server:info,2020-03-27T20:46:35.570Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 948 state to active
[ns_server:info,2020-03-27T20:46:35.570Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 947 state to active
[ns_server:info,2020-03-27T20:46:35.570Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 946 state to active
[ns_server:info,2020-03-27T20:46:35.570Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 945 state to active
[ns_server:info,2020-03-27T20:46:35.571Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 944 state to active
[ns_server:info,2020-03-27T20:46:35.571Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 943 state to active
[ns_server:info,2020-03-27T20:46:35.571Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 942 state to active
[ns_server:info,2020-03-27T20:46:35.572Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 941 state to active
[ns_server:info,2020-03-27T20:46:35.572Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 940 state to active
[ns_server:info,2020-03-27T20:46:35.572Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 939 state to active
[ns_server:info,2020-03-27T20:46:35.572Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 938 state to active
[ns_server:info,2020-03-27T20:46:35.572Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 937 state to active
[ns_server:info,2020-03-27T20:46:35.573Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 936 state to active
[ns_server:info,2020-03-27T20:46:35.573Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 935 state to active
[ns_server:info,2020-03-27T20:46:35.573Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 934 state to active
[ns_server:info,2020-03-27T20:46:35.573Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 933 state to active
[ns_server:info,2020-03-27T20:46:35.573Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 932 state to active
[ns_server:info,2020-03-27T20:46:35.574Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 931 state to active
[ns_server:info,2020-03-27T20:46:35.574Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 930 state to active
[ns_server:info,2020-03-27T20:46:35.574Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 929 state to active
[ns_server:info,2020-03-27T20:46:35.581Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 928 state to active
[ns_server:info,2020-03-27T20:46:35.582Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 927 state to active
[ns_server:info,2020-03-27T20:46:35.584Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 926 state to active
[ns_server:info,2020-03-27T20:46:35.584Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 925 state to active
[ns_server:info,2020-03-27T20:46:35.585Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 924 state to active
[ns_server:info,2020-03-27T20:46:35.600Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 923 state to active
[ns_server:info,2020-03-27T20:46:35.601Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 922 state to active
[ns_server:info,2020-03-27T20:46:35.602Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 921 state to active
[ns_server:info,2020-03-27T20:46:35.602Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 920 state to active
[ns_server:info,2020-03-27T20:46:35.603Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 919 state to active
[ns_server:info,2020-03-27T20:46:35.605Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 918 state to active
[ns_server:info,2020-03-27T20:46:35.605Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 917 state to active
[ns_server:info,2020-03-27T20:46:35.606Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 916 state to active
[ns_server:info,2020-03-27T20:46:35.607Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 915 state to active
[ns_server:info,2020-03-27T20:46:35.607Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 914 state to active
[ns_server:info,2020-03-27T20:46:35.608Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 913 state to active
[ns_server:info,2020-03-27T20:46:35.609Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 912 state to active
[ns_server:info,2020-03-27T20:46:35.609Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 911 state to active
[ns_server:info,2020-03-27T20:46:35.610Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 910 state to active
[ns_server:info,2020-03-27T20:46:35.610Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 909 state to active
[ns_server:info,2020-03-27T20:46:35.611Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 908 state to active
[ns_server:info,2020-03-27T20:46:35.611Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 907 state to active
[ns_server:info,2020-03-27T20:46:35.611Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 906 state to active
[ns_server:info,2020-03-27T20:46:35.611Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 905 state to active
[ns_server:info,2020-03-27T20:46:35.612Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 904 state to active
[ns_server:info,2020-03-27T20:46:35.612Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 903 state to active
[ns_server:info,2020-03-27T20:46:35.612Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 902 state to active
[ns_server:info,2020-03-27T20:46:35.613Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 901 state to active
[ns_server:info,2020-03-27T20:46:35.613Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 900 state to active
[ns_server:info,2020-03-27T20:46:35.613Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 899 state to active
[ns_server:info,2020-03-27T20:46:35.613Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 898 state to active
[ns_server:info,2020-03-27T20:46:35.613Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 897 state to active
[ns_server:info,2020-03-27T20:46:35.614Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 896 state to active
[ns_server:info,2020-03-27T20:46:35.614Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 895 state to active
[ns_server:info,2020-03-27T20:46:35.614Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 894 state to active
[ns_server:info,2020-03-27T20:46:35.615Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 893 state to active
[ns_server:info,2020-03-27T20:46:35.615Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 892 state to active
[ns_server:info,2020-03-27T20:46:35.615Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 891 state to active
[ns_server:info,2020-03-27T20:46:35.617Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 890 state to active
[ns_server:info,2020-03-27T20:46:35.618Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 889 state to active
[ns_server:info,2020-03-27T20:46:35.618Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 888 state to active
[ns_server:info,2020-03-27T20:46:35.619Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 887 state to active
[ns_server:info,2020-03-27T20:46:35.619Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 886 state to active
[ns_server:info,2020-03-27T20:46:35.620Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 885 state to active
[ns_server:info,2020-03-27T20:46:35.620Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 884 state to active
[ns_server:info,2020-03-27T20:46:35.621Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 883 state to active
[ns_server:info,2020-03-27T20:46:35.621Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 882 state to active
[ns_server:info,2020-03-27T20:46:35.622Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 881 state to active
[ns_server:info,2020-03-27T20:46:35.622Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 880 state to active
[ns_server:info,2020-03-27T20:46:35.623Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 879 state to active
[ns_server:info,2020-03-27T20:46:35.623Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 878 state to active
[ns_server:info,2020-03-27T20:46:35.624Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 877 state to active
[ns_server:info,2020-03-27T20:46:35.624Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 876 state to active
[ns_server:info,2020-03-27T20:46:35.624Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 875 state to active
[ns_server:info,2020-03-27T20:46:35.625Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 874 state to active
[ns_server:info,2020-03-27T20:46:35.625Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 873 state to active
[ns_server:info,2020-03-27T20:46:35.625Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 872 state to active
[ns_server:info,2020-03-27T20:46:35.626Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 871 state to active
[ns_server:info,2020-03-27T20:46:35.626Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 870 state to active
[ns_server:info,2020-03-27T20:46:35.626Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 869 state to active
[ns_server:info,2020-03-27T20:46:35.627Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 868 state to active
[ns_server:info,2020-03-27T20:46:35.627Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 867 state to active
[ns_server:info,2020-03-27T20:46:35.628Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 866 state to active
[ns_server:info,2020-03-27T20:46:35.628Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 865 state to active
[ns_server:info,2020-03-27T20:46:35.628Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 864 state to active
[ns_server:info,2020-03-27T20:46:35.629Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 863 state to active
[ns_server:info,2020-03-27T20:46:35.629Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 862 state to active
[ns_server:info,2020-03-27T20:46:35.629Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 861 state to active
[ns_server:info,2020-03-27T20:46:35.630Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 860 state to active
[ns_server:info,2020-03-27T20:46:35.630Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 859 state to active
[ns_server:info,2020-03-27T20:46:35.630Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 858 state to active
[ns_server:info,2020-03-27T20:46:35.630Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 857 state to active
[ns_server:info,2020-03-27T20:46:35.631Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 856 state to active
[ns_server:info,2020-03-27T20:46:35.631Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 855 state to active
[ns_server:info,2020-03-27T20:46:35.632Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 854 state to active
[ns_server:info,2020-03-27T20:46:35.632Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 853 state to active
[ns_server:info,2020-03-27T20:46:35.632Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 852 state to active
[ns_server:info,2020-03-27T20:46:35.633Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 851 state to active
[ns_server:info,2020-03-27T20:46:35.634Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 850 state to active
[ns_server:info,2020-03-27T20:46:35.635Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 849 state to active
[ns_server:info,2020-03-27T20:46:35.635Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 848 state to active
[ns_server:info,2020-03-27T20:46:35.635Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 847 state to active
[ns_server:info,2020-03-27T20:46:35.636Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 846 state to active
[ns_server:info,2020-03-27T20:46:35.636Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 845 state to active
[ns_server:info,2020-03-27T20:46:35.636Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 844 state to active
[ns_server:info,2020-03-27T20:46:35.636Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 843 state to active
[ns_server:info,2020-03-27T20:46:35.637Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 842 state to active
[ns_server:info,2020-03-27T20:46:35.637Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 841 state to active
[ns_server:info,2020-03-27T20:46:35.637Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 840 state to active
[ns_server:info,2020-03-27T20:46:35.638Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 839 state to active
[ns_server:info,2020-03-27T20:46:35.638Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 838 state to active
[ns_server:info,2020-03-27T20:46:35.639Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 837 state to active
[ns_server:info,2020-03-27T20:46:35.639Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 836 state to active
[ns_server:info,2020-03-27T20:46:35.639Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 835 state to active
[ns_server:info,2020-03-27T20:46:35.640Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 834 state to active
[ns_server:info,2020-03-27T20:46:35.641Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 833 state to active
[ns_server:info,2020-03-27T20:46:35.641Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 832 state to active
[ns_server:info,2020-03-27T20:46:35.641Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 831 state to active
[ns_server:info,2020-03-27T20:46:35.642Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 830 state to active
[ns_server:info,2020-03-27T20:46:35.642Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 829 state to active
[ns_server:info,2020-03-27T20:46:35.643Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 828 state to active
[ns_server:info,2020-03-27T20:46:35.645Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 827 state to active
[ns_server:info,2020-03-27T20:46:35.645Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 826 state to active
[ns_server:info,2020-03-27T20:46:35.646Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 825 state to active
[ns_server:info,2020-03-27T20:46:35.646Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 824 state to active
[ns_server:info,2020-03-27T20:46:35.647Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 823 state to active
[ns_server:info,2020-03-27T20:46:35.648Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 822 state to active
[ns_server:info,2020-03-27T20:46:35.648Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 821 state to active
[ns_server:info,2020-03-27T20:46:35.648Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 820 state to active
[ns_server:info,2020-03-27T20:46:35.649Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 819 state to active
[ns_server:info,2020-03-27T20:46:35.649Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 818 state to active
[ns_server:info,2020-03-27T20:46:35.649Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 817 state to active
[ns_server:info,2020-03-27T20:46:35.650Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 816 state to active
[ns_server:info,2020-03-27T20:46:35.650Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 815 state to active
[ns_server:info,2020-03-27T20:46:35.651Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 814 state to active
[ns_server:info,2020-03-27T20:46:35.651Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 813 state to active
[ns_server:info,2020-03-27T20:46:35.652Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 812 state to active
[ns_server:info,2020-03-27T20:46:35.652Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 811 state to active
[ns_server:info,2020-03-27T20:46:35.652Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 810 state to active
[ns_server:info,2020-03-27T20:46:35.652Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 809 state to active
[ns_server:info,2020-03-27T20:46:35.653Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 808 state to active
[ns_server:info,2020-03-27T20:46:35.653Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 807 state to active
[ns_server:info,2020-03-27T20:46:35.653Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 806 state to active
[ns_server:info,2020-03-27T20:46:35.654Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 805 state to active
[ns_server:info,2020-03-27T20:46:35.654Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 804 state to active
[ns_server:info,2020-03-27T20:46:35.655Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 803 state to active
[ns_server:info,2020-03-27T20:46:35.656Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 802 state to active
[ns_server:info,2020-03-27T20:46:35.660Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 801 state to active
[ns_server:info,2020-03-27T20:46:35.660Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 800 state to active
[ns_server:info,2020-03-27T20:46:35.660Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 799 state to active
[ns_server:info,2020-03-27T20:46:35.661Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 798 state to active
[ns_server:info,2020-03-27T20:46:35.661Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 797 state to active
[ns_server:info,2020-03-27T20:46:35.661Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 796 state to active
[ns_server:info,2020-03-27T20:46:35.662Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 795 state to active
[ns_server:info,2020-03-27T20:46:35.662Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 794 state to active
[ns_server:info,2020-03-27T20:46:35.662Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 793 state to active
[ns_server:info,2020-03-27T20:46:35.662Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 792 state to active
[ns_server:info,2020-03-27T20:46:35.662Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 791 state to active
[ns_server:info,2020-03-27T20:46:35.663Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 790 state to active
[ns_server:info,2020-03-27T20:46:35.663Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 789 state to active
[ns_server:info,2020-03-27T20:46:35.663Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 788 state to active
[ns_server:info,2020-03-27T20:46:35.663Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 787 state to active
[ns_server:info,2020-03-27T20:46:35.663Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 786 state to active
[ns_server:info,2020-03-27T20:46:35.663Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 785 state to active
[ns_server:info,2020-03-27T20:46:35.664Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 784 state to active
[ns_server:info,2020-03-27T20:46:35.664Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 783 state to active
[ns_server:info,2020-03-27T20:46:35.664Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 782 state to active
[ns_server:info,2020-03-27T20:46:35.664Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 781 state to active
[ns_server:info,2020-03-27T20:46:35.664Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 780 state to active
[ns_server:info,2020-03-27T20:46:35.665Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 779 state to active
[ns_server:info,2020-03-27T20:46:35.665Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 778 state to active
[ns_server:info,2020-03-27T20:46:35.665Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 777 state to active
[ns_server:info,2020-03-27T20:46:35.665Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 776 state to active
[ns_server:info,2020-03-27T20:46:35.665Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 775 state to active
[ns_server:info,2020-03-27T20:46:35.666Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 774 state to active
[ns_server:info,2020-03-27T20:46:35.666Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 773 state to active
[ns_server:info,2020-03-27T20:46:35.666Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 772 state to active
[ns_server:info,2020-03-27T20:46:35.666Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 771 state to active
[ns_server:info,2020-03-27T20:46:35.667Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 770 state to active
[ns_server:info,2020-03-27T20:46:35.669Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 769 state to active
[ns_server:info,2020-03-27T20:46:35.671Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 768 state to active
[ns_server:info,2020-03-27T20:46:35.672Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 767 state to active
[ns_server:info,2020-03-27T20:46:35.673Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 766 state to active
[ns_server:info,2020-03-27T20:46:35.674Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 765 state to active
[ns_server:info,2020-03-27T20:46:35.675Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 764 state to active
[ns_server:info,2020-03-27T20:46:35.677Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 763 state to active
[ns_server:info,2020-03-27T20:46:35.678Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 762 state to active
[ns_server:info,2020-03-27T20:46:35.679Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 761 state to active
[ns_server:info,2020-03-27T20:46:35.679Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 760 state to active
[ns_server:info,2020-03-27T20:46:35.681Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 759 state to active
[ns_server:info,2020-03-27T20:46:35.683Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 758 state to active
[ns_server:info,2020-03-27T20:46:35.684Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 757 state to active
[ns_server:info,2020-03-27T20:46:35.685Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 756 state to active
[ns_server:info,2020-03-27T20:46:35.686Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 755 state to active
[ns_server:info,2020-03-27T20:46:35.687Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 754 state to active
[ns_server:info,2020-03-27T20:46:35.688Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 753 state to active
[ns_server:info,2020-03-27T20:46:35.689Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 752 state to active
[ns_server:info,2020-03-27T20:46:35.690Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 751 state to active
[ns_server:info,2020-03-27T20:46:35.690Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 750 state to active
[ns_server:info,2020-03-27T20:46:35.692Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 749 state to active
[ns_server:info,2020-03-27T20:46:35.692Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 748 state to active
[ns_server:info,2020-03-27T20:46:35.693Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 747 state to active
[ns_server:info,2020-03-27T20:46:35.693Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 746 state to active
[ns_server:info,2020-03-27T20:46:35.694Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 745 state to active
[ns_server:info,2020-03-27T20:46:35.694Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 744 state to active
[ns_server:info,2020-03-27T20:46:35.695Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 743 state to active
[ns_server:info,2020-03-27T20:46:35.697Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 742 state to active
[ns_server:info,2020-03-27T20:46:35.698Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 741 state to active
[ns_server:info,2020-03-27T20:46:35.700Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 740 state to active
[ns_server:info,2020-03-27T20:46:35.701Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 739 state to active
[ns_server:info,2020-03-27T20:46:35.703Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 738 state to active
[ns_server:info,2020-03-27T20:46:35.704Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 737 state to active
[ns_server:info,2020-03-27T20:46:35.705Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 736 state to active
[ns_server:info,2020-03-27T20:46:35.706Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 735 state to active
[ns_server:info,2020-03-27T20:46:35.710Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 734 state to active
[ns_server:info,2020-03-27T20:46:35.712Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 733 state to active
[ns_server:info,2020-03-27T20:46:35.714Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 732 state to active
[ns_server:info,2020-03-27T20:46:35.718Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 731 state to active
[ns_server:info,2020-03-27T20:46:35.723Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 730 state to active
[ns_server:info,2020-03-27T20:46:35.731Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 729 state to active
[ns_server:info,2020-03-27T20:46:35.738Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 728 state to active
[ns_server:info,2020-03-27T20:46:35.739Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 727 state to active
[ns_server:info,2020-03-27T20:46:35.741Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 726 state to active
[ns_server:info,2020-03-27T20:46:35.742Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 725 state to active
[ns_server:info,2020-03-27T20:46:35.742Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 724 state to active
[ns_server:info,2020-03-27T20:46:35.742Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 723 state to active
[ns_server:info,2020-03-27T20:46:35.748Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 722 state to active
[ns_server:info,2020-03-27T20:46:35.751Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 721 state to active
[ns_server:info,2020-03-27T20:46:35.754Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 720 state to active
[ns_server:info,2020-03-27T20:46:35.754Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 719 state to active
[ns_server:info,2020-03-27T20:46:35.762Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 718 state to active
[ns_server:info,2020-03-27T20:46:35.763Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 717 state to active
[ns_server:info,2020-03-27T20:46:35.764Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 716 state to active
[ns_server:info,2020-03-27T20:46:35.767Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 715 state to active
[ns_server:info,2020-03-27T20:46:35.771Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 714 state to active
[ns_server:info,2020-03-27T20:46:35.772Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 713 state to active
[ns_server:info,2020-03-27T20:46:35.773Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 712 state to active
[ns_server:info,2020-03-27T20:46:35.774Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 711 state to active
[ns_server:info,2020-03-27T20:46:35.775Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 710 state to active
[ns_server:info,2020-03-27T20:46:35.775Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 709 state to active
[ns_server:info,2020-03-27T20:46:35.776Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 708 state to active
[ns_server:info,2020-03-27T20:46:35.776Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 707 state to active
[ns_server:info,2020-03-27T20:46:35.776Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 706 state to active
[ns_server:info,2020-03-27T20:46:35.777Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 705 state to active
[ns_server:info,2020-03-27T20:46:35.778Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 704 state to active
[ns_server:info,2020-03-27T20:46:35.779Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 703 state to active
[ns_server:info,2020-03-27T20:46:35.780Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 702 state to active
[ns_server:info,2020-03-27T20:46:35.781Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 701 state to active
[ns_server:info,2020-03-27T20:46:35.782Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 700 state to active
[ns_server:info,2020-03-27T20:46:35.783Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 699 state to active
[ns_server:info,2020-03-27T20:46:35.784Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 698 state to active
[ns_server:info,2020-03-27T20:46:35.784Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 697 state to active
[ns_server:info,2020-03-27T20:46:35.785Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 696 state to active
[ns_server:info,2020-03-27T20:46:35.785Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 695 state to active
[ns_server:info,2020-03-27T20:46:35.786Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 694 state to active
[ns_server:info,2020-03-27T20:46:35.787Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 693 state to active
[ns_server:info,2020-03-27T20:46:35.788Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 692 state to active
[ns_server:info,2020-03-27T20:46:35.789Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 691 state to active
[ns_server:info,2020-03-27T20:46:35.789Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 690 state to active
[ns_server:info,2020-03-27T20:46:35.790Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 689 state to active
[ns_server:info,2020-03-27T20:46:35.791Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 688 state to active
[ns_server:info,2020-03-27T20:46:35.793Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 687 state to active
[ns_server:info,2020-03-27T20:46:35.794Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 686 state to active
[ns_server:info,2020-03-27T20:46:35.794Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 685 state to active
[ns_server:info,2020-03-27T20:46:35.795Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 684 state to active
[ns_server:info,2020-03-27T20:46:35.796Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 683 state to active
[ns_server:info,2020-03-27T20:46:35.797Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 682 state to active
[ns_server:info,2020-03-27T20:46:35.797Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 681 state to active
[ns_server:info,2020-03-27T20:46:35.798Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 680 state to active
[ns_server:info,2020-03-27T20:46:35.798Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 679 state to active
[ns_server:info,2020-03-27T20:46:35.800Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 678 state to active
[ns_server:info,2020-03-27T20:46:35.801Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 677 state to active
[ns_server:info,2020-03-27T20:46:35.801Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 676 state to active
[ns_server:info,2020-03-27T20:46:35.802Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 675 state to active
[ns_server:info,2020-03-27T20:46:35.803Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 674 state to active
[ns_server:info,2020-03-27T20:46:35.803Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 673 state to active
[ns_server:info,2020-03-27T20:46:35.803Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 672 state to active
[ns_server:info,2020-03-27T20:46:35.804Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 671 state to active
[ns_server:info,2020-03-27T20:46:35.808Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 670 state to active
[ns_server:info,2020-03-27T20:46:35.808Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 669 state to active
[ns_server:info,2020-03-27T20:46:35.808Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 668 state to active
[ns_server:info,2020-03-27T20:46:35.810Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 667 state to active
[ns_server:info,2020-03-27T20:46:35.811Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 666 state to active
[ns_server:info,2020-03-27T20:46:35.812Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 665 state to active
[ns_server:info,2020-03-27T20:46:35.812Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 664 state to active
[ns_server:info,2020-03-27T20:46:35.813Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 663 state to active
[ns_server:info,2020-03-27T20:46:35.815Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 662 state to active
[ns_server:info,2020-03-27T20:46:35.817Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 661 state to active
[ns_server:info,2020-03-27T20:46:35.819Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 660 state to active
[ns_server:info,2020-03-27T20:46:35.824Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 659 state to active
[ns_server:info,2020-03-27T20:46:35.826Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 658 state to active
[ns_server:info,2020-03-27T20:46:35.828Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 657 state to active
[ns_server:info,2020-03-27T20:46:35.833Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 656 state to active
[ns_server:info,2020-03-27T20:46:35.835Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 655 state to active
[ns_server:info,2020-03-27T20:46:35.836Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 654 state to active
[ns_server:info,2020-03-27T20:46:35.837Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 653 state to active
[ns_server:info,2020-03-27T20:46:35.839Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 652 state to active
[ns_server:info,2020-03-27T20:46:35.839Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 651 state to active
[ns_server:info,2020-03-27T20:46:35.840Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 650 state to active
[ns_server:info,2020-03-27T20:46:35.841Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 649 state to active
[ns_server:info,2020-03-27T20:46:35.841Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 648 state to active
[ns_server:info,2020-03-27T20:46:35.843Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 647 state to active
[ns_server:info,2020-03-27T20:46:35.844Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 646 state to active
[ns_server:info,2020-03-27T20:46:35.845Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 645 state to active
[ns_server:info,2020-03-27T20:46:35.846Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 644 state to active
[ns_server:info,2020-03-27T20:46:35.847Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 643 state to active
[ns_server:info,2020-03-27T20:46:35.848Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 642 state to active
[ns_server:info,2020-03-27T20:46:35.850Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 641 state to active
[ns_server:info,2020-03-27T20:46:35.850Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 640 state to active
[ns_server:info,2020-03-27T20:46:35.852Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 639 state to active
[ns_server:info,2020-03-27T20:46:35.853Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 638 state to active
[ns_server:info,2020-03-27T20:46:35.855Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 637 state to active
[ns_server:info,2020-03-27T20:46:35.858Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 636 state to active
[ns_server:info,2020-03-27T20:46:35.859Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 635 state to active
[ns_server:info,2020-03-27T20:46:35.859Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 634 state to active
[ns_server:info,2020-03-27T20:46:35.860Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 633 state to active
[ns_server:info,2020-03-27T20:46:35.862Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 632 state to active
[ns_server:info,2020-03-27T20:46:35.863Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 631 state to active
[ns_server:info,2020-03-27T20:46:35.864Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 630 state to active
[ns_server:info,2020-03-27T20:46:35.864Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 629 state to active
[ns_server:info,2020-03-27T20:46:35.865Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 628 state to active
[ns_server:info,2020-03-27T20:46:35.868Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 627 state to active
[ns_server:info,2020-03-27T20:46:35.869Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 626 state to active
[ns_server:info,2020-03-27T20:46:35.869Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 625 state to active
[ns_server:info,2020-03-27T20:46:35.871Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 624 state to active
[ns_server:info,2020-03-27T20:46:35.873Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 623 state to active
[ns_server:info,2020-03-27T20:46:35.873Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 622 state to active
[ns_server:info,2020-03-27T20:46:35.874Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 621 state to active
[ns_server:info,2020-03-27T20:46:35.876Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 620 state to active
[ns_server:info,2020-03-27T20:46:35.876Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 619 state to active
[ns_server:info,2020-03-27T20:46:35.878Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 618 state to active
[ns_server:info,2020-03-27T20:46:35.879Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 617 state to active
[ns_server:info,2020-03-27T20:46:35.882Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 616 state to active
[ns_server:info,2020-03-27T20:46:35.884Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 615 state to active
[ns_server:info,2020-03-27T20:46:35.890Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 614 state to active
[ns_server:info,2020-03-27T20:46:35.893Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 613 state to active
[ns_server:info,2020-03-27T20:46:35.894Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 612 state to active
[ns_server:info,2020-03-27T20:46:35.898Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 611 state to active
[ns_server:info,2020-03-27T20:46:35.899Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 610 state to active
[ns_server:info,2020-03-27T20:46:35.900Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 609 state to active
[ns_server:info,2020-03-27T20:46:35.902Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 608 state to active
[ns_server:info,2020-03-27T20:46:35.905Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 607 state to active
[ns_server:info,2020-03-27T20:46:35.905Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 606 state to active
[ns_server:info,2020-03-27T20:46:35.906Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 605 state to active
[ns_server:info,2020-03-27T20:46:35.906Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 604 state to active
[ns_server:info,2020-03-27T20:46:35.907Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 603 state to active
[ns_server:info,2020-03-27T20:46:35.908Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 602 state to active
[ns_server:info,2020-03-27T20:46:35.908Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 601 state to active
[ns_server:info,2020-03-27T20:46:35.909Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 600 state to active
[ns_server:info,2020-03-27T20:46:35.917Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 599 state to active
[ns_server:info,2020-03-27T20:46:35.918Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 598 state to active
[ns_server:info,2020-03-27T20:46:35.920Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 597 state to active
[ns_server:info,2020-03-27T20:46:35.922Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 596 state to active
[ns_server:info,2020-03-27T20:46:35.923Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 595 state to active
[ns_server:info,2020-03-27T20:46:35.925Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 594 state to active
[ns_server:info,2020-03-27T20:46:35.925Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 593 state to active
[ns_server:info,2020-03-27T20:46:35.926Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 592 state to active
[ns_server:info,2020-03-27T20:46:35.926Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 591 state to active
[ns_server:info,2020-03-27T20:46:35.927Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 590 state to active
[ns_server:info,2020-03-27T20:46:35.929Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 589 state to active
[ns_server:info,2020-03-27T20:46:35.930Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 588 state to active
[ns_server:info,2020-03-27T20:46:35.931Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 587 state to active
[ns_server:info,2020-03-27T20:46:35.932Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 586 state to active
[ns_server:info,2020-03-27T20:46:35.933Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 585 state to active
[ns_server:info,2020-03-27T20:46:35.934Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 584 state to active
[ns_server:info,2020-03-27T20:46:35.934Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 583 state to active
[ns_server:info,2020-03-27T20:46:35.935Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 582 state to active
[ns_server:info,2020-03-27T20:46:35.937Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 581 state to active
[ns_server:info,2020-03-27T20:46:35.938Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 580 state to active
[ns_server:info,2020-03-27T20:46:35.939Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 579 state to active
[ns_server:info,2020-03-27T20:46:35.943Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 578 state to active
[ns_server:info,2020-03-27T20:46:35.945Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 577 state to active
[ns_server:info,2020-03-27T20:46:35.948Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 576 state to active
[ns_server:info,2020-03-27T20:46:35.950Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 575 state to active
[ns_server:info,2020-03-27T20:46:35.955Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 574 state to active
[ns_server:info,2020-03-27T20:46:35.958Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 573 state to active
[ns_server:info,2020-03-27T20:46:35.959Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 572 state to active
[ns_server:info,2020-03-27T20:46:35.961Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 571 state to active
[ns_server:info,2020-03-27T20:46:35.962Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 570 state to active
[ns_server:info,2020-03-27T20:46:35.966Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 569 state to active
[ns_server:info,2020-03-27T20:46:35.968Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 568 state to active
[ns_server:info,2020-03-27T20:46:35.968Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 567 state to active
[ns_server:info,2020-03-27T20:46:35.970Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 566 state to active
[ns_server:info,2020-03-27T20:46:35.972Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 565 state to active
[ns_server:info,2020-03-27T20:46:35.973Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 564 state to active
[ns_server:info,2020-03-27T20:46:35.975Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 563 state to active
[ns_server:info,2020-03-27T20:46:35.975Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 562 state to active
[ns_server:info,2020-03-27T20:46:35.977Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 561 state to active
[ns_server:info,2020-03-27T20:46:35.977Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 560 state to active
[ns_server:info,2020-03-27T20:46:35.977Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 559 state to active
[ns_server:info,2020-03-27T20:46:35.979Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 558 state to active
[ns_server:info,2020-03-27T20:46:35.979Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 557 state to active
[ns_server:info,2020-03-27T20:46:35.980Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 556 state to active
[ns_server:info,2020-03-27T20:46:35.985Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 555 state to active
[ns_server:info,2020-03-27T20:46:35.985Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 554 state to active
[ns_server:info,2020-03-27T20:46:35.986Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 553 state to active
[ns_server:info,2020-03-27T20:46:35.987Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 552 state to active
[ns_server:info,2020-03-27T20:46:35.989Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 551 state to active
[ns_server:info,2020-03-27T20:46:35.990Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 550 state to active
[ns_server:info,2020-03-27T20:46:35.991Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 549 state to active
[ns_server:info,2020-03-27T20:46:35.993Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 548 state to active
[ns_server:info,2020-03-27T20:46:35.993Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 547 state to active
[ns_server:info,2020-03-27T20:46:35.995Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 546 state to active
[ns_server:info,2020-03-27T20:46:35.995Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 545 state to active
[ns_server:info,2020-03-27T20:46:35.995Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 544 state to active
[ns_server:info,2020-03-27T20:46:35.997Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 543 state to active
[ns_server:info,2020-03-27T20:46:35.997Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 542 state to active
[ns_server:info,2020-03-27T20:46:35.998Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 541 state to active
[ns_server:info,2020-03-27T20:46:35.999Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 540 state to active
[ns_server:info,2020-03-27T20:46:36.000Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 539 state to active
[ns_server:info,2020-03-27T20:46:36.003Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 538 state to active
[ns_server:info,2020-03-27T20:46:36.005Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 537 state to active
[ns_server:info,2020-03-27T20:46:36.005Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 536 state to active
[ns_server:info,2020-03-27T20:46:36.007Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 535 state to active
[ns_server:info,2020-03-27T20:46:36.009Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 534 state to active
[ns_server:info,2020-03-27T20:46:36.010Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 533 state to active
[ns_server:info,2020-03-27T20:46:36.011Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 532 state to active
[ns_server:info,2020-03-27T20:46:36.012Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 531 state to active
[ns_server:info,2020-03-27T20:46:36.012Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 530 state to active
[ns_server:info,2020-03-27T20:46:36.014Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 529 state to active
[ns_server:info,2020-03-27T20:46:36.014Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 528 state to active
[ns_server:info,2020-03-27T20:46:36.015Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 527 state to active
[ns_server:info,2020-03-27T20:46:36.019Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 526 state to active
[ns_server:info,2020-03-27T20:46:36.020Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 525 state to active
[ns_server:info,2020-03-27T20:46:36.022Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 524 state to active
[ns_server:info,2020-03-27T20:46:36.023Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 523 state to active
[ns_server:info,2020-03-27T20:46:36.023Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 522 state to active
[ns_server:info,2020-03-27T20:46:36.024Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 521 state to active
[ns_server:info,2020-03-27T20:46:36.025Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 520 state to active
[ns_server:info,2020-03-27T20:46:36.026Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 519 state to active
[ns_server:info,2020-03-27T20:46:36.026Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 518 state to active
[ns_server:info,2020-03-27T20:46:36.027Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 517 state to active
[ns_server:info,2020-03-27T20:46:36.027Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 516 state to active
[ns_server:info,2020-03-27T20:46:36.028Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 515 state to active
[ns_server:info,2020-03-27T20:46:36.028Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 514 state to active
[ns_server:info,2020-03-27T20:46:36.028Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 513 state to active
[ns_server:info,2020-03-27T20:46:36.030Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 512 state to active
[ns_server:info,2020-03-27T20:46:36.032Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 511 state to active
[ns_server:info,2020-03-27T20:46:36.034Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 510 state to active
[ns_server:info,2020-03-27T20:46:36.035Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 509 state to active
[ns_server:info,2020-03-27T20:46:36.035Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 508 state to active
[ns_server:info,2020-03-27T20:46:36.035Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 507 state to active
[ns_server:info,2020-03-27T20:46:36.037Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 506 state to active
[ns_server:info,2020-03-27T20:46:36.038Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 505 state to active
[ns_server:info,2020-03-27T20:46:36.039Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 504 state to active
[ns_server:info,2020-03-27T20:46:36.040Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 503 state to active
[ns_server:info,2020-03-27T20:46:36.041Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 502 state to active
[ns_server:info,2020-03-27T20:46:36.046Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 501 state to active
[ns_server:info,2020-03-27T20:46:36.047Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 500 state to active
[ns_server:info,2020-03-27T20:46:36.048Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 499 state to active
[ns_server:info,2020-03-27T20:46:36.049Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 498 state to active
[ns_server:info,2020-03-27T20:46:36.049Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 497 state to active
[ns_server:info,2020-03-27T20:46:36.050Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 496 state to active
[ns_server:info,2020-03-27T20:46:36.051Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 495 state to active
[ns_server:info,2020-03-27T20:46:36.051Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 494 state to active
[ns_server:info,2020-03-27T20:46:36.052Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 493 state to active
[ns_server:info,2020-03-27T20:46:36.052Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 492 state to active
[ns_server:info,2020-03-27T20:46:36.052Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 491 state to active
[ns_server:info,2020-03-27T20:46:36.053Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 490 state to active
[ns_server:info,2020-03-27T20:46:36.053Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 489 state to active
[ns_server:info,2020-03-27T20:46:36.054Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 488 state to active
[ns_server:info,2020-03-27T20:46:36.054Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 487 state to active
[ns_server:info,2020-03-27T20:46:36.055Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 486 state to active
[ns_server:info,2020-03-27T20:46:36.055Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 485 state to active
[ns_server:info,2020-03-27T20:46:36.056Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 484 state to active
[ns_server:info,2020-03-27T20:46:36.057Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 483 state to active
[ns_server:info,2020-03-27T20:46:36.059Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 482 state to active
[ns_server:info,2020-03-27T20:46:36.062Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 481 state to active
[ns_server:info,2020-03-27T20:46:36.068Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 480 state to active
[ns_server:info,2020-03-27T20:46:36.069Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 479 state to active
[ns_server:info,2020-03-27T20:46:36.069Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 478 state to active
[ns_server:info,2020-03-27T20:46:36.070Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 477 state to active
[ns_server:info,2020-03-27T20:46:36.070Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 476 state to active
[ns_server:info,2020-03-27T20:46:36.071Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 475 state to active
[ns_server:info,2020-03-27T20:46:36.072Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 474 state to active
[ns_server:info,2020-03-27T20:46:36.074Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 473 state to active
[ns_server:info,2020-03-27T20:46:36.076Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 472 state to active
[ns_server:info,2020-03-27T20:46:36.076Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 471 state to active
[ns_server:info,2020-03-27T20:46:36.077Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 470 state to active
[ns_server:info,2020-03-27T20:46:36.078Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 469 state to active
[ns_server:info,2020-03-27T20:46:36.078Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 468 state to active
[ns_server:info,2020-03-27T20:46:36.079Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 467 state to active
[ns_server:info,2020-03-27T20:46:36.079Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 466 state to active
[ns_server:info,2020-03-27T20:46:36.080Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 465 state to active
[ns_server:info,2020-03-27T20:46:36.081Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 464 state to active
[ns_server:info,2020-03-27T20:46:36.083Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 463 state to active
[ns_server:info,2020-03-27T20:46:36.083Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 462 state to active
[ns_server:info,2020-03-27T20:46:36.084Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 461 state to active
[ns_server:info,2020-03-27T20:46:36.084Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 460 state to active
[ns_server:info,2020-03-27T20:46:36.085Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 459 state to active
[ns_server:info,2020-03-27T20:46:36.085Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 458 state to active
[ns_server:info,2020-03-27T20:46:36.086Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 457 state to active
[ns_server:info,2020-03-27T20:46:36.086Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 456 state to active
[ns_server:info,2020-03-27T20:46:36.088Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 455 state to active
[ns_server:info,2020-03-27T20:46:36.090Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 454 state to active
[ns_server:info,2020-03-27T20:46:36.090Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 453 state to active
[ns_server:info,2020-03-27T20:46:36.091Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 452 state to active
[ns_server:info,2020-03-27T20:46:36.092Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 451 state to active
[ns_server:info,2020-03-27T20:46:36.093Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 450 state to active
[ns_server:info,2020-03-27T20:46:36.093Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 449 state to active
[ns_server:info,2020-03-27T20:46:36.094Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 448 state to active
[ns_server:info,2020-03-27T20:46:36.095Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 447 state to active
[ns_server:info,2020-03-27T20:46:36.096Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 446 state to active
[ns_server:info,2020-03-27T20:46:36.097Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 445 state to active
[ns_server:info,2020-03-27T20:46:36.098Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 444 state to active
[ns_server:info,2020-03-27T20:46:36.099Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 443 state to active
[ns_server:info,2020-03-27T20:46:36.099Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 442 state to active
[ns_server:info,2020-03-27T20:46:36.099Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 441 state to active
[ns_server:info,2020-03-27T20:46:36.100Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 440 state to active
[ns_server:info,2020-03-27T20:46:36.101Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 439 state to active
[ns_server:info,2020-03-27T20:46:36.102Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 438 state to active
[ns_server:info,2020-03-27T20:46:36.103Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 437 state to active
[ns_server:info,2020-03-27T20:46:36.104Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 436 state to active
[ns_server:info,2020-03-27T20:46:36.105Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 435 state to active
[ns_server:info,2020-03-27T20:46:36.106Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 434 state to active
[ns_server:info,2020-03-27T20:46:36.107Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 433 state to active
[ns_server:info,2020-03-27T20:46:36.108Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 432 state to active
[ns_server:info,2020-03-27T20:46:36.108Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 431 state to active
[ns_server:info,2020-03-27T20:46:36.109Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 430 state to active
[ns_server:info,2020-03-27T20:46:36.110Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 429 state to active
[ns_server:info,2020-03-27T20:46:36.112Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 428 state to active
[ns_server:info,2020-03-27T20:46:36.113Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 427 state to active
[ns_server:info,2020-03-27T20:46:36.115Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 426 state to active
[ns_server:info,2020-03-27T20:46:36.116Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 425 state to active
[ns_server:info,2020-03-27T20:46:36.117Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 424 state to active
[ns_server:info,2020-03-27T20:46:36.119Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 423 state to active
[ns_server:info,2020-03-27T20:46:36.120Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 422 state to active
[ns_server:info,2020-03-27T20:46:36.121Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 421 state to active
[ns_server:info,2020-03-27T20:46:36.121Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 420 state to active
[ns_server:info,2020-03-27T20:46:36.123Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 419 state to active
[ns_server:info,2020-03-27T20:46:36.125Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 418 state to active
[ns_server:info,2020-03-27T20:46:36.125Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 417 state to active
[ns_server:info,2020-03-27T20:46:36.126Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 416 state to active
[ns_server:info,2020-03-27T20:46:36.126Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 415 state to active
[ns_server:info,2020-03-27T20:46:36.133Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 414 state to active
[ns_server:info,2020-03-27T20:46:36.136Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 413 state to active
[ns_server:info,2020-03-27T20:46:36.137Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 412 state to active
[ns_server:info,2020-03-27T20:46:36.139Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 411 state to active
[ns_server:info,2020-03-27T20:46:36.140Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 410 state to active
[ns_server:info,2020-03-27T20:46:36.140Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 409 state to active
[ns_server:info,2020-03-27T20:46:36.140Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 408 state to active
[ns_server:info,2020-03-27T20:46:36.141Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 407 state to active
[ns_server:info,2020-03-27T20:46:36.142Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 406 state to active
[ns_server:info,2020-03-27T20:46:36.143Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 405 state to active
[ns_server:info,2020-03-27T20:46:36.145Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 404 state to active
[ns_server:info,2020-03-27T20:46:36.148Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 403 state to active
[ns_server:info,2020-03-27T20:46:36.149Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 402 state to active
[ns_server:info,2020-03-27T20:46:36.149Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 401 state to active
[ns_server:info,2020-03-27T20:46:36.150Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 400 state to active
[ns_server:info,2020-03-27T20:46:36.151Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 399 state to active
[ns_server:info,2020-03-27T20:46:36.152Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 398 state to active
[ns_server:info,2020-03-27T20:46:36.152Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 397 state to active
[ns_server:info,2020-03-27T20:46:36.152Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 396 state to active
[ns_server:info,2020-03-27T20:46:36.153Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 395 state to active
[ns_server:info,2020-03-27T20:46:36.154Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 394 state to active
[ns_server:info,2020-03-27T20:46:36.154Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 393 state to active
[ns_server:info,2020-03-27T20:46:36.155Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 392 state to active
[ns_server:info,2020-03-27T20:46:36.156Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 391 state to active
[ns_server:info,2020-03-27T20:46:36.156Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 390 state to active
[ns_server:info,2020-03-27T20:46:36.158Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 389 state to active
[ns_server:info,2020-03-27T20:46:36.159Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 388 state to active
[ns_server:info,2020-03-27T20:46:36.159Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 387 state to active
[ns_server:info,2020-03-27T20:46:36.160Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 386 state to active
[ns_server:info,2020-03-27T20:46:36.161Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 385 state to active
[ns_server:info,2020-03-27T20:46:36.162Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 384 state to active
[ns_server:info,2020-03-27T20:46:36.162Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 383 state to active
[ns_server:info,2020-03-27T20:46:36.163Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 382 state to active
[ns_server:info,2020-03-27T20:46:36.164Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 381 state to active
[ns_server:info,2020-03-27T20:46:36.165Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 380 state to active
[ns_server:info,2020-03-27T20:46:36.166Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 379 state to active
[ns_server:info,2020-03-27T20:46:36.166Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 378 state to active
[ns_server:info,2020-03-27T20:46:36.168Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 377 state to active
[ns_server:info,2020-03-27T20:46:36.169Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 376 state to active
[ns_server:info,2020-03-27T20:46:36.169Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 375 state to active
[ns_server:info,2020-03-27T20:46:36.170Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 374 state to active
[ns_server:info,2020-03-27T20:46:36.170Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 373 state to active
[ns_server:info,2020-03-27T20:46:36.173Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 372 state to active
[ns_server:info,2020-03-27T20:46:36.174Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 371 state to active
[ns_server:info,2020-03-27T20:46:36.176Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 370 state to active
[ns_server:info,2020-03-27T20:46:36.177Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 369 state to active
[ns_server:info,2020-03-27T20:46:36.178Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 368 state to active
[ns_server:info,2020-03-27T20:46:36.179Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 367 state to active
[ns_server:info,2020-03-27T20:46:36.182Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 366 state to active
[ns_server:info,2020-03-27T20:46:36.183Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 365 state to active
[ns_server:info,2020-03-27T20:46:36.185Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 364 state to active
[ns_server:info,2020-03-27T20:46:36.187Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 363 state to active
[ns_server:info,2020-03-27T20:46:36.187Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 362 state to active
[ns_server:info,2020-03-27T20:46:36.189Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 361 state to active
[ns_server:info,2020-03-27T20:46:36.189Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 360 state to active
[ns_server:info,2020-03-27T20:46:36.192Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 359 state to active
[ns_server:info,2020-03-27T20:46:36.193Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 358 state to active
[ns_server:info,2020-03-27T20:46:36.194Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 357 state to active
[ns_server:info,2020-03-27T20:46:36.195Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 356 state to active
[ns_server:info,2020-03-27T20:46:36.196Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 355 state to active
[ns_server:info,2020-03-27T20:46:36.198Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 354 state to active
[ns_server:info,2020-03-27T20:46:36.199Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 353 state to active
[ns_server:info,2020-03-27T20:46:36.200Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 352 state to active
[ns_server:info,2020-03-27T20:46:36.201Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 351 state to active
[ns_server:info,2020-03-27T20:46:36.202Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 350 state to active
[ns_server:info,2020-03-27T20:46:36.203Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 349 state to active
[ns_server:info,2020-03-27T20:46:36.204Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 348 state to active
[ns_server:info,2020-03-27T20:46:36.208Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 347 state to active
[ns_server:info,2020-03-27T20:46:36.210Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 346 state to active
[ns_server:info,2020-03-27T20:46:36.211Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 345 state to active
[ns_server:info,2020-03-27T20:46:36.212Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 344 state to active
[ns_server:info,2020-03-27T20:46:36.212Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 343 state to active
[ns_server:info,2020-03-27T20:46:36.213Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 342 state to active
[ns_server:info,2020-03-27T20:46:36.215Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 341 state to active
[ns_server:info,2020-03-27T20:46:36.216Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 340 state to active
[ns_server:info,2020-03-27T20:46:36.217Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 339 state to active
[ns_server:info,2020-03-27T20:46:36.218Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 338 state to active
[ns_server:info,2020-03-27T20:46:36.220Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 337 state to active
[ns_server:info,2020-03-27T20:46:36.222Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 336 state to active
[ns_server:info,2020-03-27T20:46:36.223Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 335 state to active
[ns_server:info,2020-03-27T20:46:36.224Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 334 state to active
[ns_server:info,2020-03-27T20:46:36.224Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 333 state to active
[ns_server:info,2020-03-27T20:46:36.226Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 332 state to active
[ns_server:info,2020-03-27T20:46:36.227Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 331 state to active
[ns_server:info,2020-03-27T20:46:36.228Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 330 state to active
[ns_server:info,2020-03-27T20:46:36.228Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 329 state to active
[ns_server:info,2020-03-27T20:46:36.229Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 328 state to active
[ns_server:info,2020-03-27T20:46:36.231Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 327 state to active
[ns_server:info,2020-03-27T20:46:36.233Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 326 state to active
[ns_server:info,2020-03-27T20:46:36.234Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 325 state to active
[ns_server:info,2020-03-27T20:46:36.235Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 324 state to active
[ns_server:info,2020-03-27T20:46:36.236Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 323 state to active
[ns_server:info,2020-03-27T20:46:36.237Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 322 state to active
[ns_server:info,2020-03-27T20:46:36.238Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 321 state to active
[ns_server:info,2020-03-27T20:46:36.238Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 320 state to active
[ns_server:info,2020-03-27T20:46:36.240Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 319 state to active
[ns_server:info,2020-03-27T20:46:36.241Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 318 state to active
[ns_server:info,2020-03-27T20:46:36.242Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 317 state to active
[ns_server:info,2020-03-27T20:46:36.244Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 316 state to active
[ns_server:info,2020-03-27T20:46:36.245Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 315 state to active
[ns_server:info,2020-03-27T20:46:36.246Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 314 state to active
[ns_server:info,2020-03-27T20:46:36.247Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 313 state to active
[ns_server:info,2020-03-27T20:46:36.249Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 312 state to active
[ns_server:info,2020-03-27T20:46:36.251Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 311 state to active
[ns_server:info,2020-03-27T20:46:36.252Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 310 state to active
[ns_server:info,2020-03-27T20:46:36.253Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 309 state to active
[ns_server:info,2020-03-27T20:46:36.254Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 308 state to active
[ns_server:info,2020-03-27T20:46:36.256Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 307 state to active
[ns_server:info,2020-03-27T20:46:36.261Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 306 state to active
[ns_server:info,2020-03-27T20:46:36.262Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 305 state to active
[ns_server:info,2020-03-27T20:46:36.263Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 304 state to active
[ns_server:info,2020-03-27T20:46:36.264Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 303 state to active
[ns_server:info,2020-03-27T20:46:36.265Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 302 state to active
[ns_server:info,2020-03-27T20:46:36.267Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 301 state to active
[ns_server:info,2020-03-27T20:46:36.267Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 300 state to active
[ns_server:info,2020-03-27T20:46:36.270Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 299 state to active
[ns_server:info,2020-03-27T20:46:36.270Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 298 state to active
[ns_server:info,2020-03-27T20:46:36.276Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 297 state to active
[ns_server:info,2020-03-27T20:46:36.278Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 296 state to active
[ns_server:info,2020-03-27T20:46:36.279Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 295 state to active
[ns_server:info,2020-03-27T20:46:36.279Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 294 state to active
[ns_server:info,2020-03-27T20:46:36.280Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 293 state to active
[ns_server:info,2020-03-27T20:46:36.281Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 292 state to active
[ns_server:info,2020-03-27T20:46:36.282Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 291 state to active
[ns_server:info,2020-03-27T20:46:36.283Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 290 state to active
[ns_server:info,2020-03-27T20:46:36.283Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 289 state to active
[ns_server:info,2020-03-27T20:46:36.285Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 288 state to active
[ns_server:info,2020-03-27T20:46:36.286Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 287 state to active
[ns_server:info,2020-03-27T20:46:36.286Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 286 state to active
[ns_server:info,2020-03-27T20:46:36.287Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 285 state to active
[ns_server:info,2020-03-27T20:46:36.290Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 284 state to active
[ns_server:info,2020-03-27T20:46:36.292Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 283 state to active
[ns_server:info,2020-03-27T20:46:36.294Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 282 state to active
[ns_server:info,2020-03-27T20:46:36.294Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 281 state to active
[ns_server:info,2020-03-27T20:46:36.295Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 280 state to active
[ns_server:info,2020-03-27T20:46:36.297Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 279 state to active
[ns_server:info,2020-03-27T20:46:36.298Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 278 state to active
[ns_server:info,2020-03-27T20:46:36.299Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 277 state to active
[ns_server:info,2020-03-27T20:46:36.300Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 276 state to active
[ns_server:info,2020-03-27T20:46:36.300Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 275 state to active
[ns_server:info,2020-03-27T20:46:36.301Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 274 state to active
[ns_server:info,2020-03-27T20:46:36.301Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 273 state to active
[ns_server:info,2020-03-27T20:46:36.301Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 272 state to active
[ns_server:info,2020-03-27T20:46:36.302Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 271 state to active
[ns_server:info,2020-03-27T20:46:36.303Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 270 state to active
[ns_server:info,2020-03-27T20:46:36.303Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 269 state to active
[ns_server:info,2020-03-27T20:46:36.305Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 268 state to active
[ns_server:info,2020-03-27T20:46:36.306Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 267 state to active
[ns_server:info,2020-03-27T20:46:36.306Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 266 state to active
[ns_server:info,2020-03-27T20:46:36.306Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 265 state to active
[ns_server:info,2020-03-27T20:46:36.314Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 264 state to active
[ns_server:info,2020-03-27T20:46:36.315Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 263 state to active
[ns_server:info,2020-03-27T20:46:36.315Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 262 state to active
[ns_server:info,2020-03-27T20:46:36.317Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 261 state to active
[ns_server:info,2020-03-27T20:46:36.318Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 260 state to active
[ns_server:info,2020-03-27T20:46:36.320Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 259 state to active
[ns_server:info,2020-03-27T20:46:36.322Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 258 state to active
[ns_server:info,2020-03-27T20:46:36.327Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 257 state to active
[ns_server:info,2020-03-27T20:46:36.327Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 256 state to active
[ns_server:info,2020-03-27T20:46:36.328Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 255 state to active
[ns_server:info,2020-03-27T20:46:36.330Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 254 state to active
[ns_server:info,2020-03-27T20:46:36.334Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 253 state to active
[ns_server:info,2020-03-27T20:46:36.336Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 252 state to active
[ns_server:info,2020-03-27T20:46:36.340Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 251 state to active
[ns_server:info,2020-03-27T20:46:36.344Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 250 state to active
[ns_server:info,2020-03-27T20:46:36.346Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 249 state to active
[ns_server:info,2020-03-27T20:46:36.347Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 248 state to active
[ns_server:info,2020-03-27T20:46:36.348Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 247 state to active
[ns_server:info,2020-03-27T20:46:36.349Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 246 state to active
[ns_server:info,2020-03-27T20:46:36.350Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 245 state to active
[ns_server:info,2020-03-27T20:46:36.350Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 244 state to active
[ns_server:info,2020-03-27T20:46:36.350Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 243 state to active
[ns_server:info,2020-03-27T20:46:36.351Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 242 state to active
[ns_server:info,2020-03-27T20:46:36.351Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 241 state to active
[ns_server:info,2020-03-27T20:46:36.352Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 240 state to active
[ns_server:info,2020-03-27T20:46:36.352Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 239 state to active
[ns_server:info,2020-03-27T20:46:36.353Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 238 state to active
[ns_server:info,2020-03-27T20:46:36.355Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 237 state to active
[ns_server:info,2020-03-27T20:46:36.356Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 236 state to active
[ns_server:info,2020-03-27T20:46:36.357Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 235 state to active
[ns_server:info,2020-03-27T20:46:36.359Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 234 state to active
[ns_server:info,2020-03-27T20:46:36.360Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 233 state to active
[ns_server:info,2020-03-27T20:46:36.360Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 232 state to active
[ns_server:info,2020-03-27T20:46:36.362Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 231 state to active
[ns_server:info,2020-03-27T20:46:36.363Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 230 state to active
[ns_server:info,2020-03-27T20:46:36.365Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 229 state to active
[ns_server:info,2020-03-27T20:46:36.366Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 228 state to active
[ns_server:info,2020-03-27T20:46:36.368Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 227 state to active
[ns_server:info,2020-03-27T20:46:36.369Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 226 state to active
[ns_server:info,2020-03-27T20:46:36.370Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 225 state to active
[ns_server:info,2020-03-27T20:46:36.373Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 224 state to active
[ns_server:info,2020-03-27T20:46:36.375Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 223 state to active
[ns_server:info,2020-03-27T20:46:36.375Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 222 state to active
[ns_server:info,2020-03-27T20:46:36.376Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 221 state to active
[ns_server:info,2020-03-27T20:46:36.379Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 220 state to active
[ns_server:info,2020-03-27T20:46:36.381Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 219 state to active
[ns_server:info,2020-03-27T20:46:36.382Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 218 state to active
[ns_server:info,2020-03-27T20:46:36.385Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 217 state to active
[ns_server:info,2020-03-27T20:46:36.386Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 216 state to active
[ns_server:info,2020-03-27T20:46:36.386Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 215 state to active
[ns_server:info,2020-03-27T20:46:36.388Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 214 state to active
[ns_server:info,2020-03-27T20:46:36.389Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 213 state to active
[ns_server:info,2020-03-27T20:46:36.390Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 212 state to active
[ns_server:info,2020-03-27T20:46:36.392Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 211 state to active
[ns_server:info,2020-03-27T20:46:36.398Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 210 state to active
[ns_server:info,2020-03-27T20:46:36.400Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 209 state to active
[ns_server:info,2020-03-27T20:46:36.403Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 208 state to active
[ns_server:info,2020-03-27T20:46:36.403Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 207 state to active
[ns_server:info,2020-03-27T20:46:36.404Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 206 state to active
[ns_server:info,2020-03-27T20:46:36.404Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 205 state to active
[ns_server:info,2020-03-27T20:46:36.406Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 204 state to active
[ns_server:info,2020-03-27T20:46:36.408Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 203 state to active
[ns_server:info,2020-03-27T20:46:36.410Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 202 state to active
[ns_server:info,2020-03-27T20:46:36.414Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 201 state to active
[ns_server:info,2020-03-27T20:46:36.417Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 200 state to active
[ns_server:info,2020-03-27T20:46:36.421Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 199 state to active
[ns_server:info,2020-03-27T20:46:36.422Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 198 state to active
[ns_server:info,2020-03-27T20:46:36.425Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 197 state to active
[ns_server:info,2020-03-27T20:46:36.436Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 196 state to active
[ns_server:info,2020-03-27T20:46:36.436Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 195 state to active
[ns_server:info,2020-03-27T20:46:36.438Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 194 state to active
[ns_server:info,2020-03-27T20:46:36.439Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 193 state to active
[ns_server:info,2020-03-27T20:46:36.439Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 192 state to active
[ns_server:info,2020-03-27T20:46:36.441Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 191 state to active
[ns_server:info,2020-03-27T20:46:36.443Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 190 state to active
[ns_server:info,2020-03-27T20:46:36.444Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 189 state to active
[ns_server:info,2020-03-27T20:46:36.445Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 188 state to active
[ns_server:info,2020-03-27T20:46:36.451Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 187 state to active
[ns_server:info,2020-03-27T20:46:36.453Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 186 state to active
[ns_server:info,2020-03-27T20:46:36.455Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 185 state to active
[ns_server:info,2020-03-27T20:46:36.455Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 184 state to active
[ns_server:info,2020-03-27T20:46:36.457Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 183 state to active
[ns_server:info,2020-03-27T20:46:36.457Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 182 state to active
[ns_server:info,2020-03-27T20:46:36.458Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 181 state to active
[ns_server:info,2020-03-27T20:46:36.459Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 180 state to active
[ns_server:info,2020-03-27T20:46:36.461Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 179 state to active
[ns_server:info,2020-03-27T20:46:36.461Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 178 state to active
[ns_server:info,2020-03-27T20:46:36.462Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 177 state to active
[ns_server:info,2020-03-27T20:46:36.465Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 176 state to active
[ns_server:info,2020-03-27T20:46:36.469Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 175 state to active
[ns_server:info,2020-03-27T20:46:36.471Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 174 state to active
[ns_server:info,2020-03-27T20:46:36.473Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 173 state to active
[ns_server:info,2020-03-27T20:46:36.473Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 172 state to active
[ns_server:info,2020-03-27T20:46:36.475Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 171 state to active
[ns_server:info,2020-03-27T20:46:36.476Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 170 state to active
[ns_server:info,2020-03-27T20:46:36.482Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 169 state to active
[ns_server:info,2020-03-27T20:46:36.483Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 168 state to active
[ns_server:info,2020-03-27T20:46:36.488Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 167 state to active
[ns_server:info,2020-03-27T20:46:36.491Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 166 state to active
[ns_server:info,2020-03-27T20:46:36.492Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 165 state to active
[ns_server:info,2020-03-27T20:46:36.493Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 164 state to active
[ns_server:info,2020-03-27T20:46:36.513Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 163 state to active
[ns_server:info,2020-03-27T20:46:36.524Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 162 state to active
[ns_server:info,2020-03-27T20:46:36.526Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 161 state to active
[ns_server:info,2020-03-27T20:46:36.528Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 160 state to active
[ns_server:info,2020-03-27T20:46:36.528Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 159 state to active
[ns_server:info,2020-03-27T20:46:36.531Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 158 state to active
[ns_server:info,2020-03-27T20:46:36.532Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 157 state to active
[ns_server:info,2020-03-27T20:46:36.533Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 156 state to active
[ns_server:info,2020-03-27T20:46:36.535Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 155 state to active
[ns_server:info,2020-03-27T20:46:36.537Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 154 state to active
[ns_server:info,2020-03-27T20:46:36.539Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 153 state to active
[ns_server:info,2020-03-27T20:46:36.539Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 152 state to active
[ns_server:info,2020-03-27T20:46:36.540Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 151 state to active
[ns_server:info,2020-03-27T20:46:36.541Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 150 state to active
[ns_server:info,2020-03-27T20:46:36.543Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 149 state to active
[ns_server:info,2020-03-27T20:46:36.544Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 148 state to active
[ns_server:info,2020-03-27T20:46:36.544Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 147 state to active
[ns_server:info,2020-03-27T20:46:36.546Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 146 state to active
[ns_server:info,2020-03-27T20:46:36.550Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 145 state to active
[ns_server:info,2020-03-27T20:46:36.554Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 144 state to active
[ns_server:info,2020-03-27T20:46:36.555Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 143 state to active
[ns_server:info,2020-03-27T20:46:36.556Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 142 state to active
[ns_server:info,2020-03-27T20:46:36.560Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 141 state to active
[ns_server:info,2020-03-27T20:46:36.562Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 140 state to active
[ns_server:info,2020-03-27T20:46:36.563Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 139 state to active
[ns_server:info,2020-03-27T20:46:36.566Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 138 state to active
[ns_server:info,2020-03-27T20:46:36.568Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 137 state to active
[ns_server:info,2020-03-27T20:46:36.569Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 136 state to active
[ns_server:info,2020-03-27T20:46:36.570Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 135 state to active
[ns_server:info,2020-03-27T20:46:36.575Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 134 state to active
[ns_server:info,2020-03-27T20:46:36.576Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 133 state to active
[ns_server:info,2020-03-27T20:46:36.579Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 132 state to active
[ns_server:info,2020-03-27T20:46:36.586Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 131 state to active
[ns_server:info,2020-03-27T20:46:36.588Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 130 state to active
[ns_server:info,2020-03-27T20:46:36.588Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 129 state to active
[ns_server:info,2020-03-27T20:46:36.589Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 128 state to active
[ns_server:info,2020-03-27T20:46:36.590Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 127 state to active
[ns_server:info,2020-03-27T20:46:36.591Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 126 state to active
[ns_server:info,2020-03-27T20:46:36.599Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 125 state to active
[ns_server:info,2020-03-27T20:46:36.601Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 124 state to active
[ns_server:info,2020-03-27T20:46:36.601Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 123 state to active
[ns_server:info,2020-03-27T20:46:36.602Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 122 state to active
[ns_server:info,2020-03-27T20:46:36.602Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 121 state to active
[ns_server:info,2020-03-27T20:46:36.603Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 120 state to active
[ns_server:info,2020-03-27T20:46:36.603Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 119 state to active
[ns_server:info,2020-03-27T20:46:36.603Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 118 state to active
[ns_server:info,2020-03-27T20:46:36.604Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 117 state to active
[ns_server:info,2020-03-27T20:46:36.604Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 116 state to active
[ns_server:info,2020-03-27T20:46:36.604Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 115 state to active
[ns_server:info,2020-03-27T20:46:36.604Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 114 state to active
[ns_server:info,2020-03-27T20:46:36.605Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 113 state to active
[ns_server:info,2020-03-27T20:46:36.605Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 112 state to active
[ns_server:info,2020-03-27T20:46:36.605Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 111 state to active
[ns_server:info,2020-03-27T20:46:36.605Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 110 state to active
[ns_server:info,2020-03-27T20:46:36.606Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 109 state to active
[ns_server:info,2020-03-27T20:46:36.606Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 108 state to active
[ns_server:info,2020-03-27T20:46:36.606Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 107 state to active
[ns_server:info,2020-03-27T20:46:36.606Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 106 state to active
[ns_server:info,2020-03-27T20:46:36.609Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 105 state to active
[ns_server:info,2020-03-27T20:46:36.610Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 104 state to active
[ns_server:info,2020-03-27T20:46:36.611Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 103 state to active
[ns_server:info,2020-03-27T20:46:36.613Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 102 state to active
[ns_server:info,2020-03-27T20:46:36.615Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 101 state to active
[ns_server:info,2020-03-27T20:46:36.616Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 100 state to active
[ns_server:info,2020-03-27T20:46:36.617Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 99 state to active
[ns_server:info,2020-03-27T20:46:36.618Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 98 state to active
[ns_server:info,2020-03-27T20:46:36.619Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 97 state to active
[ns_server:info,2020-03-27T20:46:36.621Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 96 state to active
[ns_server:info,2020-03-27T20:46:36.623Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 95 state to active
[ns_server:info,2020-03-27T20:46:36.625Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 94 state to active
[ns_server:info,2020-03-27T20:46:36.625Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 93 state to active
[ns_server:info,2020-03-27T20:46:36.627Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 92 state to active
[ns_server:info,2020-03-27T20:46:36.630Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 91 state to active
[ns_server:info,2020-03-27T20:46:36.631Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 90 state to active
[ns_server:info,2020-03-27T20:46:36.632Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 89 state to active
[ns_server:info,2020-03-27T20:46:36.634Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 88 state to active
[ns_server:info,2020-03-27T20:46:36.635Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 87 state to active
[ns_server:info,2020-03-27T20:46:36.637Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 86 state to active
[ns_server:info,2020-03-27T20:46:36.645Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 85 state to active
[ns_server:info,2020-03-27T20:46:36.647Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 84 state to active
[ns_server:info,2020-03-27T20:46:36.647Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 83 state to active
[ns_server:info,2020-03-27T20:46:36.649Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 82 state to active
[ns_server:info,2020-03-27T20:46:36.654Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 81 state to active
[ns_server:info,2020-03-27T20:46:36.655Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 80 state to active
[ns_server:info,2020-03-27T20:46:36.657Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 79 state to active
[ns_server:info,2020-03-27T20:46:36.668Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 78 state to active
[ns_server:info,2020-03-27T20:46:36.669Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 77 state to active
[ns_server:info,2020-03-27T20:46:36.671Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 76 state to active
[ns_server:info,2020-03-27T20:46:36.673Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 75 state to active
[ns_server:info,2020-03-27T20:46:36.675Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 74 state to active
[ns_server:info,2020-03-27T20:46:36.676Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 73 state to active
[ns_server:info,2020-03-27T20:46:36.677Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 72 state to active
[ns_server:info,2020-03-27T20:46:36.678Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 71 state to active
[ns_server:info,2020-03-27T20:46:36.679Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 70 state to active
[ns_server:info,2020-03-27T20:46:36.680Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 69 state to active
[ns_server:info,2020-03-27T20:46:36.681Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 68 state to active
[ns_server:info,2020-03-27T20:46:36.684Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 67 state to active
[ns_server:info,2020-03-27T20:46:36.685Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 66 state to active
[ns_server:info,2020-03-27T20:46:36.688Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 65 state to active
[ns_server:info,2020-03-27T20:46:36.691Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 64 state to active
[ns_server:info,2020-03-27T20:46:36.692Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 63 state to active
[ns_server:info,2020-03-27T20:46:36.695Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 62 state to active
[ns_server:info,2020-03-27T20:46:36.696Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 61 state to active
[ns_server:info,2020-03-27T20:46:36.698Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 60 state to active
[ns_server:info,2020-03-27T20:46:36.699Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 59 state to active
[ns_server:info,2020-03-27T20:46:36.701Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 58 state to active
[ns_server:info,2020-03-27T20:46:36.702Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 57 state to active
[ns_server:info,2020-03-27T20:46:36.702Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 56 state to active
[ns_server:info,2020-03-27T20:46:36.704Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 55 state to active
[ns_server:info,2020-03-27T20:46:36.705Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 54 state to active
[ns_server:info,2020-03-27T20:46:36.707Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 53 state to active
[ns_server:info,2020-03-27T20:46:36.708Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 52 state to active
[ns_server:info,2020-03-27T20:46:36.709Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 51 state to active
[ns_server:info,2020-03-27T20:46:36.714Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 50 state to active
[ns_server:info,2020-03-27T20:46:36.716Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 49 state to active
[ns_server:info,2020-03-27T20:46:36.717Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 48 state to active
[ns_server:info,2020-03-27T20:46:36.720Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 47 state to active
[ns_server:info,2020-03-27T20:46:36.721Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 46 state to active
[ns_server:info,2020-03-27T20:46:36.723Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 45 state to active
[ns_server:info,2020-03-27T20:46:36.725Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 44 state to active
[ns_server:info,2020-03-27T20:46:36.726Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 43 state to active
[ns_server:info,2020-03-27T20:46:36.731Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 42 state to active
[ns_server:info,2020-03-27T20:46:36.732Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 41 state to active
[ns_server:info,2020-03-27T20:46:36.733Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 40 state to active
[ns_server:info,2020-03-27T20:46:36.734Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 39 state to active
[ns_server:info,2020-03-27T20:46:36.735Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 38 state to active
[ns_server:info,2020-03-27T20:46:36.735Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 37 state to active
[ns_server:info,2020-03-27T20:46:36.736Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 36 state to active
[ns_server:info,2020-03-27T20:46:36.738Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 35 state to active
[ns_server:info,2020-03-27T20:46:36.739Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 34 state to active
[ns_server:info,2020-03-27T20:46:36.741Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 33 state to active
[ns_server:info,2020-03-27T20:46:36.748Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 32 state to active
[ns_server:info,2020-03-27T20:46:36.751Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 31 state to active
[ns_server:info,2020-03-27T20:46:36.751Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 30 state to active
[ns_server:info,2020-03-27T20:46:36.752Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 29 state to active
[ns_server:info,2020-03-27T20:46:36.753Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 28 state to active
[ns_server:info,2020-03-27T20:46:36.754Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 27 state to active
[ns_server:info,2020-03-27T20:46:36.754Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 26 state to active
[ns_server:info,2020-03-27T20:46:36.756Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 25 state to active
[ns_server:info,2020-03-27T20:46:36.757Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 24 state to active
[ns_server:info,2020-03-27T20:46:36.758Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 23 state to active
[ns_server:info,2020-03-27T20:46:36.759Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 22 state to active
[ns_server:info,2020-03-27T20:46:36.762Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 21 state to active
[ns_server:info,2020-03-27T20:46:36.763Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 20 state to active
[ns_server:info,2020-03-27T20:46:36.765Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 19 state to active
[ns_server:info,2020-03-27T20:46:36.767Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 18 state to active
[ns_server:info,2020-03-27T20:46:36.768Z,ns_1@127.0.0.1:<0.16710.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 17 state to active
[ns_server:info,2020-03-27T20:46:36.769Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 16 state to active
[ns_server:info,2020-03-27T20:46:36.770Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 15 state to active
[ns_server:info,2020-03-27T20:46:36.771Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 14 state to active
[ns_server:info,2020-03-27T20:46:36.771Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 13 state to active
[ns_server:info,2020-03-27T20:46:36.776Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 12 state to active
[ns_server:info,2020-03-27T20:46:36.779Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 11 state to active
[ns_server:info,2020-03-27T20:46:36.781Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 10 state to active
[ns_server:info,2020-03-27T20:46:36.782Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 9 state to active
[ns_server:info,2020-03-27T20:46:36.783Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 8 state to active
[ns_server:info,2020-03-27T20:46:36.784Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 7 state to active
[ns_server:info,2020-03-27T20:46:36.785Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 6 state to active
[ns_server:info,2020-03-27T20:46:36.786Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 5 state to active
[ns_server:info,2020-03-27T20:46:36.787Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 4 state to active
[ns_server:info,2020-03-27T20:46:36.787Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 3 state to active
[ns_server:info,2020-03-27T20:46:36.792Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 2 state to active
[ns_server:info,2020-03-27T20:46:36.797Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 1 state to active
[ns_server:info,2020-03-27T20:46:36.802Z,ns_1@127.0.0.1:<0.16711.0>:ns_memcached:do_handle_call:551]Changed bucket "HelloServiceBucket" vbucket 0 state to active
[ns_server:info,2020-03-27T20:46:36.804Z,ns_1@127.0.0.1:ns_memcached-HelloServiceBucket<0.16685.0>:ns_memcached:handle_call:309]Enabling traffic to bucket "HelloServiceBucket"
[ns_server:info,2020-03-27T20:46:36.805Z,ns_1@127.0.0.1:ns_memcached-HelloServiceBucket<0.16685.0>:ns_memcached:handle_call:313]Bucket "HelloServiceBucket" marked as warmed in 2 seconds
[ns_server:debug,2020-03-27T20:46:36.806Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',buckets_with_data} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{1,63752561196}}]},
 {"HelloServiceBucket",<<"e080f4429e7bd7d930bb34ead0abb5dc">>}]
[ns_server:debug,2020-03-27T20:46:36.806Z,ns_1@127.0.0.1:ns_config_log<0.202.0>:ns_config_log:log_common:231]config change:
{local_changes_count,<<"60a6bc3db77e6c7b91c556140dcfec71">>} ->
[{'_vclock',[{<<"60a6bc3db77e6c7b91c556140dcfec71">>,{71,63752561196}}]}]
[ns_server:debug,2020-03-27T20:46:36.806Z,ns_1@127.0.0.1:ns_config_rep<0.319.0>:ns_config_rep:do_push_keys:321]Replicating some config keys ([{local_changes_count,
                                   <<"60a6bc3db77e6c7b91c556140dcfec71">>},
                               {node,'ns_1@127.0.0.1',buckets_with_data}]..)
[ns_server:info,2020-03-27T20:46:37.072Z,ns_1@127.0.0.1:ns_doctor<0.354.0>:ns_doctor:update_status:322]The following buckets became ready on node 'ns_1@127.0.0.1': ["HelloServiceBucket"]
[ns_server:debug,2020-03-27T20:46:42.442Z,ns_1@127.0.0.1:compaction_daemon<0.517.0>:compaction_daemon:process_scheduler_message:1309]Starting compaction (compact_kv) for the following buckets: 
[<<"HelloServiceBucket">>]
[ns_server:debug,2020-03-27T20:46:42.442Z,ns_1@127.0.0.1:compaction_daemon<0.517.0>:compaction_daemon:process_scheduler_message:1309]Starting compaction (compact_views) for the following buckets: 
[<<"HelloServiceBucket">>]
[ns_server:info,2020-03-27T20:46:42.442Z,ns_1@127.0.0.1:<0.17072.0>:compaction_daemon:spawn_scheduled_kv_compactor:468]Start compaction of vbuckets for bucket HelloServiceBucket with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}}]
[ns_server:debug,2020-03-27T20:46:42.444Z,ns_1@127.0.0.1:<0.17075.0>:compaction_daemon:bucket_needs_compaction:969]`HelloServiceBucket` data size is 330160, disk size is 3620108
[ns_server:debug,2020-03-27T20:46:42.444Z,ns_1@127.0.0.1:compaction_daemon<0.517.0>:compaction_daemon:process_compactors_exit:1350]Finished compaction iteration.
[ns_server:debug,2020-03-27T20:46:42.444Z,ns_1@127.0.0.1:compaction_daemon<0.517.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:info,2020-03-27T20:46:42.466Z,ns_1@127.0.0.1:<0.17074.0>:compaction_daemon:spawn_scheduled_views_compactor:494]Start compaction of indexes for bucket HelloServiceBucket with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}}]
[ns_server:debug,2020-03-27T20:46:42.467Z,ns_1@127.0.0.1:compaction_daemon<0.517.0>:compaction_daemon:process_compactors_exit:1350]Finished compaction iteration.
[ns_server:debug,2020-03-27T20:46:42.467Z,ns_1@127.0.0.1:compaction_daemon<0.517.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2020-03-27T20:47:12.445Z,ns_1@127.0.0.1:compaction_daemon<0.517.0>:compaction_daemon:process_scheduler_message:1309]Starting compaction (compact_kv) for the following buckets: 
[<<"HelloServiceBucket">>]
[ns_server:info,2020-03-27T20:47:12.445Z,ns_1@127.0.0.1:<0.18268.0>:compaction_daemon:spawn_scheduled_kv_compactor:468]Start compaction of vbuckets for bucket HelloServiceBucket with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}}]
[ns_server:debug,2020-03-27T20:47:12.446Z,ns_1@127.0.0.1:<0.18270.0>:compaction_daemon:bucket_needs_compaction:969]`HelloServiceBucket` data size is 386834, disk size is 4241408
[ns_server:debug,2020-03-27T20:47:12.446Z,ns_1@127.0.0.1:compaction_daemon<0.517.0>:compaction_daemon:process_compactors_exit:1350]Finished compaction iteration.
[ns_server:debug,2020-03-27T20:47:12.446Z,ns_1@127.0.0.1:compaction_daemon<0.517.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2020-03-27T20:47:12.467Z,ns_1@127.0.0.1:compaction_daemon<0.517.0>:compaction_daemon:process_scheduler_message:1309]Starting compaction (compact_views) for the following buckets: 
[<<"HelloServiceBucket">>]
[ns_server:info,2020-03-27T20:47:12.475Z,ns_1@127.0.0.1:<0.18271.0>:compaction_daemon:spawn_scheduled_views_compactor:494]Start compaction of indexes for bucket HelloServiceBucket with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}}]
[ns_server:debug,2020-03-27T20:47:12.475Z,ns_1@127.0.0.1:compaction_daemon<0.517.0>:compaction_daemon:process_compactors_exit:1350]Finished compaction iteration.
[ns_server:debug,2020-03-27T20:47:12.475Z,ns_1@127.0.0.1:compaction_daemon<0.517.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2020-03-27T20:47:39.440Z,ns_1@127.0.0.1:ldap_auth_cache<0.256.0>:active_cache:cleanup:231]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2020-03-27T20:47:39.462Z,ns_1@127.0.0.1:roles_cache<0.267.0>:active_cache:cleanup:231]Cache roles_cache cleanup: 0/0 records deleted
[ns_server:debug,2020-03-27T20:47:42.447Z,ns_1@127.0.0.1:compaction_daemon<0.517.0>:compaction_daemon:process_scheduler_message:1309]Starting compaction (compact_kv) for the following buckets: 
[<<"HelloServiceBucket">>]
[ns_server:info,2020-03-27T20:47:42.448Z,ns_1@127.0.0.1:<0.19475.0>:compaction_daemon:spawn_scheduled_kv_compactor:468]Start compaction of vbuckets for bucket HelloServiceBucket with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}}]
[ns_server:debug,2020-03-27T20:47:42.449Z,ns_1@127.0.0.1:<0.19477.0>:compaction_daemon:bucket_needs_compaction:969]`HelloServiceBucket` data size is 387377, disk size is 4249690
[ns_server:debug,2020-03-27T20:47:42.449Z,ns_1@127.0.0.1:compaction_daemon<0.517.0>:compaction_daemon:process_compactors_exit:1350]Finished compaction iteration.
[ns_server:debug,2020-03-27T20:47:42.449Z,ns_1@127.0.0.1:compaction_daemon<0.517.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2020-03-27T20:47:42.475Z,ns_1@127.0.0.1:compaction_daemon<0.517.0>:compaction_daemon:process_scheduler_message:1309]Starting compaction (compact_views) for the following buckets: 
[<<"HelloServiceBucket">>]
[ns_server:info,2020-03-27T20:47:42.482Z,ns_1@127.0.0.1:<0.19480.0>:compaction_daemon:spawn_scheduled_views_compactor:494]Start compaction of indexes for bucket HelloServiceBucket with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}}]
[ns_server:debug,2020-03-27T20:47:42.482Z,ns_1@127.0.0.1:compaction_daemon<0.517.0>:compaction_daemon:process_compactors_exit:1350]Finished compaction iteration.
[ns_server:debug,2020-03-27T20:47:42.482Z,ns_1@127.0.0.1:compaction_daemon<0.517.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2020-03-27T20:48:12.451Z,ns_1@127.0.0.1:compaction_daemon<0.517.0>:compaction_daemon:process_scheduler_message:1309]Starting compaction (compact_kv) for the following buckets: 
[<<"HelloServiceBucket">>]
[ns_server:info,2020-03-27T20:48:12.454Z,ns_1@127.0.0.1:<0.20677.0>:compaction_daemon:spawn_scheduled_kv_compactor:468]Start compaction of vbuckets for bucket HelloServiceBucket with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}}]
[ns_server:debug,2020-03-27T20:48:12.458Z,ns_1@127.0.0.1:<0.20679.0>:compaction_daemon:bucket_needs_compaction:969]`HelloServiceBucket` data size is 387377, disk size is 4249690
[ns_server:debug,2020-03-27T20:48:12.458Z,ns_1@127.0.0.1:compaction_daemon<0.517.0>:compaction_daemon:process_compactors_exit:1350]Finished compaction iteration.
[ns_server:debug,2020-03-27T20:48:12.458Z,ns_1@127.0.0.1:compaction_daemon<0.517.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2020-03-27T20:48:12.484Z,ns_1@127.0.0.1:compaction_daemon<0.517.0>:compaction_daemon:process_scheduler_message:1309]Starting compaction (compact_views) for the following buckets: 
[<<"HelloServiceBucket">>]
[ns_server:info,2020-03-27T20:48:12.488Z,ns_1@127.0.0.1:<0.20682.0>:compaction_daemon:spawn_scheduled_views_compactor:494]Start compaction of indexes for bucket HelloServiceBucket with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}}]
[ns_server:debug,2020-03-27T20:48:12.489Z,ns_1@127.0.0.1:compaction_daemon<0.517.0>:compaction_daemon:process_compactors_exit:1350]Finished compaction iteration.
[ns_server:debug,2020-03-27T20:48:12.489Z,ns_1@127.0.0.1:compaction_daemon<0.517.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2020-03-27T20:48:42.459Z,ns_1@127.0.0.1:compaction_daemon<0.517.0>:compaction_daemon:process_scheduler_message:1309]Starting compaction (compact_kv) for the following buckets: 
[<<"HelloServiceBucket">>]
[ns_server:info,2020-03-27T20:48:42.459Z,ns_1@127.0.0.1:<0.21889.0>:compaction_daemon:spawn_scheduled_kv_compactor:468]Start compaction of vbuckets for bucket HelloServiceBucket with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}}]
[ns_server:debug,2020-03-27T20:48:42.460Z,ns_1@127.0.0.1:<0.21891.0>:compaction_daemon:bucket_needs_compaction:969]`HelloServiceBucket` data size is 387377, disk size is 4249690
[ns_server:debug,2020-03-27T20:48:42.460Z,ns_1@127.0.0.1:compaction_daemon<0.517.0>:compaction_daemon:process_compactors_exit:1350]Finished compaction iteration.
[ns_server:debug,2020-03-27T20:48:42.460Z,ns_1@127.0.0.1:compaction_daemon<0.517.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2020-03-27T20:48:42.490Z,ns_1@127.0.0.1:compaction_daemon<0.517.0>:compaction_daemon:process_scheduler_message:1309]Starting compaction (compact_views) for the following buckets: 
[<<"HelloServiceBucket">>]
[ns_server:info,2020-03-27T20:48:42.512Z,ns_1@127.0.0.1:<0.21894.0>:compaction_daemon:spawn_scheduled_views_compactor:494]Start compaction of indexes for bucket HelloServiceBucket with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}}]
[ns_server:debug,2020-03-27T20:48:42.513Z,ns_1@127.0.0.1:compaction_daemon<0.517.0>:compaction_daemon:process_compactors_exit:1350]Finished compaction iteration.
[ns_server:debug,2020-03-27T20:48:42.513Z,ns_1@127.0.0.1:compaction_daemon<0.517.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2020-03-27T20:48:54.440Z,ns_1@127.0.0.1:ldap_auth_cache<0.256.0>:active_cache:cleanup:231]Cache ldap_auth_cache cleanup: 0/0 records deleted
